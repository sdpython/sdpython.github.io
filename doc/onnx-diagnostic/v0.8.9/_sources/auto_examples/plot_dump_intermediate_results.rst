
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_dump_intermediate_results.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_dump_intermediate_results.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_dump_intermediate_results.py:


.. _l-plot-intermediate-results:

Dumps intermediate results of a torch model
===========================================

Looking for discrepancies is quickly annoying. Discrepancies
come from two results obtained with the same models
implemented in two different ways, :epkg:`pytorch` and :epkg:`onnx`.
Models are big so where do they come from? That's the
unavoidable question. Unless there is an obvious reason,
the only way is to compare intermediate outputs alon the computation.
The first step into that direction is to dump the intermediate results
coming from :epkg:`pytorch`.
We use :func:`onnx_diagnostic.helpers.torch_helper.steal_forward` for that.

A simple LLM Model
++++++++++++++++++

See :func:`onnx_diagnostic.helpers.torch_helper.dummy_llm`
for its definition. It is mostly used for unit test or example.

.. GENERATED FROM PYTHON SOURCE LINES 23-38

.. code-block:: Python


    import numpy as np
    import pandas
    import onnx
    import torch
    import onnxruntime
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers import max_diff, string_diff, string_type
    from onnx_diagnostic.helpers.torch_helper import dummy_llm, steal_forward
    from onnx_diagnostic.helpers.mini_onnx_builder import create_input_tensors_from_onnx_model
    from onnx_diagnostic.reference import OnnxruntimeEvaluator, ReportResultComparison


    model, inputs, ds = dummy_llm(dynamic_shapes=True)








.. GENERATED FROM PYTHON SOURCE LINES 39-40

We use float16.

.. GENERATED FROM PYTHON SOURCE LINES 40-42

.. code-block:: Python

    model = model.to(torch.float16)








.. GENERATED FROM PYTHON SOURCE LINES 43-44

Let's check.

.. GENERATED FROM PYTHON SOURCE LINES 44-49

.. code-block:: Python


    print(f"type(model)={type(model)}")
    print(f"inputs={string_type(inputs, with_shape=True)}")
    print(f"ds={string_type(ds, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    type(model)=<class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.LLM'>
    inputs=(T7s2x30,)
    ds=dict(input_ids:{0:Dim(batch),1:Dim(length)})




.. GENERATED FROM PYTHON SOURCE LINES 50-51

It contains the following submodules.

.. GENERATED FROM PYTHON SOURCE LINES 51-55

.. code-block:: Python


    for name, mod in model.named_modules():
        print(f"- {name}: {type(mod)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    - : <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.LLM'>
    - embedding: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.Embedding'>
    - embedding.embedding: <class 'torch.nn.modules.sparse.Embedding'>
    - embedding.pe: <class 'torch.nn.modules.sparse.Embedding'>
    - decoder: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.DecoderLayer'>
    - decoder.attention: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.MultiAttentionBlock'>
    - decoder.attention.attention: <class 'torch.nn.modules.container.ModuleList'>
    - decoder.attention.attention.0: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.AttentionBlock'>
    - decoder.attention.attention.0.query: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.0.key: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.0.value: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.AttentionBlock'>
    - decoder.attention.attention.1.query: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1.key: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1.value: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.linear: <class 'torch.nn.modules.linear.Linear'>
    - decoder.feed_forward: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.FeedForward'>
    - decoder.feed_forward.linear_1: <class 'torch.nn.modules.linear.Linear'>
    - decoder.feed_forward.relu: <class 'torch.nn.modules.activation.ReLU'>
    - decoder.feed_forward.linear_2: <class 'torch.nn.modules.linear.Linear'>
    - decoder.norm_1: <class 'torch.nn.modules.normalization.LayerNorm'>
    - decoder.norm_2: <class 'torch.nn.modules.normalization.LayerNorm'>




.. GENERATED FROM PYTHON SOURCE LINES 56-62

Steal and dump the output of submodules
+++++++++++++++++++++++++++++++++++++++

The following context spies on the intermediate results
for the following module and submodules. It stores
in one onnx file all the input/output for those.

.. GENERATED FROM PYTHON SOURCE LINES 62-79

.. code-block:: Python


    with steal_forward(
        [
            ("model", model),
            ("model.decoder", model.decoder),
            ("model.decoder.attention", model.decoder.attention),
            ("model.decoder.feed_forward", model.decoder.feed_forward),
            ("model.decoder.norm_1", model.decoder.norm_1),
            ("model.decoder.norm_2", model.decoder.norm_2),
        ],
        dump_file="plot_dump_intermediate_results.inputs.onnx",
        verbose=1,
        storage_limit=2**28,
    ):
        expected = model(*inputs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    +model -- stolen forward for class LLM -- iteration 0
      <- args=(T7s2x30,) --- kwargs={}
    +model.decoder -- stolen forward for class DecoderLayer -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
    +model.decoder.norm_1 -- stolen forward for class LayerNorm -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.norm_1.
    -- stores key=('model.decoder.norm_1', 0), size 1Kb -- T10s2x30x16
    +model.decoder.attention -- stolen forward for class MultiAttentionBlock -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.attention.
    -- stores key=('model.decoder.attention', 0), size 1Kb -- T10s2x30x16
    +model.decoder.norm_2 -- stolen forward for class LayerNorm -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.norm_2.
    -- stores key=('model.decoder.norm_2', 0), size 1Kb -- T10s2x30x16
    +model.decoder.feed_forward -- stolen forward for class FeedForward -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.feed_forward.
    -- stores key=('model.decoder.feed_forward', 0), size 1Kb -- T10s2x30x16
      -> T10s2x30x16
    -model.decoder.
    -- stores key=('model.decoder', 0), size 1Kb -- T10s2x30x16
      -> T10s2x30x16
    -model.
    -- stores key=('model', 0), size 1Kb -- T10s2x30x16
    -- gather stored 12 objects, size=0 Mb
    -- dumps stored objects
    -- done dump stored objects




.. GENERATED FROM PYTHON SOURCE LINES 80-89

Restores saved inputs/outputs
+++++++++++++++++++++++++++++

All the intermediate tensors were saved in one unique onnx model,
every tensor is stored in a constant node.
The model can be run with any runtime to restore the inputs
and function :func:`create_input_tensors_from_onnx_model
<onnx_diagnostic.helpers.mini_onnx_builder.create_input_tensors_from_onnx_model>`
can restore their names.

.. GENERATED FROM PYTHON SOURCE LINES 89-96

.. code-block:: Python


    saved_tensors = create_input_tensors_from_onnx_model(
        "plot_dump_intermediate_results.inputs.onnx"
    )
    for k, v in saved_tensors.items():
        print(f"{k} -- {string_type(v, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ('model', 0, 'I') -- ((T7s2x30,),{})
    ('model.decoder', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_1', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_1', 0, 'O') -- T10s2x30x16
    ('model.decoder.attention', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.attention', 0, 'O') -- T10s2x30x16
    ('model.decoder.norm_2', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_2', 0, 'O') -- T10s2x30x16
    ('model.decoder.feed_forward', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.feed_forward', 0, 'O') -- T10s2x30x16
    ('model.decoder', 0, 'O') -- T10s2x30x16
    ('model', 0, 'O') -- T10s2x30x16




.. GENERATED FROM PYTHON SOURCE LINES 97-121

Let's explained the naming convention.

::

   ('model.decoder.norm_2', 0, 'I') -- ((T1s2x30x16,),{})
               |            |   |
               |            |   +--> input, the format is args, kwargs
               |            |
               |            +--> iteration, 0 means the first time the execution
               |                 went through that module
               |                 it is possible to call multiple times,
               |                 the model to store more
               |
               +--> the name given to function steal_forward

The same goes for output except ``'I'`` is replaced by ``'O'``.

::

   ('model.decoder.norm_2', 0, 'O') -- T1s2x30x16

This trick can be used to compare intermediate results coming
from pytorch to any other implementation of the same model
as long as it is possible to map the stored inputs/outputs.

.. GENERATED FROM PYTHON SOURCE LINES 123-129

Conversion to ONNX
++++++++++++++++++

The difficult point is to be able to map the saved intermediate
results to intermediate results in ONNX.
Let's create the ONNX model.

.. GENERATED FROM PYTHON SOURCE LINES 129-135

.. code-block:: Python


    ep = torch.export.export(model, inputs, dynamic_shapes=ds)
    epo = torch.onnx.export(ep)
    epo.optimize()
    epo.save("plot_dump_intermediate_results.onnx")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch.onnx] Run decomposition...
    /usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.
      return cls.__new__(cls, *args)
    [torch.onnx] Run decomposition... ✅
    [torch.onnx] Translate the graph into ONNX...
    [torch.onnx] Translate the graph into ONNX... ✅
    Applied 4 of general pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 136-143

Discrepancies
+++++++++++++

We have a torch model, intermediate results and an ONNX graph
equivalent to the torch model.
Let's see how we can check the discrepancies.
First the discrepancies of the whole model.

.. GENERATED FROM PYTHON SOURCE LINES 143-154

.. code-block:: Python


    sess = onnxruntime.InferenceSession(
        "plot_dump_intermediate_results.onnx", providers=["CPUExecutionProvider"]
    )
    feeds = dict(
        zip([i.name for i in sess.get_inputs()], [t.detach().cpu().numpy() for t in inputs])
    )
    got = sess.run(None, feeds)
    diff = max_diff(expected, got)
    print(f"discrepancies torch/ORT: {string_diff(diff)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    discrepancies torch/ORT: abs=0.00390625, rel=0.022509419326241134, n=960.0,amax=1,0,1, dev=0




.. GENERATED FROM PYTHON SOURCE LINES 155-158

What about intermediate results?
Let's use a runtime still based on :epkg:`onnxruntime`
running an eager evaluation.

.. GENERATED FROM PYTHON SOURCE LINES 158-169

.. code-block:: Python


    sess_eager = OnnxruntimeEvaluator(
        "plot_dump_intermediate_results.onnx",
        providers=["CPUExecutionProvider"],
        torch_or_numpy=True,
    )
    feeds_tensor = dict(zip([i.name for i in sess.get_inputs()], inputs))
    got = sess_eager.run(None, feeds_tensor)
    diff = max_diff(expected, got)
    print(f"discrepancies torch/eager ORT: {string_diff(diff)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    discrepancies torch/eager ORT: abs=0.001953125, rel=0.03539071347678369, n=960.0,amax=0,5,11, dev=0




.. GENERATED FROM PYTHON SOURCE LINES 170-174

They are almost the same. That's good.
Let's now dig into the intermediate results.
They are compared to the outputs stored in saved_tensors
during the execution of the model.

.. GENERATED FROM PYTHON SOURCE LINES 174-184

.. code-block:: Python

    baseline = {}
    for k, v in saved_tensors.items():
        if k[-1] == "I":  # inputs are excluded
            continue
        if isinstance(v, torch.Tensor):
            baseline[f"{k[0]}.{k[1]}".replace("model.decoder", "decoder")] = v

    report_cmp = ReportResultComparison(baseline)
    sess_eager.run(None, feeds_tensor, report_cmp=report_cmp)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [tensor([[[ 4.6729e-01, -5.0049e-01, -1.0273e+00,  2.8638e-01, -1.3105e+00,
               1.1641e+00, -8.0957e-01,  3.8145e+00,  9.0430e-01,  1.3047e+00,
              -9.8486e-01, -1.3940e-01, -1.7783e+00, -1.8418e+00, -2.2070e+00,
               7.0117e-01],
             [ 1.3926e+00, -5.8008e-01,  3.0879e+00, -1.2939e-01,  2.4414e-02,
               3.6523e-01, -1.1240e+00, -1.4111e+00,  9.5996e-01, -1.3516e+00,
               8.5986e-01,  1.3848e+00,  2.6562e-01, -1.8555e+00,  8.0664e-01,
               4.0088e-01],
             [-1.3955e+00, -2.7832e-01,  8.3057e-01, -1.8677e-01,  2.3486e-01,
              -2.3398e+00, -1.1709e+00,  1.7422e+00, -1.5215e+00,  1.3779e+00,
              -1.5430e-01,  1.3828e+00, -4.6167e-01, -1.4844e+00,  5.4785e-01,
               1.1582e+00],
             [-1.0537e+00, -1.8193e+00, -3.5522e-01, -1.3125e+00, -1.7065e-01,
              -1.8057e+00, -4.3701e-02,  4.6295e-02, -1.8965e+00, -1.0078e+00,
               1.6484e+00,  2.3711e+00,  2.4817e-01,  2.3086e+00,  5.0879e-01,
              -3.1504e+00],
             [ 4.3213e-01, -2.4512e-01, -4.5312e-01,  1.1240e+00, -1.1006e+00,
               2.4854e-01, -1.8945e-01, -2.0332e+00,  1.8613e+00, -4.2407e-01,
               3.1885e-01, -8.1104e-01, -4.7339e-01,  5.3516e-01,  1.4033e+00,
               3.7031e+00],
             [-9.4434e-01,  3.0933e-01, -1.5566e+00,  2.8770e+00, -2.8281e+00,
               3.4033e-01, -1.0264e+00,  2.9512e+00, -2.5859e+00,  1.7744e+00,
               1.6934e+00,  2.1973e+00,  5.4395e-01,  2.1855e+00,  7.4707e-01,
               2.3145e+00],
             [-5.2148e-01,  6.0352e-01, -1.5586e+00, -9.1992e-01, -1.9180e+00,
              -3.8516e+00, -8.6914e-02,  1.1855e+00,  1.1816e+00, -1.6309e+00,
              -2.0039e+00,  1.0166e+00,  3.1665e-01, -2.4756e-01, -2.6294e-01,
              -1.0449e+00],
             [ 1.1250e+00,  7.6074e-01,  1.8906e+00, -1.5146e+00,  7.4707e-02,
              -2.7686e-01, -1.1670e+00, -1.4111e-01,  1.0244e+00, -1.2842e-01,
              -1.4771e-01,  3.2168e+00, -7.5586e-01, -6.9043e-01,  2.9473e+00,
               1.0820e+00],
             [-5.7373e-01, -8.8135e-01,  2.5220e-01, -9.7461e-01,  2.0391e+00,
               1.5762e+00, -5.4492e-01,  9.3652e-01,  1.4834e+00,  2.4316e-01,
               1.2238e-01, -7.1582e-01,  6.6211e-01,  1.8428e+00, -3.1274e-01,
              -5.9180e-01],
             [-6.4258e-01,  1.5747e-01, -1.5957e+00, -2.2229e-01,  6.6211e-01,
              -1.9727e-01, -4.6729e-01, -3.4121e+00, -1.4238e+00,  2.3086e+00,
               7.1631e-01,  1.7969e+00,  1.0977e+00,  1.8262e-01, -1.2500e+00,
              -1.0566e+00],
             [ 4.0698e-01, -3.3789e+00, -2.4258e+00,  1.4287e+00, -1.0107e+00,
               1.0195e+00, -7.9297e-01,  1.7310e-01, -2.6152e+00,  1.3926e+00,
               1.1016e+00,  6.1084e-01, -2.1641e+00,  4.3115e-01,  5.0830e-01,
              -1.9932e+00],
             [-2.4141e+00,  2.3809e+00, -1.8281e+00, -2.5156e+00, -9.7559e-01,
              -6.9727e-01,  9.9609e-01, -1.4736e+00,  2.7695e+00, -2.2095e-01,
              -2.5020e+00,  8.4375e-01,  2.7539e+00,  6.9092e-02, -9.5825e-02,
              -1.7656e+00],
             [ 6.6162e-01,  1.7842e+00,  1.2930e+00,  7.2461e-01, -2.4922e+00,
               1.0605e+00, -2.4668e+00,  1.6445e+00,  5.1514e-02,  3.4424e-01,
               9.7852e-01,  4.4434e-01, -1.7102e-01, -5.9961e-01, -5.6494e-01,
               2.1729e-02],
             [ 1.4287e+00, -1.1875e+00,  8.2764e-01, -8.6279e-01,  1.5098e+00,
               1.7324e+00,  2.0728e-01,  2.0098e+00,  1.1963e-01,  6.7334e-01,
              -4.0503e-01,  1.1309e+00, -8.0762e-01,  2.2480e+00, -1.6865e+00,
               4.2676e-01],
             [ 7.7393e-02, -2.2520e+00,  9.2188e-01, -1.4004e+00,  2.1445e+00,
               2.7188e+00,  2.3401e-01,  1.4170e+00, -1.3848e+00,  1.5938e+00,
               5.2246e-01, -5.7812e-01, -2.6758e+00,  1.4805e+00, -1.6553e+00,
               1.8311e-01],
             [ 1.6836e+00,  6.7725e-01, -2.1543e+00, -9.2334e-01,  1.3848e+00,
               9.3262e-01,  3.4824e+00,  2.9102e+00, -5.0977e-01, -6.2695e-01,
              -2.6016e+00, -1.9023e+00,  5.4639e-01, -1.3275e-02, -8.9209e-01,
              -1.3457e+00],
             [ 1.1787e+00,  2.4180e+00,  5.8594e-01,  1.8516e+00,  8.3252e-01,
               7.7295e-01, -1.5752e+00,  5.1855e-01, -9.7705e-01,  6.2793e-01,
               1.7471e+00, -9.7461e-01,  2.6523e+00, -7.6074e-01, -1.4541e+00,
               4.2334e-01],
             [-6.0156e-01, -1.2292e-01, -4.3457e-01, -2.0684e+00, -1.6660e+00,
              -8.8574e-01,  1.9551e+00, -1.3096e+00,  1.1289e+00, -8.3301e-01,
              -1.8135e+00,  9.5312e-01, -4.0576e-01, -7.9297e-01,  3.0957e+00,
               1.0166e+00],
             [ 2.1973e+00, -1.7861e+00, -1.8525e+00,  1.4648e-02,  3.5391e+00,
               1.4038e-01, -7.5537e-01, -2.2656e+00, -5.7520e-01, -1.6377e+00,
              -1.7490e+00, -1.0850e+00,  3.0762e+00, -1.3818e+00, -9.9609e-01,
              -2.1191e+00],
             [ 1.7100e+00, -1.7109e+00, -4.2847e-02,  1.0986e-01, -9.6484e-01,
               1.5664e+00,  1.8984e+00, -2.6221e-01,  1.3750e+00, -7.4707e-02,
              -1.5322e+00, -2.6685e-01,  1.4717e+00, -8.2812e-01,  7.4658e-01,
              -5.9766e-01],
             [ 8.0371e-01, -6.4209e-02, -3.8574e-02, -2.0630e-02, -2.4238e+00,
               1.2295e+00,  2.8149e-01, -9.9854e-02,  1.3574e+00,  1.9971e+00,
               9.4189e-01, -1.9727e+00,  7.6758e-01,  9.1504e-01,  2.2852e+00,
               9.6826e-01],
             [-1.0977e+00,  1.8496e+00, -6.0986e-01, -1.5254e+00,  2.5562e-01,
               1.6172e+00,  3.9551e-02,  1.0986e+00,  1.2900e+00, -1.5752e+00,
              -3.0098e+00, -7.9199e-01, -2.9609e+00,  3.8086e-01,  1.6348e+00,
              -1.4141e+00],
             [-9.0332e-01, -1.8047e+00,  7.3340e-01, -1.5166e+00,  5.2344e-01,
              -9.5093e-02,  1.6416e+00,  5.4395e-01, -3.5332e+00,  1.6562e+00,
              -2.8047e+00, -1.9600e+00,  1.9023e+00, -1.1992e+00, -1.4863e+00,
               7.4951e-02],
             [-2.2148e+00, -1.0820e+00, -1.5068e+00,  8.3984e-02,  2.0586e+00,
               1.6663e-02,  5.9570e-01,  8.4863e-01,  1.8652e+00,  1.0645e+00,
              -1.5176e+00,  3.2324e-01, -1.2852e+00, -1.5957e+00,  1.2168e+00,
              -2.8906e-01],
             [ 2.2988e+00,  4.4629e-01,  9.4482e-01,  1.0322e+00,  1.7715e+00,
              -1.8188e-01, -2.0203e-01,  1.0928e+00, -1.4688e+00,  2.8320e-01,
              -3.3027e+00, -9.5947e-01, -1.3440e-01, -5.3760e-01, -2.7305e+00,
              -9.9609e-01],
             [-3.0156e+00,  9.5947e-01,  3.2812e-01,  3.7500e-01,  2.3750e+00,
              -6.2207e-01, -2.3965e+00, -2.3047e+00,  8.2617e-01, -9.0527e-01,
              -1.1396e+00,  1.8201e-01, -2.6367e-01, -1.7207e+00, -5.5615e-01,
              -1.8691e+00],
             [ 2.2705e-01, -1.6299e+00,  7.6611e-01,  2.4963e-01, -1.5723e+00,
               1.5020e+00,  3.3716e-01, -2.9321e-01, -2.7383e+00,  3.1758e+00,
              -1.5547e+00,  1.4414e+00,  4.9512e-01,  9.9609e-01, -7.5732e-01,
              -2.9150e-01],
             [ 8.6279e-01,  2.0293e+00, -7.9443e-01, -1.1074e+00,  1.8730e+00,
               1.3193e+00, -2.3359e+00,  7.0752e-01, -1.8184e+00, -1.0137e+00,
               1.9678e-01, -8.1006e-01,  1.0264e+00, -8.5754e-03,  8.4326e-01,
              -1.8994e-01],
             [ 7.1387e-01, -1.9492e+00, -1.8818e+00, -1.6406e+00, -3.0859e-01,
              -1.5801e+00,  1.7520e+00, -9.0479e-01, -1.6094e+00,  4.5288e-01,
              -1.4199e+00, -2.8662e-01,  1.0762e+00, -5.3711e-02,  1.9961e+00,
               9.1162e-01],
             [ 3.2324e-01,  8.5938e-02,  1.3467e+00, -3.4512e+00, -1.3311e+00,
              -1.9365e+00, -2.5879e-01, -1.6074e+00,  1.1592e+00, -1.3008e+00,
              -1.5576e+00, -9.1211e-01,  1.5176e+00, -2.0728e-01,  1.9326e+00,
              -1.9805e+00]],

            [[ 1.9004e+00,  4.8477e+00, -5.9766e-01, -8.7402e-02, -9.6375e-02,
               1.8945e+00, -2.9023e+00, -1.9492e+00, -2.0117e+00,  4.2773e-01,
              -1.3906e+00, -8.3252e-01,  1.6562e+00, -1.3115e+00, -2.7051e+00,
              -1.1855e+00],
             [-2.8066e+00,  1.9131e+00, -1.8320e+00,  1.4385e+00, -2.7930e-01,
               7.3633e-01, -4.4727e-01, -8.8818e-01,  1.4150e+00,  1.1445e+00,
              -9.4092e-01,  1.2695e+00,  3.7598e-02, -3.3154e-01, -2.1152e+00,
              -5.2979e-01],
             [-7.1973e-01, -4.3481e-01, -7.5244e-01, -2.1914e+00, -1.0771e+00,
               7.7051e-01, -1.9990e+00,  2.1680e+00, -9.8584e-01, -6.2012e-01,
              -1.6370e-01, -1.7676e-01, -3.0059e+00, -5.9521e-01, -6.8750e-01,
               1.9414e+00],
             [ 1.8672e+00,  1.8955e+00,  3.1250e+00,  3.4790e-01, -1.8320e+00,
              -9.5508e-01,  2.2571e-01, -1.2979e+00, -9.3262e-01,  1.6875e+00,
              -2.3438e+00, -1.2285e+00,  1.6855e+00, -1.3145e+00,  9.9121e-01,
              -1.0039e+00],
             [-8.3984e-02, -1.5850e+00,  1.4375e+00,  5.8496e-01, -7.8125e-03,
               3.3691e-01,  1.0625e+00, -5.2832e-01,  5.1611e-01, -1.7637e+00,
              -1.8896e+00, -8.3740e-02, -3.7402e-01,  1.1611e+00,  1.6328e+00,
              -1.9316e+00],
             [ 4.7363e-02,  3.9185e-01, -1.8301e+00,  2.3770e+00, -1.2168e+00,
               4.2041e-01,  1.1445e+00, -1.3477e+00, -1.0996e+00, -1.8730e+00,
              -5.2979e-02,  2.1582e-01,  2.4878e-01, -1.2793e+00,  2.6978e-01,
               1.5332e-01],
             [ 1.2314e+00, -1.5742e+00,  1.1445e+00, -7.6562e-01,  1.6094e+00,
               1.6318e+00,  4.1504e-02,  1.9248e+00,  3.5400e-01,  6.7773e-01,
              -1.8774e-01,  9.6875e-01, -6.7871e-01,  2.3496e+00, -1.6582e+00,
               4.7559e-01],
             [ 1.0820e+00, -1.4189e+00,  2.4036e-01,  4.0586e+00, -1.3730e+00,
               2.7734e+00,  1.5352e+00, -1.8281e+00, -1.0327e-01, -7.5439e-02,
               2.3555e+00, -3.9990e-01,  1.2617e+00,  3.2969e+00, -7.8906e-01,
              -1.0869e+00],
             [ 7.4121e-01, -1.4844e+00, -2.9941e+00, -1.9004e+00,  2.7832e-02,
               1.5703e+00, -3.4521e-01,  1.4570e+00, -6.7822e-01, -1.5107e+00,
              -3.2776e-02,  5.2148e-01, -9.5642e-02,  2.0508e+00,  2.9468e-01,
              -9.2383e-01],
             [ 2.2109e+00, -7.5439e-02,  1.4551e+00, -6.4648e-01, -3.0566e-01,
              -1.0918e+00, -1.7832e+00, -7.3438e-01,  2.3486e-01, -3.1855e+00,
              -1.7441e+00, -2.4109e-02, -6.1426e-01, -2.1348e+00,  1.7148e+00,
               8.5107e-01],
             [ 7.5500e-02, -5.3320e-01,  1.8301e+00, -1.4268e+00,  9.8535e-01,
              -2.4194e-01,  4.2539e+00,  6.9043e-01,  3.6084e-01,  1.0518e+00,
               1.7363e+00,  1.9756e+00, -1.9326e+00,  1.2578e+00, -6.9873e-01,
              -1.9238e+00],
             [-4.6240e-01,  3.1230e+00, -1.0176e+00, -6.0400e-01, -2.9297e-01,
               6.9678e-01,  1.0586e+00, -5.9961e-01,  2.1997e-01, -1.4082e+00,
               1.6104e+00,  2.3594e+00, -5.8301e-01,  4.8413e-01,  2.1230e+00,
               1.7373e+00],
             [ 1.9238e+00, -5.8008e-01, -1.1660e+00,  5.8984e-01,  2.8015e-02,
               1.3389e+00,  1.3818e-01,  1.8477e+00,  6.4697e-03, -1.3652e+00,
               4.1094e+00, -1.4004e+00,  1.3613e+00,  7.2363e-01, -1.0596e+00,
               1.8877e+00],
             [ 6.4014e-01, -6.7090e-01,  7.6074e-01, -7.4219e-01, -1.1963e-02,
              -3.5718e-01,  6.2012e-01,  1.7639e-01,  2.4951e-01,  4.1504e-01,
              -1.4111e+00,  8.4570e-01, -1.8774e-01,  2.8359e+00, -5.3711e-02,
               2.7422e+00],
             [ 3.0762e+00,  1.4954e-03, -5.6183e-02, -8.1201e-01,  1.0156e+00,
               2.4180e+00,  1.1445e+00,  2.5781e-01,  1.1924e+00, -2.0859e+00,
              -7.6562e-01,  2.5332e+00,  6.7139e-01, -4.6509e-01, -3.2227e-01,
              -3.2251e-01],
             [-1.6943e-01,  2.9541e-02,  9.3384e-02,  6.5967e-01,  2.3906e+00,
              -1.3398e+00,  8.4180e-01, -1.2207e+00, -2.0142e-01, -1.7847e-01,
              -2.0254e+00,  1.4922e+00,  1.0889e+00,  3.7036e-01,  1.4893e+00,
              -1.6680e+00],
             [-1.0215e+00,  1.1455e+00, -2.1216e-01, -3.0547e+00, -6.8604e-01,
               1.5000e+00,  3.4336e+00,  1.1699e+00, -1.6211e+00,  4.3628e-01,
               7.2754e-01,  7.0801e-01,  1.5259e-01,  3.5059e-01, -1.2314e+00,
               5.6689e-01],
             [-1.0391e+00, -6.4551e-01, -1.4316e+00,  1.8301e+00, -4.6338e-01,
              -2.7559e+00, -1.4258e-01,  1.7590e-01,  2.3901e-01, -1.2363e+00,
               2.4097e-01,  8.4326e-01, -3.3262e+00, -8.6377e-01,  1.7900e+00,
              -9.3994e-01],
             [ 4.2822e-01, -1.1104e+00, -2.0801e+00, -2.6855e+00, -1.1406e+00,
               2.3086e+00,  1.3750e+00, -2.3486e-01, -9.1504e-01, -2.3555e+00,
               7.0850e-01,  1.6201e+00, -1.1641e+00,  2.4844e+00, -1.6592e+00,
              -9.7412e-01],
             [-8.0225e-01, -6.4844e-01, -1.1836e+00, -3.8320e+00, -6.6748e-01,
               3.6387e+00,  2.7100e-01,  3.0029e-02,  2.6270e+00, -1.0820e+00,
              -8.3008e-01, -2.1621e+00, -1.1006e+00,  1.2969e+00, -1.1094e+00,
              -1.4238e+00],
             [ 4.5264e-01, -1.1206e-01, -4.8828e-02, -9.9854e-01,  3.6157e-01,
               1.5332e+00,  7.8760e-01, -1.8535e+00,  1.3379e+00, -3.4131e-01,
               1.0273e+00, -2.0654e-01, -8.8037e-01, -2.3027e+00, -2.0195e+00,
              -4.4800e-01],
             [-7.3291e-01,  1.5244e+00, -1.1719e-01,  1.1543e+00,  3.2715e+00,
               2.1252e-01,  7.7539e-01, -1.8018e+00, -2.2832e+00, -3.0918e+00,
               2.1523e+00,  1.0342e+00,  2.3359e+00, -5.7129e-01, -7.6855e-01,
              -6.8359e-01],
             [ 3.8086e-02, -1.1182e+00, -9.1003e-02, -2.5898e+00, -2.5391e-01,
               1.7168e+00,  2.0918e+00, -2.5938e+00, -3.0127e-01,  7.3828e-01,
              -9.4678e-01,  4.6191e-01,  1.9229e+00, -2.4375e+00, -6.6699e-01,
              -1.5371e+00],
             [-1.4980e+00, -2.5312e+00,  1.2510e+00, -9.2871e-01,  8.8672e-01,
              -1.1436e+00,  6.2500e-01,  2.5527e+00,  7.0410e-01,  7.8027e-01,
              -1.9226e-01, -8.8770e-01,  2.9736e-01, -3.8232e-01, -7.8418e-01,
              -1.1836e+00],
             [ 5.5176e-01, -1.5742e+00,  1.3467e+00, -2.9688e-01, -1.1523e+00,
               1.7065e-01, -6.6455e-01, -3.7148e+00,  3.3984e-01,  1.7041e+00,
              -1.0293e+00,  1.1499e-01, -2.3457e+00,  3.5962e-01,  5.2734e-01,
               3.6211e+00],
             [ 1.1514e+00, -1.3984e+00,  3.4106e-01,  1.0693e+00, -4.0063e-01,
               7.7100e-01, -1.3076e+00,  2.5391e+00, -1.0264e+00,  1.1221e+00,
               2.2129e+00, -3.0176e+00, -1.1240e+00,  1.7168e+00, -2.4570e+00,
               3.1189e-02],
             [ 6.3428e-01, -1.3398e+00,  1.8535e+00, -1.3604e+00,  1.8916e+00,
               2.5244e-01, -8.2910e-01, -1.6250e+00, -1.9395e+00, -2.1562e+00,
               4.6191e-01,  3.6011e-01,  1.7207e+00,  2.2192e-01, -1.3989e-01,
              -2.0156e+00],
             [ 1.3779e+00, -7.7637e-01,  1.8105e+00,  1.2012e+00, -1.6436e+00,
              -1.2051e+00,  3.0420e-01, -9.3994e-03, -8.7524e-02, -2.1621e+00,
               1.4990e+00,  2.8652e+00, -4.4385e-01,  1.4658e+00, -3.3081e-02,
              -3.0457e-02],
             [-5.7031e-01,  7.5977e-01, -5.0977e-01,  1.1045e+00, -6.2549e-01,
               5.8398e-01, -9.3213e-01,  1.1963e+00,  1.5698e-01, -1.3701e+00,
               2.9414e+00,  2.7783e-01, -3.6774e-02,  1.6934e+00,  9.9121e-01,
               1.8994e+00],
             [-1.1465e+00,  2.6416e-01,  2.2988e+00,  9.6924e-01,  7.3291e-01,
               2.3359e+00,  1.6279e+00,  3.8867e-01, -2.4731e-01,  1.1621e+00,
              -1.3452e-01,  1.4551e+00,  1.4531e+00,  2.3535e+00,  1.4868e-01,
              -6.9141e-01]]], dtype=torch.float16)]



.. GENERATED FROM PYTHON SOURCE LINES 185-186

Let's see the results.

.. GENERATED FROM PYTHON SOURCE LINES 186-192

.. code-block:: Python


    data = report_cmp.data
    df = pandas.DataFrame(data)
    piv = df.pivot(index=("run_index", "run_name"), columns="ref_name", values="abs")
    print(piv)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ref_name                decoder.0  decoder.attention.0  decoder.feed_forward.0  decoder.norm_1.0  decoder.norm_2.0   model.0
    run_index run_name                                                                                                          
    1         embedding      3.389648             2.915955                3.490967          2.666016          2.641602  3.389648
    2         embedding_1    2.810547             3.657349                3.229675          2.794922          2.802734  2.810547
    3         add_8          1.050049             5.153442                5.155273          2.185547          2.308594  1.050049
    4         layer_norm     2.126953             2.967896                3.389648          0.000000          0.560425  2.126953
    5         linear         4.730957             1.741089                2.087891          3.583984          3.623047  4.730957
    6         linear_1       4.337402             1.851501                1.847168          3.257812          3.208984  4.337402
    7         linear_2       5.205078             1.855591                2.004883          3.846680          3.797852  5.205078
    17        matmul_1       4.793457             1.809082                1.269043          3.291504          3.525879  4.793457
    18        linear_3       5.321289             1.822266                1.926758          3.885742          3.909180  5.321289
    19        linear_4       4.394287             2.039795                2.385254          3.359863          3.383301  4.394287
    20        linear_5       4.724121             1.751953                1.953125          3.200684          3.125977  4.724121
    30        matmul_3       4.306885             1.587036                1.416504          2.755615          2.945068  4.306885
    32        val_48         5.024902             0.174927                1.371582          2.897949          2.783051  5.024902
    33        linear_6       5.094849             0.000244                1.467285          2.967896          2.913818  5.094849
    34        add_115        1.118652             4.907349                5.006836          2.269531          2.249023  1.118652
    35        layer_norm_1   2.250000             2.913818                3.340820          0.560425          0.001953  2.250000
    39        val_54         4.609375             1.383301                0.088501          3.321777          3.272949  4.609375
    40        linear_8       4.661255             1.467285                0.000977          3.389648          3.340820  4.661255
    41        add_136        0.001953             5.094849                4.661255          2.126953          2.250000  0.001953




.. GENERATED FROM PYTHON SOURCE LINES 193-194

Let's clean a little bit.

.. GENERATED FROM PYTHON SOURCE LINES 194-197

.. code-block:: Python

    piv[piv >= 1] = np.nan
    print(piv.dropna(axis=0, how="all"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ref_name                decoder.0  decoder.attention.0  decoder.feed_forward.0  decoder.norm_1.0  decoder.norm_2.0   model.0
    run_index run_name                                                                                                          
    4         layer_norm          NaN                  NaN                     NaN          0.000000          0.560425       NaN
    32        val_48              NaN             0.174927                     NaN               NaN               NaN       NaN
    33        linear_6            NaN             0.000244                     NaN               NaN               NaN       NaN
    35        layer_norm_1        NaN                  NaN                     NaN          0.560425          0.001953       NaN
    39        val_54              NaN                  NaN                0.088501               NaN               NaN       NaN
    40        linear_8            NaN                  NaN                0.000977               NaN               NaN       NaN
    41        add_136        0.001953                  NaN                     NaN               NaN               NaN  0.001953




.. GENERATED FROM PYTHON SOURCE LINES 198-199

We can identity which results is mapped to which expected tensor.

.. GENERATED FROM PYTHON SOURCE LINES 201-203

Picture of the model
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 203-207

.. code-block:: Python


    onx = onnx.load("plot_dump_intermediate_results.onnx")
    doc.plot_dot(onx)




.. image-sg:: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_001.png
   :alt: plot dump intermediate results
   :srcset: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 208-209

.. code-block:: Python

    doc.plot_legend("steal and dump\nintermediate\nresults", "steal_forward", "blue")



.. image-sg:: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_002.png
   :alt: plot dump intermediate results
   :srcset: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_002.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 6.120 seconds)


.. _sphx_glr_download_auto_examples_plot_dump_intermediate_results.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_dump_intermediate_results.ipynb <plot_dump_intermediate_results.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_dump_intermediate_results.py <plot_dump_intermediate_results.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_dump_intermediate_results.zip <plot_dump_intermediate_results.zip>`


.. include:: plot_dump_intermediate_results.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
