
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_failing_onnxruntime_evaluator.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_failing_onnxruntime_evaluator.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_failing_onnxruntime_evaluator.py:


.. _l-plot-failing-onnxruntime-evaluator:

Intermediate results with onnxruntime
=====================================

Example :ref:`l-plot-failing-reference-evaluator` demonstrated
how to run a python runtime on a model but it may very slow sometimes
and it could show some discrepancies if the only provider is not CPU.
Let's use :class:`OnnxruntimeEvaluator <onnx_diagnostic.reference.OnnxruntimeEvaluator>`.
It splits the model into node and runs them independently until it succeeds
or fails. This class converts every node into model based on the types
discovered during the execution. It relies on :class:`InferenceSessionForTorch
<onnx_diagnostic.ort_session.InferenceSessionForTorch>` or
:class:`InferenceSessionForNumpy
<onnx_diagnostic.ort_session.InferenceSessionForNumpy>`
for the execution. This example uses torch tensor and
bfloat16.

A failing model
+++++++++++++++

The issue here is a an operator ``Cast`` trying to convert a result
into a non-existing type.

.. GENERATED FROM PYTHON SOURCE LINES 26-59

.. code-block:: Python


    import onnx
    import onnx.helper as oh
    import torch
    import onnxruntime
    from onnx_diagnostic import doc
    from onnx_diagnostic.ext_test_case import has_cuda
    from onnx_diagnostic.helpers import from_array_extended
    from onnx_diagnostic.reference import OnnxruntimeEvaluator

    TBFLOAT16 = onnx.TensorProto.BFLOAT16

    model = oh.make_model(
        oh.make_graph(
            [
                oh.make_node("Mul", ["X", "Y"], ["xy"], name="n0"),
                oh.make_node("Sigmoid", ["xy"], ["sy"], name="n1"),
                oh.make_node("Add", ["sy", "one"], ["C"], name="n2"),
                oh.make_node("Cast", ["C"], ["X999"], to=999, name="failing"),
                oh.make_node("CastLike", ["X999", "Y"], ["Z"], name="n4"),
            ],
            "-nd-",
            [
                oh.make_tensor_value_info("X", TBFLOAT16, ["a", "b", "c"]),
                oh.make_tensor_value_info("Y", TBFLOAT16, ["a", "b", "c"]),
            ],
            [oh.make_tensor_value_info("Z", TBFLOAT16, ["a", "b", "c"])],
            [from_array_extended(torch.tensor([1], dtype=torch.bfloat16), name="one")],
        ),
        opset_imports=[oh.make_opsetid("", 18)],
        ir_version=9,
    )








.. GENERATED FROM PYTHON SOURCE LINES 60-61

We check it is failing.

.. GENERATED FROM PYTHON SOURCE LINES 61-68

.. code-block:: Python


    try:
        onnxruntime.InferenceSession(model.SerializeToString(), providers=["CPUExecutionProvider"])
    except onnxruntime.capi.onnxruntime_pybind11_state.Fail as e:
        print(e)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [ONNXRuntimeError] : 1 : FAIL : Node (failing) Op (Cast) [TypeInferenceError] Attribute to does not specify a valid type in .




.. GENERATED FROM PYTHON SOURCE LINES 69-76

OnnxruntimeEvaluator
++++++++++++++++++++++++++

This class extends :class:`onnx.reference.ReferenceEvaluator`
with operators outside the standard but defined by :epkg:`onnxruntime`.
`verbose=10` tells the class to print as much as possible,
`verbose=0` prints nothing. Intermediate values for more or less verbosity.

.. GENERATED FROM PYTHON SOURCE LINES 76-87

.. code-block:: Python


    ref = OnnxruntimeEvaluator(model, verbose=10)
    feeds = dict(
        X=torch.rand((3, 4), dtype=torch.bfloat16), Y=torch.rand((3, 4), dtype=torch.bfloat16)
    )
    try:
        ref.run(None, feeds)
    except Exception as e:
        print("ERROR", type(e), e)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

     +C one: bfloat16:(1,):[1.0]
     +I X: D-1:torch.bfloat16:torch.Size([3, 4]):0.75,0.43359375,0.5390625,0.28125,0.2265625,0.36328125,0.03515625,0.67578125,0.06640625,0.87109375...
     +I Y: D-1:torch.bfloat16:torch.Size([3, 4]):0.9453125,0.484375,0.1484375,0.8828125,0.828125,0.828125,0.828125,0.94921875,0.35546875,0.734375...
    Mul(X, Y) -> xy
    ERROR <class 'TypeError'> expected str, bytes or os.PathLike object, not NoneType




.. GENERATED FROM PYTHON SOURCE LINES 88-90

:epkg:`onnxruntime` may not support bfloat16 on CPU.
See :epkg:`onnxruntime kernels`.

.. GENERATED FROM PYTHON SOURCE LINES 90-101

.. code-block:: Python


    if has_cuda():
        ref = OnnxruntimeEvaluator(model, providers="cuda", verbose=10)
        feeds = dict(
            X=torch.rand((3, 4), dtype=torch.bfloat16), Y=torch.rand((3, 4), dtype=torch.bfloat16)
        )
        try:
            ref.run(None, feeds)
        except Exception as e:
            print("ERROR", type(e), e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

     +C one: bfloat16:(1,):[1.0]
     +I X: D-1:torch.bfloat16:torch.Size([3, 4]):0.01171875,0.30859375,0.7734375,0.3828125,0.08984375,0.58203125,0.55859375,0.6953125,0.86328125,0.19140625...
     +I Y: D-1:torch.bfloat16:torch.Size([3, 4]):0.19921875,0.50390625,0.1015625,0.38671875,0.42578125,0.9765625,0.54296875,0.70703125,0.1796875,0.87109375...
    Mul(X, Y) -> xy
     + xy: D-1:torch.bfloat16:torch.Size([3, 4]):0.0023345947265625,0.1552734375,0.07861328125,0.1484375,0.038330078125,0.5703125,0.302734375,0.4921875,0.1552734375,0.1669921875...
    Sigmoid(xy) -> sy
     + sy: D-1:torch.bfloat16:torch.Size([3, 4]):0.5,0.5390625,0.51953125,0.5390625,0.51171875,0.640625,0.578125,0.62109375,0.5390625,0.54296875...
    Add(sy, one) -> C
     + C: bfloat16:(3, 4):1.5,1.5390625,1.515625,1.5390625,1.515625,1.640625,1.578125,1.625,1.5390625,1.546875...
    Cast(C) -> X999
    ERROR <class 'RuntimeError'> Unable to infer a session with inputs
    #1[A16r2]
    due to [ONNXRuntimeError] : 1 : FAIL : Node (failing) Op (Cast) [TypeInferenceError] Attribute to does not specify a valid type in .
    opset: domain='' version=18
    input: name='C' type=dtype('float32') shape=[3, 4]
    Cast(C, to=999) -> X999
    output: name='X999' type='NOTENSOR' shape=None




.. GENERATED FROM PYTHON SOURCE LINES 102-108

We can see it run until it reaches `Cast` and stops.
The error message is not always obvious to interpret.
It gets improved every time from time to time.
This runtime is useful when it fails for a numerical reason.
It is possible to insert prints in the python code to print
more information or debug if needed.

.. GENERATED FROM PYTHON SOURCE LINES 108-110

.. code-block:: Python


    doc.plot_legend("onnxruntime running\nstep by step", "OnnxruntimeEvaluator", "lightgrey")



.. image-sg:: /auto_examples/images/sphx_glr_plot_failing_onnxruntime_evaluator_001.png
   :alt: plot failing onnxruntime evaluator
   :srcset: /auto_examples/images/sphx_glr_plot_failing_onnxruntime_evaluator_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 6.938 seconds)


.. _sphx_glr_download_auto_examples_plot_failing_onnxruntime_evaluator.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_failing_onnxruntime_evaluator.ipynb <plot_failing_onnxruntime_evaluator.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_failing_onnxruntime_evaluator.py <plot_failing_onnxruntime_evaluator.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_failing_onnxruntime_evaluator.zip <plot_failing_onnxruntime_evaluator.zip>`


.. include:: plot_failing_onnxruntime_evaluator.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
