{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export with DynamicCache and dynamic shapes\n\nEvery LLMs implemented in :epkg:`transformers` use cache.\nOne of the most used is :class:`transformers.cache_utils.DynamicCache`.\nThe cache size is dynamic to cope with the growing context.\nThe example shows a tool which determines the dynamic shapes\nfor :func:`torch.export.export` based on a set of valid inputs.\n\n## Simple Examples\n\nWe first look at examples playing positional and names parameters\nto understand how :func:`torch.export.export` works.\n\n### args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pprint\nimport torch\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.cache_helpers import make_dynamic_cache\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.export import ModelInputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need addition import in case ``transformers<4.50``.\nExporting DynamicCache is not supported before that.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnx_diagnostic.ext_test_case import has_transformers\nfrom onnx_diagnostic.torch_export_patches import bypass_export_some_errors\n\n\nclass Model(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n\nmodel = Model()\nx = torch.randn((5, 6))\ny = torch.randn((1, 6))\nmodel(x, y)  # to check it works\n\nep = torch.export.export(model, (x, y))\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected there is no dynamic shapes.\nWe use :class:`onnx_diagnostic.export.ModelInputs`\nto define them from two set of valid inputs.\nThese inputs must have different value for the dynamic\ndimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs = [(x, y), (torch.randn((7, 8)), torch.randn((1, 8)))]\nmi = ModelInputs(Model(), inputs)\nds = mi.guess_dynamic_shapes()\npprint.pprint(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function returns a tuple with two objects.\nThe first one for the positional arguments, the other one\nfor the named arguments. There is no named arguments. We\nwe used the first result to export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = torch.export.export(model, (x, y), dynamic_shapes=ds[0])\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### kwargs\n\nWe do the same with named arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n\nmodel = Model()\nx = torch.randn((5, 6))\ny = torch.randn((1, 6))\nmodel(x=x, y=y)  # to check it works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Two sets of valid inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs = [dict(x=x, y=y), dict(x=torch.randn((7, 8)), y=torch.randn((1, 8)))]\nmi = ModelInputs(Model(), inputs)\nds = mi.guess_dynamic_shapes()\npprint.pprint(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = torch.export.export(model, (), kwargs=dict(x=x, y=y), dynamic_shapes=ds[1])\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### args and kwargs\n\n:func:`torch.export.export` does not like having dynami shapes\nfor both args and kwargs. We need to define them using one mechanism.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n\nmodel = Model()\nx = torch.randn((5, 6))\ny = torch.randn((1, 6))\nmodel(x, y=y)  # to check it works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Two sets of valid inputs with positional and names arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs = [((x,), dict(y=y)), ((torch.randn((7, 8)),), dict(y=torch.randn((1, 8))))]\nmi = ModelInputs(Model(), inputs)\nds = mi.guess_dynamic_shapes()\npprint.pprint(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This does not work with :func:`torch.export.export` so\nwe use a method to move the positional dynamic shapes to\nnamed one. The method relies on the signature of the\nforward method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_args, new_kwargs, new_ds = mi.move_to_kwargs(*mi.inputs[0], ds)\npprint.pprint(new_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = torch.export.export(model, new_args, kwargs=new_kwargs, dynamic_shapes=new_ds[1])\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DynamicCache\n\n:func:`torch.export.export` serializes caches and any custom class\nif these serialization functions are provided with is the case for\n:class:`transformers.cache_utils.DynamicCache` and ``transformers>=4.50``.\nThe dynamic shapes must be provided following the serialized form.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n    def forward(self, cache, z):\n        return (\n            z\n            + cache.key_cache[0]\n            + cache.key_cache[1]\n            + cache.value_cache[0]\n            + cache.value_cache[1]\n        )\n\n\nmodel = Model()\n\nn_layers = 2\nbsize, nheads, slen, dim = 2, 4, 3, 7\ncache = make_dynamic_cache(\n    [\n        (torch.randn(bsize, nheads, slen, dim), torch.randn(bsize, nheads, slen, dim))\n        for i in range(n_layers)\n    ]\n)\nz = torch.randn((1, 1, 1, 7))\nmodel(cache, z)  # to check it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cache looks like this:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(string_type(cache, with_shape=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache2 = make_dynamic_cache(\n    [\n        (\n            torch.randn(bsize + 1, nheads, slen + 1, dim + 1),\n            torch.randn(bsize + 1, nheads, slen + 1, dim + 1),\n        )\n        for i in range(n_layers)\n    ]\n)\ninputs = [\n    (cache, z),\n    (cache2, torch.randn((1, 1, 1, 8))),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the first set of inputs looks like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(string_type(inputs[0], with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compute the dynamic shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mi = ModelInputs(Model(), inputs)\nds = mi.guess_dynamic_shapes()\npprint.pprint(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally the export.\nThe export is simple if ``transformers>=4.50``, otherwise,\ntransformers needs to be patched.\n:func:`onnx_diagnostic.torch_export_patches.bypass_export_some_errors`\nregisters functions to serialize ``DynamicCache``. This one is modified to make\nthe shape inference implemented in :epkg:`torch` happy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if has_transformers(\"4.50\"):\n    ep = torch.export.export(model, inputs[0], dynamic_shapes=ds[0], strict=False)\nelse:\n    with bypass_export_some_errors(patch_transformers=True) as modificator:\n        ep = torch.export.export(\n            model, modificator(inputs[0]), dynamic_shapes=ds[0], strict=False\n        )\nprint(ep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"dynamic shapes\", \"torch.export.export\", \"tomato\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}