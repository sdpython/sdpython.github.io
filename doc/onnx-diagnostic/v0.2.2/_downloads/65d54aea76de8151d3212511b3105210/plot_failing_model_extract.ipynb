{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Find where a model is failing by running submodels\n\nLet's assume :epkg:`onnxruntime` crashes without telling why or where.\nThe first thing is do is to locate where. For that, we extract every submodel\nstarting from the inputs and running the first *n* nodes of the model.\nThe model is likely to fail for some *n*. Then the failing is known.\n\nThis method only works if the model only contains operator coming\nfrom the main domain *ai.onnx* otherwise shape inference stops\nat the first non standard operator and the algorithm fails at\nproducing :class:`onnx.ModelProto` including the non standard operators.\n\n## A failing model\n\nThe issue here is a an operator ``Cast`` trying to convert a result\ninto a non-existing type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport onnx\nimport onnx.helper as oh\nimport onnxruntime\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import from_array_extended\nfrom onnx_diagnostic.ort_session import investigate_onnxruntime_issue\n\nTFLOAT = onnx.TensorProto.FLOAT\n\nmodel = oh.make_model(\n    oh.make_graph(\n        [\n            oh.make_node(\"Mul\", [\"X\", \"Y\"], [\"xy\"], name=\"n0\"),\n            oh.make_node(\"Sigmoid\", [\"xy\"], [\"sy\"], name=\"n1\"),\n            oh.make_node(\"Add\", [\"sy\", \"one\"], [\"C\"], name=\"n2\"),\n            oh.make_node(\"Cast\", [\"C\"], [\"X999\"], to=999, name=\"failing\"),\n            oh.make_node(\"CastLike\", [\"X999\", \"Y\"], [\"Z\"], name=\"n4\"),\n        ],\n        \"-nd-\",\n        [\n            oh.make_tensor_value_info(\"X\", TFLOAT, [\"a\", \"b\", \"c\"]),\n            oh.make_tensor_value_info(\"Y\", TFLOAT, [\"a\", \"b\", \"c\"]),\n        ],\n        [oh.make_tensor_value_info(\"Z\", TFLOAT, [\"a\", \"b\", \"c\"])],\n        [from_array_extended(np.array([1], dtype=np.float32), name=\"one\")],\n    ),\n    opset_imports=[oh.make_opsetid(\"\", 18)],\n    ir_version=9,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check it is failing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    onnxruntime.InferenceSession(model.SerializeToString(), providers=[\"CPUExecutionProvider\"])\nexcept onnxruntime.capi.onnxruntime_pybind11_state.Fail as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shape Inference\n\nBuilding submodels requires to known the output type.\nWe run shape inference on the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shaped_model = onnx.shape_inference.infer_shapes(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looping over the nodes\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "failing = investigate_onnxruntime_issue(shaped_model, providers=\"cpu\", verbose=1, quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print the failing node.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(failing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detect an issue with shape Inference\n\nWe could have caught the error sooner by asking shape inference\nto raise an exception if one node could not be processed.\nIt means either the node is a custom node\nand shape inference has no way to guess the output type and shape\nfor this node or shape inference failed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    onnx.shape_inference.infer_shapes(model, strict_mode=True)\nexcept onnx.onnx_cpp2py_export.shape_inference.InferenceError as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"Run until it fails\", \"onnxruntime.InferenceSession\", \"lightgrey\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}