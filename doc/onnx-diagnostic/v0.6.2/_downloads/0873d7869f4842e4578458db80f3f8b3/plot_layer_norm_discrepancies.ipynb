{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# LayerNormalization implementation cannot be exchanged\n\nThis example applies what was illustrated\n`l-plot-parallelized-reduction`, reduction operations\nare sensitive to parallelization.\n\nWe consider a small model including a layer normalization\nfollowed by a matrix multiplication and we show that replacing\na kernel by another one may significantly impact the output.\n\n## The model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nimport pandas\nimport onnx\nimport onnx.helper as oh\nimport onnxruntime\nimport torch\nfrom onnx_array_api.plotting.graphviz_helper import plot_dot\nfrom onnx_diagnostic.ext_test_case import unit_test_going\nfrom onnx_diagnostic.helpers import max_diff, string_diff, string_type\nfrom onnx_diagnostic.helpers.onnx_helper import onnx_dtype_name, onnx_dtype_to_np_dtype\nfrom onnx_diagnostic.helpers.torch_helper import onnx_dtype_to_torch_dtype\nfrom onnx_diagnostic.helpers.doc_helper import LayerNormalizationOrt, MatMulOrt\nfrom onnx_diagnostic.reference import TorchOnnxEvaluator\n\nTFLOAT = onnx.TensorProto.FLOAT\nTFLOAT16 = onnx.TensorProto.FLOAT16\n\n\ndef get_model(itype: int = TFLOAT16):\n    return oh.make_model(\n        oh.make_graph(\n            [\n                oh.make_node(\"LayerNormalization\", [\"X\", \"scale\", \"bias\"], [\"norm\"], axis=-1),\n                oh.make_node(\"MatMul\", [\"norm\", \"weights\"], [\"mm\"]),\n                oh.make_node(\"Add\", [\"mm\", \"bias2\"], [\"Z\"]),\n            ],\n            \"layer_norm_matmul_add\",\n            [\n                oh.make_tensor_value_info(\"X\", itype, [\"a\", \"b\", \"c\"]),\n                oh.make_tensor_value_info(\"scale\", itype, [\"c\"]),\n                oh.make_tensor_value_info(\"bias\", itype, [\"c\"]),\n                oh.make_tensor_value_info(\"weights\", itype, [\"c\", \"c\"]),\n                oh.make_tensor_value_info(\"bias2\", itype, [\"c\"]),\n            ],\n            [oh.make_tensor_value_info(\"Z\", itype, [\"a\", \"b\", \"c\"])],\n        ),\n        ir_version=9,\n        opset_imports=[oh.make_opsetid(\"\", 18)],\n    )\n\n\nmodel = get_model()\nplot_dot(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's compare two runtimes\n\nThat will be :epkg:`onnxruntime` and\n:class:`onnx_diagnostic.reference.TorchOnnxEvaluator`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "last_dim = 64 if unit_test_going() else 1152\n\n\ndef make_feeds(last_dim: int):\n    return {\n        \"X\": (torch.rand((32, 1024, last_dim), dtype=torch.float16) - 0.5) * 120,\n        \"scale\": torch.rand((last_dim,), dtype=torch.float16),\n        \"bias\": torch.rand((last_dim,), dtype=torch.float16),\n        \"weights\": torch.rand((last_dim, last_dim), dtype=torch.float16),\n        \"bias2\": torch.rand((last_dim,), dtype=torch.float16),\n    }\n\n\ndef cast_feeds(itype, provider, feeds):\n    np_feeds = {k: v.detach().numpy() for k, v in feeds.items()}\n    if provider == \"CUDA\":\n        if not torch.cuda.is_available():\n            return None, None\n        tch_feeds = {k: v.to(\"cuda\") for k, v in feeds.items()}\n        ort_feeds = np_feeds\n    else:\n        tch_feeds = feeds.copy()\n        tch_feeds[\"X\"] = tch_feeds[\"X\"][:2]  # too long otherwise\n        ort_feeds = np_feeds.copy()\n        ort_feeds[\"X\"] = ort_feeds[\"X\"][:2]\n    tch_feeds = {k: v.to(ttype) for k, v in tch_feeds.items()}\n    ort_feeds = {k: v.astype(np_dtype) for k, v in ort_feeds.items()}\n    return tch_feeds, ort_feeds\n\n\nfeeds = make_feeds(last_dim)\nkws = dict(with_shape=True, with_min_max=True, with_device=True)\ndata = []\nbaseline = {}\n\nfor provider, itype in itertools.product([\"CPU\", \"CUDA\"], [TFLOAT, TFLOAT16]):\n    ttype = onnx_dtype_to_torch_dtype(itype)\n    np_dtype = onnx_dtype_to_np_dtype(itype)\n    tch_feeds, ort_feeds = cast_feeds(itype, provider, feeds)\n    if tch_feeds is None:\n        continue\n\n    model = get_model(itype)\n    print()\n    print(f\"-- running on {provider} with {onnx_dtype_name(itype)}\")\n    print(\"-- running with torch\")\n    torch_sess = TorchOnnxEvaluator(model, providers=[f\"{provider}ExecutionProvider\"])\n    expected = torch_sess.run(None, tch_feeds)\n    baseline[itype, provider, \"torch\"] = expected\n    print(f\"-- torch: {string_type(expected, **kws)}\")\n\n    print(\"-- running with ort\")\n    ort_sess = onnxruntime.InferenceSession(\n        model.SerializeToString(), providers=[f\"{provider}ExecutionProvider\"]\n    )\n    got = ort_sess.run(None, ort_feeds)\n    baseline[itype, provider, \"ort\"] = got\n    print(f\"-- ort: {string_type(got, **kws)}\")\n    diff = max_diff(expected, got, hist=True)\n    print(f\"-- diff {string_diff(diff)}\")\n\n    # memorize the data\n    diff[\"dtype\"] = onnx_dtype_name(itype)\n    diff[\"provider\"] = provider\n    diff.update(diff[\"rep\"])\n    del diff[\"rep\"]\n    del diff[\"dnan\"]\n    del diff[\">100.0\"]\n    del diff[\">10.0\"]\n    data.append(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pandas.DataFrame(data).set_index([\"provider\", \"dtype\"])\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df[\"abs\"].plot.bar(title=\"Discrepancies ORT / torch for LayerNorm(X) @ W + B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The discrepancies are significant on CUDA, higher for float16.\nLet's see which operator is responsible for them,\n*LayerNormalization* or *MatMul*.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The discrepancies come from?\n\nWe mix torch and onnxruntime to execute the kernels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\n\nfor mod, provider, itype in itertools.product(\n    [\"ORT-TORCH\", \"TORCH-ORT\"], [\"CPU\", \"CUDA\"], [TFLOAT, TFLOAT16]\n):\n    ttype = onnx_dtype_to_torch_dtype(itype)\n    np_dtype = onnx_dtype_to_np_dtype(itype)\n    tch_feeds, _ = cast_feeds(itype, provider, feeds)\n    if tch_feeds is None:\n        continue\n\n    custom_kernels = (\n        {(\"\", \"LayerNormalization\"): LayerNormalizationOrt}\n        if mod == \"ORT-TORCH\"\n        else {(\"\", \"MatMul\"): MatMulOrt}\n    )\n\n    model = get_model(itype)\n    print()\n    print(f\"-- {mod} running on {provider} with {onnx_dtype_name(itype)}\")\n    sess = TorchOnnxEvaluator(\n        model,\n        custom_kernels=custom_kernels,\n        providers=[f\"{provider}ExecutionProvider\"],\n    )\n    got = sess.run(None, tch_feeds)\n    print(f\"-- {mod}: {string_type(got, **kws)}\")\n\n    difft = max_diff(baseline[itype, provider, \"torch\"], got)\n    print(f\"-- diff with torch {string_diff(difft)}\")\n    diffo = max_diff(baseline[itype, provider, \"ort\"], got)\n    print(f\"-- diff with ort {string_diff(diffo)}\")\n\n    data.append(\n        dict(\n            model=mod,\n            dtype=onnx_dtype_name(itype),\n            provider=provider,\n            diff_ort=diffo[\"abs\"],\n            diff_torch=difft[\"abs\"],\n        )\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pandas.DataFrame(data).set_index([\"model\", \"provider\", \"dtype\"])\ndf = df.sort_index()\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df[[\"diff_ort\", \"diff_torch\"]].plot.bar(\n    title=\"ORT/Torch or Torch/ORT for LayerNorm(X) @ W + B\"\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}