
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_technical/plot_parallelized_reduction.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_technical_plot_parallelized_reduction.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_technical_plot_parallelized_reduction.py:


.. _l-plot-parallelized-reduction:

Reproducible Parallelized Reduction is difficult
================================================

A reduction is a frequent operation with neural networks. It appears in layer normalization,
softmax... Because of the float precision, the result of the computation
changes based on the order of the elements. The following examples show the variation
based on different hypothesis on the vector distribution.
We consider a vector :math:`X = (x_1, ..., x_n)`.
It computes the average:

.. math::

    mean(X) = \frac{\sum_{i=1}^n x_i}{n}

Or the normalization of the vector:

.. math::

    norm(X)_i = \frac{ X_i  - \mathbb{E}X}{ \sqrt{ \mathbb{V}X}}

With :math:`\mathbb{E}X = mean(X)`,
:math:`\mathbb{V}X = mean\left(\left(X - mean(X)\right)^2\right)`.

Methodology
+++++++++++

**Permutation should not change the average.**

We draw 128 random permutations of X. The average or mean should not change.
And the normalized vector should have the same values. In the first case, we compute
the difference between the highest and the lowest values obtained for the average.
In the second case, we look for the maximum difference between the original normalized
vector and the permuted one, both sorted.

The computation code
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 41-125

.. code-block:: Python


    import itertools
    from tqdm import tqdm
    import numpy as np
    import pandas

    DATA = []


    def str_dtype(dtype):
        """Displays numpy dtype in a nicer way."""
        if dtype == np.float64:
            return "fp64"
        if dtype == np.float32:
            return "fp32"
        if dtype == np.float16:
            return "fp16"
        raise ValueError(f"Unexpected value {dtype}")


    def layer_norm(a, eps=1e-6):
        """
        Normalized the vector a.
        The computation is done in float32 or float64.
        """
        ctype = np.float32 if a.dtype == np.float16 else a.dtype
        a32 = a.astype(ctype)
        m = a32.mean(axis=-1, keepdims=True)
        c = a32 - m
        va = np.sqrt((c * c).mean(axis=-1, keepdims=True))
        va += eps
        return (c / va).astype(a.dtype)


    def compute(values, fct):
        """
        Compare the results of function ``fct`` on a sample.
        Loops over multiple sizes, dtypes. Tries 128 times.
        """

        def make_value(base, value):
            if value.size > 1:
                return np.abs(np.sort(base) - np.sort(value)).max()
            return value

        sizes = [2, 4, 8, 16, 512, 1024, 2048, 4096, 8192]
        dtypes = [np.float64, np.float32, np.float16]
        N = list(range(128))
        exps = list(itertools.product(sizes, dtypes, N))
        data = []
        ech = None
        for size, dtype, n in tqdm(exps):
            if n == 0:
                ech = values[:size].astype(dtype)
                base = fct(ech)
                assert base.dtype == ech.dtype
                obs = dict(
                    n=n, size=size, dtype=str_dtype(ech.dtype), value=make_value(base, fct(ech))
                )
                data.append(obs)

            if n == 1:
                new_ech = np.sort(ech)
            elif n == 2:
                new_ech = np.sort(ech)[::-1]
            else:
                new_ech = np.random.permutation(ech)
            assert new_ech.dtype == ech.dtype
            assert new_ech.shape == ech.shape
            obs = dict(
                n=n + 1,
                size=size,
                dtype=str_dtype(new_ech.dtype),
                value=make_value(base, fct(new_ech)),
            )
            data.append(obs)

        df = pandas.DataFrame(data)
        agg = df.drop("n", axis=1).groupby(["dtype", "size"], as_index=False).agg(["min", "max"])
        agg["value", "delta"] = agg["value", "max"] - agg["value", "min"]
        piv = agg.pivot(index="size", columns="dtype", values=("value", "delta"))
        return piv









.. GENERATED FROM PYTHON SOURCE LINES 126-131

Normal Law
++++++++++

Let's see what it returns an on random sample following a normal law.
First the average.

.. GENERATED FROM PYTHON SOURCE LINES 131-137

.. code-block:: Python


    values = np.random.randn(4096)
    mean = compute(values, lambda x: np.mean(x).astype(x.dtype))
    mean["name"] = "normal"
    print(mean)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     95%|█████████▌| 3291/3456 [00:00<00:00, 32878.36it/s]    100%|██████████| 3456/3456 [00:00<00:00, 28144.72it/s]
    dtype  fp16          fp32          fp64    name
    size                                           
    2       0.0  0.000000e+00  0.000000e+00  normal
    4       0.0  0.000000e+00  0.000000e+00  normal
    8       0.0  8.940697e-08  1.110223e-16  normal
    16      0.0  5.960464e-08  1.110223e-16  normal
    512     0.0  8.940697e-08  1.249001e-16  normal
    1024    0.0  2.235174e-08  5.551115e-17  normal
    2048    0.0  4.097819e-08  4.857226e-17  normal
    4096    0.0  1.303852e-08  6.938894e-17  normal
    8192    0.0  1.117587e-08  7.285839e-17  normal




.. GENERATED FROM PYTHON SOURCE LINES 138-139

Then the layer normalization.

.. GENERATED FROM PYTHON SOURCE LINES 139-145

.. code-block:: Python


    ln = compute(values, layer_norm)
    ln["name"] = "normal"
    DATA.append(ln.reset_index(drop=True).max(axis=0))
    print(ln)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     71%|███████▏  | 2465/3456 [00:00<00:00, 24641.27it/s]    100%|██████████| 3456/3456 [00:00<00:00, 8371.52it/s] 
    dtype      fp16  ...    name
    size             ...        
    2      0.000000  ...  normal
    4      0.000000  ...  normal
    8      0.000000  ...  normal
    16     0.000000  ...  normal
    512    0.000000  ...  normal
    1024   0.000488  ...  normal
    2048   0.000000  ...  normal
    4096   0.000244  ...  normal
    8192   0.000000  ...  normal

    [9 rows x 4 columns]




.. GENERATED FROM PYTHON SOURCE LINES 146-150

Fixed values
++++++++++++

We try a fixed vector with one very high value and all the others are small.

.. GENERATED FROM PYTHON SOURCE LINES 150-157

.. code-block:: Python


    values[:] = -1e-4
    values[::128] = 100
    mean = compute(values, lambda x: np.mean(x).astype(x.dtype))
    mean["name"] = "fixed"
    print(mean)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     96%|█████████▌| 3314/3456 [00:00<00:00, 33125.89it/s]    100%|██████████| 3456/3456 [00:00<00:00, 29557.95it/s]
    dtype  fp16          fp32          fp64   name
    size                                          
    2       0.0  0.000000e+00  0.000000e+00  fixed
    4       0.0  0.000000e+00  3.552714e-15  fixed
    8       0.0  0.000000e+00  0.000000e+00  fixed
    16      0.0  0.000000e+00  0.000000e+00  fixed
    512     0.0  1.788139e-07  4.440892e-16  fixed
    1024    0.0  1.788139e-07  4.440892e-16  fixed
    2048    0.0  4.768372e-07  4.440892e-16  fixed
    4096    0.0  2.384186e-07  9.992007e-16  fixed
    8192    0.0  2.384186e-07  9.992007e-16  fixed




.. GENERATED FROM PYTHON SOURCE LINES 158-159

And the normalized vector.

.. GENERATED FROM PYTHON SOURCE LINES 159-164

.. code-block:: Python

    ln = compute(values, layer_norm)
    ln["name"] = "fixed"
    DATA.append(ln.reset_index(drop=True).max(axis=0))
    print(ln)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     79%|███████▉  | 2723/3456 [00:00<00:00, 27216.14it/s]    100%|██████████| 3456/3456 [00:00<00:00, 15979.85it/s]
    dtype  fp16      fp32          fp64   name
    size                                      
    2       0.0  0.000000  0.000000e+00  fixed
    4       0.0  0.000000  2.220446e-16  fixed
    8       0.0  0.000000  0.000000e+00  fixed
    16      0.0  0.000000  0.000000e+00  fixed
    512     0.0  0.000002  5.329071e-15  fixed
    1024    0.0  0.000002  1.776357e-15  fixed
    2048    0.0  0.000004  7.105427e-15  fixed
    4096    0.0  0.000002  1.776357e-15  fixed
    8192    0.0  0.000002  1.776357e-15  fixed




.. GENERATED FROM PYTHON SOURCE LINES 165-169

Pareto Distribution
+++++++++++++++++++

A law with a long tail.

.. GENERATED FROM PYTHON SOURCE LINES 169-177

.. code-block:: Python


    values = np.random.pareto(1, (4096,))
    print(values)

    mean = compute(values, lambda x: np.mean(x).astype(x.dtype))
    mean["name"] = "normal"
    print(mean)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [0.60690077 0.39940303 0.65340433 ... 0.62058226 2.26779759 0.31579773]
      0%|          | 0/3456 [00:00<?, ?it/s]     96%|█████████▌| 3324/3456 [00:00<00:00, 33215.56it/s]    100%|██████████| 3456/3456 [00:00<00:00, 30408.81it/s]
    dtype  fp16          fp32          fp64    name
    size                                           
    2       0.0  0.000000e+00  0.000000e+00  normal
    4       0.0  1.192093e-07  0.000000e+00  normal
    8       0.0  1.192093e-07  2.220446e-16  normal
    16      0.0  2.384186e-07  6.661338e-16  normal
    512     0.0  2.288818e-05  4.263256e-14  normal
    1024    0.0  1.525879e-05  2.842171e-14  normal
    2048    0.0  1.144409e-05  1.776357e-14  normal
    4096    0.0  4.768372e-06  8.881784e-15  normal
    8192    0.0  3.814697e-06  5.329071e-15  normal




.. GENERATED FROM PYTHON SOURCE LINES 178-179

And the normalized vector.

.. GENERATED FROM PYTHON SOURCE LINES 179-184

.. code-block:: Python

    ln = compute(values, layer_norm)
    ln["name"] = "pareto"
    DATA.append(ln.reset_index(drop=True).max(axis=0))
    print(ln)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     74%|███████▍  | 2562/3456 [00:00<00:00, 25523.15it/s]    100%|██████████| 3456/3456 [00:00<00:00, 9567.41it/s] 
    dtype          fp16  ...    name
    size                 ...        
    2      0.000000e+00  ...  pareto
    4      0.000000e+00  ...  pareto
    8      0.000000e+00  ...  pareto
    16     0.000000e+00  ...  pareto
    512    0.000000e+00  ...  pareto
    1024   3.051758e-05  ...  pareto
    2048   2.384186e-07  ...  pareto
    4096   1.525879e-05  ...  pareto
    8192   1.525879e-05  ...  pareto

    [9 rows x 4 columns]




.. GENERATED FROM PYTHON SOURCE LINES 185-189

Summary
+++++++

We consider the maximum difference obtained for any sample size.

.. GENERATED FROM PYTHON SOURCE LINES 189-193

.. code-block:: Python


    df = pandas.DataFrame(DATA).set_index("name")
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dtype       fp16          fp32          fp64
    name                                        
    normal  0.000488  7.152557e-07  8.881784e-16
    fixed   0.000000  3.814697e-06  7.105427e-15
    pareto  0.000031  7.629395e-06  2.131628e-14




.. GENERATED FROM PYTHON SOURCE LINES 194-195

Visually.

.. GENERATED FROM PYTHON SOURCE LINES 195-201

.. code-block:: Python


    ax = df.plot.bar(logy=True)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
    fig = ax.get_figure()
    fig.savefig("plot_parallelized_reduction.png")




.. image-sg:: /auto_technical/images/sphx_glr_plot_parallelized_reduction_001.png
   :alt: plot parallelized reduction
   :srcset: /auto_technical/images/sphx_glr_plot_parallelized_reduction_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 202-211

In a deep neural network
++++++++++++++++++++++++

Some of the vector have 500 values, 16x32x1024x1024. A layer normalization
does 16x32x1024 ~ 2M reductions, over 20 layers.
When a deep neural network is computed with a different code
doing a different parallelization (GPU/CPU for example),
the order of the reduction may change and therefore,
some errors will appear and propagate.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.573 seconds)


.. _sphx_glr_download_auto_technical_plot_parallelized_reduction.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_parallelized_reduction.ipynb <plot_parallelized_reduction.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_parallelized_reduction.py <plot_parallelized_reduction.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_parallelized_reduction.zip <plot_parallelized_reduction.zip>`


.. include:: plot_parallelized_reduction.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
