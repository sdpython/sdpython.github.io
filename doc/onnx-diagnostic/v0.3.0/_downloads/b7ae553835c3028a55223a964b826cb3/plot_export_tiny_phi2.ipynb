{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Untrained microsoft/phi-2\n\n:epkg:`microsoft/phi-2` is not a big models but still quite big\nwhen it comes to write unittest. Function\n:func:`onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs`\ncan be used to create a reduced untrained version of a model coming from\n:epkg:`HuggingFace`. It downloads the configuration from the website\nbut creates a dummy model with 1 or 2 hidden layers in order to reduce\nthe size and get a fast execution. The goal is usually to test\nthe export or to compare performance. The relevance does not matter.\n\n## Create the dummy model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nimport pprint\nimport warnings\nimport torch\nimport onnxruntime\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import max_diff, string_diff, string_type\nfrom onnx_diagnostic.helpers.cache_helper import is_cache_dynamic_registered\nfrom onnx_diagnostic.helpers.ort_session import make_feeds\nfrom onnx_diagnostic.torch_export_patches import bypass_export_some_errors\nfrom onnx_diagnostic.torch_models.hghub import (\n    get_untrained_model_with_inputs,\n)\n\nwarnings.simplefilter(\"ignore\")\n\n# another tiny id: arnir0/Tiny-LLM\ndata = get_untrained_model_with_inputs(\"microsoft/phi-2\")\nuntrained_model, inputs, dynamic_shapes, config, size, n_weights = (\n    data[\"model\"],\n    data[\"inputs\"],\n    data[\"dynamic_shapes\"],\n    data[\"configuration\"],\n    data[\"size\"],\n    data[\"n_weights\"],\n)\n\nprint(f\"model {size / 2**10:1.3f} Kb with {n_weights} parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The original model has 2.7 billion parameters. It was divided by more than 10.\nLet's see the configuration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(string_type(inputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With min/max values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(string_type(inputs, with_shape=True, with_min_max=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the dynamic shapes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint.pprint(dynamic_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We execute the model to produce expected outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expected = untrained_model(**copy.deepcopy(inputs))\nprint(f\"expected: {string_type(expected, with_shape=True, with_min_max=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with bypass_export_some_errors(patch_transformers=True) as modificator:\n\n    # Unnecessary steps but useful in case of an error\n    # We check the cache is registered.\n    assert is_cache_dynamic_registered()\n\n    # We check there is no discrepancies when the cache is applied.\n    d = max_diff(expected, untrained_model(**copy.deepcopy(inputs)))\n    assert (\n        d[\"abs\"] < 1e-5\n    ), f\"The model with patches produces different outputs: {string_diff(d)}\"\n\n    # Then we export.\n    ep = torch.export.export(\n        untrained_model,\n        (),\n        kwargs=modificator(copy.deepcopy(inputs)),\n        dynamic_shapes=dynamic_shapes,\n        strict=False,  # mandatory for torch==2.6\n    )\n\n    # We check the exported program produces the same results as well.\n    d = max_diff(expected, ep.module()(**copy.deepcopy(inputs)))\n    assert d[\"abs\"] < 1e-5, f\"The exported model different outputs: {string_diff(d)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export to ONNX\n\nThe export works. We can export to ONNX now.\nPatches are still needed because the export\napplies :meth:`torch.export.ExportedProgram.run_decompositions`\nmay export local pieces of the model again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with bypass_export_some_errors(patch_transformers=True):\n    epo = torch.onnx.export(\n        ep, (), kwargs=copy.deepcopy(inputs), dynamic_shapes=dynamic_shapes, dynamo=True\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epo.save(\"plot_export_tiny_phi2.onnx\", external_data=True)\n\n# Or directly get the :class:`onnx.ModelProto`.\nonx = epo.model_proto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discrepancies\n\nThe we check the conversion to ONNX.\nLet's make sure the ONNX model produces the same outputs.\nIt takes flatten inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feeds = make_feeds(onx, copy.deepcopy(inputs), use_numpy=True, copy=True)\n\nprint(f\"torch inputs: {string_type(inputs)}\")\nprint(f\"onxrt inputs: {string_type(feeds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create a :class:`onnxruntime.InferenceSession`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = onnxruntime.InferenceSession(\n    onx.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "got = sess.run(None, feeds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally the discrepancies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diff = max_diff(expected, got, flatten=True)\nprint(f\"onnx discrepancies: {string_diff(diff)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks good.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"untrained smaller\\nmicrosoft/phi-2\", \"torch.onnx.export\", \"green\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}