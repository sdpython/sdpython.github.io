
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_dump_intermediate_results.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_dump_intermediate_results.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_dump_intermediate_results.py:


.. _l-plot-intermediate-results:

Dumps intermediate results of a torch model
===========================================

Looking for discrepancies is quickly annoying. Discrepancies
come from two results obtained with the same models
implemented in two different ways, :epkg:`pytorch` and :epkg:`onnx`.
Models are big so where do they come from? That's the
unavoidable question. Unless there is an obvious reason,
the only way is to compare intermediate outputs alon the computation.
The first step into that direction is to dump the intermediate results
coming from :epkg:`pytorch`.
We use :func:`onnx_diagnostic.helpers.torch_helper.steal_forward` for that.

A simple LLM Model
++++++++++++++++++

See :func:`onnx_diagnostic.helpers.torch_helper.dummy_llm`
for its definition. It is mostly used for unit test or example.

.. GENERATED FROM PYTHON SOURCE LINES 23-39

.. code-block:: Python


    import numpy as np
    import pandas
    import onnx
    import torch
    import onnxruntime
    from onnx_array_api.plotting.graphviz_helper import plot_dot
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers import max_diff, string_diff, string_type
    from onnx_diagnostic.helpers.torch_helper import dummy_llm, steal_forward
    from onnx_diagnostic.helpers.mini_onnx_builder import create_input_tensors_from_onnx_model
    from onnx_diagnostic.reference import OnnxruntimeEvaluator, ReportResultComparison


    model, inputs, ds = dummy_llm(dynamic_shapes=True)








.. GENERATED FROM PYTHON SOURCE LINES 40-41

We use float16.

.. GENERATED FROM PYTHON SOURCE LINES 41-43

.. code-block:: Python

    model = model.to(torch.float16)








.. GENERATED FROM PYTHON SOURCE LINES 44-45

Let's check.

.. GENERATED FROM PYTHON SOURCE LINES 45-50

.. code-block:: Python


    print(f"type(model)={type(model)}")
    print(f"inputs={string_type(inputs, with_shape=True)}")
    print(f"ds={string_type(ds, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    type(model)=<class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.LLM'>
    inputs=(T7s2x30,)
    ds=dict(input_ids:{0:Dim(batch),1:Dim(length)})




.. GENERATED FROM PYTHON SOURCE LINES 51-52

It contains the following submodules.

.. GENERATED FROM PYTHON SOURCE LINES 52-56

.. code-block:: Python


    for name, mod in model.named_modules():
        print(f"- {name}: {type(mod)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    - : <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.LLM'>
    - embedding: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.Embedding'>
    - embedding.embedding: <class 'torch.nn.modules.sparse.Embedding'>
    - embedding.pe: <class 'torch.nn.modules.sparse.Embedding'>
    - decoder: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.DecoderLayer'>
    - decoder.attention: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.MultiAttentionBlock'>
    - decoder.attention.attention: <class 'torch.nn.modules.container.ModuleList'>
    - decoder.attention.attention.0: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.AttentionBlock'>
    - decoder.attention.attention.0.query: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.0.key: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.0.value: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.AttentionBlock'>
    - decoder.attention.attention.1.query: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1.key: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1.value: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.linear: <class 'torch.nn.modules.linear.Linear'>
    - decoder.feed_forward: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.FeedForward'>
    - decoder.feed_forward.linear_1: <class 'torch.nn.modules.linear.Linear'>
    - decoder.feed_forward.relu: <class 'torch.nn.modules.activation.ReLU'>
    - decoder.feed_forward.linear_2: <class 'torch.nn.modules.linear.Linear'>
    - decoder.norm_1: <class 'torch.nn.modules.normalization.LayerNorm'>
    - decoder.norm_2: <class 'torch.nn.modules.normalization.LayerNorm'>




.. GENERATED FROM PYTHON SOURCE LINES 57-63

Steal and dump the output of submodules
+++++++++++++++++++++++++++++++++++++++

The following context spies on the intermediate results
for the following module and submodules. It stores
in one onnx file all the input/output for those.

.. GENERATED FROM PYTHON SOURCE LINES 63-80

.. code-block:: Python


    with steal_forward(
        [
            ("model", model),
            ("model.decoder", model.decoder),
            ("model.decoder.attention", model.decoder.attention),
            ("model.decoder.feed_forward", model.decoder.feed_forward),
            ("model.decoder.norm_1", model.decoder.norm_1),
            ("model.decoder.norm_2", model.decoder.norm_2),
        ],
        dump_file="plot_dump_intermediate_results.inputs.onnx",
        verbose=1,
        storage_limit=2**28,
    ):
        expected = model(*inputs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    +model -- stolen forward for class LLM -- iteration 0
      <- args=(T7s2x30,) --- kwargs={}
    +model.decoder -- stolen forward for class DecoderLayer -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
    +model.decoder.norm_1 -- stolen forward for class LayerNorm -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.norm_1.
    -- stores key=('model.decoder.norm_1', 0), size 1Kb -- T10s2x30x16
    +model.decoder.attention -- stolen forward for class MultiAttentionBlock -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.attention.
    -- stores key=('model.decoder.attention', 0), size 1Kb -- T10s2x30x16
    +model.decoder.norm_2 -- stolen forward for class LayerNorm -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.norm_2.
    -- stores key=('model.decoder.norm_2', 0), size 1Kb -- T10s2x30x16
    +model.decoder.feed_forward -- stolen forward for class FeedForward -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.feed_forward.
    -- stores key=('model.decoder.feed_forward', 0), size 1Kb -- T10s2x30x16
      -> T10s2x30x16
    -model.decoder.
    -- stores key=('model.decoder', 0), size 1Kb -- T10s2x30x16
      -> T10s2x30x16
    -model.
    -- stores key=('model', 0), size 1Kb -- T10s2x30x16
    -- gather stored 12 objects, size=0 Mb
    -- dumps stored objects
    -- done dump stored objects




.. GENERATED FROM PYTHON SOURCE LINES 81-90

Restores saved inputs/outputs
+++++++++++++++++++++++++++++

All the intermediate tensors were saved in one unique onnx model,
every tensor is stored in a constant node.
The model can be run with any runtime to restore the inputs
and function :func:`create_input_tensors_from_onnx_model
<onnx_diagnostic.helpers.mini_onnx_builder.create_input_tensors_from_onnx_model>`
can restore their names.

.. GENERATED FROM PYTHON SOURCE LINES 90-97

.. code-block:: Python


    saved_tensors = create_input_tensors_from_onnx_model(
        "plot_dump_intermediate_results.inputs.onnx"
    )
    for k, v in saved_tensors.items():
        print(f"{k} -- {string_type(v, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ('model', 0, 'I') -- ((T7s2x30,),{})
    ('model.decoder', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_1', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_1', 0, 'O') -- T10s2x30x16
    ('model.decoder.attention', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.attention', 0, 'O') -- T10s2x30x16
    ('model.decoder.norm_2', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_2', 0, 'O') -- T10s2x30x16
    ('model.decoder.feed_forward', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.feed_forward', 0, 'O') -- T10s2x30x16
    ('model.decoder', 0, 'O') -- T10s2x30x16
    ('model', 0, 'O') -- T10s2x30x16




.. GENERATED FROM PYTHON SOURCE LINES 98-122

Let's explained the naming convention.

::

   ('model.decoder.norm_2', 0, 'I') -- ((T1s2x30x16,),{})
               |            |   |
               |            |   +--> input, the format is args, kwargs
               |            |
               |            +--> iteration, 0 means the first time the execution
               |                 went through that module
               |                 it is possible to call multiple times,
               |                 the model to store more
               |
               +--> the name given to function steal_forward

The same goes for output except ``'I'`` is replaced by ``'O'``.

::

   ('model.decoder.norm_2', 0, 'O') -- T1s2x30x16

This trick can be used to compare intermediate results coming
from pytorch to any other implementation of the same model
as long as it is possible to map the stored inputs/outputs.

.. GENERATED FROM PYTHON SOURCE LINES 124-130

Conversion to ONNX
++++++++++++++++++

The difficult point is to be able to map the saved intermediate
results to intermediate results in ONNX.
Let's create the ONNX model.

.. GENERATED FROM PYTHON SOURCE LINES 130-135

.. code-block:: Python


    epo = torch.onnx.export(model, inputs, dynamic_shapes=ds, dynamo=True)
    epo.optimize()
    epo.save("plot_dump_intermediate_results.onnx")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch.onnx] Obtain model graph for `LLM([...]` with `torch.export.export(..., strict=False)`...
    [torch.onnx] Obtain model graph for `LLM([...]` with `torch.export.export(..., strict=False)`... ❌
    [torch.onnx] Obtain model graph for `LLM([...]` with `torch.export.export(..., strict=True)`...
    [torch.onnx] Obtain model graph for `LLM([...]` with `torch.export.export(..., strict=True)`... ❌
    [torch.onnx] Obtain model graph for `LLM([...]` with `torch.export draft_export`...
    [torch.onnx] Obtain model graph for `LLM([...]` with `torch.export draft_export`... ✅
    [torch.onnx] Run decomposition...
    [torch.onnx] Run decomposition... ✅
    [torch.onnx] Translate the graph into ONNX...
    [torch.onnx] Translate the graph into ONNX... ✅
    Applied 4 of general pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 136-143

Discrepancies
+++++++++++++

We have a torch model, intermediate results and an ONNX graph
equivalent to the torch model.
Let's see how we can check the discrepancies.
First the discrepancies of the whole model.

.. GENERATED FROM PYTHON SOURCE LINES 143-154

.. code-block:: Python


    sess = onnxruntime.InferenceSession(
        "plot_dump_intermediate_results.onnx", providers=["CPUExecutionProvider"]
    )
    feeds = dict(
        zip([i.name for i in sess.get_inputs()], [t.detach().cpu().numpy() for t in inputs])
    )
    got = sess.run(None, feeds)
    diff = max_diff(expected, got)
    print(f"discrepancies torch/ORT: {string_diff(diff)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    discrepancies torch/ORT: abs=0.001953125, rel=0.031203195207189213, n=960.0,amax=0,0,6




.. GENERATED FROM PYTHON SOURCE LINES 155-158

What about intermediate results?
Let's use a runtime still based on :epkg:`onnxruntime`
running an eager evaluation.

.. GENERATED FROM PYTHON SOURCE LINES 158-169

.. code-block:: Python


    sess_eager = OnnxruntimeEvaluator(
        "plot_dump_intermediate_results.onnx",
        providers=["CPUExecutionProvider"],
        torch_or_numpy=True,
    )
    feeds_tensor = dict(zip([i.name for i in sess.get_inputs()], inputs))
    got = sess_eager.run(None, feeds_tensor)
    diff = max_diff(expected, got)
    print(f"discrepancies torch/eager ORT: {string_diff(diff)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    discrepancies torch/eager ORT: abs=0.001953125, rel=0.0511561285041948, n=960.0,amax=0,1,11




.. GENERATED FROM PYTHON SOURCE LINES 170-174

They are almost the same. That's good.
Let's now dig into the intermediate results.
They are compared to the outputs stored in saved_tensors
during the execution of the model.

.. GENERATED FROM PYTHON SOURCE LINES 174-184

.. code-block:: Python

    baseline = {}
    for k, v in saved_tensors.items():
        if k[-1] == "I":  # inputs are excluded
            continue
        if isinstance(v, torch.Tensor):
            baseline[f"{k[0]}.{k[1]}".replace("model.decoder", "decoder")] = v

    report_cmp = ReportResultComparison(baseline)
    sess_eager.run(None, feeds_tensor, report_cmp=report_cmp)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [tensor([[[ 1.1445e+00, -1.5674e-01,  1.4336e+00, -1.8145e+00, -1.4883e+00,
               9.0234e-01, -2.8379e+00, -1.5127e+00, -1.8726e-01, -2.4597e-01,
               5.0732e-01, -8.1543e-01,  5.9082e-02,  2.8857e-01,  3.8379e-01,
              -2.6807e-01],
             [ 8.5938e-02, -3.3887e-01, -1.6162e+00, -1.0605e+00, -9.4434e-01,
              -1.4355e-01,  2.3926e+00,  1.2920e+00, -3.7036e-01,  8.1396e-01,
               3.2959e-02, -2.1621e+00, -1.3037e-01, -3.8574e-02, -3.2793e+00,
              -1.5635e+00],
             [ 2.0625e+00,  2.9688e-01, -1.9863e+00, -6.1523e-02, -1.0752e+00,
              -1.2363e+00, -1.8877e+00, -2.7285e+00, -1.3611e-02, -1.0303e+00,
              -1.5137e+00, -3.3569e-01,  1.7676e+00,  2.9434e+00,  2.2949e-02,
               9.2188e-01],
             [-9.6143e-01, -3.7036e-01, -1.6631e+00, -4.4043e-01,  3.0054e-01,
               2.2070e+00,  1.1475e+00, -5.3516e-01, -2.5146e-01, -1.7041e-01,
              -7.0312e-02, -2.5635e-01,  1.5881e-01,  6.7236e-01,  1.0522e-01,
               1.3330e+00],
             [ 3.3325e-01,  2.8047e+00,  2.4902e+00,  1.6377e+00,  1.4453e-01,
              -6.2793e-01, -1.7080e+00, -1.3232e+00,  2.6538e-01,  2.5020e+00,
              -5.7471e-01, -1.9775e+00, -1.6660e+00,  4.5117e-01, -1.3691e+00,
              -2.7295e-01],
             [ 2.0483e-01, -5.4248e-01,  1.1338e+00,  2.4336e+00, -1.6572e+00,
              -4.2773e-01,  7.8125e-01, -9.6582e-01, -2.3789e+00,  2.7012e+00,
              -2.8242e+00,  1.3877e+00,  1.0312e+00, -1.0342e+00, -1.3555e+00,
               9.0576e-01],
             [-1.4756e+00,  2.3022e-01,  1.6641e+00, -7.5049e-01,  1.3965e+00,
              -2.4688e+00,  5.8496e-01, -8.5254e-01,  7.0020e-01,  2.2969e+00,
              -1.8291e+00, -2.3828e+00,  1.2764e+00,  5.6348e-01, -1.5664e+00,
               5.2832e-01],
             [-1.7798e-01, -2.4336e+00,  7.2949e-01,  1.8193e+00,  1.6924e+00,
               1.1055e+00, -7.9639e-01,  2.4805e+00,  1.1611e+00,  1.2354e-01,
               1.0674e+00,  1.1113e+00, -4.9658e-01,  5.4590e-01, -1.4819e-01,
              -1.0168e-01],
             [-5.2100e-01, -4.1064e-01,  3.5957e+00,  7.4805e-01, -1.6035e+00,
               1.7100e+00, -2.5879e+00,  3.8745e-01,  4.4629e-01, -2.6465e+00,
              -2.5806e-01,  1.0322e+00,  2.9883e-01,  1.1875e+00, -3.2734e+00,
              -8.5645e-01],
             [ 1.8887e+00, -2.5586e-01, -1.9104e-02, -1.8604e+00, -7.8320e-01,
              -1.1191e+00, -7.3682e-01, -2.9785e-01,  3.3008e-01,  8.7012e-01,
               1.3330e-01,  1.2822e+00,  2.0039e+00, -5.5469e-01,  5.2930e-01,
               9.5508e-01],
             [-2.2246e+00,  9.3201e-02, -3.1494e-01,  7.4902e-01, -2.6914e+00,
               3.4629e+00, -7.0020e-01, -1.3086e+00,  1.5698e-01,  3.7246e+00,
               1.7285e+00, -1.6689e+00,  2.6113e+00, -5.7861e-01, -2.0762e+00,
               1.1621e-01],
             [ 5.5176e-01, -2.6582e+00,  1.9092e-01,  1.5527e+00, -1.2842e-01,
              -1.8877e+00, -1.4429e-01, -4.5337e-01, -1.3691e+00,  2.5410e+00,
               2.7793e+00,  8.9111e-03, -6.6797e-01,  1.8125e+00, -6.0400e-01,
              -3.8037e-01],
             [-1.4531e+00,  1.2119e+00, -2.5859e+00,  8.9355e-02,  2.4551e+00,
               1.7461e+00,  9.8145e-02,  1.2432e+00,  1.5662e-01,  1.7715e+00,
              -1.3408e+00,  1.2725e+00,  3.7520e+00,  2.6270e-01,  1.7070e+00,
               6.3086e-01],
             [ 4.4092e-01, -1.2119e+00, -1.6621e+00,  1.1074e+00, -2.0566e+00,
              -3.6133e-01, -1.6846e-01,  9.2676e-01,  1.3496e+00,  5.5566e-01,
               1.5537e+00,  7.1387e-01,  1.1108e-01,  1.0508e+00,  1.0176e+00,
               7.3682e-01],
             [ 8.9746e-01,  2.9453e+00,  3.8047e+00, -2.6133e+00,  4.1260e-01,
              -1.6445e+00,  3.6992e+00,  3.1348e-01,  2.3809e+00, -1.6592e+00,
               3.3325e-02, -1.3330e+00,  5.8594e-02, -8.9307e-01, -1.6289e+00,
               1.7163e-01],
             [ 8.2031e-01,  3.0444e-01,  2.5977e-01, -3.2544e-01, -2.8301e+00,
               2.3457e+00,  1.7939e+00, -2.4922e+00,  2.3711e+00,  6.4795e-01,
               2.1216e-01,  3.0981e-01,  9.3262e-02, -3.8037e-01, -1.1719e+00,
               1.6602e+00],
             [ 3.8574e-01,  2.8760e-01,  2.8271e-01, -4.0674e-01, -7.2461e-01,
              -1.4883e+00,  2.0264e-01, -1.5527e+00,  2.1802e-01,  1.5098e+00,
              -2.9639e-01, -2.9541e-01,  7.8369e-02,  1.7529e+00, -6.0449e-01,
               1.1445e+00],
             [ 1.7109e+00, -1.6299e+00,  2.5195e-01, -1.9180e+00, -9.6973e-01,
               8.3838e-01, -1.5020e+00, -3.3496e+00, -1.7705e+00,  4.2627e-01,
               2.7617e+00,  7.4463e-01, -8.5059e-01,  3.4033e-01, -2.5332e+00,
              -5.2734e-01],
             [-4.0063e-01, -6.4209e-01,  1.2979e+00, -5.7080e-01,  8.5254e-01,
              -7.0435e-02, -1.9766e+00,  1.0781e+00,  9.0869e-01,  1.4746e+00,
              -1.2910e+00, -5.9473e-01, -7.4902e-01, -4.4678e-01, -4.9609e-01,
              -1.0479e+00],
             [ 9.1113e-01, -3.1641e+00, -1.1641e+00, -7.6172e-02,  2.6289e+00,
               2.8691e+00, -2.0469e+00, -9.6680e-01,  2.6758e+00, -7.5928e-01,
              -2.0039e+00,  5.3809e-01,  3.9551e-01,  4.5776e-01,  1.6475e+00,
               1.4590e+00],
             [-2.0840e+00, -1.9971e+00,  2.1133e+00,  2.6953e+00,  1.0469e+00,
              -7.8564e-01, -7.5537e-01,  1.3574e-01, -1.1162e+00, -5.8398e-01,
               2.2937e-01, -5.4004e-01,  2.8418e+00,  2.4648e+00,  6.1426e-01,
              -4.9805e-01],
             [ 4.6753e-01, -1.3203e+00,  2.3223e+00, -2.3418e+00,  1.3105e+00,
              -2.8589e-01, -1.0049e+00,  3.5522e-01, -6.7432e-01, -1.0586e+00,
               6.2158e-01,  7.2363e-01, -1.0000e+00,  2.2656e+00, -1.0137e+00,
              -4.3579e-01],
             [ 1.3613e+00, -9.7266e-01,  4.0234e-01,  1.6484e+00, -5.7539e+00,
              -3.2373e-01,  1.3281e+00,  8.4863e-01,  9.1797e-01,  1.2227e+00,
              -1.6006e-02, -1.0205e+00, -2.1643e-01,  1.2773e+00,  1.0400e+00,
              -8.3154e-01],
             [-5.3857e-01, -1.2021e+00,  1.8984e+00, -8.6621e-01, -2.3086e+00,
               3.9966e-01,  3.8281e-01,  7.0703e-01,  1.7520e+00,  2.4658e-02,
               1.4189e+00, -1.3955e+00, -4.3823e-02,  1.9814e+00, -1.6387e+00,
               1.9932e+00],
             [-1.1455e+00, -3.1562e+00, -4.6338e-01, -1.7578e+00, -1.6211e-01,
              -2.9419e-02,  3.5107e-01,  1.3164e+00,  2.2207e+00,  1.0156e+00,
              -2.1133e+00,  1.4062e+00,  9.4043e-01,  3.1934e-01, -2.4072e-01,
               2.0859e+00],
             [ 1.0693e+00, -5.4321e-03, -1.4131e+00, -6.7969e-01, -7.3975e-02,
              -1.0527e+00,  2.6172e-01, -4.5801e-01, -1.9434e+00,  6.9189e-01,
              -3.3398e-01, -8.8135e-02,  8.1885e-01,  1.7803e+00, -1.5576e+00,
              -2.7490e-01],
             [-2.0508e+00, -1.8965e+00,  2.2278e-01, -2.1660e+00,  2.3359e+00,
              -2.0801e-01, -4.5044e-01,  3.3252e-01,  1.2236e+00, -7.8906e-01,
              -9.2334e-01,  1.3223e+00, -1.6123e+00, -1.1699e+00, -6.8945e-01,
               2.1914e+00],
             [ 1.1279e+00, -4.6826e-01,  1.1829e-01, -2.1606e-01, -1.8125e+00,
               3.9331e-01,  1.0117e+00, -8.6719e-01, -2.2227e+00,  1.2539e+00,
               1.6855e+00,  6.9678e-01,  7.3389e-01,  6.9385e-01,  1.0801e+00,
               2.0957e+00],
             [ 2.3984e+00, -7.8613e-01, -2.9883e-01, -1.5957e+00, -2.1621e+00,
               2.0630e-01,  1.4473e+00,  3.8647e-01,  1.9570e+00,  2.2812e+00,
               3.1660e+00, -1.1758e+00, -3.7305e-01,  3.4707e+00,  2.9004e+00,
              -5.0928e-01],
             [-1.5195e+00, -3.9819e-01,  2.5732e-01, -1.4717e+00, -6.1670e-01,
              -2.0312e+00,  8.6914e-01, -2.3535e+00,  1.2607e+00,  6.1426e-01,
               6.8848e-01,  7.2559e-01,  2.0254e+00, -3.0054e-01, -9.7168e-01,
              -4.2480e-01]],

            [[-1.6504e+00, -7.9834e-01, -1.8188e-01,  3.8940e-01,  8.1934e-01,
               1.2041e+00, -9.6777e-01,  7.3047e-01,  5.8057e-01, -6.8115e-01,
               2.3315e-02,  1.3281e+00,  1.5684e+00, -3.8184e-01, -1.4355e+00,
               2.2246e+00],
             [-1.6973e+00, -2.9629e+00, -5.2521e-02, -1.5674e+00,  1.9348e-01,
              -1.5869e-01, -1.2573e-01,  1.4404e+00,  2.1484e+00,  1.1309e+00,
              -1.9326e+00,  1.7070e+00,  9.3262e-01,  9.3262e-01, -6.6650e-01,
               1.9951e+00],
             [-2.7051e-01,  9.9609e-01,  1.3652e+00,  2.6953e-01,  1.9287e-02,
              -1.7227e+00, -3.9160e-01,  1.5547e+00, -2.4365e-01,  2.6904e-01,
               1.1738e+00, -1.4883e+00,  2.2324e+00,  1.1299e+00, -1.5020e+00,
              -3.4131e-01],
             [ 2.1172e+00, -1.0049e+00,  3.6304e-01, -1.8574e+00, -2.4590e+00,
               2.4570e+00,  5.7910e-01, -1.6045e+00,  9.5410e-01, -3.8354e-01,
               1.2891e+00,  3.6914e-01, -4.0771e-01,  1.7090e-02, -5.6836e-01,
              -1.9551e+00],
             [-1.6094e+00,  3.0420e-01,  1.1348e+00, -4.2212e-01,  3.3691e-01,
               2.8066e+00,  6.1768e-01, -7.1191e-01, -5.7373e-02,  3.0098e+00,
               2.7832e+00,  8.1299e-01, -7.3242e-01,  2.2461e+00, -1.6250e+00,
              -3.5352e-01],
             [ 1.2354e+00,  3.3203e-01, -1.6055e+00,  1.4697e-01, -6.9580e-01,
              -5.8740e-01,  6.4746e-01, -1.4038e-01,  4.7534e-01, -1.1455e+00,
               2.0117e+00,  1.0859e+00, -1.9385e+00,  3.3838e-01,  1.0342e+00,
               2.9492e-01],
             [-1.0732e+00,  2.0957e+00, -1.0654e+00, -8.1787e-01,  2.0752e-01,
               8.3496e-01,  1.4355e+00,  7.1191e-01, -1.1377e+00, -4.1162e-01,
               1.7310e-01, -1.6235e-01,  3.9551e-02,  3.9590e+00,  1.1221e+00,
               3.4521e-01],
             [-1.6885e+00, -1.3721e+00, -2.7832e-01, -1.6406e+00,  1.1572e+00,
              -7.2607e-01, -1.0039e+00, -8.5645e-01, -3.3154e-01, -1.7617e+00,
              -2.6538e-01, -7.4756e-01, -1.1133e-01,  4.1650e-01,  7.7783e-01,
              -9.1260e-01],
             [-4.3408e-01, -1.4343e-01,  1.1816e-01,  2.7173e-01, -1.3135e-01,
               1.1914e-01, -8.0566e-03,  9.5020e-01, -2.1562e+00, -1.7549e+00,
              -1.1807e+00,  3.1958e-01, -4.0088e-01, -1.5811e+00,  3.4512e+00,
               4.7144e-01],
             [ 1.0186e+00,  1.3037e+00, -1.3545e+00,  6.9092e-01, -9.6631e-01,
              -1.2939e+00, -1.2324e+00, -2.9219e+00,  1.8242e+00,  1.3145e+00,
              -2.8066e+00, -4.5020e-01,  3.1592e-01,  1.4512e+00,  4.1406e-01,
              -1.3252e+00],
             [ 6.0840e-01,  6.5625e-01,  8.8184e-01,  1.1230e+00, -1.0723e+00,
               7.6758e-01,  6.4551e-01, -2.3906e+00, -6.9238e-01, -3.6992e+00,
               1.4385e+00,  5.7520e-01, -8.1885e-01,  5.0684e-01, -1.3208e-01,
               1.7529e-01],
             [-1.1436e+00, -5.3369e-01, -1.6670e+00,  4.0967e-01, -2.9810e-01,
               2.0840e+00, -5.5420e-01,  5.9717e-01, -1.8574e+00,  4.9707e-01,
               1.2607e+00,  5.8203e-01,  1.9180e+00,  2.1328e+00,  9.2773e-01,
               1.9805e+00],
             [-1.5430e+00, -1.3945e+00, -1.3203e+00,  1.6592e+00, -2.9321e-01,
              -2.5664e+00, -2.9883e-01,  5.4102e-01,  5.6445e-01,  2.1143e-01,
              -1.6875e+00,  1.1133e+00, -2.3320e+00, -2.3301e+00,  1.7139e+00,
              -1.0225e+00],
             [ 9.4580e-01,  2.1699e+00, -1.8213e+00,  1.3555e+00, -3.6035e+00,
               8.4961e-02,  1.3174e+00,  3.1934e-01,  1.3555e+00,  1.3086e+00,
               5.4395e-01,  5.1807e-01, -6.4160e-01,  2.5254e+00,  6.9238e-01,
               5.6934e-01],
             [-2.8149e-01, -1.7666e+00,  3.5425e-01,  3.7793e-01, -1.8281e+00,
              -8.0957e-01, -2.4121e+00, -5.8350e-02,  6.8311e-01, -2.0195e+00,
              -7.7441e-01, -8.0518e-01,  1.3203e+00,  2.4688e+00,  4.5117e-01,
              -5.3271e-01],
             [-1.4648e-02, -5.8691e-01, -4.4141e-01,  2.1045e-01,  3.5913e-01,
               3.3301e-01,  7.5488e-01, -1.2441e+00,  5.7715e-01, -1.8906e+00,
              -6.7627e-02,  2.6484e+00,  2.3145e+00,  3.3789e-01, -9.6729e-01,
               9.0527e-01],
             [-2.4141e+00,  6.5332e-01,  3.0469e+00,  8.0664e-01,  7.3242e-01,
              -1.9414e+00, -1.8135e+00,  1.2588e+00, -1.2627e+00, -6.2061e-01,
               2.2441e+00,  9.9219e-01, -1.0928e+00, -2.7100e-01,  1.3701e+00,
              -1.4775e+00],
             [-2.8467e-01,  1.0703e+00,  4.4507e-01,  1.5906e-01, -2.5938e+00,
              -1.4062e-01, -3.3984e-01, -1.0332e+00,  4.1138e-01, -1.1279e+00,
              -8.7256e-01, -2.4072e-01,  8.4131e-01, -7.0996e-01, -1.2090e+00,
              -1.3818e+00],
             [-7.8320e-01,  2.4277e+00,  2.0820e+00, -1.7158e+00,  3.6255e-01,
              -5.6641e-01,  2.8184e+00,  1.0117e+00,  2.6211e+00,  4.9976e-01,
              -2.5024e-03, -5.6885e-01,  1.7031e+00,  1.5635e+00, -8.0371e-01,
               8.9551e-01],
             [-4.6362e-01, -9.0234e-01, -1.4512e+00, -5.3613e-01,  5.5481e-02,
               4.2725e-01, -7.2363e-01, -1.6807e+00,  1.4316e+00,  1.8525e+00,
               2.5137e+00,  1.7959e+00,  9.3701e-01,  1.0107e+00,  8.1836e-01,
              -8.6230e-01],
             [-9.1309e-02,  8.4473e-01,  6.6748e-01, -3.7427e-01,  4.8535e-01,
              -1.7217e+00,  1.8030e-01,  1.3945e+00,  2.2773e+00,  2.6699e+00,
               1.5039e+00, -2.4902e+00, -1.4023e+00,  3.6743e-01, -2.0195e+00,
              -8.3398e-01],
             [-7.6611e-01,  1.1895e+00,  5.6445e-01, -2.4863e+00, -2.2246e+00,
               6.2549e-01,  6.7676e-01, -8.7305e-01, -6.3379e-01,  4.4727e-01,
              -2.0176e+00, -1.2344e+00,  2.5146e-02,  6.1182e-01, -2.1738e+00,
              -2.7676e+00],
             [-2.9999e-02,  1.0488e+00, -1.3555e+00,  1.2598e+00,  8.9160e-01,
               6.0242e-02, -2.2070e-01,  1.5850e+00, -5.1855e-01,  8.4375e-01,
               2.4023e+00,  2.4199e+00,  1.0449e+00, -1.2783e+00,  5.1123e-01,
               5.8594e-03],
             [-8.1641e-01,  3.4668e-01, -1.0742e-01, -7.2900e-01, -2.3574e+00,
              -1.3955e+00,  1.2061e+00, -1.7549e+00, -2.5928e-01, -2.3379e+00,
              -5.6250e-01,  2.3418e+00,  2.0625e+00, -9.6973e-01,  1.0615e+00,
               4.1602e-01],
             [-1.4246e-01,  2.7090e+00, -4.5703e-01,  2.5703e+00, -4.3213e-02,
               3.3438e+00,  6.4795e-01,  1.8530e-01,  3.7183e-01,  1.0742e-02,
              -8.0566e-01, -3.1094e+00,  1.4814e+00,  1.5454e-01,  3.5571e-01,
               7.5586e-01],
             [-8.9551e-01,  2.3281e+00,  1.8730e+00,  1.8740e+00, -8.8770e-01,
               4.5459e-01,  1.8652e+00,  1.2646e+00,  1.5352e+00,  1.4673e-01,
              -7.7393e-01, -6.2988e-01,  1.0166e+00,  1.4102e+00, -7.4707e-01,
              -4.8486e-01],
             [-4.6143e-01,  6.5283e-01, -1.5146e+00,  2.3809e+00,  1.8408e-01,
               1.0625e+00, -1.0244e+00, -8.1726e-02, -1.7969e+00,  1.0117e+00,
               2.4844e+00,  8.5010e-01,  3.0781e+00,  9.3652e-01,  2.0195e+00,
              -1.9141e+00],
             [-5.2100e-01,  2.2930e+00,  1.2861e+00,  2.0078e+00, -9.8145e-01,
               2.7578e+00, -1.3584e+00, -1.1836e+00,  1.0635e+00,  4.6826e-01,
              -1.7070e+00, -8.8184e-01, -1.3691e+00, -1.7090e+00, -1.0703e+00,
              -7.8027e-01],
             [-7.0801e-01,  3.3066e+00, -7.6953e-01, -7.7002e-01,  2.6489e-01,
               1.7480e+00, -3.1982e-01, -1.2246e+00,  3.1836e+00,  1.5410e+00,
              -3.1689e-01, -1.8096e+00, -8.9307e-01, -6.3818e-01,  8.3057e-01,
              -5.5957e-01],
             [ 4.6631e-01, -4.9103e-02,  2.5488e-01, -2.0059e+00,  1.6543e+00,
              -1.6719e+00, -2.6875e+00, -4.8389e-01,  4.2031e+00,  1.6719e+00,
              -3.5293e+00,  5.6934e-01, -1.7344e+00, -4.1602e-01,  1.4062e+00,
               2.6914e+00]]], dtype=torch.float16)]



.. GENERATED FROM PYTHON SOURCE LINES 185-186

Let's see the results.

.. GENERATED FROM PYTHON SOURCE LINES 186-192

.. code-block:: Python


    data = report_cmp.data
    df = pandas.DataFrame(data)
    piv = df.pivot(index=("run_index", "run_name"), columns="ref_name", values="abs")
    print(piv)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ref_name                decoder.0  decoder.attention.0  decoder.feed_forward.0  decoder.norm_1.0  decoder.norm_2.0   model.0
    run_index run_name                                                                                                          
    1         embedding      2.996094             3.878662                3.618042          2.970703          2.858398  2.996094
    2         embedding_1    3.785156             2.952820                3.129150          2.944580          2.954346  3.785156
    3         add_8          1.230469             5.614502                5.120605          2.332031          2.371094  1.230469
    4         layer_norm     2.589844             3.282471                2.788574          0.000015          0.655762  2.589844
    5         linear         6.066406             1.751465                2.259766          3.860352          3.801758  6.066406
    6         linear_1       5.943970             2.042969                2.123535          4.269531          4.279297  5.943970
    7         linear_2       6.389648             1.721558                1.742981          3.799805          3.760742  6.389648
    17        matmul_1       5.748718             1.636841                1.725586          3.158875          3.119812  5.748718
    18        linear_3       6.195557             1.748596                2.259766          3.605713          3.566650  6.195557
    19        linear_4       5.208008             2.228027                1.920776          3.596680          3.559570  5.208008
    20        linear_5       5.813385             2.295410                2.010010          3.293945          3.303711  5.813385
    30        matmul_3       5.744553             2.302246                2.001465          3.154709          3.115646  5.744553
    32        val_48         5.956787             0.144043                0.909790          3.366943          3.327881  5.956787
    33        linear_6       5.872375             0.000488                0.826172          3.282532          3.243469  5.872375
    34        add_115        0.752930             5.497314                5.003418          2.214844          2.253906  0.752930
    35        layer_norm_1   2.628906             3.243408                2.826416          0.655762          0.000977  2.628906
    39        val_54         5.409424             0.865234                0.087646          2.819580          2.865234  5.409424
    40        linear_8       5.378174             0.826172                0.000488          2.788330          2.826416  5.378174
    41        add_136        0.001953             5.872314                5.378418          2.589844          2.628906  0.001953




.. GENERATED FROM PYTHON SOURCE LINES 193-194

Let's clean a little bit.

.. GENERATED FROM PYTHON SOURCE LINES 194-197

.. code-block:: Python

    piv[piv >= 1] = np.nan
    print(piv.dropna(axis=0, how="all"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ref_name                decoder.0  decoder.attention.0  decoder.feed_forward.0  decoder.norm_1.0  decoder.norm_2.0   model.0
    run_index run_name                                                                                                          
    4         layer_norm          NaN                  NaN                     NaN          0.000015          0.655762       NaN
    32        val_48              NaN             0.144043                0.909790               NaN               NaN       NaN
    33        linear_6            NaN             0.000488                0.826172               NaN               NaN       NaN
    34        add_115        0.752930                  NaN                     NaN               NaN               NaN  0.752930
    35        layer_norm_1        NaN                  NaN                     NaN          0.655762          0.000977       NaN
    39        val_54              NaN             0.865234                0.087646               NaN               NaN       NaN
    40        linear_8            NaN             0.826172                0.000488               NaN               NaN       NaN
    41        add_136        0.001953                  NaN                     NaN               NaN               NaN  0.001953




.. GENERATED FROM PYTHON SOURCE LINES 198-199

We can identity which results is mapped to which expected tensor.

.. GENERATED FROM PYTHON SOURCE LINES 201-203

Picture of the model
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 203-207

.. code-block:: Python


    onx = onnx.load("plot_dump_intermediate_results.onnx")
    plot_dot(onx)




.. image-sg:: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_001.png
   :alt: plot dump intermediate results
   :srcset: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 208-209

.. code-block:: Python

    doc.plot_legend("steal and dump\nintermediate\nresults", "steal_forward", "blue")



.. image-sg:: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_002.png
   :alt: plot dump intermediate results
   :srcset: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_002.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 18.734 seconds)


.. _sphx_glr_download_auto_examples_plot_dump_intermediate_results.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_dump_intermediate_results.ipynb <plot_dump_intermediate_results.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_dump_intermediate_results.py <plot_dump_intermediate_results.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_dump_intermediate_results.zip <plot_dump_intermediate_results.zip>`


.. include:: plot_dump_intermediate_results.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
