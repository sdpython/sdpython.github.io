
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_tiny_phi2.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_tiny_phi2.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_tiny_phi2.py:


.. _l-plot-export_tiny_phi2:

======================
Export microsoft/phi-2
======================

This function exports an smaller untrained model with the same architecture.
It is faster than the pretrained model.
When this works, the untrained model can be replaced by the trained one.

:epkg:`microsoft/phi-2` is not a big model but still quite big
when it comes to write unittests. Function
:func:`onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs`
can be used to create a reduced untrained version of a model coming from
:epkg:`HuggingFace`. It downloads the configuration from the website
but creates a dummy model with 1 or 2 hidden layers in order to reduce
the size and get a fast execution. The goal is usually to test
the export or to compare performance. The relevance does not matter.

Create the dummy model
======================

.. GENERATED FROM PYTHON SOURCE LINES 24-54

.. code-block:: Python


    import copy
    import pprint
    import warnings
    import torch
    import onnxruntime
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers import max_diff, string_diff, string_type
    from onnx_diagnostic.helpers.cache_helper import is_cache_dynamic_registered
    from onnx_diagnostic.helpers.rt_helper import make_feeds
    from onnx_diagnostic.torch_export_patches import torch_export_patches
    from onnx_diagnostic.torch_export_patches.patch_inputs import use_dyn_not_str
    from onnx_diagnostic.torch_models.hghub import (
        get_untrained_model_with_inputs,
    )

    warnings.simplefilter("ignore")

    # another tiny id: arnir0/Tiny-LLM
    data = get_untrained_model_with_inputs("microsoft/phi-2")
    untrained_model, inputs, dynamic_shapes, config, size, n_weights = (
        data["model"],
        data["inputs"],
        data["dynamic_shapes"],
        data["configuration"],
        data["size"],
        data["n_weights"],
    )

    print(f"model {size / 2**20:1.1f} Mb with {n_weights // 1000} thousands of parameters.")




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    model 432.3 Mb with 113332 thousands of parameters.




.. GENERATED FROM PYTHON SOURCE LINES 55-59

The original model has 2.7 billion parameters. It was divided by more than 10.
However, it can still be used with
``get_untrained_model_with_inputs("microsoft/phi-2", same_as_pretrained=True)``.
Let's see the configuration.

.. GENERATED FROM PYTHON SOURCE LINES 59-62

.. code-block:: Python

    print(config)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    PhiConfig {
      "_attn_implementation_autoset": true,
      "architectures": [
        "PhiForCausalLM"
      ],
      "attention_dropout": 0.0,
      "bos_token_id": 50256,
      "embd_pdrop": 0.0,
      "eos_token_id": 50256,
      "head_dim": 80,
      "hidden_act": "gelu_new",
      "hidden_size": 768,
      "initializer_range": 0.02,
      "intermediate_size": 6144,
      "layer_norm_eps": 1e-05,
      "max_position_embeddings": 2048,
      "model_type": "phi",
      "num_attention_heads": 32,
      "num_hidden_layers": 2,
      "num_key_value_heads": 32,
      "partial_rotary_factor": 0.4,
      "qk_layernorm": false,
      "resid_pdrop": 0.1,
      "rope_scaling": null,
      "rope_theta": 10000.0,
      "subfolder": null,
      "tie_word_embeddings": false,
      "torch_dtype": "float16",
      "transformers_version": "4.52.0.dev0",
      "use_cache": true,
      "vocab_size": 51200
    }





.. GENERATED FROM PYTHON SOURCE LINES 63-64

Inputs:

.. GENERATED FROM PYTHON SOURCE LINES 64-67

.. code-block:: Python


    print(string_type(inputs, with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x80,T1s2x32x30x80], value_cache=#2[T1s2x32x30x80,T1s2x32x30x80]))




.. GENERATED FROM PYTHON SOURCE LINES 68-69

With min/max values.

.. GENERATED FROM PYTHON SOURCE LINES 69-71

.. code-block:: Python

    print(string_type(inputs, with_shape=True, with_min_max=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dict(input_ids:T7s2x3[5868,42369:A28806.5],attention_mask:T7s2x33[1,1:A1.0],position_ids:T7s2x3[30,32:A31.0],past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x80[-4.339683532714844,4.168929100036621:A-0.00015688629796453135],T1s2x32x30x80[-4.691745758056641,4.495841979980469:A-0.0017857432339830719]], value_cache=#2[T1s2x32x30x80[-4.3160271644592285,4.298698902130127:A-0.0037800918842078777],T1s2x32x30x80[-4.749629974365234,5.0046281814575195:A0.00033762750999332054]]))




.. GENERATED FROM PYTHON SOURCE LINES 72-73

And the dynamic shapes

.. GENERATED FROM PYTHON SOURCE LINES 73-75

.. code-block:: Python

    pprint.pprint(dynamic_shapes)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'attention_mask': {0: Dim('batch', min=1, max=1024), 1: 'cache+seq'},
     'input_ids': {0: Dim('batch', min=1, max=1024), 1: 'seq_length'},
     'past_key_values': [[{0: Dim('batch', min=1, max=1024), 2: 'cache_length'},
                          {0: Dim('batch', min=1, max=1024), 2: 'cache_length'}],
                         [{0: Dim('batch', min=1, max=1024), 2: 'cache_length'},
                          {0: Dim('batch', min=1, max=1024), 2: 'cache_length'}]],
     'position_ids': {0: Dim('batch', min=1, max=1024), 1: 'cache+seq'}}




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We execute the model to produce expected outputs.

.. GENERATED FROM PYTHON SOURCE LINES 77-81

.. code-block:: Python

    expected = untrained_model(**copy.deepcopy(inputs))
    print(f"expected: {string_type(expected, with_shape=True, with_min_max=True)}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    expected: CausalLMOutputWithPast(logits:T1s2x3x51200[-2.425957202911377,2.6620113849639893:A-0.00048338043225195786],past_key_values:DynamicCache(key_cache=#2[T1s2x32x33x80[-4.339683532714844,4.168929100036621:A-0.0001798408414748936],T1s2x32x33x80[-4.691745758056641,4.495841979980469:A-0.001198434475900593]], value_cache=#2[T1s2x32x33x80[-4.3160271644592285,4.298698902130127:A-0.0041093787574674355],T1s2x32x33x80[-4.749629974365234,5.0046281814575195:A-0.0005269664030454032]]))




.. GENERATED FROM PYTHON SOURCE LINES 82-89

Export to fx.Graph
==================

:func:`torch.export.export` is the first step before converting
a model into ONNX. The inputs are duplicated (with ``copy.deepcopy``)
because the model may modify them inline (a cache for example).
Shapes may not match on the second call with the modified inputs.

.. GENERATED FROM PYTHON SOURCE LINES 89-117

.. code-block:: Python



    with torch_export_patches(patch_transformers=True):

        # Two unnecessary steps but useful in case of an error
        # We check the cache is registered.
        assert is_cache_dynamic_registered()

        # We check there is no discrepancies when the cache is applied.
        d = max_diff(expected, untrained_model(**copy.deepcopy(inputs)))
        assert (
            d["abs"] < 1e-5
        ), f"The model with patches produces different outputs: {string_diff(d)}"

        # Then we export: the only import line in this section.
        ep = torch.export.export(
            untrained_model,
            (),
            kwargs=copy.deepcopy(inputs),
            dynamic_shapes=use_dyn_not_str(dynamic_shapes),
            strict=False,  # mandatory for torch==2.6
        )

        # We check the exported program produces the same results as well.
        # This step is again unnecessary.
        d = max_diff(expected, ep.module()(**copy.deepcopy(inputs)))
        assert d["abs"] < 1e-5, f"The exported model different outputs: {string_diff(d)}"








.. GENERATED FROM PYTHON SOURCE LINES 118-126

Export to ONNX
==============

The export works. We can export to ONNX now
:func:`torch.onnx.export`.
Patches are still needed because the export
applies :meth:`torch.export.ExportedProgram.run_decompositions`
may export local pieces of the model again.

.. GENERATED FROM PYTHON SOURCE LINES 126-132

.. code-block:: Python


    with torch_export_patches(patch_transformers=True):
        epo = torch.onnx.export(
            ep, (), kwargs=copy.deepcopy(inputs), dynamic_shapes=dynamic_shapes, dynamo=True
        )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch.onnx] Run decomposition...
    [torch.onnx] Run decomposition... ✅
    [torch.onnx] Translate the graph into ONNX...
    [torch.onnx] Translate the graph into ONNX... ✅
    Applied 53 of general pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 133-134

We can save it.

.. GENERATED FROM PYTHON SOURCE LINES 134-140

.. code-block:: Python

    epo.save("plot_export_tiny_phi2.onnx", external_data=True)

    # Or directly get the :class:`onnx.ModelProto`.
    onx = epo.model_proto









.. GENERATED FROM PYTHON SOURCE LINES 141-147

Discrepancies
+++++++++++++

The we check the conversion to ONNX.
Let's make sure the ONNX model produces the same outputs.
It takes flatten inputs.

.. GENERATED FROM PYTHON SOURCE LINES 147-153

.. code-block:: Python


    feeds = make_feeds(onx, copy.deepcopy(inputs), use_numpy=True, copy=True)

    print(f"torch inputs: {string_type(inputs)}")
    print(f"onxrt inputs: {string_type(feeds)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch inputs: dict(input_ids:T7r2,attention_mask:T7r2,position_ids:T7r2,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    onxrt inputs: dict(input_ids:A7r2,attention_mask:A7r2,position_ids:A7r2,past_key_values_key_cache_0:A1r4,past_key_values_key_cache_1:A1r4,past_key_values_value_cache_0:A1r4,past_key_values_value_cache_1:A1r4)




.. GENERATED FROM PYTHON SOURCE LINES 154-155

We then create a :class:`onnxruntime.InferenceSession`.

.. GENERATED FROM PYTHON SOURCE LINES 155-160

.. code-block:: Python


    sess = onnxruntime.InferenceSession(
        onx.SerializeToString(), providers=["CPUExecutionProvider"]
    )








.. GENERATED FROM PYTHON SOURCE LINES 161-162

Let's run.

.. GENERATED FROM PYTHON SOURCE LINES 162-164

.. code-block:: Python

    got = sess.run(None, feeds)








.. GENERATED FROM PYTHON SOURCE LINES 165-166

And finally the discrepancies.

.. GENERATED FROM PYTHON SOURCE LINES 166-170

.. code-block:: Python


    diff = max_diff(expected, got, flatten=True)
    print(f"onnx discrepancies: {string_diff(diff)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    onnx discrepancies: abs=2.205371856689453e-06, rel=0.0012052791646639817, n=983040.0




.. GENERATED FROM PYTHON SOURCE LINES 171-172

It looks good.

.. GENERATED FROM PYTHON SOURCE LINES 174-176

.. code-block:: Python

    doc.plot_legend("export\nuntrained smaller\nmicrosoft/phi-2", "torch.onnx.export", "orange")




.. image-sg:: /auto_examples/images/sphx_glr_plot_export_tiny_phi2_001.png
   :alt: plot export tiny phi2
   :srcset: /auto_examples/images/sphx_glr_plot_export_tiny_phi2_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 177-223

Possible Issues
===============

Unknown task
++++++++++++

Function :func:`onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs`
is unabl to guess a task associated to the model.
A different set of dummy inputs is defined for every task.
The user needs to explicitly give that information to the function.
Tasks are the same as the one defined by
`HuggingFace/models <https://huggingface.co/models>`_.

Inputs are incorrect
++++++++++++++++++++

Example :ref:`l-plot-tiny-llm-export` explains
how to retrieve that information. If you cannot guess the dynamic
shapes - a cache can be tricky sometimes, follow example
:ref:`l-plot-export-with-args-kwargs`.

DynamicCache or any other cache cannot be exported
++++++++++++++++++++++++++++++++++++++++++++++++++

That's the role of :func:`onnx_diagnostic.torch_export_patches.torch_export_patches`.
It registers the necessary information into pytorch to make the export
work with these. Its need should slowly disappear until :epkg:`transformers`
includes the serialization functions.

Control Flow
++++++++++++

Every mixture of models goes through a control flow (a test).
It also happens when a cache is truncated. The code of the model
needs to be changed. See example :ref:`l-plot-export-cond`.
Loops are not supported yet.

Issue with dynamic shapes
+++++++++++++++++++++++++

Example :ref:`l-plot-dynamic-shapes-python-int` gives one reason
this process may fail but that's not the only one.
Example :ref:`l-plot-export-locale-issue` gives an way to locate
the cause but that does not cover all the possible causes.
Raising an issue on github would be the recommended option
until it is fixed.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 23.760 seconds)


.. _sphx_glr_download_auto_examples_plot_export_tiny_phi2.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_tiny_phi2.ipynb <plot_export_tiny_phi2.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_tiny_phi2.py <plot_export_tiny_phi2.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_tiny_phi2.zip <plot_export_tiny_phi2.zip>`


.. include:: plot_export_tiny_phi2.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
