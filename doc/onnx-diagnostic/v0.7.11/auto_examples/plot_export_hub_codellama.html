<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Common Export Issues" href="../auto_recipes/index.html" /><link rel="prev" title="Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)" href="plot_export_tiny_llm.html" />
        <link rel="prefetch" href="../_static/logo.png" as="image" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2025.07.19 -->
        <title>Test the export on untrained models - onnx-diagnostic 0.7.11 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=25af2a20" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a7360d90" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">onnx-diagnostic 0.7.11 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">onnx-diagnostic 0.7.11 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../patches.html">Patches Explained</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Patches Explained</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../status/index.html">Exporter Status</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Exporter Status</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../status/exported_program_dynamic.html">Exported Programs with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/exporter_dynamic.html">Exported ONNX with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/patches_coverage.html">Coverage of the Patches</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API of onnx_diagnostic</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of API of onnx_diagnostic</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/export/index.html">onnx_diagnostic.export</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.export</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/export/dynamic_shapes.html">onnx_diagnostic.export.dynamic_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/shape_helper.html">onnx_diagnostic.export.shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/validate.html">onnx_diagnostic.export.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/helpers/index.html">onnx_diagnostic.helpers</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.helpers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/args_helper.html">onnx_diagnostic.helpers.args_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/bench_run.html">onnx_diagnostic.helpers.bench_run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/cache_helper.html">onnx_diagnostic.helpers.cache_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/config_helper.html">onnx_diagnostic.helpers.config_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/doc_helper.html">onnx_diagnostic.helpers.doc_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/graph_helper.html">onnx_diagnostic.helpers.graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/helper.html">onnx_diagnostic.helpers.helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/_log_helper.html">onnx_diagnostic.helpers._log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/log_helper.html">onnx_diagnostic.helpers.log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/memory_peak.html">onnx_diagnostic.helpers.memory_peak</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/mini_onnx_builder.html">onnx_diagnostic.helpers.mini_onnx_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/model_builder_helper.html">onnx_diagnostic.helpers.model_builder_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/onnx_helper.html">onnx_diagnostic.helpers.onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/ort_session.html">onnx_diagnostic.helpers.ort_session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/rt_helper.html">onnx_diagnostic.helpers.rt_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_helper.html">onnx_diagnostic.helpers.torch_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">onnx_diagnostic.reference</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">onnx_diagnostic.reference.ops</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">onnx_diagnostic.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">onnx_diagnostic.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">onnx_diagnostic.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">onnx_diagnostic.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">onnx_diagnostic.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">onnx_diagnostic.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">onnx_diagnostic.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">onnx_diagnostic.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">onnx_diagnostic.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">onnx_diagnostic.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">onnx_diagnostic.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">onnx_diagnostic.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">onnx_diagnostic.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">onnx_diagnostic.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">onnx_diagnostic.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">onnx_diagnostic.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">onnx_diagnostic.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">onnx_diagnostic.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">onnx_diagnostic.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">onnx_diagnostic.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">onnx_diagnostic.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">onnx_diagnostic.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">onnx_diagnostic.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/torch_ops/index.html">onnx_diagnostic.reference.torch_ops</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference.torch_ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/access_ops.html">onnx_diagnostic.reference.torch_ops.access_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/binary_ops.html">onnx_diagnostic.reference.torch_ops.binary_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/controlflow_ops.html">onnx_diagnostic.reference.torch_ops.controlflow_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/generator_ops.html">onnx_diagnostic.reference.torch_ops.generator_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/nn_ops.html">onnx_diagnostic.reference.torch_ops.nn_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/other_ops.html">onnx_diagnostic.reference.torch_ops.other_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/reduce_ops.html">onnx_diagnostic.reference.torch_ops.reduce_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/sequence_ops.html">onnx_diagnostic.reference.torch_ops.sequence_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/shape_ops.html">onnx_diagnostic.reference.torch_ops.shape_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/unary_ops.html">onnx_diagnostic.reference.torch_ops.unary_ops</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">onnx_diagnostic.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">onnx_diagnostic.reference.quantized_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">onnx_diagnostic.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/report_results_comparison.html">onnx_diagnostic.reference.report_results_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/torch_evaluator.html">onnx_diagnostic.reference.torch_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/tasks/index.html">onnx_diagnostic.tasks</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.tasks</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/automatic_speech_recognition.html">onnx_diagnostic.tasks.automatic_speech_recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/fill_mask.html">onnx_diagnostic.tasks.fill_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/feature_extraction.html">onnx_diagnostic.tasks.feature_extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_classification.html">onnx_diagnostic.tasks.image_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_text_to_text.html">onnx_diagnostic.export.image_text_to_text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/mixture_of_expert.html">onnx_diagnostic.tasks.mixture_of_expert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/object_detection.html">onnx_diagnostic.tasks.object_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/sentence_similarity.html">onnx_diagnostic.tasks.sentence_similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/summarization.html">onnx_diagnostic.tasks.summarization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_classification.html">onnx_diagnostic.tasks.text_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_generation.html">onnx_diagnostic.tasks.text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_to_image.html">onnx_diagnostic.tasks.text_to_image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text2text_generation.html">onnx_diagnostic.tasks.text2text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/zero_shot_image_classification.html">onnx_diagnostic.tasks.zero_shot_image_classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_export_patches/index.html">onnx_diagnostic.torch_export_patches</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/eval/index.html">onnx_diagnostic.torch_export_patches.eval</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches.eval</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/eval/model_cases.html">onnx_diagnostic.torch_export_patches.eval.model_cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_errors.html">onnx_diagnostic.torch_export_patches.onnx_export_errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_serialization.html">onnx_diagnostic.torch_export_patches.onnx_export_serialization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/patches/index.html">onnx_diagnostic.torch_export_patches.patches</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches.patches</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_torch.html">onnx_diagnostic.torch_export_patches.patches.patch_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_transformers.html">onnx_diagnostic.torch_export_patches.patches.patch_transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_expressions.html">onnx_diagnostic.torch_export_patches.patch_expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_inputs.html">onnx_diagnostic.torch_export_patches.patch_inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module.html">onnx_diagnostic.torch_export_patches.patch_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module_helper.html">onnx_diagnostic.torch_export_patches.patch_module_helper</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/serialization/index.html">onnx_diagnostic.torch_export_patches.serialization</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches.serialization</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/diffusers_impl.html">onnx_diagnostic.torch_export_patches.serialization.diffusers_impl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/transformers_impl.html">onnx_diagnostic.torch_export_patches.serialization.transformers_impl</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">onnx_diagnostic.torch_models</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_models</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_models/hghub/index.html">onnx_diagnostic.torch_models.hghub</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_models.hghub</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_api.html">onnx_diagnostic.torch_models.hghub.hub_api</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_data.html">onnx_diagnostic.torch_models.hghub.hub_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/model_inputs.html">onnx_diagnostic.torch_models.hghub.model_inputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llms.html">onnx_diagnostic.torch_models.llms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/validate.html">onnx_diagnostic.torch_models.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_onnx/index.html">onnx_diagnostic.torch_onnx</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_onnx</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/runtime_info.html">onnx_diagnostic.torch_onnx.runtime_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs.html">onnx_diagnostic.torch_onnx.sbs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/api.html">onnx_diagnostic.api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">onnx_diagnostic.ext_test_case</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cmds/index.html">Command Lines</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of Command Lines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cmds/config.html">-m onnx_diagnostic config … prints the config for a model id</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/validate.html">-m onnx_diagnostic validate … validate a model id</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Examples Gallery</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of Examples Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_dump_intermediate_results.html">Dumps intermediate results of a torch model</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_args_kwargs.html">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_patched.html">Export Tiny-LLM with patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_phi2.html">Export microsoft/phi-2</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_dynamic_cache.html">Export with DynamicCache and guessed dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_locate_issue.html">Find and fix an export issue due to dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_model_extract.html">Find where a model is failing by running submodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_reference_evaluator.html">Intermediate results with (ONNX) ReferenceEvaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_onnxruntime_evaluator.html">Intermediate results with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm.html">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Test the export on untrained models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_recipes/index.html">Common Export Issues</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle navigation of Common Export Issues</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_dim1.html">0, 1, 2 for a Dynamic Dimension in the dummy example to export a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_what.html">Builds dynamic shapes from any input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_max.html">Cannot export <code class="docutils literal notranslate"><span class="pre">torch.sym_max(x.shape[0],</span> <span class="pre">y.shape[0])</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_python_int.html">Do not use python int with dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_cond.html">Export a model with a control flow (If)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_nonzero.html">Half certain nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_json.html">JSON returns list when the original dynamic shapes are list or tuple</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_with_dynamic.html">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_technical/index.html">Technical Details</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle navigation of Technical Details</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_layer_norm_discrepancies.html">LayerNormalization implementation cannot be exchanged</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_parallelized_reduction.html">Reproducible Parallelized Reduction is difficult</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_examples/plot_export_hub_codellama.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-export-hub-codellama-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="test-the-export-on-untrained-models">
<span id="l-plot-export-hub-codellama"></span><span id="sphx-glr-auto-examples-plot-export-hub-codellama-py"></span><h1>Test the export on untrained models<a class="headerlink" href="#test-the-export-on-untrained-models" title="Link to this heading">¶</a></h1>
<p>Checking the exporter on a whole model takes time as it is
usually big but we can create a smaller version with
the same architecture. Then fix export issues on such a
small model is faster.</p>
<section id="codellama-codellama-7b-python-hf">
<h2>codellama/CodeLlama-7b-Python-hf<a class="headerlink" href="#codellama-codellama-7b-python-hf" title="Link to this heading">¶</a></h2>
<p>Let’s grab some information about this model.
This reuses <a class="reference external" href="https://github.com/huggingface/huggingface_hub">huggingface_hub</a> API.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic</span> <span class="kn">import</span> <span class="n">doc</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.ext_test_case</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/ext_test_case.html#onnx_diagnostic.ext_test_case.unit_test_going" title="onnx_diagnostic.ext_test_case.unit_test_going" class="sphx-glr-backref-module-onnx_diagnostic-ext_test_case sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/ext_test_case.html#onnx_diagnostic.ext_test_case.unit_test_going" title="onnx_diagnostic.ext_test_case.unit_test_going" class="sphx-glr-backref-module-onnx_diagnostic-ext_test_case sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/ext_test_case.html#onnx_diagnostic.ext_test_case.unit_test_going" title="onnx_diagnostic.ext_test_case.unit_test_going" class="sphx-glr-backref-module-onnx_diagnostic-ext_test_case sphx-glr-backref-type-py-function"><span class="n">unit_test_going</span></a></a></a>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.helpers</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_models.hghub</span> <span class="kn">import</span> <span class="p">(</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><span class="n">get_untrained_model_with_inputs</span></a></a></a><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_models.hghub.hub_api</span> <span class="kn">import</span> <span class="p">(</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_model_info</span></a></a></a><span class="p">,</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_pretrained_config</span></a></a></a><span class="p">,</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">task_from_id</span></a></a></a><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_export_patches</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a></a>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_export_patches.patch_inputs</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a>

<a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a></a> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;HuggingFaceM4/tiny-random-idefics&quot;</span>
    <span class="k">if</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/ext_test_case.html#onnx_diagnostic.ext_test_case.unit_test_going" title="onnx_diagnostic.ext_test_case.unit_test_going" class="sphx-glr-backref-module-onnx_diagnostic-ext_test_case sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/ext_test_case.html#onnx_diagnostic.ext_test_case.unit_test_going" title="onnx_diagnostic.ext_test_case.unit_test_going" class="sphx-glr-backref-module-onnx_diagnostic-ext_test_case sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/ext_test_case.html#onnx_diagnostic.ext_test_case.unit_test_going" title="onnx_diagnostic.ext_test_case.unit_test_going" class="sphx-glr-backref-module-onnx_diagnostic-ext_test_case sphx-glr-backref-type-py-function"><span class="n">unit_test_going</span></a></a></a><span class="p">()</span>
    <span class="k">else</span> <span class="s2">&quot;codellama/CodeLlama-7b-Python-hf&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_id=</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a></a><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;info&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_model_info</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>model_id=&#39;codellama/CodeLlama-7b-Python-hf&#39;
info ModelInfo(id=&#39;codellama/CodeLlama-7b-Python-hf&#39;, author=&#39;codellama&#39;, sha=&#39;d4178f5d2eead875e627ec487b23679266319b7f&#39;, created_at=datetime.datetime(2023, 8, 24, 16, 31, 28, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 4, 12, 14, 16, 26, tzinfo=datetime.timezone.utc), private=False, disabled=False, downloads=5472, downloads_all_time=None, gated=False, gguf=None, inference=None, inference_provider_mapping=None, likes=143, library_name=&#39;transformers&#39;, tags=[&#39;transformers&#39;, &#39;pytorch&#39;, &#39;safetensors&#39;, &#39;llama&#39;, &#39;text-generation&#39;, &#39;llama-2&#39;, &#39;code&#39;, &#39;arxiv:2308.12950&#39;, &#39;license:llama2&#39;, &#39;autotrain_compatible&#39;, &#39;text-generation-inference&#39;, &#39;endpoints_compatible&#39;, &#39;region:us&#39;], pipeline_tag=&#39;text-generation&#39;, mask_token=None, card_data={&#39;base_model&#39;: None, &#39;datasets&#39;: None, &#39;eval_results&#39;: None, &#39;language&#39;: [&#39;code&#39;], &#39;library_name&#39;: None, &#39;license&#39;: &#39;llama2&#39;, &#39;license_name&#39;: None, &#39;license_link&#39;: None, &#39;metrics&#39;: None, &#39;model_name&#39;: None, &#39;pipeline_tag&#39;: &#39;text-generation&#39;, &#39;tags&#39;: [&#39;llama-2&#39;]}, widget_data=None, model_index=None, config={&#39;architectures&#39;: [&#39;LlamaForCausalLM&#39;], &#39;model_type&#39;: &#39;llama&#39;, &#39;tokenizer_config&#39;: {&#39;bos_token&#39;: {&#39;__type&#39;: &#39;AddedToken&#39;, &#39;content&#39;: &#39;&lt;s&gt;&#39;, &#39;lstrip&#39;: False, &#39;normalized&#39;: True, &#39;rstrip&#39;: False, &#39;single_word&#39;: False}, &#39;eos_token&#39;: {&#39;__type&#39;: &#39;AddedToken&#39;, &#39;content&#39;: &#39;&lt;/s&gt;&#39;, &#39;lstrip&#39;: False, &#39;normalized&#39;: True, &#39;rstrip&#39;: False, &#39;single_word&#39;: False}, &#39;pad_token&#39;: None, &#39;unk_token&#39;: {&#39;__type&#39;: &#39;AddedToken&#39;, &#39;content&#39;: &#39;&lt;unk&gt;&#39;, &#39;lstrip&#39;: False, &#39;normalized&#39;: True, &#39;rstrip&#39;: False, &#39;single_word&#39;: False}}}, transformers_info=TransformersInfo(auto_model=&#39;AutoModelForCausalLM&#39;, custom_class=None, pipeline_tag=&#39;text-generation&#39;, processor=&#39;AutoTokenizer&#39;), trending_score=None, siblings=[RepoSibling(rfilename=&#39;.gitattributes&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;LICENSE&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;README.md&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;USE_POLICY.md&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;config.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;generation_config.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;model-00001-of-00002.safetensors&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;model-00002-of-00002.safetensors&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;model.safetensors.index.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model-00001-of-00003.bin&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model-00002-of-00003.bin&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model-00003-of-00003.bin&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model.bin.index.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;special_tokens_map.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;tokenizer.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;tokenizer.model&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;tokenizer_config.json&#39;, size=None, blob_id=None, lfs=None)], spaces=[&#39;bigcode/bigcode-models-leaderboard&#39;, &#39;Intel/low_bit_open_llm_leaderboard&#39;, &#39;BAAI/open_cn_llm_leaderboard&#39;, &#39;qiantong-xu/toolbench-leaderboard&#39;, &#39;gsaivinay/open_llm_leaderboard&#39;, &#39;EvanTHU/MotionLLM&#39;, &#39;GTBench/GTBench&#39;, &#39;Vikhrmodels/small-shlepa-lb&#39;, &#39;kz-transformers/kaz-llm-lb&#39;, &#39;Vikhrmodels/DOoM-lb&#39;, &#39;21world/bigcode-models-leaderboard&#39;, &#39;felixz/open_llm_leaderboard&#39;, &#39;BAAI/open_flageval_vlm_leaderboard&#39;, &#39;HemaAM/GPT_train_on_LLaMa&#39;, &#39;OPTML-Group/UnlearnCanvas-Benchmark&#39;, &#39;atlasas/bigcode-models-leaderboard&#39;, &#39;whackthejacker/CodeTuneStudio&#39;, &#39;BAAI/EmbodiedVerse&#39;, &#39;anantgupta129/LitGPT-Pythia-160M&#39;, &#39;neubla/neubla-llm-evaluation-board&#39;, &#39;PrarthanaTS/tsai-gpt-from-scratch&#39;, &#39;MadhurGarg/TSAIGPTRedPajama&#39;, &#39;RaviNaik/ERA-SESSION22&#39;, &#39;theangkko/codellama-CodeLlama-7b-Python-hf&#39;, &#39;rodrigomasini/data_only_open_llm_leaderboard&#39;, &#39;Docfile/open_llm_leaderboard&#39;, &#39;Sijuade/GPTNEXTWORD&#39;, &#39;VDebugger/VDebugger-generalist-for-VQA&#39;, &#39;Temuzin64/code_helper&#39;, &#39;Agents-MCP-Hackathon/universal-api-translator&#39;, &#39;Noveumai/NovaEval&#39;, &#39;piyushgrover/MiniGPT_S22&#39;, &#39;supra-e-acc/Pythia-160M-text-generate&#39;, &#39;venkyyuvy/GPT_redpajama&#39;, &#39;mkthoma/GPT_From_Scratch&#39;, &#39;VarunSivamani/GPT-From-Scratch&#39;, &#39;sanjanatule/GPTNext&#39;, &#39;RashiAgarwal/TSAIGPTRedPajama&#39;, &#39;neuralorbs/DialogGen&#39;, &#39;Navyabhat/ERAV1-Session-22&#39;, &#39;GunaKoppula/ERA-Session-22&#39;, &#39;Vaish2705/ERA_S22&#39;, &#39;smothiki/open_llm_leaderboard&#39;, &#39;aemonge/codellama-CodeLlama-7b-Python-hf&#39;, &#39;sid92/codellama-CodeLlama-7b-Python-hf&#39;, &#39;poubellearman/codellama-CodeLlama-7b-Python-hf&#39;, &#39;IPF/codellama-CodeLlama-7b-Python-hf&#39;, &#39;Chris4K/codellama-CodeLlama-7b-Python-hf&#39;, &#39;CuriosityPdf/codellama-CodeLlama-7b-Python-hf&#39;, &#39;shreefhamed/codellama-CodeLlama-7b-Python-hf&#39;, &#39;markl11/codellama-CodeLlama-7b-Python-hf&#39;, &#39;0x1668/open_llm_leaderboard&#39;, &#39;rpratl/codellama-CodeLlama-7b-Python-hf&#39;, &#39;pngwn/open_llm_leaderboard-check&#39;, &#39;asir0z/open_llm_leaderboard&#39;, &#39;LovelySweet/codellama-CodeLlama-7b-Python-hf&#39;, &#39;kbmlcoding/open_llm_leaderboard_free&#39;, &#39;ToletiSri/TSAI_S22&#39;, &#39;aichampions/open_llm_leaderboard&#39;, &#39;Adeco/open_llm_leaderboard&#39;, &#39;anirudh937/open_llm_leaderboard&#39;, &#39;smothiki/open_llm_leaderboard2&#39;, &#39;mjalg/IFEvalTR&#39;, &#39;lastsamuraii/LitGPT-Pythia-160M&#39;, &#39;Wazahat/Pyco&#39;, &#39;feryelb/python-coder&#39;, &#39;moshabann/Virtual_Teachers&#39;, &#39;ahmedsqrd/model_trace&#39;], safetensors=SafeTensorsInfo(parameters={&#39;BF16&#39;: 6738415616}, total=6738415616), security_repo_status=None, xet_enabled=None)
</pre></div>
</div>
<p>The configuration.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_pretrained_config</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>config LlamaConfig {
  &quot;architectures&quot;: [
    &quot;LlamaForCausalLM&quot;
  ],
  &quot;attention_bias&quot;: false,
  &quot;attention_dropout&quot;: 0.0,
  &quot;bos_token_id&quot;: 1,
  &quot;dtype&quot;: &quot;bfloat16&quot;,
  &quot;eos_token_id&quot;: 2,
  &quot;head_dim&quot;: 128,
  &quot;hidden_act&quot;: &quot;silu&quot;,
  &quot;hidden_size&quot;: 4096,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 11008,
  &quot;max_position_embeddings&quot;: 16384,
  &quot;mlp_bias&quot;: false,
  &quot;model_type&quot;: &quot;llama&quot;,
  &quot;num_attention_heads&quot;: 32,
  &quot;num_hidden_layers&quot;: 32,
  &quot;num_key_value_heads&quot;: 32,
  &quot;pretraining_tp&quot;: 1,
  &quot;rms_norm_eps&quot;: 1e-05,
  &quot;rope_scaling&quot;: null,
  &quot;rope_theta&quot;: 1000000,
  &quot;tie_word_embeddings&quot;: false,
  &quot;transformers_version&quot;: &quot;4.57.0.dev0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 32000
}
</pre></div>
</div>
<p>The task determines the set of inputs which needs
to be created for this input.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;task&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">task_from_id</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>task text-generation
</pre></div>
</div>
</section>
<section id="untrained-model">
<h2>Untrained model<a class="headerlink" href="#untrained-model" title="Link to this heading">¶</a></h2>
<p>The function <a class="reference internal" href="../api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_untrained_model_with_inputs</span></code></a>.
It loads the pretrained configuration, extracts the task associated
to the model and them creates random inputs and dynamic shapes
for <a class="reference external" href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="(in PyTorch v2.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><span class="n">get_untrained_model_with_inputs</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a></a><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;model size:&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;size&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of weights:&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;n_weights&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;fields:&quot;</span><span class="p">,</span> <span class="nb">set</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[get_untrained_model_with_inputs] model_id=&#39;codellama/CodeLlama-7b-Python-hf&#39;
[get_untrained_model_with_inputs] use preinstalled &#39;codellama/CodeLlama-7b-Python-hf&#39;
[get_untrained_model_with_inputs] architectures=[&#39;LlamaForCausalLM&#39;]
[get_untrained_model_with_inputs] cls=&#39;LlamaConfig&#39;
[get_untrained_model_with_inputs] task=&#39;text-generation&#39;
[get_untrained_model_with_inputs] -- updated config
{&#39;num_hidden_layers&#39;: &#39;32 -&gt; 2&#39;}
[get_untrained_model_with_inputs] --
[get_untrained_model_with_inputs] default config._attn_implementation=None
[get_untrained_model_with_inputs] use fct=&lt;function get_inputs at 0x7f2e65ad9f80&gt;
model size: 2667659264
number of weights: 666914816
fields: {&#39;configuration&#39;, &#39;model_kwargs&#39;, &#39;inputs2&#39;, &#39;model&#39;, &#39;n_weights&#39;, &#39;dump_info&#39;, &#39;input_kwargs&#39;, &#39;task&#39;, &#39;dynamic_shapes&#39;, &#39;inputs&#39;, &#39;size&#39;}
</pre></div>
</div>
<p>Inputs</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs:&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>inputs: dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x128,T1s2x32x30x128], value_cache=#2[T1s2x32x30x128,T1s2x32x30x128]))
</pre></div>
</div>
<p>Dynamic Shapes</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dynamic shapes:&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/pprint.html#pprint.pformat" title="pprint.pformat" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pformat" title="pprint.pformat" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pformat" title="pprint.pformat" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span><span class="o">.</span><span class="n">pformat</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dynamic shapes: {&#39;attention_mask&#39;: {0: &#39;batch&#39;, 1: &#39;cache+seq&#39;},
 &#39;input_ids&#39;: {0: &#39;batch&#39;, 1: &#39;seq_length&#39;},
 &#39;past_key_values&#39;: [[{0: &#39;batch&#39;, 2: &#39;cache_length&#39;},
                      {0: &#39;batch&#39;, 2: &#39;cache_length&#39;}],
                     [{0: &#39;batch&#39;, 2: &#39;cache_length&#39;},
                      {0: &#39;batch&#39;, 2: &#39;cache_length&#39;}]],
 &#39;position_ids&#39;: {0: &#39;batch&#39;, 1: &#39;cache+seq&#39;}}
</pre></div>
</div>
<p>Let’s check the model runs. We still needs to
copy the inputs before using the models, the cache
is usually modified inplace.
Expected outputs can be used later to compute
discrepancies.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs_copy</span></a></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span>
<span class="n">expected_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs_copy</span></a></a></a><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;outputs:&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a><span class="p">(</span><span class="n">expected_outputs</span><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>outputs: CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#2[T1s2x32x33x128,T1s2x32x33x128], value_cache=#2[T1s2x32x33x128,T1s2x32x33x128]))
</pre></div>
</div>
<p>It works.</p>
</section>
<section id="export">
<h2>Export<a class="headerlink" href="#export" title="Link to this heading">¶</a></h2>
<p>The model uses <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.cache_utils.DynamicCache</span></code>.
It still requires patches to be exportable (control flow).
See <a class="reference internal" href="../api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.torch_export_patches.torch_export_patches()</span></code></a></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a></a><span class="p">(</span><span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><span class="n">f</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]),</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">]),</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_embed_tokens_weight: &quot;f32[32000, 4096]&quot;, p_model_layers_0_self_attn_q_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_0_self_attn_k_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_0_self_attn_v_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_0_self_attn_o_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_0_mlp_gate_proj_weight: &quot;f32[11008, 4096]&quot;, p_model_layers_0_mlp_up_proj_weight: &quot;f32[11008, 4096]&quot;, p_model_layers_0_mlp_down_proj_weight: &quot;f32[4096, 11008]&quot;, p_model_layers_0_input_layernorm_weight: &quot;f32[4096]&quot;, p_model_layers_0_post_attention_layernorm_weight: &quot;f32[4096]&quot;, p_model_layers_1_self_attn_q_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_1_self_attn_k_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_1_self_attn_v_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_1_self_attn_o_proj_weight: &quot;f32[4096, 4096]&quot;, p_model_layers_1_mlp_gate_proj_weight: &quot;f32[11008, 4096]&quot;, p_model_layers_1_mlp_up_proj_weight: &quot;f32[11008, 4096]&quot;, p_model_layers_1_mlp_down_proj_weight: &quot;f32[4096, 11008]&quot;, p_model_layers_1_input_layernorm_weight: &quot;f32[4096]&quot;, p_model_layers_1_post_attention_layernorm_weight: &quot;f32[4096]&quot;, p_model_norm_weight: &quot;f32[4096]&quot;, p_lm_head_weight: &quot;f32[32000, 4096]&quot;, b_model_rotary_emb_inv_freq: &quot;f32[64]&quot;, input_ids: &quot;i64[s23, s70]&quot;, attention_mask: &quot;i64[s43, s53]&quot;, position_ids: &quot;i64[s23, s70]&quot;, past_key_values_key_cache_0: &quot;f32[s23, 32, s31, 128]&quot;, past_key_values_key_cache_1: &quot;f32[s23, 32, s31, 128]&quot;, past_key_values_value_cache_0: &quot;f32[s23, 32, s11, 128]&quot;, past_key_values_value_cache_1: &quot;f32[s23, 32, s54, 128]&quot;):
             #
            sym_size_int_16: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(input_ids, 1)
            sym_size_int_21: &quot;Sym(s23)&quot; = torch.ops.aten.sym_size.int(past_key_values_key_cache_0, 0)
            sym_size_int_22: &quot;Sym(s31)&quot; = torch.ops.aten.sym_size.int(past_key_values_key_cache_0, 2)

            # No stacktrace found for following nodes
            empty: &quot;f32[s23, 32, 0, 128]&quot; = torch.ops.aten.empty.memory_format([sym_size_int_21, 32, 0, 128], dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
            empty_1: &quot;f32[s23, 32, 0, 128]&quot; = torch.ops.aten.empty.memory_format([sym_size_int_21, 32, 0, 128], dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
            cat: &quot;f32[s23, 32, s31, 128]&quot; = torch.ops.aten.cat.default([empty, past_key_values_key_cache_0], -2);  empty = past_key_values_key_cache_0 = None
            cat_1: &quot;f32[s23, 32, s11, 128]&quot; = torch.ops.aten.cat.default([empty_1, past_key_values_value_cache_0], -2);  empty_1 = past_key_values_value_cache_0 = None
            empty_2: &quot;f32[s23, 32, 0, 128]&quot; = torch.ops.aten.empty.memory_format([sym_size_int_21, 32, 0, 128], dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
            empty_3: &quot;f32[s23, 32, 0, 128]&quot; = torch.ops.aten.empty.memory_format([sym_size_int_21, 32, 0, 128], dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
            cat_2: &quot;f32[s23, 32, s31, 128]&quot; = torch.ops.aten.cat.default([empty_2, past_key_values_key_cache_1], -2);  empty_2 = past_key_values_key_cache_1 = None
            cat_3: &quot;f32[s23, 32, s54, 128]&quot; = torch.ops.aten.cat.default([empty_3, past_key_values_value_cache_1], -2);  empty_3 = past_key_values_value_cache_1 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
            embedding: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:376 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            add: &quot;Sym(s31 + s70)&quot; = sym_size_int_22 + sym_size_int_16

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:375 in forward, code: cache_position: torch.Tensor = torch.arange(
            arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int_22, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_22 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:382 in forward, code: causal_mask = create_causal_mask(
            _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(attention_mask, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default = None
            to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(attention_mask, device(type=&#39;cpu&#39;), torch.bool);  attention_mask = None
            arange_1: &quot;i64[s31 + s70]&quot; = torch.ops.aten.arange.default(add, device = device(type=&#39;cpu&#39;), pin_memory = False)
            add_: &quot;i64[s31 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0);  arange_1 = None
            arange_2: &quot;i64[s23]&quot; = torch.ops.aten.arange.default(sym_size_int_21, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
            reshape: &quot;i64[s23, 1, 1, 1]&quot; = torch.ops.aten.reshape.default(arange_2, [-1, 1, 1, 1]);  arange_2 = None
            reshape_1: &quot;i64[1, 1, 1, 1]&quot; = torch.ops.aten.reshape.default(arange_3, [1, -1, 1, 1]);  arange_3 = None
            reshape_2: &quot;i64[1, 1, s70, 1]&quot; = torch.ops.aten.reshape.default(arange, [1, 1, -1, 1]);  arange = None
            reshape_3: &quot;i64[1, 1, 1, s31 + s70]&quot; = torch.ops.aten.reshape.default(add_, [1, 1, 1, -1]);  add_ = None
            expand: &quot;i64[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.expand.default(reshape, [sym_size_int_21, 1, sym_size_int_16, add]);  reshape = None
            expand_1: &quot;i64[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.expand.default(reshape_1, [sym_size_int_21, 1, sym_size_int_16, add]);  reshape_1 = expand_1 = None
            expand_2: &quot;i64[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.expand.default(reshape_2, [sym_size_int_21, 1, sym_size_int_16, add]);  reshape_2 = None
            expand_3: &quot;i64[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.expand.default(reshape_3, [sym_size_int_21, 1, sym_size_int_16, add]);  reshape_3 = None
            new_ones: &quot;b8[]&quot; = torch.ops.aten.new_ones.default(expand_2, [], dtype = torch.bool, pin_memory = False)
            le: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.le.Tensor(expand_3, expand_2);  expand_2 = None
            _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(le, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
            to_1: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.to.dtype_layout(le, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  le = None
            and_1: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.__and__.Tensor(new_ones, to_1);  new_ones = to_1 = None
            index: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.index.Tensor(to, [expand, expand_3]);  to = expand = expand_3 = None
            _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(index, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
            to_2: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.to.dtype_layout(index, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  index = None
            and_2: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.__and__.Tensor(and_1, to_2);  and_1 = to_2 = None

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1075 in forward, code: self.inv_freq[None, :, None]
            unsqueeze: &quot;f32[1, 64]&quot; = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
            unsqueeze_1: &quot;f32[1, 64, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1076 in forward, code: .float()
            _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
            to_3: &quot;f32[1, 64, 1]&quot; = torch.ops.aten.to.dtype(unsqueeze_1, torch.float32);  unsqueeze_1 = None

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1077 in forward, code: .expand(position_ids.shape[0], -1, 1)
            expand_4: &quot;f32[s23, 64, 1]&quot; = torch.ops.aten.expand.default(to_3, [sym_size_int_21, -1, 1]);  to_3 = None

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1078 in forward, code: .to(x.device)
            _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(expand_4, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
            to_4: &quot;f32[s23, 64, 1]&quot; = torch.ops.aten.to.dtype_layout(expand_4, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  expand_4 = None

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1080 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
            slice_1: &quot;i64[s23, s70]&quot; = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
            unsqueeze_2: &quot;i64[s23, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
            slice_2: &quot;i64[s23, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
            _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(slice_2, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
            to_5: &quot;f32[s23, 1, s70]&quot; = torch.ops.aten.to.dtype(slice_2, torch.float32);  slice_2 = None

            # No stacktrace found for following nodes
            submod_3 = self.submod_1
            wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast(&#39;cpu&#39;, torch.bfloat16, False, False, submod_3, to_4, to_5);  submod_3 = to_4 = to_5 = None

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1090 in forward, code: cos = emb.cos() * self.attention_scaling
            mul: &quot;f32[s23, s70, 128]&quot; = wrap_with_autocast[0]

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1091 in forward, code: sin = emb.sin() * self.attention_scaling
            mul_1: &quot;f32[s23, s70, 128]&quot; = wrap_with_autocast[1];  wrap_with_autocast = None

             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1093 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
            _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
            to_8: &quot;f32[s23, s70, 128]&quot; = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
            _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
            to_9: &quot;f32[s23, s70, 128]&quot; = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
            to_10: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
            mean: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_3: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_3);  add_3 = None
            mul_2: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(to_10, rsqrt);  rsqrt = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
            to_11: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
            mul_3: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_11);  p_model_layers_0_input_layernorm_weight = to_11 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:236 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.view.default(linear, [sym_size_int_21, sym_size_int_16, -1, 128]);  linear = None
            transpose_1: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.transpose.int(view, 1, 2);  view = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_1: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:237 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_1: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.view.default(linear_1, [sym_size_int_21, sym_size_int_16, -1, 128]);  linear_1 = None
            transpose_2: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_2: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:238 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_2: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.view.default(linear_2, [sym_size_int_21, sym_size_int_16, -1, 128]);  linear_2 = None
            transpose_3: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:241 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_3: &quot;f32[s23, 1, s70, 128]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1)
            unsqueeze_4: &quot;f32[s23, 1, s70, 128]&quot; = torch.ops.aten.unsqueeze.default(to_9, 1)
            mul_4: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_3)
            slice_3: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 64)
            slice_4: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 64, 9223372036854775807);  transpose_1 = None
            neg: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.neg.default(slice_4);  slice_4 = None
            cat_5: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.cat.default([neg, slice_3], -1);  neg = slice_3 = None
            mul_5: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(cat_5, unsqueeze_4);  cat_5 = None
            add_4: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            mul_6: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_3);  unsqueeze_3 = None
            slice_5: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 64)
            slice_6: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 64, 9223372036854775807);  transpose_2 = None
            neg_1: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.neg.default(slice_6);  slice_6 = None
            cat_6: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.cat.default([neg_1, slice_5], -1);  neg_1 = slice_5 = None
            mul_7: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(cat_6, unsqueeze_4);  cat_6 = unsqueeze_4 = None
            add_5: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:246 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_7: &quot;f32[s23, 32, s31 + s70, 128]&quot; = torch.ops.aten.cat.default([cat, add_5], -2);  cat = add_5 = None
            cat_8: &quot;f32[s23, 32, s11 + s70, 128]&quot; = torch.ops.aten.cat.default([cat_1, transpose_3], -2);  cat_1 = transpose_3 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:252 in forward, code: attn_output, attn_weights = attention_interface(
            slice_7: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.slice.Tensor(and_2, 3, None, add)
            scaled_dot_product_attention: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.scaled_dot_product_attention.default(add_4, cat_7, cat_8, slice_7, scale = 0.08838834764831845);  add_4 = slice_7 = None
            transpose_4: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:263 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_4: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.reshape.default(transpose_4, [sym_size_int_21, sym_size_int_16, -1]);  transpose_4 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_3: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(reshape_4, p_model_layers_0_self_attn_o_proj_weight);  reshape_4 = p_model_layers_0_self_attn_o_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:304 in forward, code: hidden_states = residual + hidden_states
            add_6: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.add.Tensor(to_10, linear_3);  to_10 = linear_3 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_6, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
            to_12: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(add_6, torch.float32);  add_6 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
            mean_1: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_7: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_7);  add_7 = None
            mul_16: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(to_12, rsqrt_1);  rsqrt_1 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_16, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
            to_13: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(mul_16, torch.float32);  mul_16 = None
            mul_17: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_13);  p_model_layers_0_post_attention_layernorm_weight = to_13 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_4: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)
            silu: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.silu.default(linear_4);  linear_4 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_5: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_up_proj_weight);  mul_17 = p_model_layers_0_mlp_up_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:155 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_18: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_6: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_0_mlp_down_proj_weight);  mul_18 = p_model_layers_0_mlp_down_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:310 in forward, code: hidden_states = residual + hidden_states
            add_8: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.add.Tensor(to_12, linear_6);  to_12 = linear_6 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_8, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
            to_14: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(add_8, torch.float32);  add_8 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
            mean_2: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_9: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_9);  add_9 = None
            mul_19: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(to_14, rsqrt_2);  rsqrt_2 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_19, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
            to_15: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(mul_19, torch.float32);  mul_19 = None
            mul_20: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_1_input_layernorm_weight, to_15);  p_model_layers_1_input_layernorm_weight = to_15 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_7: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_20, p_model_layers_1_self_attn_q_proj_weight);  p_model_layers_1_self_attn_q_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:236 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_3: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.view.default(linear_7, [sym_size_int_21, sym_size_int_16, -1, 128]);  linear_7 = None
            transpose_5: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_8: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_20, p_model_layers_1_self_attn_k_proj_weight);  p_model_layers_1_self_attn_k_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:237 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_4: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.view.default(linear_8, [sym_size_int_21, sym_size_int_16, -1, 128]);  linear_8 = None
            transpose_6: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_9: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_20, p_model_layers_1_self_attn_v_proj_weight);  mul_20 = p_model_layers_1_self_attn_v_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:238 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_5: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.view.default(linear_9, [sym_size_int_21, sym_size_int_16, -1, 128]);  linear_9 = None
            transpose_7: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:241 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_5: &quot;f32[s23, 1, s70, 128]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
            unsqueeze_6: &quot;f32[s23, 1, s70, 128]&quot; = torch.ops.aten.unsqueeze.default(to_9, 1);  to_9 = None
            mul_21: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_5, unsqueeze_5)
            slice_8: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_5, 3, 0, 64)
            slice_9: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_5, 3, 64, 9223372036854775807);  transpose_5 = None
            neg_2: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.neg.default(slice_9);  slice_9 = None
            cat_9: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.cat.default([neg_2, slice_8], -1);  neg_2 = slice_8 = None
            mul_22: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(cat_9, unsqueeze_6);  cat_9 = None
            add_10: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.add.Tensor(mul_21, mul_22);  mul_21 = mul_22 = None
            mul_23: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_6, unsqueeze_5);  unsqueeze_5 = None
            slice_10: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_6, 3, 0, 64)
            slice_11: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_6, 3, 64, 9223372036854775807);  transpose_6 = None
            neg_3: &quot;f32[s23, 32, s70, 64]&quot; = torch.ops.aten.neg.default(slice_11);  slice_11 = None
            cat_10: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.cat.default([neg_3, slice_10], -1);  neg_3 = slice_10 = None
            mul_24: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.mul.Tensor(cat_10, unsqueeze_6);  cat_10 = unsqueeze_6 = None
            add_11: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.add.Tensor(mul_23, mul_24);  mul_23 = mul_24 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:246 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_11: &quot;f32[s23, 32, s31 + s70, 128]&quot; = torch.ops.aten.cat.default([cat_2, add_11], -2);  cat_2 = add_11 = None
            cat_12: &quot;f32[s23, 32, s54 + s70, 128]&quot; = torch.ops.aten.cat.default([cat_3, transpose_7], -2);  cat_3 = transpose_7 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:252 in forward, code: attn_output, attn_weights = attention_interface(
            slice_12: &quot;b8[s23, 1, s70, s31 + s70]&quot; = torch.ops.aten.slice.Tensor(and_2, 3, None, add);  and_2 = add = None
            scaled_dot_product_attention_1: &quot;f32[s23, 32, s70, 128]&quot; = torch.ops.aten.scaled_dot_product_attention.default(add_10, cat_11, cat_12, slice_12, scale = 0.08838834764831845);  add_10 = slice_12 = None
            transpose_8: &quot;f32[s23, s70, 32, 128]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:263 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_5: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.reshape.default(transpose_8, [sym_size_int_21, sym_size_int_16, -1]);  transpose_8 = sym_size_int_21 = sym_size_int_16 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_10: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(reshape_5, p_model_layers_1_self_attn_o_proj_weight);  reshape_5 = p_model_layers_1_self_attn_o_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:304 in forward, code: hidden_states = residual + hidden_states
            add_12: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.add.Tensor(to_14, linear_10);  to_14 = linear_10 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_16 = torch.ops.aten._assert_tensor_metadata.default(add_12, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_16 = None
            to_16: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(add_12, torch.float32);  add_12 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_4: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_16, 2)
            mean_3: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_13: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_3, 1e-05);  mean_3 = None
            rsqrt_3: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_13);  add_13 = None
            mul_25: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(to_16, rsqrt_3);  rsqrt_3 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_17 = torch.ops.aten._assert_tensor_metadata.default(mul_25, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_17 = None
            to_17: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(mul_25, torch.float32);  mul_25 = None
            mul_26: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_1_post_attention_layernorm_weight, to_17);  p_model_layers_1_post_attention_layernorm_weight = to_17 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_11: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.linear.default(mul_26, p_model_layers_1_mlp_gate_proj_weight);  p_model_layers_1_mlp_gate_proj_weight = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)
            silu_1: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.silu.default(linear_11);  linear_11 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_12: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.linear.default(mul_26, p_model_layers_1_mlp_up_proj_weight);  mul_26 = p_model_layers_1_mlp_up_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:155 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_27: &quot;f32[s23, s70, 11008]&quot; = torch.ops.aten.mul.Tensor(silu_1, linear_12);  silu_1 = linear_12 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_13: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.linear.default(mul_27, p_model_layers_1_mlp_down_proj_weight);  mul_27 = p_model_layers_1_mlp_down_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:310 in forward, code: hidden_states = residual + hidden_states
            add_14: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.add.Tensor(to_16, linear_13);  to_16 = linear_13 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_18 = torch.ops.aten._assert_tensor_metadata.default(add_14, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_18 = None
            to_18: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(add_14, torch.float32);  add_14 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_5: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_18, 2)
            mean_4: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_5, [-1], True);  pow_5 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_15: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_4, 1e-05);  mean_4 = None
            rsqrt_4: &quot;f32[s23, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_15);  add_15 = None
            mul_28: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(to_18, rsqrt_4);  to_18 = rsqrt_4 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_19 = torch.ops.aten._assert_tensor_metadata.default(mul_28, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_19 = None
            to_19: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.to.dtype(mul_28, torch.float32);  mul_28 = None
            mul_29: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_19);  p_model_norm_weight = to_19 = None

             # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:473 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
            slice_13: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.slice.Tensor(mul_29, 0, 0, 9223372036854775807);  mul_29 = None
            slice_14: &quot;f32[s23, s70, 4096]&quot; = torch.ops.aten.slice.Tensor(slice_13, 1, 0, 9223372036854775807);  slice_13 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_14: &quot;f32[s23, s70, 32000]&quot; = torch.ops.aten.linear.default(slice_14, p_lm_head_weight);  slice_14 = p_lm_head_weight = None
            return (linear_14, cat_7, cat_11, cat_8, cat_12)

        class submod_1(torch.nn.Module):
            def forward(self, to_4: &quot;f32[s23, 64, 1]&quot;, to_5: &quot;f32[s23, 1, s70]&quot;):
                 # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1088 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                to_6: &quot;f32[s23, 64, 1]&quot; = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(to_5, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                to_7: &quot;f32[s23, 1, s70]&quot; = torch.ops.aten.to.dtype(to_5, torch.float32);  to_5 = None
                matmul: &quot;f32[s23, 64, s70]&quot; = torch.ops.aten.matmul.default(to_6, to_7);  to_6 = to_7 = None
                transpose: &quot;f32[s23, s70, 64]&quot; = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None

                 # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1089 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                cat_4: &quot;f32[s23, s70, 128]&quot; = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None

                 # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1090 in forward, code: cos = emb.cos() * self.attention_scaling
                cos: &quot;f32[s23, s70, 128]&quot; = torch.ops.aten.cos.default(cat_4)
                mul: &quot;f32[s23, s70, 128]&quot; = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None

                 # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1091 in forward, code: sin = emb.sin() * self.attention_scaling
                sin: &quot;f32[s23, s70, 128]&quot; = torch.ops.aten.sin.default(cat_4);  cat_4 = None
                mul_1: &quot;f32[s23, s70, 128]&quot; = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                return (mul, mul_1)

Graph signature:
    # inputs
    p_model_embed_tokens_weight: PARAMETER target=&#39;model.embed_tokens.weight&#39;
    p_model_layers_0_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.q_proj.weight&#39;
    p_model_layers_0_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.k_proj.weight&#39;
    p_model_layers_0_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.v_proj.weight&#39;
    p_model_layers_0_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.o_proj.weight&#39;
    p_model_layers_0_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.gate_proj.weight&#39;
    p_model_layers_0_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.up_proj.weight&#39;
    p_model_layers_0_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.down_proj.weight&#39;
    p_model_layers_0_input_layernorm_weight: PARAMETER target=&#39;model.layers.0.input_layernorm.weight&#39;
    p_model_layers_0_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.0.post_attention_layernorm.weight&#39;
    p_model_layers_1_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.q_proj.weight&#39;
    p_model_layers_1_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.k_proj.weight&#39;
    p_model_layers_1_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.v_proj.weight&#39;
    p_model_layers_1_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.o_proj.weight&#39;
    p_model_layers_1_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.gate_proj.weight&#39;
    p_model_layers_1_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.up_proj.weight&#39;
    p_model_layers_1_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.down_proj.weight&#39;
    p_model_layers_1_input_layernorm_weight: PARAMETER target=&#39;model.layers.1.input_layernorm.weight&#39;
    p_model_layers_1_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.1.post_attention_layernorm.weight&#39;
    p_model_norm_weight: PARAMETER target=&#39;model.norm.weight&#39;
    p_lm_head_weight: PARAMETER target=&#39;lm_head.weight&#39;
    b_model_rotary_emb_inv_freq: BUFFER target=&#39;model.rotary_emb.inv_freq&#39; persistent=False
    input_ids: USER_INPUT
    attention_mask: USER_INPUT
    position_ids: USER_INPUT
    past_key_values_key_cache_0: USER_INPUT
    past_key_values_key_cache_1: USER_INPUT
    past_key_values_value_cache_0: USER_INPUT
    past_key_values_value_cache_1: USER_INPUT

    # outputs
    linear_14: USER_OUTPUT
    cat_7: USER_OUTPUT
    cat_11: USER_OUTPUT
    cat_8: USER_OUTPUT
    cat_12: USER_OUTPUT

Range constraints: {s23: VR[2, int_oo], s70: VR[2, int_oo], s43: VR[2, int_oo], s53: VR[4, int_oo], s31: VR[2, int_oo], s11: VR[2, int_oo], s54: VR[2, int_oo]}
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span><span class="o">.</span><span class="n">plot_legend</span><span class="p">(</span>
    <span class="s2">&quot;untrained</span><span class="se">\n</span><span class="s2">codellama/</span><span class="se">\n</span><span class="s2">CodeLlama-7b-Python-hf&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.export.export&quot;</span><span class="p">,</span> <span class="s2">&quot;tomato&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_export_hub_codellama_001.png" srcset="../_images/sphx_glr_plot_export_hub_codellama_001.png" alt="plot export hub codellama" class = "sphx-glr-single-img"/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 12.226 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-export-hub-codellama-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/ddd9c1f67aff5f3035228e4ae64ecf6d/plot_export_hub_codellama.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_export_hub_codellama.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/a4876826f2dd4d37a228cdab8e0fe187/plot_export_hub_codellama.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_export_hub_codellama.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b6c0f575e324bf8b84d99b1a8df56381/plot_export_hub_codellama.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_export_hub_codellama.zip</span></code></a></p>
</div>
</div>
<p class="rubric">Related examples</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This function exports an smaller untrained model with the same architecture. It is faster than the pretrained model. When this works, the untrained model can be replaced by the trained one."><img alt="" src="../_images/sphx_glr_plot_export_tiny_phi2_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_phi2.html#sphx-glr-auto-examples-plot-export-tiny-phi2-py"><span class="std std-ref">Export microsoft/phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export microsoft/phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Inputs are always dynamic with LLMs that is why dynamic shapes needs to be specified when a LLM is exported with torch.export.export. Most of the examples on HuggingFace use method transformers.GenerationMixin.generate but we only want to export the model and its method forward."><img alt="" src="../_images/sphx_glr_plot_export_tiny_llm_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_llm.html#sphx-glr-auto-examples-plot-export-tiny-llm-py"><span class="std std-ref">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Let&#x27;s assume onnxruntime crashes without telling why or where. The first thing is do is to locate where. For that, we run a python runtime which is going to run until it fails."><img alt="" src="../_images/sphx_glr_plot_failing_reference_evaluator_thumb.png" />
<p><a class="reference internal" href="plot_failing_reference_evaluator.html#sphx-glr-auto-examples-plot-failing-reference-evaluator-py"><span class="std std-ref">Intermediate results with (ONNX) ReferenceEvaluator</span></a></p>
  <div class="sphx-glr-thumbnail-title">Intermediate results with (ONNX) ReferenceEvaluator</div>
</div></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../auto_recipes/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Common Export Issues</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_export_tiny_llm.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Test the export on untrained models</a><ul>
<li><a class="reference internal" href="#codellama-codellama-7b-python-hf">codellama/CodeLlama-7b-Python-hf</a></li>
<li><a class="reference internal" href="#untrained-model">Untrained model</a></li>
<li><a class="reference internal" href="#export">Export</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=c79aef8b"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>