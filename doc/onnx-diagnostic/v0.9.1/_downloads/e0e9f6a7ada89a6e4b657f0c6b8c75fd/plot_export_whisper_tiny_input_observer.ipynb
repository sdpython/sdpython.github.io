{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export whisper-tiny with InputObserver\n\nThis reuses the recipe introduced by example `l-plot-tiny-llm-export-input-observer`\nfor model [openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny).\n\n## The model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.export.api import to_onnx\nfrom onnx_diagnostic.torch_export_patches import (\n    register_additional_serialization_functions,\n    torch_export_patches,\n)\nfrom onnx_diagnostic.investigate.input_observer import InputObserver\n\n# load model and processor\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\nmodel.config.forced_decoder_ids = None\n\n# load dummy dataset and read audio files\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nsamples = [ds[0][\"audio\"], ds[2][\"audio\"]]\nfor s in samples:\n    print(s[\"array\"].shape, s[\"array\"].min(), s[\"array\"].max(), s[\"sampling_rate\"])\ninput_features = [\n    processor(\n        sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\"\n    ).input_features\n    for sample in samples\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captures inputs and outputs for the encoder, decoder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "observer_encoder, observer_decoder = InputObserver(), InputObserver()\nwith register_additional_serialization_functions(patch_transformers=True):\n    for features in input_features:\n        with (\n            observer_encoder(model.model.encoder, store_n_calls=4),\n            observer_decoder(model.model.decoder, store_n_calls=4),\n        ):\n            predicted_ids = model.generate(features)\n\n\nprint(f\"{observer_encoder.num_obs()} observations stored for encoder.\")\nprint(f\"{observer_decoder.num_obs()} observations stored for decoder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export the encoder\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kwargs = observer_encoder.infer_arguments()\ndynamic_shapes = observer_encoder.infer_dynamic_shapes(set_batch_dimension_for=True)\nprint(f\"encoder kwargs={string_type(kwargs, with_shape=True)}\")\nprint(f\"encoder dynamic_shapes={dynamic_shapes}\")\nfor candidate in observer_encoder.info.inputs:\n    print(\n        \"   \",\n        candidate,\n        candidate.str_obs(),\n        string_type(candidate.aligned_flat_list, with_shape=True),\n    )\n\n\nfilename_encoder = \"plot_export_whisper_tiny_input_observer_encoder.onnx\"\nwith torch_export_patches(patch_transformers=True):\n    to_onnx(\n        model.model.encoder,\n        args=(),\n        filename=filename_encoder,\n        kwargs=kwargs,\n        dynamic_shapes=dynamic_shapes,\n        exporter=\"custom\",\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's measure the discrepancies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = observer_encoder.check_discrepancies(filename_encoder, progress_bar=True)\nprint(pandas.DataFrame(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export the decoder\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kwargs = observer_decoder.infer_arguments()\ndynamic_shapes = observer_decoder.infer_dynamic_shapes(set_batch_dimension_for=True)\nprint(f\"decoder kwargs={string_type(kwargs, with_shape=True)}\")\nprint(f\"decoder dynamic_shapes={dynamic_shapes}\")\n\nfilename_decoder = \"plot_export_whisper_tiny_input_observer_decoder.onnx\"\nwith torch_export_patches(patch_transformers=True):\n    to_onnx(\n        model.model.decoder,\n        args=(),\n        filename=filename_decoder,\n        kwargs=observer_decoder.infer_arguments(),\n        dynamic_shapes=observer_decoder.infer_dynamic_shapes(set_batch_dimension_for=True),\n        exporter=\"custom\",\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's measure the discrepancies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = observer_decoder.check_discrepancies(filename_decoder, progress_bar=True, atol=1e-3)\nprint(pandas.DataFrame(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.save_fig(doc.plot_dot(filename_decoder), f\"{filename_decoder}.png\", dpi=400)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}