{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export OptiMind-SFT with InputObserver\n\nThis reuses the recipe introduced by example `l-plot-tiny-llm-export-input-observer`\nfor model [microsoft/OptiMind-SFT](https://huggingface.co/microsoft/OptiMind-SFT).\nWe only export class ``GptOssExperts``.\n\n## Let's create a random model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.export.api import to_onnx\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.torch_export_patches import (\n    register_additional_serialization_functions,\n    torch_export_patches,\n)\nfrom onnx_diagnostic.investigate.input_observer import InputObserver\n\ndevice = \"cuda\"\nmodel_id = \"microsoft/OptiMind-SFT\"\nprint(f\"get tokenizer {model_id!r}\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nprint(f\"get config {model_id!r}\")\nconfig = AutoConfig.from_pretrained(model_id)\nconfig.num_hidden_layers = 2\nconfig.layer_types = config.layer_types[:2]\nprint(f\"create model from config for {model_id!r}\")\nmodel = AutoModelForCausalLM.from_config(config)\nprint(f\"the model is created with {len(list(model.named_modules()))} subdmodules.\")\nmodel = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We need to only export class GptOssExperts\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "export_module = None\nfor _name, sub in model.named_modules():\n    if sub.__class__.__name__ == \"GptOssExperts\":\n        export_module = sub\n\nassert export_module is not None, (\n    f\"Unable to find a submodule from class GptOssExperts in \"\n    f\"{set(sub.__class__.__name__ for _, sub in model.named_modules())}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the model and capture inputs and outputs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_text(\n    prompt,\n    model,\n    tokenizer,\n    max_length=50,\n    temperature=0.01,\n    top_k=50,\n    top_p=0.95,\n    do_sample=True,\n):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        do_sample=do_sample,\n    )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n\nprompt = \"Continue: it rains, what should I do?\"\nobserver = InputObserver()\nwith (\n    register_additional_serialization_functions(patch_transformers=True),\n    observer(export_module),\n):\n    generate_text(prompt, model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export\n\nFirst, what was inferred.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "args = observer.infer_arguments()\ndynamic_shapes = observer.infer_dynamic_shapes()\nprint(f\"args={string_type(args, with_shape=True, with_device=True)}\")\nprint(f\"dynamic_shapes={dynamic_shapes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, the export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"plot_export_optimind_experts_input_observer.onnx\"\nwith torch_export_patches(patch_transformers=True):\n    to_onnx(\n        export_module,\n        args=args,\n        filename=filename,\n        dynamic_shapes=dynamic_shapes,\n        exporter=\"custom\",\n        verbose=1,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's measure the discrepancies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = observer.check_discrepancies(filename, progress_bar=True, atol=1e-2, include_io=True)\ndf = pandas.DataFrame(data)\ndf.to_excel(\"plot_export_optimind_input_observer.xlsx\")\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's show the errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for row in data:\n    if not row[\"SUCCESS\"] and \"error\" in row:\n        print(row[\"error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.save_fig(doc.plot_dot(filename), f\"{filename}.png\", dpi=400)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}