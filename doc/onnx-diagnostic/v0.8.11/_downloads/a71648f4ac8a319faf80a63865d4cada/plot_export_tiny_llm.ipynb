{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)\n\nInputs are always dynamic with LLMs that is why dynamic shapes\nneeds to be specified when a LLM is exported with :func:`torch.export.export`.\nMost of the examples on :epkg:`HuggingFace` use method\n:meth:`transformers.GenerationMixin.generate` but we only want to\nexport the model and its method ``forward``.\n\nThat example shows to guess the inputs of this method even though the model\nis executed through meth ``generate``.\n\nWe focus on the model :epkg:`arnir0/Tiny-LLM`.\nTo avoid downloading any weights, we write a function creating a\nrandom model based on the same architecture.\n\n## Steel the forward method\n\nThe first step is to guess the dummy inputs.\nLet's use the true model for that.\nWe use the dummy example from the model page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nimport pprint\nimport torch\nimport transformers\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.helpers.torch_helper import steal_forward\nfrom onnx_diagnostic.torch_models.llms import get_tiny_llm\nfrom onnx_diagnostic.torch_export_patches.patch_inputs import use_dyn_not_str\n\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We rewrite the forward method to print the cache dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _forward_(*args, _f=None, **kwargs):\n    assert _f is not None\n    if not hasattr(torch.compiler, \"is_exporting\") or not torch.compiler.is_exporting():\n        # torch.compiler.is_exporting requires torch>=2.7\n        print(\"<-\", string_type((args, kwargs), with_shape=True, with_min_max=True))\n    res = _f(*args, **kwargs)\n    if not hasattr(torch.compiler, \"is_exporting\") or not torch.compiler.is_exporting():\n        print(\"->\", string_type(res, with_shape=True, with_min_max=True))\n    return res\n\n\nkeep_model_forward = model.forward\nmodel.forward = lambda *args, _f=keep_model_forward, **kwargs: _forward_(\n    *args, _f=_f, **kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prompt = \"Continue: it rains...\"\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n\noutputs = model.generate(\n    inputs, max_length=50, temperature=1, top_k=50, top_p=0.95, do_sample=True\n)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"-- prompt\", prompt)\nprint(\"-- answer\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's restore the forward as it was.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.forward = keep_model_forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another syntax with :func:`onnx_diagnostic.helpers.torch_helper.steal_forward`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with steal_forward(model):\n    model.generate(inputs, max_length=50, temperature=1, top_k=50, top_p=0.95, do_sample=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Untrained model\n\nThis part can skipped if you are only interested in exporting\nthe original model. It is useful to create a unit test to ensure\na specific architecture can be exported despite the many changes\nbrought to :epkg:`torch` or :epkg:`transformers`.\n\nLet's create an untrained model using the config file provided\n[config.json](https://huggingface.co/arnir0/Tiny-LLM/blob/main/config.json)\nto create an untrained model:\n:func:`onnx_diagnostic.torch_models.llms.get_tiny_llm`.\nThen let's use it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "experiment = get_tiny_llm()\nuntrained_model, inputs, dynamic_shapes = (\n    experiment[\"model\"],\n    experiment[\"inputs\"],\n    experiment[\"dynamic_shapes\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we run it, we make a copy of the inputs as the cache\nget modified by the execution. Then it is no longer valid\nassociated with the previous input_ids and mask.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cloned_inputs = copy.deepcopy(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"input type before\", string_type(inputs, with_shape=True))\n\nexpected_output = untrained_model(**inputs)\n\nprint(\"input type after-\", string_type(inputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The outputs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"result type\", string_type(expected_output, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works.\n\n## ExportedProgram\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    ep = torch.export.export(\n        untrained_model,\n        (),\n        kwargs=cloned_inputs,\n        dynamic_shapes=use_dyn_not_str(dynamic_shapes),\n        strict=False,\n    )\n    print(\"It worked:\")\n    print(ep)\nexcept Exception as e:\n    # To work, it needs at least PRs:\n    # * https://github.com/huggingface/transformers/pull/36311\n    # * https://github.com/huggingface/transformers/pull/36652\n    print(\"It failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Back to the original model\n\nLet's use the same dummy inputs but we use the downloaded model.\nDummy inputs and dynamic shapes are created by function\n:func:`onnx_diagnostic.torch_models.llms.get_tiny_llm`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = get_tiny_llm()\ninputs, dynamic_shapes = data[\"inputs\"], data[\"dynamic_shapes\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print the inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(string_type(inputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint.pprint(dynamic_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And Let's finally export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    ep = torch.export.export(\n        model,\n        (),\n        kwargs=cloned_inputs,\n        dynamic_shapes=use_dyn_not_str(dynamic_shapes),\n        strict=False,\n    )\n    print(\"It worked:\")\n    print(ep)\nexcept Exception as e:\n    # To work, it needs at least PRs:\n    # * https://github.com/huggingface/transformers/pull/36311\n    # * https://github.com/huggingface/transformers/pull/36652\n    print(\"It failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have any error, then look at example\n`l-plot-tiny-llm-export-patched`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"Tiny-LLM\\nforward inputs\\nbehind generate\", \"torch.export.export\", \"tomato\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}