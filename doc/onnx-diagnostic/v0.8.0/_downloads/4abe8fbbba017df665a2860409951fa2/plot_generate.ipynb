{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# From a LLM to processing a prompt\n\nMethod ``generate`` generates the model answer for a given prompt.\nLet's implement our own to understand better how it works and\nthen apply it to an ONNX model.\n\n## Example with Phi 1.5\n\nepkg:`microsoft/Phi-1.5` is a small LLM. The example given\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport time\nimport sys\nimport pandas\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom onnx_diagnostic.ext_test_case import unit_test_going\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.helpers.torch_helper import to_any, get_weight_type\nfrom onnx_diagnostic.helpers.rt_helper import onnx_generate\nfrom onnx_diagnostic.torch_export_patches import torch_export_patches\nfrom onnx_diagnostic.torch_models.hghub import get_untrained_model_with_inputs\nfrom onnx_diagnostic.torch_models.hghub.hub_api import get_pretrained_config, task_from_id\nfrom onnx_diagnostic.tasks import random_input_kwargs\nfrom onnx_diagnostic.export.api import to_onnx\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndata = []\n\nprint(\"-- load the model...\")\nif unit_test_going():\n    # unit_test_going() returns True if UNITTEST_GOING is 1\n    # The example switches to a faster scenario.\n    model_id = \"arnir0/Tiny-LLM\"\n    data_export = get_untrained_model_with_inputs(model_id)\n    model = data_export[\"model\"]\n    export_inputs = data_export[\"inputs\"]\n    export_shapes = data_export[\"dynamic_shapes\"]\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\nelse:\n    model_id = \"microsoft/phi-1_5\"\n    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    config = get_pretrained_config(model_id)\n    task = task = task_from_id(model_id)\n    kwargs, fct = random_input_kwargs(config, task)\n    res = fct(model, config, add_second_input=False, **kwargs)\n    export_inputs = res[\"inputs\"]\n    export_shapes = res[\"dynamic_shapes\"]\nmodel = model.to(device)\nprint(\"-- done.\")\n\nprint(\"-- tokenize the prompt...\")\ninputs = tokenizer(\n    '''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''',\n    return_tensors=\"pt\",\n    return_attention_mask=False,\n).to(device)\nprint(\"-- done.\")\n\nprint(\"-- compute the answer...\")\nbegin = time.perf_counter()\noutputs = model.generate(**inputs, max_new_tokens=100)\nduration = time.perf_counter() - begin\nprint(f\"-- done in {duration}\")\ndata.append(dict(name=\"generate\", duration=duration))\nprint(\"output shape:\", string_type(outputs, with_shape=True, with_min_max=True))\nprint(\"-- decode the answer...\")\ntext = tokenizer.batch_decode(outputs)[0]\nprint(\"-- done.\")\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## eos_token_id?\n\nThis token means the end of the answer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"eos_token_id=\", tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom method generate\n\nLet's implement a simple function replicating when method\n``generate`` does.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def simple_generate_with_cache(\n    model, input_ids: torch.Tensor, eos_token_id: int, max_new_tokens: int = 100\n):\n    # First call: prefill\n    outputs = model(input_ids, use_cache=True)\n\n    # Next calls: decode\n    for _ in tqdm(list(range(max_new_tokens))):\n        next_token_logits = outputs.logits[:, -1, :]\n        past_key_values = outputs.past_key_values\n\n        # The most probable next token is chosen.\n        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n        # But we could select it using a multinomial law\n        # <<< probs = torch.softmax(next_token_logits / temperature, dim=-1)\n        # <<< top_probs, top_indices = torch.topk(probs, top_k)\n        # <<< next_token_id = top_indices[torch.multinomial(top_probs, 1)]\n\n        if next_token_id.item() == eos_token_id:\n            break\n        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n\n        # Feed only the new token, but with the cache\n        outputs = model(next_token_id, use_cache=True, past_key_values=past_key_values)\n\n    return input_ids\n\n\nprint(\"-- compute the answer with custom generate...\")\nbegin = time.perf_counter()\noutputs = simple_generate_with_cache(\n    model, inputs.input_ids, eos_token_id=tokenizer.eos_token_id, max_new_tokens=100\n)\nduration = time.perf_counter() - begin\nprint(f\"-- done in {duration}\")\ndata.append(dict(name=\"custom\", duration=duration))\n\nprint(\"-- done.\")\nprint(\"output shape:\", string_type(outputs, with_shape=True, with_min_max=True))\nprint(\"-- decode the answer...\")\ntext = tokenizer.batch_decode(outputs)[0]\nprint(\"-- done.\")\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method generate for onnx models\n\nWe first need to export the model into ONNX.\n\n### ONNX Conversion\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"position_ids\" in export_inputs:\n    del export_inputs[\"position_ids\"]\n    del export_shapes[\"position_ids\"]\ndtype = get_weight_type(model)\nprint(\"-- model dtype:\", dtype)\nexport_inputs[\"past_key_values\"] = to_any(export_inputs[\"past_key_values\"], dtype)\nexporter = \"onnx-dynamo\" if \"dynamo\" in sys.argv else \"custom\"\nmodel_name = f\"model_{model_id.replace('/', '-')}.{exporter}.onnx\"\nif not os.path.exists(model_name):\n    # This step is slow so let's skip it if it was already done.\n    print(\"-- conversion to ONNX.\")\n    begin = time.perf_counter()\n    with torch_export_patches(patch_transformers=True):\n        to_onnx(\n            model,\n            (),\n            kwargs=to_any(export_inputs, device),\n            dynamic_shapes=export_shapes,\n            filename=model_name,\n            verbose=1,\n            exporter=exporter,\n        )\n    duration = time.perf_counter() - begin\n    print(f\"-- done in {duration}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### onnx_generate\n\nThen we can call method generate for two tokens.\nThis function is part of :mod:`onnx_diagnostic` but follows the implementation\nseen earlier for a torch model.\nLet's ask first the function to return the session to avoid creating on the second call.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_res, session = onnx_generate(\n    model_name, inputs.input_ids, 2, max_new_tokens=2, return_session=True\n)\n\n# And now the full answer.\nprint(\"-- compute the answer with custom generate...\")\nbegin = time.perf_counter()\noutputs = onnx_generate(\n    session, inputs.input_ids, eos_token_id=tokenizer.eos_token_id, max_new_tokens=100\n)\nduration = time.perf_counter() - begin\nprint(f\"-- done in {duration}\")\ndata.append(dict(name=\"onnx\", duration=duration))\n\nprint(\"-- done.\")\nprint(\"output shape:\", string_type(outputs, with_shape=True, with_min_max=True))\nprint(\"-- decode the answer...\")\ntext = tokenizer.batch_decode(outputs)[0]\nprint(\"-- done.\")\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pandas.DataFrame(data).set_index(\"name\")\nprint(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = df.plot(kind=\"bar\", title=\"Time (s) comparison to generate a prompt.\", rot=45)\nax.figure.tight_layout()\nax.figure.savefig(\"plot_generate.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}