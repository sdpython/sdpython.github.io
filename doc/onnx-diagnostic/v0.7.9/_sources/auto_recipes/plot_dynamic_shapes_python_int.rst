
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_recipes/plot_dynamic_shapes_python_int.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_recipes_plot_dynamic_shapes_python_int.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_recipes_plot_dynamic_shapes_python_int.py:


.. _l-plot-dynamic-shapes-python-int:

Do not use python int with dynamic shapes
=========================================

:func:`torch.export.export` uses :class:`torch.SymInt` to operate on shapes and
optimizes the graph it produces. It checks if two tensors share the same dimension,
if the shapes can be broadcast, ... To do that, python types must not be used
or the algorithm looses information.

Wrong Model
+++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 15-36

.. code-block:: Python


    import math
    import torch
    from onnx_diagnostic import doc
    from onnx_diagnostic.torch_export_patches import torch_export_patches


    class Model(torch.nn.Module):
        def dim(self, i, divisor):
            return int(math.ceil(i / divisor))  # noqa: RUF046

        def forward(self, x):
            new_shape = (self.dim(x.shape[0], 8), x.shape[1])
            return torch.zeros(new_shape)


    model = Model()
    x = torch.rand((10, 15))
    y = model(x)
    print(f"x.shape={x.shape}, y.shape={y.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    x.shape=torch.Size([10, 15]), y.shape=torch.Size([2, 15])




.. GENERATED FROM PYTHON SOURCE LINES 37-39

Export
++++++

.. GENERATED FROM PYTHON SOURCE LINES 39-44

.. code-block:: Python


    DYN = torch.export.Dim.DYNAMIC
    ep = torch.export.export(model, (x,), dynamic_shapes=(({0: DYN, 1: DYN}),))
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s77, s27]"):
                 # 
                sym_size_int_3: "Sym(s27)" = torch.ops.aten.sym_size.int(x, 1);  x = None
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/recipes/plot_dynamic_shapes_python_int.py:28 in forward, code: return torch.zeros(new_shape)
                zeros: "f32[2, s27]" = torch.ops.aten.zeros.default([2, sym_size_int_3], device = device(type='cpu'), pin_memory = False);  sym_size_int_3 = None
                return (zeros,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
    
        # outputs
        zeros: USER_OUTPUT
    
    Range constraints: {s77: VR[2, int_oo], s27: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 45-51

The last dimension became static. We must not use int.
:func:`math.ceil` should be avoided as well since it is a python operation.
The exporter may fail to detect it is operating on shapes.

Rewrite
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 51-66

.. code-block:: Python



    class RewrittenModel(torch.nn.Module):
        def dim(self, i, divisor):
            return (i + divisor - 1) // divisor

        def forward(self, x):
            new_shape = (self.dim(x.shape[0], 8), x.shape[1])
            return torch.zeros(new_shape)


    rewritten_model = RewrittenModel()
    y = rewritten_model(x)
    print(f"x.shape={x.shape}, y.shape={y.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    x.shape=torch.Size([10, 15]), y.shape=torch.Size([2, 15])




.. GENERATED FROM PYTHON SOURCE LINES 67-69

Export
++++++

.. GENERATED FROM PYTHON SOURCE LINES 69-74

.. code-block:: Python


    ep = torch.export.export(rewritten_model, (x,), dynamic_shapes=({0: DYN, 1: DYN},))
    print(ep)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s77, s27]"):
                 # 
                sym_size_int_2: "Sym(s77)" = torch.ops.aten.sym_size.int(x, 0)
                sym_size_int_3: "Sym(s27)" = torch.ops.aten.sym_size.int(x, 1);  x = None
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/recipes/plot_dynamic_shapes_python_int.py:58 in forward, code: new_shape = (self.dim(x.shape[0], 8), x.shape[1])
                add: "Sym(s77 + 8)" = sym_size_int_2 + 8;  sym_size_int_2 = None
                sub: "Sym(s77 + 7)" = add - 1;  add = None
                floordiv: "Sym(((s77 + 7)//8))" = sub // 8;  sub = None
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/recipes/plot_dynamic_shapes_python_int.py:59 in forward, code: return torch.zeros(new_shape)
                zeros: "f32[((s77 + 7)//8), s27]" = torch.ops.aten.zeros.default([floordiv, sym_size_int_3], device = device(type='cpu'), pin_memory = False);  floordiv = sym_size_int_3 = None
                return (zeros,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
    
        # outputs
        zeros: USER_OUTPUT
    
    Range constraints: {s77: VR[2, int_oo], s27: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 75-81

Find the error
++++++++++++++

Function :func:`onnx_diagnostic.torch_export_patches.torch_export_patches`
has a parameter ``stop_if_static`` which patches torch to raise exception
when something like that is happening.

.. GENERATED FROM PYTHON SOURCE LINES 81-87

.. code-block:: Python



    with torch_export_patches(stop_if_static=True):
        ep = torch.export.export(model, (x,), dynamic_shapes=({0: DYN, 1: DYN},))
        print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s77, s27]"):
                 # 
                sym_size_int_3: "Sym(s27)" = torch.ops.aten.sym_size.int(x, 1);  x = None
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/recipes/plot_dynamic_shapes_python_int.py:28 in forward, code: return torch.zeros(new_shape)
                zeros: "f32[2, s27]" = torch.ops.aten.zeros.default([2, sym_size_int_3], device = device(type='cpu'), pin_memory = False);  sym_size_int_3 = None
                return (zeros,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
    
        # outputs
        zeros: USER_OUTPUT
    
    Range constraints: {s77: VR[2, int_oo], s27: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 88-89

.. code-block:: Python

    doc.plot_legend("dynamic shapes\ndo not cast to\npython int", "dynamic shapes", "yellow")



.. image-sg:: /auto_recipes/images/sphx_glr_plot_dynamic_shapes_python_int_001.png
   :alt: plot dynamic shapes python int
   :srcset: /auto_recipes/images/sphx_glr_plot_dynamic_shapes_python_int_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.223 seconds)


.. _sphx_glr_download_auto_recipes_plot_dynamic_shapes_python_int.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_dynamic_shapes_python_int.ipynb <plot_dynamic_shapes_python_int.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_dynamic_shapes_python_int.py <plot_dynamic_shapes_python_int.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_dynamic_shapes_python_int.zip <plot_dynamic_shapes_python_int.zip>`


.. include:: plot_dynamic_shapes_python_int.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
