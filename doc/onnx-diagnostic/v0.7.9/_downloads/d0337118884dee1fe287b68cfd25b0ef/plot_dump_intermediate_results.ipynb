{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Dumps intermediate results of a torch model\n\nLooking for discrepancies is quickly annoying. Discrepancies\ncome from two results obtained with the same models\nimplemented in two different ways, :epkg:`pytorch` and :epkg:`onnx`.\nModels are big so where do they come from? That's the\nunavoidable question. Unless there is an obvious reason,\nthe only way is to compare intermediate outputs alon the computation.\nThe first step into that direction is to dump the intermediate results\ncoming from :epkg:`pytorch`.\nWe use :func:`onnx_diagnostic.helpers.torch_helper.steal_forward` for that.\n\n## A simple LLM Model\n\nSee :func:`onnx_diagnostic.helpers.torch_helper.dummy_llm`\nfor its definition. It is mostly used for unit test or example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas\nimport onnx\nimport torch\nimport onnxruntime\nfrom onnx_array_api.plotting.graphviz_helper import plot_dot\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import max_diff, string_diff, string_type\nfrom onnx_diagnostic.helpers.torch_helper import dummy_llm, steal_forward\nfrom onnx_diagnostic.helpers.mini_onnx_builder import create_input_tensors_from_onnx_model\nfrom onnx_diagnostic.reference import OnnxruntimeEvaluator, ReportResultComparison\n\n\nmodel, inputs, ds = dummy_llm(dynamic_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use float16.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = model.to(torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"type(model)={type(model)}\")\nprint(f\"inputs={string_type(inputs, with_shape=True)}\")\nprint(f\"ds={string_type(ds, with_shape=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It contains the following submodules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for name, mod in model.named_modules():\n    print(f\"- {name}: {type(mod)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Steal and dump the output of submodules\n\nThe following context spies on the intermediate results\nfor the following module and submodules. It stores\nin one onnx file all the input/output for those.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with steal_forward(\n    [\n        (\"model\", model),\n        (\"model.decoder\", model.decoder),\n        (\"model.decoder.attention\", model.decoder.attention),\n        (\"model.decoder.feed_forward\", model.decoder.feed_forward),\n        (\"model.decoder.norm_1\", model.decoder.norm_1),\n        (\"model.decoder.norm_2\", model.decoder.norm_2),\n    ],\n    dump_file=\"plot_dump_intermediate_results.inputs.onnx\",\n    verbose=1,\n    storage_limit=2**28,\n):\n    expected = model(*inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Restores saved inputs/outputs\n\nAll the intermediate tensors were saved in one unique onnx model,\nevery tensor is stored in a constant node.\nThe model can be run with any runtime to restore the inputs\nand function :func:`create_input_tensors_from_onnx_model\n<onnx_diagnostic.helpers.mini_onnx_builder.create_input_tensors_from_onnx_model>`\ncan restore their names.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "saved_tensors = create_input_tensors_from_onnx_model(\n    \"plot_dump_intermediate_results.inputs.onnx\"\n)\nfor k, v in saved_tensors.items():\n    print(f\"{k} -- {string_type(v, with_shape=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's explained the naming convention.\n\n::\n\n   ('model.decoder.norm_2', 0, 'I') -- ((T1s2x30x16,),{})\n               |            |   |\n               |            |   +--> input, the format is args, kwargs\n               |            |\n               |            +--> iteration, 0 means the first time the execution\n               |                 went through that module\n               |                 it is possible to call multiple times,\n               |                 the model to store more\n               |\n               +--> the name given to function steal_forward\n\nThe same goes for output except ``'I'`` is replaced by ``'O'``.\n\n::\n\n   ('model.decoder.norm_2', 0, 'O') -- T1s2x30x16\n\nThis trick can be used to compare intermediate results coming\nfrom pytorch to any other implementation of the same model\nas long as it is possible to map the stored inputs/outputs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversion to ONNX\n\nThe difficult point is to be able to map the saved intermediate\nresults to intermediate results in ONNX.\nLet's create the ONNX model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = torch.export.export(model, inputs, dynamic_shapes=ds)\nepo = torch.onnx.export(ep, dynamo=True)\nepo.optimize()\nepo.save(\"plot_dump_intermediate_results.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discrepancies\n\nWe have a torch model, intermediate results and an ONNX graph\nequivalent to the torch model.\nLet's see how we can check the discrepancies.\nFirst the discrepancies of the whole model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = onnxruntime.InferenceSession(\n    \"plot_dump_intermediate_results.onnx\", providers=[\"CPUExecutionProvider\"]\n)\nfeeds = dict(\n    zip([i.name for i in sess.get_inputs()], [t.detach().cpu().numpy() for t in inputs])\n)\ngot = sess.run(None, feeds)\ndiff = max_diff(expected, got)\nprint(f\"discrepancies torch/ORT: {string_diff(diff)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What about intermediate results?\nLet's use a runtime still based on :epkg:`onnxruntime`\nrunning an eager evaluation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess_eager = OnnxruntimeEvaluator(\n    \"plot_dump_intermediate_results.onnx\",\n    providers=[\"CPUExecutionProvider\"],\n    torch_or_numpy=True,\n)\nfeeds_tensor = dict(zip([i.name for i in sess.get_inputs()], inputs))\ngot = sess_eager.run(None, feeds_tensor)\ndiff = max_diff(expected, got)\nprint(f\"discrepancies torch/eager ORT: {string_diff(diff)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They are almost the same. That's good.\nLet's now dig into the intermediate results.\nThey are compared to the outputs stored in saved_tensors\nduring the execution of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baseline = {}\nfor k, v in saved_tensors.items():\n    if k[-1] == \"I\":  # inputs are excluded\n        continue\n    if isinstance(v, torch.Tensor):\n        baseline[f\"{k[0]}.{k[1]}\".replace(\"model.decoder\", \"decoder\")] = v\n\nreport_cmp = ReportResultComparison(baseline)\nsess_eager.run(None, feeds_tensor, report_cmp=report_cmp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = report_cmp.data\ndf = pandas.DataFrame(data)\npiv = df.pivot(index=(\"run_index\", \"run_name\"), columns=\"ref_name\", values=\"abs\")\nprint(piv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's clean a little bit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv[piv >= 1] = np.nan\nprint(piv.dropna(axis=0, how=\"all\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can identity which results is mapped to which expected tensor.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Picture of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = onnx.load(\"plot_dump_intermediate_results.onnx\")\nplot_dot(onx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"steal and dump\\nintermediate\\nresults\", \"steal_forward\", \"blue\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}