{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export Tiny-LLM with patches\n\nMany models from :epkg:`transformers` cannot be converted because\nthe implementation uses cache classes. Let's see how to get around that.\nWe focus on the model :epkg:`arnir0/Tiny-LLM`.\nTo avoid downloading any weights, we write a function creating a\nrandom model based on the same architecture.\nThis continues example `l-plot-tiny-llm-export`.\n\n## Errors\n\nThey depend on transformers version.\n\n``transformers>=4.40,<4.50`` cannot serialize DynamicCache and cannot\nmap dynamic shapes to instances of DynamicCache. The following errors\nwould appear:\n\n::\n\n  torch._dynamo.exc.UserError: Cannot associate shape\n      [[{0: <class '....batch'>, 2: <class '....cache_length'>}],\n       [{0: <class '....batch'>, 2: <class '....cache_length'>}]]\n      specified at `dynamic_shapes['past_key_values']`\n      to non-tensor type <class 'transformers.cache_utils.DynamicCache'>\n      at `inputs['past_key_values']` (expected None)\n  For more information about this error,\n  see: https://docs.pytorch.org/docs/stable/generated/exportdb/index.html#dynamic-shapes-validation\n\nWith ``transformers==4.50``, it shows the following:\n\n::\n\n  torch._dynamo.exc.UserError: Constraints violated (batch)!\n  For more information, run with TORCH_LOGS=\"+dynamic\".\n      - Not all values of batch = L['args'][1]['input_ids'].size()[0]\n          in the specified range batch <= 1024 are valid\n          because batch was inferred to be a constant (2).\n      - Not all values of batch = L['args'][1]['attention_mask'].size()[0]\n          in the specified range batch <= 1024 are valid\n          because batch was inferred to be a constant (2).\n      - Not all values of batch = L['args'][1]['past_key_values']['key_cache'][0].size()[0]\n          in the specified range batch <= 1024 are valid\n          because batch was inferred to be a constant (2).\n      - Not all values of batch = L['args'][1]['past_key_values']['value_cache'][0].size()[0]\n          in the specified range batch <= 1024 are valid\n          because batch was inferred to be a constant (2).\n   Suggested fixes:\n       batch = 2\n\nHowever, this package implements a patch mechanism\nwith replaces the part causing these issues.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>restart after an export failure\n\n    If the export fails, it is better to start executing again,\n    or restart the kernel if you are in the notebook.\n    The export may leave :epkg:`torch` in one unstable state.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nimport pprint\nimport torch\nimport transformers\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers.cache_helper import is_cache_dynamic_registered\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.torch_export_patches import torch_export_patches\nfrom onnx_diagnostic.torch_export_patches.patch_inputs import use_dyn_not_str\nfrom onnx_diagnostic.torch_models.llms import get_tiny_llm\n\n\nexperiment = get_tiny_llm()\nuntrained_model, inputs, dynamic_shapes = (\n    experiment[\"model\"],\n    experiment[\"inputs\"],\n    experiment[\"dynamic_shapes\"],\n)\n\ncloned_inputs = copy.deepcopy(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's show this inputs, this was inferred in\nexample `l-plot-tiny-llm-export`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(string_type(inputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the dynamic shapes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint.pprint(dynamic_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before exporting, we check :class:`transformers.cache_utils.DynamicCache`\ncan serialized and deserialized otherwise :func:`torch.export.export`\nfails.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"-- DynamicCache registered: \", is_cache_dynamic_registered())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If they are not registered, function\n:func:`onnx_diagnostic.torch_export_patches.torch_export_patches`\nshould take care of it. Then we export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch_export_patches(patch_transformers=True, verbose=10) as modificator:\n    assert is_cache_dynamic_registered()  # it must be true here\n    ep = torch.export.export(\n        untrained_model,\n        (),\n        kwargs=modificator(cloned_inputs),\n        dynamic_shapes=use_dyn_not_str(dynamic_shapes),\n        strict=False,  # mandatory for torch==2.6\n    )\n    print(\"It worked:\")\n    print(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With the original model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\ncloned_inputs = copy.deepcopy(inputs)\n\nwith torch_export_patches(patch_transformers=True, verbose=10) as modificator:\n    ep = torch.export.export(\n        model,\n        (),\n        kwargs=modificator(cloned_inputs),\n        dynamic_shapes=use_dyn_not_str(dynamic_shapes),\n        strict=False,  # mandatory for torch==2.6\n    )\n    print(\"It worked:\")\n    print(ep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"Tiny-LLM patched\", \"torch.export.export\", \"green\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}