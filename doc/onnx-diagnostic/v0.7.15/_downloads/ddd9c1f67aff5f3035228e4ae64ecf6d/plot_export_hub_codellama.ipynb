{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Test the export on untrained models\n\nChecking the exporter on a whole model takes time as it is\nusually big but we can create a smaller version with\nthe same architecture. Then fix export issues on such a\nsmall model is faster.\n\n## codellama/CodeLlama-7b-Python-hf\n\nLet's grab some information about this model.\nThis reuses :epkg:`huggingface_hub` API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nimport pprint\nimport torch\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.ext_test_case import unit_test_going\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.torch_models.hghub import (\n    get_untrained_model_with_inputs,\n)\nfrom onnx_diagnostic.torch_models.hghub.hub_api import (\n    get_model_info,\n    get_pretrained_config,\n    task_from_id,\n)\nfrom onnx_diagnostic.torch_export_patches import torch_export_patches\nfrom onnx_diagnostic.torch_export_patches.patch_inputs import use_dyn_not_str\n\nmodel_id = (\n    \"HuggingFaceM4/tiny-random-idefics\"\n    if unit_test_going()\n    else \"codellama/CodeLlama-7b-Python-hf\"\n)\nprint(f\"model_id={model_id!r}\")\nprint(\"info\", get_model_info(model_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The configuration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"config\", get_pretrained_config(model_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The task determines the set of inputs which needs\nto be created for this input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"task\", task_from_id(model_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Untrained model\n\nThe function :func:`get_untrained_model_with_inputs\n<onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs>`.\nIt loads the pretrained configuration, extracts the task associated\nto the model and them creates random inputs and dynamic shapes\nfor :func:`torch.export.export`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = get_untrained_model_with_inputs(model_id, verbose=1)\nprint(\"model size:\", data[\"size\"])\nprint(\"number of weights:\", data[\"n_weights\"])\nprint(\"fields:\", set(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inputs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"inputs:\", string_type(data[\"inputs\"], with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dynamic Shapes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"dynamic shapes:\", pprint.pformat(data[\"dynamic_shapes\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the model runs. We still needs to\ncopy the inputs before using the models, the cache\nis usually modified inplace.\nExpected outputs can be used later to compute\ndiscrepancies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs_copy = copy.deepcopy(data[\"inputs\"])\nmodel = data[\"model\"]\nexpected_outputs = model(**inputs_copy)\n\nprint(\"outputs:\", string_type(expected_outputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works.\n\n## Export\n\nThe model uses :class:`transformers.cache_utils.DynamicCache`.\nIt still requires patches to be exportable (control flow).\nSee :func:`onnx_diagnostic.torch_export_patches.torch_export_patches`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch_export_patches(patch_torch=False, patch_transformers=True) as f:\n    ep = torch.export.export(\n        model,\n        (),\n        kwargs=f(data[\"inputs\"]),\n        dynamic_shapes=use_dyn_not_str(data[\"dynamic_shapes\"]),\n        strict=False,\n    )\n    print(ep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\n    \"untrained\\ncodellama/\\nCodeLlama-7b-Python-hf\", \"torch.export.export\", \"tomato\"\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}