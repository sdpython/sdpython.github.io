
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_tiny_llm_dim01.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_tiny_llm_dim01.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_tiny_llm_dim01.py:


.. _l-plot-tiny-llm-export-dim01:

Export with dynamic dimensions in ``{0,1}``
===========================================

The first version of :func:`torch.export.export` did not support
any tensor with a dimension equal to 0, 1 if the dimension was expected
to be dynamic. The latest versions offers more options. Let's check it works.
The experiments consists in exporting the model with different sets of inputs
and checking the exported models works with all set of inputs.

Available input sets
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 17-37

.. code-block:: Python


    import itertools
    from tqdm import tqdm
    import numpy as np
    import pandas
    import torch
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers import max_diff, string_type
    from onnx_diagnostic.helpers.torch_helper import torch_deepcopy
    from onnx_diagnostic.torch_models.hghub.model_inputs import get_untrained_model_with_inputs
    from onnx_diagnostic.torch_export_patches.patch_inputs import use_dyn_not_str
    from onnx_diagnostic.torch_export_patches import (
        torch_export_patches,
        register_additional_serialization_functions,
    )


    data = get_untrained_model_with_inputs("arnir0/Tiny-LLM", add_second_input=True)
    model, dynamic_shapes = data["model"], data["dynamic_shapes"]








.. GENERATED FROM PYTHON SOURCE LINES 38-45

The trained model can be obtained with:

.. code-block:: python

  MODEL_NAME = "arnir0/Tiny-LLM"
  tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)
  model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)

.. GENERATED FROM PYTHON SOURCE LINES 45-51

.. code-block:: Python


    input_sets = {k: v for k, v in data.items() if k.startswith("inputs")}

    for k, v in input_sets.items():
        print(f"{k:20}: {string_type(v, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    inputs              : dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x30x96], value_cache=#1[T1s2x1x30x96]))
    inputs2             : dict(input_ids:T7s3x4,attention_mask:T7s3x35,position_ids:T7s3x4,past_key_values:DynamicCache(key_cache=#1[T1s3x1x31x96], value_cache=#1[T1s3x1x31x96]))
    inputs_empty_cache  : dict(input_ids:T7s2x3,attention_mask:T7s2x3,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x0x96], value_cache=#1[T1s2x1x0x96]))
    inputs_batch1       : dict(input_ids:T7s1x3,attention_mask:T7s1x33,position_ids:T7s1x3,past_key_values:DynamicCache(key_cache=#1[T1s1x1x30x96], value_cache=#1[T1s1x1x30x96]))




.. GENERATED FROM PYTHON SOURCE LINES 52-53

The dynamic shapes are:

.. GENERATED FROM PYTHON SOURCE LINES 53-56

.. code-block:: Python


    print(f"dynamic_shapes: {string_type(dynamic_shapes)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dynamic_shapes: dict(input_ids:{0:DYN(batch),1:DYN(seq_length)},attention_mask:{0:DYN(batch),1:DYN(cache+seq)},position_ids:{0:DYN(batch),1:DYN(cache+seq)},past_key_values:#2[#1[{0:DYN(batch),2:DYN(cache_length)}],#1[{0:DYN(batch),2:DYN(cache_length)}]])




.. GENERATED FROM PYTHON SOURCE LINES 57-61

.. code-block:: Python


    dynamic_shapes = use_dyn_not_str(dynamic_shapes)
    print(f"dynamic_shapes: {string_type(dynamic_shapes)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dynamic_shapes: dict(input_ids:{0:DYNAMIC,1:DYNAMIC},attention_mask:{0:DYNAMIC,1:DYNAMIC},position_ids:{0:DYNAMIC,1:DYNAMIC},past_key_values:#2[#1[{0:DYNAMIC,2:DYNAMIC}],#1[{0:DYNAMIC,2:DYNAMIC}]])




.. GENERATED FROM PYTHON SOURCE LINES 62-64

Let's check they all work and compute the expected values.
We use deepcopy because caches are usually modified inplace.

.. GENERATED FROM PYTHON SOURCE LINES 64-70

.. code-block:: Python


    expected = {}
    for k, v in input_sets.items():
        expected[k] = model(**torch_deepcopy(v))
        print(f"{k:20}: {string_type(expected[k], with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    inputs              : CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s2x1x33x96], value_cache=#1[T1s2x1x33x96]))
    inputs2             : CausalLMOutputWithPast(logits:T1s3x4x32000,past_key_values:DynamicCache(key_cache=#1[T1s3x1x35x96], value_cache=#1[T1s3x1x35x96]))
    inputs_empty_cache  : CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s2x1x3x96], value_cache=#1[T1s2x1x3x96]))
    inputs_batch1       : CausalLMOutputWithPast(logits:T1s1x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x33x96], value_cache=#1[T1s1x1x33x96]))




.. GENERATED FROM PYTHON SOURCE LINES 71-87

Export with options
+++++++++++++++++++

We try to export with the following options:

- cache registration: register cache serialization with
  :func:`onnx_diagnostic.torch_export_patches.register_additional_serialization_functions`

- oblivious: an option to remove some the exception raises by the exporter

- rt: see ``prefer_deferred_runtime_asserts_over_guards`` in :func:`torch.export.export`

- cache_patch: patches the model before exporting with
  :func:`onnx_diagnostic.torch_export_patches.torch_export_patches`

Some function first.

.. GENERATED FROM PYTHON SOURCE LINES 87-161

.. code-block:: Python



    def export_model(
        model,
        dynamic_shapes,
        inputs,
        cache=False,
        oblivious=False,
        rt=False,
        cache_patch=False,
        strict=False,
    ):
        if cache and not cache_patch:
            with register_additional_serialization_functions(patch_transformers=True):
                return export_model(
                    model, dynamic_shapes, inputs, oblivious=oblivious, rt=rt, strict=strict
                )
        if cache_patch:
            with torch_export_patches(
                patch_torch=cache_patch in ("all", "torch", True, 1),
                patch_transformers=cache_patch in ("all", "transformers", True, 1),
            ):
                return export_model(
                    model, dynamic_shapes, inputs, oblivious=oblivious, rt=rt, strict=strict
                )
        if oblivious:
            with torch.fx.experimental._config.patch(backed_size_oblivious=True):
                return export_model(model, dynamic_shapes, inputs, rt=rt, strict=strict)
        return torch.export.export(
            model,
            (),
            inputs,
            dynamic_shapes=dynamic_shapes,
            strict=strict,
            prefer_deferred_runtime_asserts_over_guards=rt,
        )


    def try_export_model(
        model,
        dynamic_shapes,
        inputs,
        cache=False,
        oblivious=False,
        rt=False,
        cache_patch=False,
        strict=False,
    ):
        try:
            return export_model(
                model,
                dynamic_shapes,
                inputs,
                cache=cache,
                oblivious=oblivious,
                rt=rt,
                cache_patch=cache_patch,
                strict=strict,
            )
        except Exception as e:
            return e


    def validation(ep, input_sets, expected):
        mod = ep.module()
        for k, v in input_sets.items():
            try:
                got = mod(**torch_deepcopy(v))
            except Exception as e:
                yield k, e
                continue
            yield k, max_diff(expected[k], got, verbose=0)









.. GENERATED FROM PYTHON SOURCE LINES 162-164

The main loop
+++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 164-215

.. code-block:: Python


    results = []

    possibilities = [*[[0, 1] for _ in range(5)], list(input_sets)]
    possibilities[1] = [0, "all", "torch", "transformers"]
    with tqdm(list(itertools.product(*possibilities))) as pbar:
        for cache, cache_patch, strict, oblivious, rt, inputs in pbar:
            if cache_patch and not cache:
                # patches include caches.
                continue
            kwargs = dict(
                cache=cache, cache_patch=cache_patch, strict=strict, oblivious=oblivious, rt=rt
            )
            legend = "-".join(
                (k if isinstance(v, int) else f"{k}:{v}") for k, v in kwargs.items() if v
            )
            legend = f"{legend}/{inputs}"
            pbar.set_description(f"{legend} EXPORT")

            # export
            ep = try_export_model(
                model, dynamic_shapes, torch_deepcopy(input_sets[inputs]), **kwargs
            )
            if isinstance(ep, Exception):
                obs = {
                    **kwargs,
                    "export_with": inputs,
                    "EXPORT": 0,
                    "ERR-EXPORT": str(ep).split("\n")[0],
                }
                results.append(obs)
                continue

            pbar.set_description(f"{legend} VALIDATE")
            common = {**kwargs, "export_with": inputs, "EXPORT": 1}
            for inp, res in validation(ep, input_sets, expected):
                if isinstance(res, Exception):
                    obs = {
                        **common,
                        "run_with": inp,
                        "ERR-RUN": str(res).split("\n")[0],
                        "WORKS": 0,
                    }
                else:
                    obs = {
                        **common,
                        "run_with": inp,
                        "WORKS": int(~np.isnan(res["abs"]) and res["abs"] < 1e-3),
                    }
                results.append(obs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/256 [00:00<?, ?it/s]    /inputs EXPORT:   0%|          | 0/256 [00:00<?, ?it/s]    /inputs EXPORT:   0%|          | 1/256 [00:00<01:14,  3.44it/s]    /inputs2 EXPORT:   0%|          | 1/256 [00:00<01:14,  3.44it/s]    /inputs2 EXPORT:   1%|          | 2/256 [00:00<01:05,  3.88it/s]    /inputs_empty_cache EXPORT:   1%|          | 2/256 [00:00<01:05,  3.88it/s]    /inputs_empty_cache EXPORT:   1%|          | 3/256 [00:00<01:01,  4.14it/s]    /inputs_batch1 EXPORT:   1%|          | 3/256 [00:00<01:01,  4.14it/s]         /inputs_batch1 EXPORT:   2%|▏         | 4/256 [00:00<01:01,  4.10it/s]    rt/inputs EXPORT:   2%|▏         | 4/256 [00:00<01:01,  4.10it/s]         rt/inputs EXPORT:   2%|▏         | 5/256 [00:01<00:57,  4.38it/s]    rt/inputs2 EXPORT:   2%|▏         | 5/256 [00:01<00:57,  4.38it/s]    rt/inputs2 EXPORT:   2%|▏         | 6/256 [00:01<00:57,  4.33it/s]    rt/inputs_empty_cache EXPORT:   2%|▏         | 6/256 [00:01<00:57,  4.33it/s]    rt/inputs_empty_cache EXPORT:   3%|▎         | 7/256 [00:01<00:52,  4.78it/s]    rt/inputs_batch1 EXPORT:   3%|▎         | 7/256 [00:01<00:52,  4.78it/s]         rt/inputs_batch1 EXPORT:   3%|▎         | 8/256 [00:01<00:53,  4.66it/s]    oblivious/inputs EXPORT:   3%|▎         | 8/256 [00:01<00:53,  4.66it/s]    oblivious/inputs EXPORT:   4%|▎         | 9/256 [00:02<00:53,  4.61it/s]    oblivious/inputs2 EXPORT:   4%|▎         | 9/256 [00:02<00:53,  4.61it/s]    oblivious/inputs2 EXPORT:   4%|▍         | 10/256 [00:02<00:52,  4.66it/s]    oblivious/inputs_empty_cache EXPORT:   4%|▍         | 10/256 [00:02<00:52,  4.66it/s]    oblivious/inputs_empty_cache EXPORT:   4%|▍         | 11/256 [00:02<00:52,  4.67it/s]    oblivious/inputs_batch1 EXPORT:   4%|▍         | 11/256 [00:02<00:52,  4.67it/s]         oblivious/inputs_batch1 EXPORT:   5%|▍         | 12/256 [00:02<00:51,  4.74it/s]    oblivious-rt/inputs EXPORT:   5%|▍         | 12/256 [00:02<00:51,  4.74it/s]        oblivious-rt/inputs EXPORT:   5%|▌         | 13/256 [00:02<00:50,  4.79it/s]    oblivious-rt/inputs2 EXPORT:   5%|▌         | 13/256 [00:02<00:50,  4.79it/s]    oblivious-rt/inputs2 EXPORT:   5%|▌         | 14/256 [00:03<00:49,  4.85it/s]    oblivious-rt/inputs_empty_cache EXPORT:   5%|▌         | 14/256 [00:03<00:49,  4.85it/s]    oblivious-rt/inputs_empty_cache EXPORT:   6%|▌         | 15/256 [00:03<00:49,  4.86it/s]    oblivious-rt/inputs_batch1 EXPORT:   6%|▌         | 15/256 [00:03<00:49,  4.86it/s]         oblivious-rt/inputs_batch1 EXPORT:   6%|▋         | 16/256 [00:03<00:49,  4.89it/s]    strict/inputs EXPORT:   6%|▋         | 16/256 [00:03<00:49,  4.89it/s]                 strict/inputs EXPORT:   7%|▋         | 17/256 [00:04<01:25,  2.81it/s]    strict/inputs2 EXPORT:   7%|▋         | 17/256 [00:04<01:25,  2.81it/s]    strict/inputs2 EXPORT:   7%|▋         | 18/256 [00:04<01:36,  2.47it/s]    strict/inputs_empty_cache EXPORT:   7%|▋         | 18/256 [00:04<01:36,  2.47it/s]    strict/inputs_empty_cache EXPORT:   7%|▋         | 19/256 [00:05<01:43,  2.30it/s]    strict/inputs_batch1 EXPORT:   7%|▋         | 19/256 [00:05<01:43,  2.30it/s]         strict/inputs_batch1 EXPORT:   8%|▊         | 20/256 [00:05<01:48,  2.18it/s]    strict-rt/inputs EXPORT:   8%|▊         | 20/256 [00:05<01:48,  2.18it/s]        strict-rt/inputs EXPORT:   8%|▊         | 21/256 [00:06<01:48,  2.18it/s]    strict-rt/inputs2 EXPORT:   8%|▊         | 21/256 [00:06<01:48,  2.18it/s]    strict-rt/inputs2 EXPORT:   9%|▊         | 22/256 [00:06<01:42,  2.28it/s]    strict-rt/inputs_empty_cache EXPORT:   9%|▊         | 22/256 [00:06<01:42,  2.28it/s]    strict-rt/inputs_empty_cache EXPORT:   9%|▉         | 23/256 [00:07<02:15,  1.72it/s]    strict-rt/inputs_batch1 EXPORT:   9%|▉         | 23/256 [00:07<02:15,  1.72it/s]         strict-rt/inputs_batch1 EXPORT:   9%|▉         | 24/256 [00:07<02:02,  1.90it/s]    strict-oblivious/inputs EXPORT:   9%|▉         | 24/256 [00:07<02:02,  1.90it/s]    strict-oblivious/inputs EXPORT:  10%|▉         | 25/256 [00:08<01:55,  2.01it/s]    strict-oblivious/inputs2 EXPORT:  10%|▉         | 25/256 [00:08<01:55,  2.01it/s]    strict-oblivious/inputs2 EXPORT:  10%|█         | 26/256 [00:08<01:50,  2.09it/s]    strict-oblivious/inputs_empty_cache EXPORT:  10%|█         | 26/256 [00:08<01:50,  2.09it/s]    strict-oblivious/inputs_empty_cache EXPORT:  11%|█         | 27/256 [00:09<01:59,  1.92it/s]    strict-oblivious/inputs_batch1 EXPORT:  11%|█         | 27/256 [00:09<01:59,  1.92it/s]         strict-oblivious/inputs_batch1 EXPORT:  11%|█         | 28/256 [00:10<02:07,  1.79it/s]    strict-oblivious-rt/inputs EXPORT:  11%|█         | 28/256 [00:10<02:07,  1.79it/s]        strict-oblivious-rt/inputs EXPORT:  11%|█▏        | 29/256 [00:10<02:06,  1.79it/s]    strict-oblivious-rt/inputs2 EXPORT:  11%|█▏        | 29/256 [00:10<02:06,  1.79it/s]    strict-oblivious-rt/inputs2 EXPORT:  12%|█▏        | 30/256 [00:11<02:03,  1.84it/s]    strict-oblivious-rt/inputs_empty_cache EXPORT:  12%|█▏        | 30/256 [00:11<02:03,  1.84it/s]    strict-oblivious-rt/inputs_empty_cache EXPORT:  12%|█▏        | 31/256 [00:11<02:05,  1.79it/s]    strict-oblivious-rt/inputs_batch1 EXPORT:  12%|█▏        | 31/256 [00:11<02:05,  1.79it/s]         strict-oblivious-rt/inputs_batch1 EXPORT:  12%|█▎        | 32/256 [00:12<01:56,  1.92it/s]    cache/inputs EXPORT:  12%|█▎        | 32/256 [00:12<01:56,  1.92it/s]                         cache/inputs EXPORT:  50%|█████     | 129/256 [00:12<00:01, 68.59it/s]    cache/inputs2 EXPORT:  50%|█████     | 129/256 [00:12<00:01, 68.59it/s]    cache/inputs_empty_cache EXPORT:  50%|█████     | 129/256 [00:12<00:01, 68.59it/s]    cache/inputs_batch1 EXPORT:  50%|█████     | 129/256 [00:12<00:01, 68.59it/s]         cache-rt/inputs EXPORT:  50%|█████     | 129/256 [00:13<00:01, 68.59it/s]        cache-rt/inputs2 EXPORT:  50%|█████     | 129/256 [00:13<00:01, 68.59it/s]    cache-rt/inputs_empty_cache EXPORT:  50%|█████     | 129/256 [00:13<00:01, 68.59it/s]    cache-rt/inputs_batch1 EXPORT:  50%|█████     | 129/256 [00:13<00:01, 68.59it/s]         cache-oblivious/inputs EXPORT:  50%|█████     | 129/256 [00:13<00:01, 68.59it/s]    cache-oblivious/inputs2 EXPORT:  50%|█████     | 129/256 [00:14<00:01, 68.59it/s]    cache-oblivious/inputs_empty_cache EXPORT:  50%|█████     | 129/256 [00:14<00:01, 68.59it/s]    cache-oblivious/inputs_batch1 EXPORT:  50%|█████     | 129/256 [00:14<00:01, 68.59it/s]         cache-oblivious-rt/inputs EXPORT:  50%|█████     | 129/256 [00:14<00:01, 68.59it/s]        cache-oblivious-rt/inputs2 EXPORT:  50%|█████     | 129/256 [00:14<00:01, 68.59it/s]    cache-oblivious-rt/inputs2 EXPORT:  55%|█████▌    | 142/256 [00:15<00:05, 21.95it/s]    cache-oblivious-rt/inputs_empty_cache EXPORT:  55%|█████▌    | 142/256 [00:15<00:05, 21.95it/s]    cache-oblivious-rt/inputs_batch1 EXPORT:  55%|█████▌    | 142/256 [00:15<00:05, 21.95it/s]         cache-strict/inputs EXPORT:  55%|█████▌    | 142/256 [00:15<00:05, 21.95it/s]                 cache-strict/inputs2 EXPORT:  55%|█████▌    | 142/256 [00:15<00:05, 21.95it/s]    cache-strict/inputs_empty_cache EXPORT:  55%|█████▌    | 142/256 [00:15<00:05, 21.95it/s]    cache-strict/inputs_batch1 EXPORT:  55%|█████▌    | 142/256 [00:16<00:05, 21.95it/s]         cache-strict-rt/inputs EXPORT:  55%|█████▌    | 142/256 [00:17<00:05, 21.95it/s]        cache-strict-rt/inputs2 EXPORT:  55%|█████▌    | 142/256 [00:17<00:05, 21.95it/s]    cache-strict-rt/inputs_empty_cache EXPORT:  55%|█████▌    | 142/256 [00:18<00:05, 21.95it/s]    cache-strict-rt/inputs_batch1 EXPORT:  55%|█████▌    | 142/256 [00:18<00:05, 21.95it/s]         cache-strict-rt/inputs_batch1 EXPORT:  59%|█████▉    | 152/256 [00:19<00:10, 10.15it/s]    cache-strict-oblivious/inputs EXPORT:  59%|█████▉    | 152/256 [00:19<00:10, 10.15it/s]    cache-strict-oblivious/inputs2 EXPORT:  59%|█████▉    | 152/256 [00:19<00:10, 10.15it/s]    cache-strict-oblivious/inputs_empty_cache EXPORT:  59%|█████▉    | 152/256 [00:20<00:10, 10.15it/s]    cache-strict-oblivious/inputs_batch1 EXPORT:  59%|█████▉    | 152/256 [00:20<00:10, 10.15it/s]         cache-strict-oblivious-rt/inputs EXPORT:  59%|█████▉    | 152/256 [00:21<00:10, 10.15it/s]        cache-strict-oblivious-rt/inputs2 EXPORT:  59%|█████▉    | 152/256 [00:21<00:10, 10.15it/s]    cache-strict-oblivious-rt/inputs_empty_cache EXPORT:  59%|█████▉    | 152/256 [00:22<00:10, 10.15it/s]    cache-strict-oblivious-rt/inputs_empty_cache EXPORT:  62%|██████▏   | 159/256 [00:22<00:14,  6.60it/s]    cache-strict-oblivious-rt/inputs_batch1 EXPORT:  62%|██████▏   | 159/256 [00:22<00:14,  6.60it/s]         cache-cache_patch:all/inputs EXPORT:  62%|██████▏   | 159/256 [00:23<00:14,  6.60it/s]               cache-cache_patch:all/inputs VALIDATE:  62%|██████▏   | 159/256 [00:24<00:14,  6.60it/s]    cache-cache_patch:all/inputs2 EXPORT:  62%|██████▏   | 159/256 [00:24<00:14,  6.60it/s]     cache-cache_patch:all/inputs2 VALIDATE:  62%|██████▏   | 159/256 [00:25<00:14,  6.60it/s]    cache-cache_patch:all/inputs_empty_cache EXPORT:  62%|██████▏   | 159/256 [00:25<00:14,  6.60it/s]    cache-cache_patch:all/inputs_empty_cache VALIDATE:  62%|██████▏   | 159/256 [00:26<00:14,  6.60it/s]    cache-cache_patch:all/inputs_batch1 EXPORT:  62%|██████▏   | 159/256 [00:26<00:14,  6.60it/s]           cache-cache_patch:all/inputs_batch1 VALIDATE:  62%|██████▏   | 159/256 [00:27<00:14,  6.60it/s]    cache-cache_patch:all/inputs_batch1 VALIDATE:  64%|██████▍   | 164/256 [00:27<00:22,  4.16it/s]    cache-cache_patch:all-rt/inputs EXPORT:  64%|██████▍   | 164/256 [00:27<00:22,  4.16it/s]          cache-cache_patch:all-rt/inputs VALIDATE:  64%|██████▍   | 164/256 [00:28<00:22,  4.16it/s]    cache-cache_patch:all-rt/inputs2 EXPORT:  64%|██████▍   | 164/256 [00:28<00:22,  4.16it/s]     cache-cache_patch:all-rt/inputs2 VALIDATE:  64%|██████▍   | 164/256 [00:30<00:22,  4.16it/s]    cache-cache_patch:all-rt/inputs_empty_cache EXPORT:  64%|██████▍   | 164/256 [00:30<00:22,  4.16it/s]    cache-cache_patch:all-rt/inputs_empty_cache VALIDATE:  64%|██████▍   | 164/256 [00:31<00:22,  4.16it/s]    cache-cache_patch:all-rt/inputs_batch1 EXPORT:  64%|██████▍   | 164/256 [00:31<00:22,  4.16it/s]           cache-cache_patch:all-rt/inputs_batch1 VALIDATE:  64%|██████▍   | 164/256 [00:31<00:22,  4.16it/s]    cache-cache_patch:all-rt/inputs_batch1 VALIDATE:  66%|██████▌   | 168/256 [00:32<00:30,  2.86it/s]    cache-cache_patch:all-oblivious/inputs EXPORT:  66%|██████▌   | 168/256 [00:32<00:30,  2.86it/s]      cache-cache_patch:all-oblivious/inputs VALIDATE:  66%|██████▌   | 168/256 [00:33<00:30,  2.86it/s]    cache-cache_patch:all-oblivious/inputs2 EXPORT:  66%|██████▌   | 168/256 [00:33<00:30,  2.86it/s]     cache-cache_patch:all-oblivious/inputs2 VALIDATE:  66%|██████▌   | 168/256 [00:34<00:30,  2.86it/s]    cache-cache_patch:all-oblivious/inputs_empty_cache EXPORT:  66%|██████▌   | 168/256 [00:35<00:30,  2.86it/s]    cache-cache_patch:all-oblivious/inputs_empty_cache VALIDATE:  66%|██████▌   | 168/256 [00:36<00:30,  2.86it/s]    cache-cache_patch:all-oblivious/inputs_empty_cache VALIDATE:  67%|██████▋   | 171/256 [00:36<00:40,  2.12it/s]    cache-cache_patch:all-oblivious/inputs_batch1 EXPORT:  67%|██████▋   | 171/256 [00:36<00:40,  2.12it/s]           cache-cache_patch:all-oblivious/inputs_batch1 VALIDATE:  67%|██████▋   | 171/256 [00:37<00:40,  2.12it/s]    cache-cache_patch:all-oblivious-rt/inputs EXPORT:  67%|██████▋   | 171/256 [00:37<00:40,  2.12it/s]          cache-cache_patch:all-oblivious-rt/inputs VALIDATE:  67%|██████▋   | 171/256 [00:39<00:40,  2.12it/s]    cache-cache_patch:all-oblivious-rt/inputs VALIDATE:  68%|██████▊   | 173/256 [00:39<00:46,  1.77it/s]    cache-cache_patch:all-oblivious-rt/inputs2 EXPORT:  68%|██████▊   | 173/256 [00:39<00:46,  1.77it/s]     cache-cache_patch:all-oblivious-rt/inputs2 VALIDATE:  68%|██████▊   | 173/256 [00:40<00:46,  1.77it/s]    cache-cache_patch:all-oblivious-rt/inputs_empty_cache EXPORT:  68%|██████▊   | 173/256 [00:40<00:46,  1.77it/s]    cache-cache_patch:all-oblivious-rt/inputs_empty_cache VALIDATE:  68%|██████▊   | 173/256 [00:42<00:46,  1.77it/s]    cache-cache_patch:all-oblivious-rt/inputs_empty_cache VALIDATE:  68%|██████▊   | 175/256 [00:42<00:56,  1.43it/s]    cache-cache_patch:all-oblivious-rt/inputs_batch1 EXPORT:  68%|██████▊   | 175/256 [00:42<00:56,  1.43it/s]           cache-cache_patch:all-oblivious-rt/inputs_batch1 VALIDATE:  68%|██████▊   | 175/256 [00:43<00:56,  1.43it/s]    cache-cache_patch:all-oblivious-rt/inputs_batch1 VALIDATE:  69%|██████▉   | 176/256 [00:43<00:59,  1.34it/s]    cache-cache_patch:all-strict/inputs EXPORT:  69%|██████▉   | 176/256 [00:43<00:59,  1.34it/s]                   cache-cache_patch:all-strict/inputs EXPORT:  69%|██████▉   | 177/256 [00:46<01:10,  1.11it/s]    cache-cache_patch:all-strict/inputs2 EXPORT:  69%|██████▉   | 177/256 [00:46<01:10,  1.11it/s]    cache-cache_patch:all-strict/inputs2 EXPORT:  70%|██████▉   | 178/256 [00:47<01:10,  1.10it/s]    cache-cache_patch:all-strict/inputs_empty_cache EXPORT:  70%|██████▉   | 178/256 [00:47<01:10,  1.10it/s]    cache-cache_patch:all-strict/inputs_empty_cache EXPORT:  70%|██████▉   | 179/256 [00:48<01:17,  1.00s/it]    cache-cache_patch:all-strict/inputs_batch1 EXPORT:  70%|██████▉   | 179/256 [00:48<01:17,  1.00s/it]         cache-cache_patch:all-strict/inputs_batch1 EXPORT:  70%|███████   | 180/256 [00:56<02:44,  2.17s/it]    cache-cache_patch:all-strict-rt/inputs EXPORT:  70%|███████   | 180/256 [00:56<02:44,  2.17s/it]        cache-cache_patch:all-strict-rt/inputs EXPORT:  71%|███████   | 181/256 [01:00<03:09,  2.52s/it]    cache-cache_patch:all-strict-rt/inputs2 EXPORT:  71%|███████   | 181/256 [01:00<03:09,  2.52s/it]    cache-cache_patch:all-strict-rt/inputs2 EXPORT:  71%|███████   | 182/256 [01:04<03:38,  2.95s/it]    cache-cache_patch:all-strict-rt/inputs_empty_cache EXPORT:  71%|███████   | 182/256 [01:04<03:38,  2.95s/it]    cache-cache_patch:all-strict-rt/inputs_empty_cache EXPORT:  71%|███████▏  | 183/256 [01:08<03:49,  3.14s/it]    cache-cache_patch:all-strict-rt/inputs_batch1 EXPORT:  71%|███████▏  | 183/256 [01:08<03:49,  3.14s/it]         cache-cache_patch:all-strict-rt/inputs_batch1 EXPORT:  72%|███████▏  | 184/256 [01:11<03:49,  3.18s/it]    cache-cache_patch:all-strict-oblivious/inputs EXPORT:  72%|███████▏  | 184/256 [01:11<03:49,  3.18s/it]    cache-cache_patch:all-strict-oblivious/inputs EXPORT:  72%|███████▏  | 185/256 [01:13<03:21,  2.84s/it]    cache-cache_patch:all-strict-oblivious/inputs2 EXPORT:  72%|███████▏  | 185/256 [01:13<03:21,  2.84s/it]    cache-cache_patch:all-strict-oblivious/inputs2 EXPORT:  73%|███████▎  | 186/256 [01:15<02:56,  2.52s/it]    cache-cache_patch:all-strict-oblivious/inputs_empty_cache EXPORT:  73%|███████▎  | 186/256 [01:15<02:56,  2.52s/it]    cache-cache_patch:all-strict-oblivious/inputs_empty_cache EXPORT:  73%|███████▎  | 187/256 [01:16<02:33,  2.22s/it]    cache-cache_patch:all-strict-oblivious/inputs_batch1 EXPORT:  73%|███████▎  | 187/256 [01:16<02:33,  2.22s/it]         cache-cache_patch:all-strict-oblivious/inputs_batch1 EXPORT:  73%|███████▎  | 188/256 [01:19<02:49,  2.49s/it]    cache-cache_patch:all-strict-oblivious-rt/inputs EXPORT:  73%|███████▎  | 188/256 [01:19<02:49,  2.49s/it]        cache-cache_patch:all-strict-oblivious-rt/inputs EXPORT:  74%|███████▍  | 189/256 [01:23<03:16,  2.94s/it]    cache-cache_patch:all-strict-oblivious-rt/inputs2 EXPORT:  74%|███████▍  | 189/256 [01:23<03:16,  2.94s/it]    cache-cache_patch:all-strict-oblivious-rt/inputs2 EXPORT:  74%|███████▍  | 190/256 [01:28<03:52,  3.52s/it]    cache-cache_patch:all-strict-oblivious-rt/inputs_empty_cache EXPORT:  74%|███████▍  | 190/256 [01:28<03:52,  3.52s/it]    cache-cache_patch:all-strict-oblivious-rt/inputs_empty_cache EXPORT:  75%|███████▍  | 191/256 [01:32<03:55,  3.62s/it]    cache-cache_patch:all-strict-oblivious-rt/inputs_batch1 EXPORT:  75%|███████▍  | 191/256 [01:32<03:55,  3.62s/it]         cache-cache_patch:all-strict-oblivious-rt/inputs_batch1 EXPORT:  75%|███████▌  | 192/256 [01:36<03:56,  3.70s/it]    cache-cache_patch:torch/inputs EXPORT:  75%|███████▌  | 192/256 [01:36<03:56,  3.70s/it]                         


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch/inputs EXPORT:  75%|███████▌  | 193/256 [01:36<02:50,  2.70s/it]    cache-cache_patch:torch/inputs2 EXPORT:  75%|███████▌  | 193/256 [01:36<02:50,  2.70s/it]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch/inputs2 EXPORT:  76%|███████▌  | 194/256 [01:37<02:03,  2.00s/it]    cache-cache_patch:torch/inputs_empty_cache EXPORT:  76%|███████▌  | 194/256 [01:37<02:03,  2.00s/it]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, 0, 96]", arg17_1: "f32[s4, 1, 0, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, 0, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = cat = None
        cat_1: "f32[s4, 1, 0, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s70)" = 0 + sym_size_int;  sym_size_int = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(0, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s70)" = 0 + sym_size_int_1
        add_2: "Sym(s70)" = add_1 + 0
        sym_size_int_2: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(-s53 + s70)" = add_2 - sym_size_int_2;  add_2 = None
        gt: "Sym(-s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s70)" = sym_size_int_2 > add_1;  sym_size_int_2 = gt_1 = None
        arange_1: "i64[s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_3: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_3, device = device(type='cpu'), pin_memory = False);  sym_size_int_3 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
        unsqueeze_2: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_5: "Sym(s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, 0, 96]", arg17_1: "f32[s4, 1, 0, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, 0, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = cat = None
        cat_1: "f32[s4, 1, 0, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s70)" = 0 + sym_size_int;  sym_size_int = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(0, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s70)" = 0 + sym_size_int_1
        add_2: "Sym(s70)" = add_1 + 0
        sym_size_int_2: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(-s53 + s70)" = add_2 - sym_size_int_2;  add_2 = None
        gt: "Sym(-s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s70)" = sym_size_int_2 > add_1;  sym_size_int_2 = gt_1 = None
        arange_1: "i64[s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_3: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_3, device = device(type='cpu'), pin_memory = False);  sym_size_int_3 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
        unsqueeze_2: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_5: "Sym(s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None
    
    cache-cache_patch:torch/inputs_empty_cache EXPORT:  76%|███████▌  | 195/256 [01:37<01:29,  1.46s/it]    cache-cache_patch:torch/inputs_batch1 EXPORT:  76%|███████▌  | 195/256 [01:37<01:29,  1.46s/it]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[1, s70]", arg14_1: "i64[1, s53]", arg15_1: "i64[1, s9]", arg16_1: "f32[1, 1, s31, 96]", arg17_1: "f32[1, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[1, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[1, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[1, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[1, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        arange_2: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[1]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
        select: "i64[]" = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
        movedim_1: "i64[1]" = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
        select_1: "i64[]" = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
        movedim_2: "i64[s70]" = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
        unsqueeze: "i64[1]" = torch.ops.aten.unsqueeze.default(select, 0);  select = None
        expand: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1]" = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
        expand_1: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_4: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s70, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[1, s70]", arg14_1: "i64[1, s53]", arg15_1: "i64[1, s9]", arg16_1: "f32[1, 1, s31, 96]", arg17_1: "f32[1, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[1, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[1, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[1, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[1, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        arange_2: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[1]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
        select: "i64[]" = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
        movedim_1: "i64[1]" = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
        select_1: "i64[]" = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
        movedim_2: "i64[s70]" = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
        unsqueeze: "i64[1]" = torch.ops.aten.unsqueeze.default(select, 0);  select = None
        expand: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1]" = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
        expand_1: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_4: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s70, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None
    
    cache-cache_patch:torch/inputs_batch1 EXPORT:  77%|███████▋  | 196/256 [01:37<01:05,  1.09s/it]    cache-cache_patch:torch-rt/inputs EXPORT:  77%|███████▋  | 196/256 [01:37<01:05,  1.09s/it]    


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs EXPORT:  77%|███████▋  | 197/256 [01:37<00:49,  1.18it/s]    cache-cache_patch:torch-rt/inputs2 EXPORT:  77%|███████▋  | 197/256 [01:37<00:49,  1.18it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs2 EXPORT:  77%|███████▋  | 198/256 [01:38<00:39,  1.48it/s]    cache-cache_patch:torch-rt/inputs_empty_cache EXPORT:  77%|███████▋  | 198/256 [01:38<00:39,  1.48it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, 0, 96]", arg17_1: "f32[s4, 1, 0, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, 0, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = cat = None
        cat_1: "f32[s4, 1, 0, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s70)" = 0 + sym_size_int;  sym_size_int = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(0, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s70)" = 0 + sym_size_int_1
        add_2: "Sym(s70)" = add_1 + 0
        sym_size_int_2: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(-s53 + s70)" = add_2 - sym_size_int_2;  add_2 = None
        gt: "Sym(-s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s70)" = sym_size_int_2 > add_1;  sym_size_int_2 = gt_1 = None
        arange_1: "i64[s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_3: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_3, device = device(type='cpu'), pin_memory = False);  sym_size_int_3 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
        unsqueeze_2: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_5: "Sym(s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, 0, 96]", arg17_1: "f32[s4, 1, 0, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, 0, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = cat = None
        cat_1: "f32[s4, 1, 0, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s70)" = 0 + sym_size_int;  sym_size_int = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(0, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s70)" = 0 + sym_size_int_1
        add_2: "Sym(s70)" = add_1 + 0
        sym_size_int_2: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(-s53 + s70)" = add_2 - sym_size_int_2;  add_2 = None
        gt: "Sym(-s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s70)" = sym_size_int_2 > add_1;  sym_size_int_2 = gt_1 = None
        arange_1: "i64[s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_3: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_3, device = device(type='cpu'), pin_memory = False);  sym_size_int_3 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
        unsqueeze_2: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_5: "Sym(s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs_empty_cache EXPORT:  78%|███████▊  | 199/256 [01:38<00:30,  1.87it/s]    cache-cache_patch:torch-rt/inputs_batch1 EXPORT:  78%|███████▊  | 199/256 [01:38<00:30,  1.87it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[1, s70]", arg14_1: "i64[1, s53]", arg15_1: "i64[1, s9]", arg16_1: "f32[1, 1, s31, 96]", arg17_1: "f32[1, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[1, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[1, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[1, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[1, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        arange_2: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[1]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
        select: "i64[]" = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
        movedim_1: "i64[1]" = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
        select_1: "i64[]" = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
        movedim_2: "i64[s70]" = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
        unsqueeze: "i64[1]" = torch.ops.aten.unsqueeze.default(select, 0);  select = None
        expand: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1]" = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
        expand_1: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_4: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s70, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[1, s70]", arg14_1: "i64[1, s53]", arg15_1: "i64[1, s9]", arg16_1: "f32[1, 1, s31, 96]", arg17_1: "f32[1, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[1, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[1, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[1, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[1, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        arange_2: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[1]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
        select: "i64[]" = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
        movedim_1: "i64[1]" = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
        select_1: "i64[]" = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
        movedim_2: "i64[s70]" = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
        unsqueeze: "i64[1]" = torch.ops.aten.unsqueeze.default(select, 0);  select = None
        expand: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1]" = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
        expand_1: "i64[s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_4: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s70, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs_batch1 EXPORT:  78%|███████▊  | 200/256 [01:38<00:24,  2.31it/s]    cache-cache_patch:torch-oblivious/inputs EXPORT:  78%|███████▊  | 200/256 [01:38<00:24,  2.31it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs EXPORT:  79%|███████▊  | 201/256 [01:38<00:21,  2.57it/s]    cache-cache_patch:torch-oblivious/inputs2 EXPORT:  79%|███████▊  | 201/256 [01:38<00:21,  2.57it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs2 EXPORT:  79%|███████▉  | 202/256 [01:39<00:19,  2.74it/s]    cache-cache_patch:torch-oblivious/inputs_empty_cache EXPORT:  79%|███████▉  | 202/256 [01:39<00:19,  2.74it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs_empty_cache EXPORT:  79%|███████▉  | 203/256 [01:39<00:17,  2.98it/s]    cache-cache_patch:torch-oblivious/inputs_batch1 EXPORT:  79%|███████▉  | 203/256 [01:39<00:17,  2.98it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs_batch1 EXPORT:  80%|███████▉  | 204/256 [01:39<00:16,  3.13it/s]    cache-cache_patch:torch-oblivious-rt/inputs EXPORT:  80%|███████▉  | 204/256 [01:39<00:16,  3.13it/s]    


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs EXPORT:  80%|████████  | 205/256 [01:39<00:15,  3.33it/s]    cache-cache_patch:torch-oblivious-rt/inputs2 EXPORT:  80%|████████  | 205/256 [01:39<00:15,  3.33it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs2 EXPORT:  80%|████████  | 206/256 [01:40<00:14,  3.46it/s]    cache-cache_patch:torch-oblivious-rt/inputs_empty_cache EXPORT:  80%|████████  | 206/256 [01:40<00:14,  3.46it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs_empty_cache EXPORT:  81%|████████  | 207/256 [01:40<00:13,  3.54it/s]    cache-cache_patch:torch-oblivious-rt/inputs_batch1 EXPORT:  81%|████████  | 207/256 [01:40<00:13,  3.54it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs_batch1 EXPORT:  81%|████████▏ | 208/256 [01:40<00:13,  3.65it/s]    cache-cache_patch:torch-strict/inputs EXPORT:  81%|████████▏ | 208/256 [01:40<00:13,  3.65it/s]                 cache-cache_patch:torch-strict/inputs EXPORT:  82%|████████▏ | 209/256 [01:40<00:11,  4.00it/s]    cache-cache_patch:torch-strict/inputs2 EXPORT:  82%|████████▏ | 209/256 [01:40<00:11,  4.00it/s]    cache-cache_patch:torch-strict/inputs2 EXPORT:  82%|████████▏ | 210/256 [01:41<00:11,  4.00it/s]    cache-cache_patch:torch-strict/inputs_empty_cache EXPORT:  82%|████████▏ | 210/256 [01:41<00:11,  4.00it/s]    cache-cache_patch:torch-strict/inputs_empty_cache EXPORT:  82%|████████▏ | 211/256 [01:41<00:10,  4.41it/s]    cache-cache_patch:torch-strict/inputs_batch1 EXPORT:  82%|████████▏ | 211/256 [01:41<00:10,  4.41it/s]         cache-cache_patch:torch-strict/inputs_batch1 EXPORT:  83%|████████▎ | 212/256 [01:41<00:08,  4.97it/s]    cache-cache_patch:torch-strict-rt/inputs EXPORT:  83%|████████▎ | 212/256 [01:41<00:08,  4.97it/s]        cache-cache_patch:torch-strict-rt/inputs EXPORT:  83%|████████▎ | 213/256 [01:41<00:08,  5.04it/s]    cache-cache_patch:torch-strict-rt/inputs2 EXPORT:  83%|████████▎ | 213/256 [01:41<00:08,  5.04it/s]    cache-cache_patch:torch-strict-rt/inputs2 EXPORT:  84%|████████▎ | 214/256 [01:41<00:08,  4.72it/s]    cache-cache_patch:torch-strict-rt/inputs_empty_cache EXPORT:  84%|████████▎ | 214/256 [01:41<00:08,  4.72it/s]    cache-cache_patch:torch-strict-rt/inputs_empty_cache EXPORT:  84%|████████▍ | 215/256 [01:41<00:08,  4.94it/s]    cache-cache_patch:torch-strict-rt/inputs_batch1 EXPORT:  84%|████████▍ | 215/256 [01:41<00:08,  4.94it/s]         cache-cache_patch:torch-strict-rt/inputs_batch1 EXPORT:  84%|████████▍ | 216/256 [01:42<00:07,  5.31it/s]    cache-cache_patch:torch-strict-oblivious/inputs EXPORT:  84%|████████▍ | 216/256 [01:42<00:07,  5.31it/s]    cache-cache_patch:torch-strict-oblivious/inputs EXPORT:  85%|████████▍ | 217/256 [01:42<00:07,  5.47it/s]    cache-cache_patch:torch-strict-oblivious/inputs2 EXPORT:  85%|████████▍ | 217/256 [01:42<00:07,  5.47it/s]    cache-cache_patch:torch-strict-oblivious/inputs2 EXPORT:  85%|████████▌ | 218/256 [01:42<00:07,  5.04it/s]    cache-cache_patch:torch-strict-oblivious/inputs_empty_cache EXPORT:  85%|████████▌ | 218/256 [01:42<00:07,  5.04it/s]    cache-cache_patch:torch-strict-oblivious/inputs_empty_cache EXPORT:  86%|████████▌ | 219/256 [01:42<00:06,  5.31it/s]    cache-cache_patch:torch-strict-oblivious/inputs_batch1 EXPORT:  86%|████████▌ | 219/256 [01:42<00:06,  5.31it/s]         cache-cache_patch:torch-strict-oblivious/inputs_batch1 EXPORT:  86%|████████▌ | 220/256 [01:42<00:07,  4.87it/s]    cache-cache_patch:torch-strict-oblivious-rt/inputs EXPORT:  86%|████████▌ | 220/256 [01:42<00:07,  4.87it/s]        cache-cache_patch:torch-strict-oblivious-rt/inputs EXPORT:  86%|████████▋ | 221/256 [01:43<00:06,  5.02it/s]    cache-cache_patch:torch-strict-oblivious-rt/inputs2 EXPORT:  86%|████████▋ | 221/256 [01:43<00:06,  5.02it/s]    cache-cache_patch:torch-strict-oblivious-rt/inputs2 EXPORT:  87%|████████▋ | 222/256 [01:43<00:07,  4.79it/s]    cache-cache_patch:torch-strict-oblivious-rt/inputs_empty_cache EXPORT:  87%|████████▋ | 222/256 [01:43<00:07,  4.79it/s]    cache-cache_patch:torch-strict-oblivious-rt/inputs_empty_cache EXPORT:  87%|████████▋ | 223/256 [01:43<00:06,  4.79it/s]    cache-cache_patch:torch-strict-oblivious-rt/inputs_batch1 EXPORT:  87%|████████▋ | 223/256 [01:43<00:06,  4.79it/s]         cache-cache_patch:torch-strict-oblivious-rt/inputs_batch1 EXPORT:  88%|████████▊ | 224/256 [01:43<00:06,  4.78it/s]    cache-cache_patch:transformers/inputs EXPORT:  88%|████████▊ | 224/256 [01:43<00:06,  4.78it/s]                        cache-cache_patch:transformers/inputs VALIDATE:  88%|████████▊ | 224/256 [01:45<00:06,  4.78it/s]    cache-cache_patch:transformers/inputs VALIDATE:  88%|████████▊ | 225/256 [01:45<00:24,  1.25it/s]    cache-cache_patch:transformers/inputs2 EXPORT:  88%|████████▊ | 225/256 [01:45<00:24,  1.25it/s]     cache-cache_patch:transformers/inputs2 VALIDATE:  88%|████████▊ | 225/256 [01:47<00:24,  1.25it/s]    cache-cache_patch:transformers/inputs2 VALIDATE:  88%|████████▊ | 226/256 [01:47<00:26,  1.12it/s]    cache-cache_patch:transformers/inputs_empty_cache EXPORT:  88%|████████▊ | 226/256 [01:47<00:26,  1.12it/s]    cache-cache_patch:transformers/inputs_empty_cache EXPORT:  89%|████████▊ | 227/256 [01:48<00:26,  1.10it/s]    cache-cache_patch:transformers/inputs_batch1 EXPORT:  89%|████████▊ | 227/256 [01:48<00:26,  1.10it/s]         cache-cache_patch:transformers/inputs_batch1 EXPORT:  89%|████████▉ | 228/256 [01:48<00:24,  1.15it/s]    cache-cache_patch:transformers-rt/inputs EXPORT:  89%|████████▉ | 228/256 [01:48<00:24,  1.15it/s]        cache-cache_patch:transformers-rt/inputs VALIDATE:  89%|████████▉ | 228/256 [01:49<00:24,  1.15it/s]    cache-cache_patch:transformers-rt/inputs VALIDATE:  89%|████████▉ | 229/256 [01:50<00:26,  1.02it/s]    cache-cache_patch:transformers-rt/inputs2 EXPORT:  89%|████████▉ | 229/256 [01:50<00:26,  1.02it/s]     cache-cache_patch:transformers-rt/inputs2 VALIDATE:  89%|████████▉ | 229/256 [01:50<00:26,  1.02it/s]    cache-cache_patch:transformers-rt/inputs2 VALIDATE:  90%|████████▉ | 230/256 [01:50<00:20,  1.24it/s]    cache-cache_patch:transformers-rt/inputs_empty_cache EXPORT:  90%|████████▉ | 230/256 [01:50<00:20,  1.24it/s]    cache-cache_patch:transformers-rt/inputs_empty_cache EXPORT:  90%|█████████ | 231/256 [01:51<00:21,  1.15it/s]    cache-cache_patch:transformers-rt/inputs_batch1 EXPORT:  90%|█████████ | 231/256 [01:51<00:21,  1.15it/s]         cache-cache_patch:transformers-rt/inputs_batch1 EXPORT:  91%|█████████ | 232/256 [01:52<00:20,  1.16it/s]    cache-cache_patch:transformers-oblivious/inputs EXPORT:  91%|█████████ | 232/256 [01:52<00:20,  1.16it/s]    cache-cache_patch:transformers-oblivious/inputs VALIDATE:  91%|█████████ | 232/256 [01:53<00:20,  1.16it/s]    cache-cache_patch:transformers-oblivious/inputs VALIDATE:  91%|█████████ | 233/256 [01:53<00:22,  1.01it/s]    cache-cache_patch:transformers-oblivious/inputs2 EXPORT:  91%|█████████ | 233/256 [01:53<00:22,  1.01it/s]     cache-cache_patch:transformers-oblivious/inputs2 VALIDATE:  91%|█████████ | 233/256 [01:54<00:22,  1.01it/s]    cache-cache_patch:transformers-oblivious/inputs2 VALIDATE:  91%|█████████▏| 234/256 [01:54<00:22,  1.02s/it]    cache-cache_patch:transformers-oblivious/inputs_empty_cache EXPORT:  91%|█████████▏| 234/256 [01:54<00:22,  1.02s/it]    cache-cache_patch:transformers-oblivious/inputs_empty_cache VALIDATE:  91%|█████████▏| 234/256 [01:55<00:22,  1.02s/it]    cache-cache_patch:transformers-oblivious/inputs_empty_cache VALIDATE:  92%|█████████▏| 235/256 [01:55<00:21,  1.03s/it]    cache-cache_patch:transformers-oblivious/inputs_batch1 EXPORT:  92%|█████████▏| 235/256 [01:55<00:21,  1.03s/it]           cache-cache_patch:transformers-oblivious/inputs_batch1 VALIDATE:  92%|█████████▏| 235/256 [01:56<00:21,  1.03s/it]    cache-cache_patch:transformers-oblivious/inputs_batch1 VALIDATE:  92%|█████████▏| 236/256 [01:56<00:20,  1.04s/it]    cache-cache_patch:transformers-oblivious-rt/inputs EXPORT:  92%|█████████▏| 236/256 [01:56<00:20,  1.04s/it]          cache-cache_patch:transformers-oblivious-rt/inputs VALIDATE:  92%|█████████▏| 236/256 [01:57<00:20,  1.04s/it]    cache-cache_patch:transformers-oblivious-rt/inputs VALIDATE:  93%|█████████▎| 237/256 [01:57<00:20,  1.09s/it]    cache-cache_patch:transformers-oblivious-rt/inputs2 EXPORT:  93%|█████████▎| 237/256 [01:57<00:20,  1.09s/it]     cache-cache_patch:transformers-oblivious-rt/inputs2 VALIDATE:  93%|█████████▎| 237/256 [01:58<00:20,  1.09s/it]    cache-cache_patch:transformers-oblivious-rt/inputs2 VALIDATE:  93%|█████████▎| 238/256 [01:59<00:19,  1.09s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache EXPORT:  93%|█████████▎| 238/256 [01:59<00:19,  1.09s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache VALIDATE:  93%|█████████▎| 238/256 [02:00<00:19,  1.09s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache VALIDATE:  93%|█████████▎| 239/256 [02:00<00:18,  1.09s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_batch1 EXPORT:  93%|█████████▎| 239/256 [02:00<00:18,  1.09s/it]           cache-cache_patch:transformers-oblivious-rt/inputs_batch1 VALIDATE:  93%|█████████▎| 239/256 [02:01<00:18,  1.09s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_batch1 VALIDATE:  94%|█████████▍| 240/256 [02:01<00:17,  1.10s/it]    cache-cache_patch:transformers-strict/inputs EXPORT:  94%|█████████▍| 240/256 [02:01<00:17,  1.10s/it]                   cache-cache_patch:transformers-strict/inputs EXPORT:  94%|█████████▍| 241/256 [02:02<00:18,  1.21s/it]    cache-cache_patch:transformers-strict/inputs2 EXPORT:  94%|█████████▍| 241/256 [02:02<00:18,  1.21s/it]    cache-cache_patch:transformers-strict/inputs2 EXPORT:  95%|█████████▍| 242/256 [02:05<00:23,  1.65s/it]    cache-cache_patch:transformers-strict/inputs_empty_cache EXPORT:  95%|█████████▍| 242/256 [02:05<00:23,  1.65s/it]    cache-cache_patch:transformers-strict/inputs_empty_cache EXPORT:  95%|█████████▍| 243/256 [02:06<00:20,  1.56s/it]    cache-cache_patch:transformers-strict/inputs_batch1 EXPORT:  95%|█████████▍| 243/256 [02:06<00:20,  1.56s/it]         cache-cache_patch:transformers-strict/inputs_batch1 EXPORT:  95%|█████████▌| 244/256 [02:08<00:20,  1.71s/it]    cache-cache_patch:transformers-strict-rt/inputs EXPORT:  95%|█████████▌| 244/256 [02:08<00:20,  1.71s/it]        cache-cache_patch:transformers-strict-rt/inputs EXPORT:  96%|█████████▌| 245/256 [02:11<00:21,  1.94s/it]    cache-cache_patch:transformers-strict-rt/inputs2 EXPORT:  96%|█████████▌| 245/256 [02:11<00:21,  1.94s/it]    cache-cache_patch:transformers-strict-rt/inputs2 EXPORT:  96%|█████████▌| 246/256 [02:13<00:21,  2.12s/it]    cache-cache_patch:transformers-strict-rt/inputs_empty_cache EXPORT:  96%|█████████▌| 246/256 [02:13<00:21,  2.12s/it]    cache-cache_patch:transformers-strict-rt/inputs_empty_cache EXPORT:  96%|█████████▋| 247/256 [02:16<00:20,  2.29s/it]    cache-cache_patch:transformers-strict-rt/inputs_batch1 EXPORT:  96%|█████████▋| 247/256 [02:16<00:20,  2.29s/it]         cache-cache_patch:transformers-strict-rt/inputs_batch1 EXPORT:  97%|█████████▋| 248/256 [02:18<00:17,  2.19s/it]    cache-cache_patch:transformers-strict-oblivious/inputs EXPORT:  97%|█████████▋| 248/256 [02:18<00:17,  2.19s/it]    cache-cache_patch:transformers-strict-oblivious/inputs EXPORT:  97%|█████████▋| 249/256 [02:19<00:13,  1.96s/it]    cache-cache_patch:transformers-strict-oblivious/inputs2 EXPORT:  97%|█████████▋| 249/256 [02:19<00:13,  1.96s/it]    cache-cache_patch:transformers-strict-oblivious/inputs2 EXPORT:  98%|█████████▊| 250/256 [02:21<00:10,  1.77s/it]    cache-cache_patch:transformers-strict-oblivious/inputs_empty_cache EXPORT:  98%|█████████▊| 250/256 [02:21<00:10,  1.77s/it]    cache-cache_patch:transformers-strict-oblivious/inputs_empty_cache EXPORT:  98%|█████████▊| 251/256 [02:21<00:07,  1.43s/it]    cache-cache_patch:transformers-strict-oblivious/inputs_batch1 EXPORT:  98%|█████████▊| 251/256 [02:21<00:07,  1.43s/it]         cache-cache_patch:transformers-strict-oblivious/inputs_batch1 EXPORT:  98%|█████████▊| 252/256 [02:25<00:08,  2.05s/it]    cache-cache_patch:transformers-strict-oblivious-rt/inputs EXPORT:  98%|█████████▊| 252/256 [02:25<00:08,  2.05s/it]        cache-cache_patch:transformers-strict-oblivious-rt/inputs EXPORT:  99%|█████████▉| 253/256 [02:29<00:07,  2.58s/it]    cache-cache_patch:transformers-strict-oblivious-rt/inputs2 EXPORT:  99%|█████████▉| 253/256 [02:29<00:07,  2.58s/it]    cache-cache_patch:transformers-strict-oblivious-rt/inputs2 EXPORT:  99%|█████████▉| 254/256 [02:34<00:06,  3.25s/it]    cache-cache_patch:transformers-strict-oblivious-rt/inputs_empty_cache EXPORT:  99%|█████████▉| 254/256 [02:34<00:06,  3.25s/it]    cache-cache_patch:transformers-strict-oblivious-rt/inputs_empty_cache EXPORT: 100%|█████████▉| 255/256 [02:37<00:03,  3.34s/it]    cache-cache_patch:transformers-strict-oblivious-rt/inputs_batch1 EXPORT: 100%|█████████▉| 255/256 [02:37<00:03,  3.34s/it]         cache-cache_patch:transformers-strict-oblivious-rt/inputs_batch1 EXPORT: 100%|██████████| 256/256 [02:41<00:00,  3.42s/it]    cache-cache_patch:transformers-strict-oblivious-rt/inputs_batch1 EXPORT: 100%|██████████| 256/256 [02:41<00:00,  1.59it/s]




.. GENERATED FROM PYTHON SOURCE LINES 216-217

Let's save the results.

.. GENERATED FROM PYTHON SOURCE LINES 217-222

.. code-block:: Python


    df = pandas.DataFrame(results)
    df.to_excel("plot_export_tiny_llm_dim01.xlsx")
    df






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>cache</th>
          <th>cache_patch</th>
          <th>strict</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>export_with</th>
          <th>EXPORT</th>
          <th>ERR-EXPORT</th>
          <th>run_with</th>
          <th>WORKS</th>
          <th>ERR-RUN</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123134473163344)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs2</td>
          <td>0</td>
          <td>8*s72 (123136028097376)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>8*s72 (123136026772784)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>8*s31 + 8*s70 (123135932371536)is not tracked ...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123135935588896)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>239</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>0</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>240</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>241</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs2</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>242</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>243</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    <p>244 rows × 11 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 223-228

.. code-block:: Python


    no_export = df[df.EXPORT == 0]
    no_export.to_excel("plot_export_tiny_llm_dim01.no_export.xlsx")
    no_export






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>cache</th>
          <th>cache_patch</th>
          <th>strict</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>export_with</th>
          <th>EXPORT</th>
          <th>ERR-EXPORT</th>
          <th>run_with</th>
          <th>WORKS</th>
          <th>ERR-RUN</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123134473163344)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs2</td>
          <td>0</td>
          <td>8*s72 (123136028097376)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>8*s72 (123136026772784)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>8*s31 + 8*s70 (123135932371536)is not tracked ...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123135935588896)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>239</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>0</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>240</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>241</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs2</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>242</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>243</th>
          <td>1</td>
          <td>transformers</td>
          <td>1</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    <p>132 rows × 11 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 229-230

The validation failures.

.. GENERATED FROM PYTHON SOURCE LINES 230-239

.. code-block:: Python


    invalid = df[(df.EXPORT == 1) & (df.WORKS == 0)].pivot(
        index=["cache", "cache_patch", "strict", "oblivious", "rt", "export_with"],
        columns=["run_with"],
        values=["WORKS", "ERR-RUN"],
    )
    invalid.to_excel("plot_export_tiny_llm_dim01.invalid.xlsx")
    invalid






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead tr th {
            text-align: left;
        }

        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th colspan="4" halign="left">WORKS</th>
          <th colspan="4" halign="left">ERR-RUN</th>
        </tr>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th>run_with</th>
          <th>inputs</th>
          <th>inputs2</th>
          <th>inputs_batch1</th>
          <th>inputs_empty_cache</th>
          <th>inputs</th>
          <th>inputs2</th>
          <th>inputs_batch1</th>
          <th>inputs_empty_cache</th>
        </tr>
        <tr>
          <th>cache</th>
          <th>cache_patch</th>
          <th>strict</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>export_with</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="28" valign="top">1</th>
          <th rowspan="16" valign="top">all</th>
          <th rowspan="16" valign="top">0</th>
          <th rowspan="8" valign="top">0</th>
          <th rowspan="4" valign="top">0</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>Guard failed: input_ids.size()[0] == 1</td>
          <td>Guard failed: input_ids.size()[0] == 1</td>
          <td>NaN</td>
          <td>Guard failed: input_ids.size()[0] == 1</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>Guard failed: past_key_values['key_cache'][0]....</td>
          <td>Guard failed: past_key_values['key_cache'][0]....</td>
          <td>Guard failed: past_key_values['key_cache'][0]....</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="4" valign="top">1</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>Guard failed: input_ids.size()[0] == 1</td>
          <td>Guard failed: input_ids.size()[0] == 1</td>
          <td>NaN</td>
          <td>Guard failed: input_ids.size()[0] == 1</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>Guard failed: past_key_values['key_cache'][0]....</td>
          <td>Guard failed: past_key_values['key_cache'][0]....</td>
          <td>Guard failed: past_key_values['key_cache'][0]....</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="8" valign="top">1</th>
          <th rowspan="4" valign="top">0</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] != 1</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] != 1</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>Guard failed: position_ids.size()[0] == 1</td>
          <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] == 1</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] != 1</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="4" valign="top">1</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Ne(s44...</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Ne(s44...</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>Runtime assertion failed for expression Eq(s44...</td>
          <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Eq(s44...</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Ne(s44...</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="12" valign="top">transformers</th>
          <th rowspan="12" valign="top">0</th>
          <th rowspan="4" valign="top">0</th>
          <th rowspan="2" valign="top">0</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">1</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
        </tr>
        <tr>
          <th rowspan="8" valign="top">1</th>
          <th rowspan="4" valign="top">0</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] != 1</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] != 1</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>Guard failed: position_ids.size()[0] == 1</td>
          <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] == 1</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Guard failed: position_ids.size()[0] != 1</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="4" valign="top">1</th>
          <th>inputs</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Ne(s44...</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Ne(s44...</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>Runtime assertion failed for expression Eq(s44...</td>
          <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Eq(s44...</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Runtime assertion failed for expression Ne(s44...</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 240-250

.. code-block:: Python


    success = df[(df.EXPORT == 1) & (df.WORKS == 1)].pivot(
        index=["cache", "cache_patch", "strict", "oblivious", "rt", "export_with"],
        columns=["run_with"],
        values=["WORKS"],
    )
    success.to_excel("plot_export_tiny_llm_dim01.success.xlsx")
    success







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead tr th {
            text-align: left;
        }

        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th colspan="4" halign="left">WORKS</th>
        </tr>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th>run_with</th>
          <th>inputs</th>
          <th>inputs2</th>
          <th>inputs_batch1</th>
          <th>inputs_empty_cache</th>
        </tr>
        <tr>
          <th>cache</th>
          <th>cache_patch</th>
          <th>strict</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>export_with</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="28" valign="top">1</th>
          <th rowspan="16" valign="top">all</th>
          <th rowspan="16" valign="top">0</th>
          <th rowspan="8" valign="top">0</th>
          <th rowspan="4" valign="top">0</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="4" valign="top">1</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="8" valign="top">1</th>
          <th rowspan="4" valign="top">0</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="4" valign="top">1</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="12" valign="top">transformers</th>
          <th rowspan="12" valign="top">0</th>
          <th rowspan="4" valign="top">0</th>
          <th rowspan="2" valign="top">0</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">1</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th rowspan="8" valign="top">1</th>
          <th rowspan="4" valign="top">0</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="4" valign="top">1</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>NaN</td>
          <td>1.0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 251-253

If you have any error, then look at example
:ref:`l-plot-tiny-llm-export-patched`.

.. GENERATED FROM PYTHON SOURCE LINES 253-255

.. code-block:: Python


    doc.plot_legend("Tiny-LLM\nexport with\ndimension in {0,1}", "torch.export.export", "tomato")



.. image-sg:: /auto_examples/images/sphx_glr_plot_export_tiny_llm_dim01_001.png
   :alt: plot export tiny llm dim01
   :srcset: /auto_examples/images/sphx_glr_plot_export_tiny_llm_dim01_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 41.626 seconds)


.. _sphx_glr_download_auto_examples_plot_export_tiny_llm_dim01.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_tiny_llm_dim01.ipynb <plot_export_tiny_llm_dim01.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_tiny_llm_dim01.py <plot_export_tiny_llm_dim01.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_tiny_llm_dim01.zip <plot_export_tiny_llm_dim01.zip>`


.. include:: plot_export_tiny_llm_dim01.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
