
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_tiny_llm_dim01_onnx_custom.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_tiny_llm_dim01_onnx_custom.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_tiny_llm_dim01_onnx_custom.py:


.. _l-plot-tiny-llm-export-dim01-onnx-custom:

Export with dynamic dimensions in ``{0,1}`` into ONNX (custom)
==============================================================

This duplicates the example :ref:`l-plot-tiny-llm-export-dim01` but for
:func:`experimental_experiment.torch_interpreter.to_onnx`.
It checks what inputs can be used to export and with which inputs it can work.

Available input sets
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 15-36

.. code-block:: Python


    import itertools
    from tqdm import tqdm
    import numpy as np
    import pandas
    import onnxruntime
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers import max_diff, string_type, flatten_object
    from onnx_diagnostic.helpers.torch_helper import torch_deepcopy
    from onnx_diagnostic.helpers.rt_helper import make_feeds
    from onnx_diagnostic.torch_models.hghub.model_inputs import get_untrained_model_with_inputs
    from onnx_diagnostic.torch_export_patches import (
        torch_export_patches,
        register_additional_serialization_functions,
    )
    from experimental_experiment.torch_interpreter import to_onnx, ExportOptions


    data = get_untrained_model_with_inputs("arnir0/Tiny-LLM", add_second_input=True)
    model, dynamic_shapes = data["model"], data["dynamic_shapes"]








.. GENERATED FROM PYTHON SOURCE LINES 37-44

The trained model can be obtained with:

.. code-block:: python

  MODEL_NAME = "arnir0/Tiny-LLM"
  tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)
  model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)

.. GENERATED FROM PYTHON SOURCE LINES 44-50

.. code-block:: Python


    input_sets = {k: v for k, v in data.items() if k.startswith("inputs")}

    for k, v in input_sets.items():
        print(f"{k:20}: {string_type(v, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    inputs              : dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x30x96], value_cache=#1[T1s2x1x30x96]))
    inputs2             : dict(input_ids:T7s3x4,attention_mask:T7s3x35,position_ids:T7s3x4,past_key_values:DynamicCache(key_cache=#1[T1s3x1x31x96], value_cache=#1[T1s3x1x31x96]))
    inputs_empty_cache  : dict(input_ids:T7s2x3,attention_mask:T7s2x3,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x0x96], value_cache=#1[T1s2x1x0x96]))
    inputs_batch1       : dict(input_ids:T7s1x3,attention_mask:T7s1x33,position_ids:T7s1x3,past_key_values:DynamicCache(key_cache=#1[T1s1x1x30x96], value_cache=#1[T1s1x1x30x96]))




.. GENERATED FROM PYTHON SOURCE LINES 51-52

The dynamic shapes are:

.. GENERATED FROM PYTHON SOURCE LINES 52-55

.. code-block:: Python


    print(f"dynamic_shapes: {string_type(dynamic_shapes)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dynamic_shapes: dict(input_ids:{0:DYN(batch),1:DYN(seq_length)},attention_mask:{0:DYN(batch),1:DYN(cache+seq)},position_ids:{0:DYN(batch),1:DYN(cache+seq)},past_key_values:#2[#1[{0:DYN(batch),2:DYN(cache_length)}],#1[{0:DYN(batch),2:DYN(cache_length)}]])




.. GENERATED FROM PYTHON SOURCE LINES 56-58

Let's check they all work and compute the expected values.
We use deepcopy because caches are usually modified inplace.

.. GENERATED FROM PYTHON SOURCE LINES 58-64

.. code-block:: Python


    expected = {}
    for k, v in input_sets.items():
        expected[k] = model(**torch_deepcopy(v))
        print(f"{k:20}: {string_type(expected[k], with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    inputs              : CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s2x1x33x96], value_cache=#1[T1s2x1x33x96]))
    inputs2             : CausalLMOutputWithPast(logits:T1s3x4x32000,past_key_values:DynamicCache(key_cache=#1[T1s3x1x35x96], value_cache=#1[T1s3x1x35x96]))
    inputs_empty_cache  : CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s2x1x3x96], value_cache=#1[T1s2x1x3x96]))
    inputs_batch1       : CausalLMOutputWithPast(logits:T1s1x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x33x96], value_cache=#1[T1s1x1x33x96]))




.. GENERATED FROM PYTHON SOURCE LINES 65-81

Export with options
+++++++++++++++++++

We try to export with the following options:

- cache registration: register cache serialization with
  :func:`onnx_diagnostic.torch_export_patches.register_additional_serialization_functions`

- oblivious: an option to remove some the exception raises by the exporter

- rt: see ``prefer_deferred_runtime_asserts_over_guards`` in :func:`torch.export.export`

- cache_patch: patches the model before exporting with
  :func:`onnx_diagnostic.torch_export_patches.torch_export_patches`

Some function first.

.. GENERATED FROM PYTHON SOURCE LINES 81-160

.. code-block:: Python



    def export_model(
        model,
        dynamic_shapes,
        inputs,
        cache=False,
        oblivious=False,
        rt=False,
        cache_patch=False,
        strict=False,
    ):
        if cache and not cache_patch:
            with register_additional_serialization_functions(patch_transformers=True):
                return export_model(
                    model, dynamic_shapes, inputs, oblivious=oblivious, rt=rt, strict=strict
                )
        if cache_patch:
            with torch_export_patches(
                patch_torch=cache_patch in ("all", "torch", True, 1),
                patch_transformers=cache_patch in ("all", "transformers", True, 1),
            ):
                return export_model(
                    model, dynamic_shapes, inputs, oblivious=oblivious, rt=rt, strict=strict
                )
        return to_onnx(
            model,
            (),
            kwargs=inputs,
            dynamic_shapes=dynamic_shapes,
            export_options=ExportOptions(
                prefer_deferred_runtime_asserts_over_guards=rt,
                backed_size_oblivious=oblivious,
                strict=strict,
            ),
        )


    def try_export_model(
        model,
        dynamic_shapes,
        inputs,
        cache=False,
        oblivious=False,
        rt=False,
        cache_patch=False,
        strict=False,
    ):
        try:
            return export_model(
                model,
                dynamic_shapes,
                inputs,
                cache=cache,
                oblivious=oblivious,
                rt=rt,
                cache_patch=cache_patch,
                strict=strict,
            )
        except Exception as e:
            return e


    def validation(onx, input_sets, expected, catch_exception=True):
        sess = onnxruntime.InferenceSession(
            onx.SerializeToString(), providers=["CPUExecutionProvider"]
        )
        for k, v in input_sets.items():
            feeds = make_feeds(sess, torch_deepcopy(v), use_numpy=True)
            try:
                got = sess.run(None, feeds)
            except Exception as e:
                if not catch_exception:
                    raise
                yield k, e
                continue
            yield k, max_diff(flatten_object(expected[k], drop_keys=True), got)









.. GENERATED FROM PYTHON SOURCE LINES 161-162

Verification an example known to be working is.

.. GENERATED FROM PYTHON SOURCE LINES 162-173

.. code-block:: Python


    ep = export_model(
        model,
        dynamic_shapes,
        torch_deepcopy(input_sets["inputs"]),
        cache_patch=True,
    )
    res = list(validation(ep, dict(inputs=input_sets["inputs"]), expected, catch_exception=False))
    assert res[0][1]["abs"] < 1e-5, f"Unexpected issue with res={res}"









.. GENERATED FROM PYTHON SOURCE LINES 174-176

The main loop
+++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 176-233

.. code-block:: Python


    results = []

    possibilities = [
        [0, 1],
        [0, "all", "torch", "transformers"],
        [0, 1],
        [0, 1, "auto", "half"],
        [0, 1],
        list(input_sets),
    ]
    with tqdm(list(itertools.product(*possibilities))) as pbar:
        for cache, cache_patch, strict, oblivious, rt, inputs in pbar:
            if cache_patch and not cache:
                # patches include caches.
                continue
            kwargs = dict(
                cache=cache, cache_patch=cache_patch, oblivious=oblivious, rt=rt, strict=strict
            )
            legend = "-".join(
                (k if isinstance(v, int) else f"{k}:{v}") for k, v in kwargs.items() if v
            )
            legend = f"{legend}/{inputs}"
            pbar.set_description(f"{legend} EXPORT")

            # export
            ep = try_export_model(
                model, dynamic_shapes, torch_deepcopy(input_sets[inputs]), **kwargs
            )
            if isinstance(ep, Exception):
                obs = {
                    **kwargs,
                    "export_with": inputs,
                    "EXPORT": 0,
                    "ERR-EXPORT": str(ep).split("\n")[0],
                }
                results.append(obs)
                continue

            pbar.set_description(f"{legend} VALIDATE")
            common = {**kwargs, "export_with": inputs, "EXPORT": 1}
            for inp, res in validation(ep, input_sets, expected):
                if isinstance(res, Exception):
                    obs = {
                        **common,
                        "run_with": inp,
                        "ERR-RUN": str(res).split("\n")[0],
                        "WORKS": 0,
                    }
                else:
                    obs = {
                        **common,
                        "run_with": inp,
                        "WORKS": int(~np.isnan(res["abs"]) and res["abs"] < 1e-3),
                    }
                results.append(obs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/512 [00:00<?, ?it/s]    /inputs EXPORT:   0%|          | 0/512 [00:00<?, ?it/s]    /inputs EXPORT:   0%|          | 1/512 [00:00<02:19,  3.67it/s]    /inputs2 EXPORT:   0%|          | 1/512 [00:00<02:19,  3.67it/s]    /inputs2 EXPORT:   0%|          | 2/512 [00:00<02:14,  3.80it/s]    /inputs_empty_cache EXPORT:   0%|          | 2/512 [00:00<02:14,  3.80it/s]    /inputs_empty_cache EXPORT:   1%|          | 3/512 [00:00<02:00,  4.23it/s]    /inputs_batch1 EXPORT:   1%|          | 3/512 [00:00<02:00,  4.23it/s]         /inputs_batch1 EXPORT:   1%|          | 4/512 [00:01<02:16,  3.71it/s]    rt/inputs EXPORT:   1%|          | 4/512 [00:01<02:16,  3.71it/s]         rt/inputs EXPORT:   1%|          | 5/512 [00:01<02:11,  3.85it/s]    rt/inputs2 EXPORT:   1%|          | 5/512 [00:01<02:11,  3.85it/s]    rt/inputs2 EXPORT:   1%|          | 6/512 [00:01<02:08,  3.92it/s]    rt/inputs_empty_cache EXPORT:   1%|          | 6/512 [00:01<02:08,  3.92it/s]    rt/inputs_empty_cache EXPORT:   1%|▏         | 7/512 [00:01<02:04,  4.05it/s]    rt/inputs_batch1 EXPORT:   1%|▏         | 7/512 [00:01<02:04,  4.05it/s]         rt/inputs_batch1 EXPORT:   2%|▏         | 8/512 [00:02<03:05,  2.72it/s]    oblivious/inputs EXPORT:   2%|▏         | 8/512 [00:02<03:05,  2.72it/s]    oblivious/inputs EXPORT:   2%|▏         | 9/512 [00:02<03:08,  2.67it/s]    oblivious/inputs2 EXPORT:   2%|▏         | 9/512 [00:02<03:08,  2.67it/s]    oblivious/inputs2 EXPORT:   2%|▏         | 10/512 [00:03<03:07,  2.67it/s]    oblivious/inputs_empty_cache EXPORT:   2%|▏         | 10/512 [00:03<03:07,  2.67it/s]    oblivious/inputs_empty_cache EXPORT:   2%|▏         | 11/512 [00:03<03:10,  2.64it/s]    oblivious/inputs_batch1 EXPORT:   2%|▏         | 11/512 [00:03<03:10,  2.64it/s]         oblivious/inputs_batch1 EXPORT:   2%|▏         | 12/512 [00:03<03:03,  2.72it/s]    oblivious-rt/inputs EXPORT:   2%|▏         | 12/512 [00:03<03:03,  2.72it/s]        oblivious-rt/inputs EXPORT:   3%|▎         | 13/512 [00:04<02:49,  2.94it/s]    oblivious-rt/inputs2 EXPORT:   3%|▎         | 13/512 [00:04<02:49,  2.94it/s]    oblivious-rt/inputs2 EXPORT:   3%|▎         | 14/512 [00:04<02:39,  3.13it/s]    oblivious-rt/inputs_empty_cache EXPORT:   3%|▎         | 14/512 [00:04<02:39,  3.13it/s]    oblivious-rt/inputs_empty_cache EXPORT:   3%|▎         | 15/512 [00:04<02:52,  2.89it/s]    oblivious-rt/inputs_batch1 EXPORT:   3%|▎         | 15/512 [00:04<02:52,  2.89it/s]         oblivious-rt/inputs_batch1 EXPORT:   3%|▎         | 16/512 [00:05<02:57,  2.79it/s]    oblivious:auto/inputs EXPORT:   3%|▎         | 16/512 [00:05<02:57,  2.79it/s]         oblivious:auto/inputs EXPORT:   3%|▎         | 17/512 [00:05<03:15,  2.53it/s]    oblivious:auto/inputs2 EXPORT:   3%|▎         | 17/512 [00:05<03:15,  2.53it/s]    oblivious:auto/inputs2 EXPORT:   4%|▎         | 18/512 [00:06<03:06,  2.64it/s]    oblivious:auto/inputs_empty_cache EXPORT:   4%|▎         | 18/512 [00:06<03:06,  2.64it/s]    oblivious:auto/inputs_empty_cache EXPORT:   4%|▎         | 19/512 [00:06<03:11,  2.57it/s]    oblivious:auto/inputs_batch1 EXPORT:   4%|▎         | 19/512 [00:06<03:11,  2.57it/s]         oblivious:auto/inputs_batch1 EXPORT:   4%|▍         | 20/512 [00:06<02:57,  2.78it/s]    oblivious:auto-rt/inputs EXPORT:   4%|▍         | 20/512 [00:06<02:57,  2.78it/s]        oblivious:auto-rt/inputs EXPORT:   4%|▍         | 21/512 [00:07<03:07,  2.62it/s]    oblivious:auto-rt/inputs2 EXPORT:   4%|▍         | 21/512 [00:07<03:07,  2.62it/s]    oblivious:auto-rt/inputs2 EXPORT:   4%|▍         | 22/512 [00:07<02:57,  2.76it/s]    oblivious:auto-rt/inputs_empty_cache EXPORT:   4%|▍         | 22/512 [00:07<02:57,  2.76it/s]    oblivious:auto-rt/inputs_empty_cache EXPORT:   4%|▍         | 23/512 [00:07<02:45,  2.96it/s]    oblivious:auto-rt/inputs_batch1 EXPORT:   4%|▍         | 23/512 [00:07<02:45,  2.96it/s]         oblivious:auto-rt/inputs_batch1 EXPORT:   5%|▍         | 24/512 [00:08<02:31,  3.22it/s]    oblivious:half/inputs EXPORT:   5%|▍         | 24/512 [00:08<02:31,  3.22it/s]              oblivious:half/inputs EXPORT:   5%|▍         | 25/512 [00:08<02:35,  3.14it/s]    oblivious:half/inputs2 EXPORT:   5%|▍         | 25/512 [00:08<02:35,  3.14it/s]    oblivious:half/inputs2 EXPORT:   5%|▌         | 26/512 [00:08<02:29,  3.26it/s]    oblivious:half/inputs_empty_cache EXPORT:   5%|▌         | 26/512 [00:08<02:29,  3.26it/s]    oblivious:half/inputs_empty_cache EXPORT:   5%|▌         | 27/512 [00:09<02:37,  3.09it/s]    oblivious:half/inputs_batch1 EXPORT:   5%|▌         | 27/512 [00:09<02:37,  3.09it/s]         oblivious:half/inputs_batch1 EXPORT:   5%|▌         | 28/512 [00:09<02:35,  3.10it/s]    oblivious:half-rt/inputs EXPORT:   5%|▌         | 28/512 [00:09<02:35,  3.10it/s]        oblivious:half-rt/inputs EXPORT:   6%|▌         | 29/512 [00:09<02:47,  2.88it/s]    oblivious:half-rt/inputs2 EXPORT:   6%|▌         | 29/512 [00:09<02:47,  2.88it/s]    oblivious:half-rt/inputs2 EXPORT:   6%|▌         | 30/512 [00:10<02:53,  2.78it/s]    oblivious:half-rt/inputs_empty_cache EXPORT:   6%|▌         | 30/512 [00:10<02:53,  2.78it/s]    oblivious:half-rt/inputs_empty_cache EXPORT:   6%|▌         | 31/512 [00:10<02:47,  2.87it/s]    oblivious:half-rt/inputs_batch1 EXPORT:   6%|▌         | 31/512 [00:10<02:47,  2.87it/s]         oblivious:half-rt/inputs_batch1 EXPORT:   6%|▋         | 32/512 [00:10<02:34,  3.10it/s]    strict/inputs EXPORT:   6%|▋         | 32/512 [00:10<02:34,  3.10it/s]                      strict/inputs EXPORT:   6%|▋         | 33/512 [00:11<03:48,  2.10it/s]    strict/inputs2 EXPORT:   6%|▋         | 33/512 [00:11<03:48,  2.10it/s]    strict/inputs2 EXPORT:   7%|▋         | 34/512 [00:12<04:29,  1.78it/s]    strict/inputs_empty_cache EXPORT:   7%|▋         | 34/512 [00:12<04:29,  1.78it/s]    strict/inputs_empty_cache EXPORT:   7%|▋         | 35/512 [00:12<04:42,  1.69it/s]    strict/inputs_batch1 EXPORT:   7%|▋         | 35/512 [00:12<04:42,  1.69it/s]         strict/inputs_batch1 EXPORT:   7%|▋         | 36/512 [00:13<04:57,  1.60it/s]    rt-strict/inputs EXPORT:   7%|▋         | 36/512 [00:13<04:57,  1.60it/s]        rt-strict/inputs EXPORT:   7%|▋         | 37/512 [00:14<04:52,  1.63it/s]    rt-strict/inputs2 EXPORT:   7%|▋         | 37/512 [00:14<04:52,  1.63it/s]    rt-strict/inputs2 EXPORT:   7%|▋         | 38/512 [00:14<04:37,  1.71it/s]    rt-strict/inputs_empty_cache EXPORT:   7%|▋         | 38/512 [00:14<04:37,  1.71it/s]    rt-strict/inputs_empty_cache EXPORT:   8%|▊         | 39/512 [00:15<04:27,  1.77it/s]    rt-strict/inputs_batch1 EXPORT:   8%|▊         | 39/512 [00:15<04:27,  1.77it/s]         rt-strict/inputs_batch1 EXPORT:   8%|▊         | 40/512 [00:15<04:24,  1.78it/s]    oblivious-strict/inputs EXPORT:   8%|▊         | 40/512 [00:15<04:24,  1.78it/s]    oblivious-strict/inputs EXPORT:   8%|▊         | 41/512 [00:16<04:27,  1.76it/s]    oblivious-strict/inputs2 EXPORT:   8%|▊         | 41/512 [00:16<04:27,  1.76it/s]    oblivious-strict/inputs2 EXPORT:   8%|▊         | 42/512 [00:16<04:20,  1.81it/s]    oblivious-strict/inputs_empty_cache EXPORT:   8%|▊         | 42/512 [00:16<04:20,  1.81it/s]    oblivious-strict/inputs_empty_cache EXPORT:   8%|▊         | 43/512 [00:17<04:34,  1.71it/s]    oblivious-strict/inputs_batch1 EXPORT:   8%|▊         | 43/512 [00:17<04:34,  1.71it/s]         oblivious-strict/inputs_batch1 EXPORT:   9%|▊         | 44/512 [00:18<04:30,  1.73it/s]    oblivious-rt-strict/inputs EXPORT:   9%|▊         | 44/512 [00:18<04:30,  1.73it/s]        oblivious-rt-strict/inputs EXPORT:   9%|▉         | 45/512 [00:18<04:31,  1.72it/s]    oblivious-rt-strict/inputs2 EXPORT:   9%|▉         | 45/512 [00:18<04:31,  1.72it/s]    oblivious-rt-strict/inputs2 EXPORT:   9%|▉         | 46/512 [00:19<04:35,  1.69it/s]    oblivious-rt-strict/inputs_empty_cache EXPORT:   9%|▉         | 46/512 [00:19<04:35,  1.69it/s]    oblivious-rt-strict/inputs_empty_cache EXPORT:   9%|▉         | 47/512 [00:20<04:42,  1.64it/s]    oblivious-rt-strict/inputs_batch1 EXPORT:   9%|▉         | 47/512 [00:20<04:42,  1.64it/s]         oblivious-rt-strict/inputs_batch1 EXPORT:   9%|▉         | 48/512 [00:20<05:01,  1.54it/s]    oblivious:auto-strict/inputs EXPORT:   9%|▉         | 48/512 [00:20<05:01,  1.54it/s]         oblivious:auto-strict/inputs EXPORT:  10%|▉         | 49/512 [00:21<05:32,  1.39it/s]    oblivious:auto-strict/inputs2 EXPORT:  10%|▉         | 49/512 [00:21<05:32,  1.39it/s]    oblivious:auto-strict/inputs2 EXPORT:  10%|▉         | 50/512 [00:22<05:21,  1.44it/s]    oblivious:auto-strict/inputs_empty_cache EXPORT:  10%|▉         | 50/512 [00:22<05:21,  1.44it/s]    oblivious:auto-strict/inputs_empty_cache EXPORT:  10%|▉         | 51/512 [00:22<05:18,  1.45it/s]    oblivious:auto-strict/inputs_batch1 EXPORT:  10%|▉         | 51/512 [00:22<05:18,  1.45it/s]         oblivious:auto-strict/inputs_batch1 EXPORT:  10%|█         | 52/512 [00:23<05:16,  1.45it/s]    oblivious:auto-rt-strict/inputs EXPORT:  10%|█         | 52/512 [00:23<05:16,  1.45it/s]        oblivious:auto-rt-strict/inputs EXPORT:  10%|█         | 53/512 [00:24<05:12,  1.47it/s]    oblivious:auto-rt-strict/inputs2 EXPORT:  10%|█         | 53/512 [00:24<05:12,  1.47it/s]    oblivious:auto-rt-strict/inputs2 EXPORT:  11%|█         | 54/512 [00:25<05:25,  1.41it/s]    oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  11%|█         | 54/512 [00:25<05:25,  1.41it/s]    oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  11%|█         | 55/512 [00:25<05:20,  1.43it/s]    oblivious:auto-rt-strict/inputs_batch1 EXPORT:  11%|█         | 55/512 [00:25<05:20,  1.43it/s]         oblivious:auto-rt-strict/inputs_batch1 EXPORT:  11%|█         | 56/512 [00:26<05:08,  1.48it/s]    oblivious:half-strict/inputs EXPORT:  11%|█         | 56/512 [00:26<05:08,  1.48it/s]              oblivious:half-strict/inputs EXPORT:  11%|█         | 57/512 [00:27<05:11,  1.46it/s]    oblivious:half-strict/inputs2 EXPORT:  11%|█         | 57/512 [00:27<05:11,  1.46it/s]    oblivious:half-strict/inputs2 EXPORT:  11%|█▏        | 58/512 [00:27<05:00,  1.51it/s]    oblivious:half-strict/inputs_empty_cache EXPORT:  11%|█▏        | 58/512 [00:27<05:00,  1.51it/s]    oblivious:half-strict/inputs_empty_cache EXPORT:  12%|█▏        | 59/512 [00:28<04:33,  1.66it/s]    oblivious:half-strict/inputs_batch1 EXPORT:  12%|█▏        | 59/512 [00:28<04:33,  1.66it/s]         oblivious:half-strict/inputs_batch1 EXPORT:  12%|█▏        | 60/512 [00:28<04:30,  1.67it/s]    oblivious:half-rt-strict/inputs EXPORT:  12%|█▏        | 60/512 [00:28<04:30,  1.67it/s]        oblivious:half-rt-strict/inputs EXPORT:  12%|█▏        | 61/512 [00:29<04:25,  1.70it/s]    oblivious:half-rt-strict/inputs2 EXPORT:  12%|█▏        | 61/512 [00:29<04:25,  1.70it/s]    oblivious:half-rt-strict/inputs2 EXPORT:  12%|█▏        | 62/512 [00:29<04:13,  1.77it/s]    oblivious:half-rt-strict/inputs_empty_cache EXPORT:  12%|█▏        | 62/512 [00:29<04:13,  1.77it/s]    oblivious:half-rt-strict/inputs_empty_cache EXPORT:  12%|█▏        | 63/512 [00:30<04:20,  1.72it/s]    oblivious:half-rt-strict/inputs_batch1 EXPORT:  12%|█▏        | 63/512 [00:30<04:20,  1.72it/s]         oblivious:half-rt-strict/inputs_batch1 EXPORT:  12%|█▎        | 64/512 [00:30<04:07,  1.81it/s]    cache/inputs EXPORT:  12%|█▎        | 64/512 [00:30<04:07,  1.81it/s]                              cache/inputs EXPORT:  50%|█████     | 257/512 [00:32<00:04, 62.89it/s]    cache/inputs2 EXPORT:  50%|█████     | 257/512 [00:32<00:04, 62.89it/s]    cache/inputs_empty_cache EXPORT:  50%|█████     | 257/512 [00:33<00:04, 62.89it/s]    cache/inputs_batch1 EXPORT:  50%|█████     | 257/512 [00:33<00:04, 62.89it/s]         cache-rt/inputs EXPORT:  50%|█████     | 257/512 [00:33<00:04, 62.89it/s]        cache-rt/inputs EXPORT:  51%|█████     | 261/512 [00:34<00:06, 39.61it/s]    cache-rt/inputs2 EXPORT:  51%|█████     | 261/512 [00:34<00:06, 39.61it/s]    cache-rt/inputs_empty_cache EXPORT:  51%|█████     | 261/512 [00:34<00:06, 39.61it/s]    cache-rt/inputs_batch1 EXPORT:  51%|█████     | 261/512 [00:34<00:06, 39.61it/s]         cache-rt/inputs_batch1 EXPORT:  52%|█████▏    | 264/512 [00:35<00:08, 30.12it/s]    cache-oblivious/inputs EXPORT:  52%|█████▏    | 264/512 [00:35<00:08, 30.12it/s]    cache-oblivious/inputs2 EXPORT:  52%|█████▏    | 264/512 [00:35<00:08, 30.12it/s]    cache-oblivious/inputs2 EXPORT:  52%|█████▏    | 266/512 [00:35<00:09, 25.08it/s]    cache-oblivious/inputs_empty_cache EXPORT:  52%|█████▏    | 266/512 [00:35<00:09, 25.08it/s]    cache-oblivious/inputs_batch1 EXPORT:  52%|█████▏    | 266/512 [00:35<00:09, 25.08it/s]         cache-oblivious/inputs_batch1 EXPORT:  52%|█████▏    | 268/512 [00:36<00:12, 20.15it/s]    cache-oblivious-rt/inputs EXPORT:  52%|█████▏    | 268/512 [00:36<00:12, 20.15it/s]        cache-oblivious-rt/inputs2 EXPORT:  52%|█████▏    | 268/512 [00:36<00:12, 20.15it/s]    cache-oblivious-rt/inputs2 EXPORT:  53%|█████▎    | 270/512 [00:36<00:14, 16.54it/s]    cache-oblivious-rt/inputs_empty_cache EXPORT:  53%|█████▎    | 270/512 [00:36<00:14, 16.54it/s]    cache-oblivious-rt/inputs_empty_cache EXPORT:  53%|█████▎    | 271/512 [00:36<00:16, 14.85it/s]    cache-oblivious-rt/inputs_batch1 EXPORT:  53%|█████▎    | 271/512 [00:36<00:16, 14.85it/s]         cache-oblivious-rt/inputs_batch1 EXPORT:  53%|█████▎    | 272/512 [00:37<00:18, 13.13it/s]    cache-oblivious:auto/inputs EXPORT:  53%|█████▎    | 272/512 [00:37<00:18, 13.13it/s]         cache-oblivious:auto/inputs EXPORT:  53%|█████▎    | 273/512 [00:37<00:20, 11.50it/s]    cache-oblivious:auto/inputs2 EXPORT:  53%|█████▎    | 273/512 [00:37<00:20, 11.50it/s]    cache-oblivious:auto/inputs2 EXPORT:  54%|█████▎    | 274/512 [00:37<00:24,  9.78it/s]    cache-oblivious:auto/inputs_empty_cache EXPORT:  54%|█████▎    | 274/512 [00:37<00:24,  9.78it/s]    cache-oblivious:auto/inputs_empty_cache EXPORT:  54%|█████▎    | 275/512 [00:37<00:28,  8.28it/s]    cache-oblivious:auto/inputs_batch1 EXPORT:  54%|█████▎    | 275/512 [00:37<00:28,  8.28it/s]         cache-oblivious:auto/inputs_batch1 EXPORT:  54%|█████▍    | 276/512 [00:38<00:34,  6.86it/s]    cache-oblivious:auto-rt/inputs EXPORT:  54%|█████▍    | 276/512 [00:38<00:34,  6.86it/s]        cache-oblivious:auto-rt/inputs EXPORT:  54%|█████▍    | 277/512 [00:38<00:40,  5.83it/s]    cache-oblivious:auto-rt/inputs2 EXPORT:  54%|█████▍    | 277/512 [00:38<00:40,  5.83it/s]    cache-oblivious:auto-rt/inputs2 EXPORT:  54%|█████▍    | 278/512 [00:38<00:45,  5.13it/s]    cache-oblivious:auto-rt/inputs_empty_cache EXPORT:  54%|█████▍    | 278/512 [00:38<00:45,  5.13it/s]    cache-oblivious:auto-rt/inputs_empty_cache EXPORT:  54%|█████▍    | 279/512 [00:39<00:50,  4.66it/s]    cache-oblivious:auto-rt/inputs_batch1 EXPORT:  54%|█████▍    | 279/512 [00:39<00:50,  4.66it/s]         cache-oblivious:auto-rt/inputs_batch1 EXPORT:  55%|█████▍    | 280/512 [00:39<00:52,  4.43it/s]    cache-oblivious:half/inputs EXPORT:  55%|█████▍    | 280/512 [00:39<00:52,  4.43it/s]              cache-oblivious:half/inputs EXPORT:  55%|█████▍    | 281/512 [00:39<00:54,  4.26it/s]    cache-oblivious:half/inputs2 EXPORT:  55%|█████▍    | 281/512 [00:39<00:54,  4.26it/s]    cache-oblivious:half/inputs2 EXPORT:  55%|█████▌    | 282/512 [00:39<00:55,  4.17it/s]    cache-oblivious:half/inputs_empty_cache EXPORT:  55%|█████▌    | 282/512 [00:39<00:55,  4.17it/s]    cache-oblivious:half/inputs_empty_cache EXPORT:  55%|█████▌    | 283/512 [00:40<00:58,  3.91it/s]    cache-oblivious:half/inputs_batch1 EXPORT:  55%|█████▌    | 283/512 [00:40<00:58,  3.91it/s]         cache-oblivious:half/inputs_batch1 EXPORT:  55%|█████▌    | 284/512 [00:40<00:58,  3.91it/s]    cache-oblivious:half-rt/inputs EXPORT:  55%|█████▌    | 284/512 [00:40<00:58,  3.91it/s]        cache-oblivious:half-rt/inputs EXPORT:  56%|█████▌    | 285/512 [00:40<00:57,  3.93it/s]    cache-oblivious:half-rt/inputs2 EXPORT:  56%|█████▌    | 285/512 [00:40<00:57,  3.93it/s]    cache-oblivious:half-rt/inputs2 EXPORT:  56%|█████▌    | 286/512 [00:41<01:02,  3.62it/s]    cache-oblivious:half-rt/inputs_empty_cache EXPORT:  56%|█████▌    | 286/512 [00:41<01:02,  3.62it/s]    cache-oblivious:half-rt/inputs_empty_cache EXPORT:  56%|█████▌    | 287/512 [00:41<01:00,  3.70it/s]    cache-oblivious:half-rt/inputs_batch1 EXPORT:  56%|█████▌    | 287/512 [00:41<01:00,  3.70it/s]         cache-oblivious:half-rt/inputs_batch1 EXPORT:  56%|█████▋    | 288/512 [00:41<01:00,  3.67it/s]    cache-strict/inputs EXPORT:  56%|█████▋    | 288/512 [00:41<01:00,  3.67it/s]                      cache-strict/inputs EXPORT:  56%|█████▋    | 289/512 [00:42<01:16,  2.91it/s]    cache-strict/inputs2 EXPORT:  56%|█████▋    | 289/512 [00:42<01:16,  2.91it/s]    cache-strict/inputs2 EXPORT:  57%|█████▋    | 290/512 [00:42<01:27,  2.53it/s]    cache-strict/inputs_empty_cache EXPORT:  57%|█████▋    | 290/512 [00:42<01:27,  2.53it/s]    cache-strict/inputs_empty_cache EXPORT:  57%|█████▋    | 291/512 [00:43<01:32,  2.40it/s]    cache-strict/inputs_batch1 EXPORT:  57%|█████▋    | 291/512 [00:43<01:32,  2.40it/s]         cache-strict/inputs_batch1 EXPORT:  57%|█████▋    | 292/512 [00:43<01:39,  2.20it/s]    cache-rt-strict/inputs EXPORT:  57%|█████▋    | 292/512 [00:43<01:39,  2.20it/s]        cache-rt-strict/inputs EXPORT:  57%|█████▋    | 293/512 [00:44<01:51,  1.96it/s]    cache-rt-strict/inputs2 EXPORT:  57%|█████▋    | 293/512 [00:44<01:51,  1.96it/s]    cache-rt-strict/inputs2 EXPORT:  57%|█████▋    | 294/512 [00:44<01:57,  1.85it/s]    cache-rt-strict/inputs_empty_cache EXPORT:  57%|█████▋    | 294/512 [00:44<01:57,  1.85it/s]    cache-rt-strict/inputs_empty_cache EXPORT:  58%|█████▊    | 295/512 [00:45<01:59,  1.82it/s]    cache-rt-strict/inputs_batch1 EXPORT:  58%|█████▊    | 295/512 [00:45<01:59,  1.82it/s]         cache-rt-strict/inputs_batch1 EXPORT:  58%|█████▊    | 296/512 [00:46<01:57,  1.84it/s]    cache-oblivious-strict/inputs EXPORT:  58%|█████▊    | 296/512 [00:46<01:57,  1.84it/s]    cache-oblivious-strict/inputs EXPORT:  58%|█████▊    | 297/512 [00:46<01:53,  1.90it/s]    cache-oblivious-strict/inputs2 EXPORT:  58%|█████▊    | 297/512 [00:46<01:53,  1.90it/s]    cache-oblivious-strict/inputs2 EXPORT:  58%|█████▊    | 298/512 [00:47<01:55,  1.85it/s]    cache-oblivious-strict/inputs_empty_cache EXPORT:  58%|█████▊    | 298/512 [00:47<01:55,  1.85it/s]    cache-oblivious-strict/inputs_empty_cache EXPORT:  58%|█████▊    | 299/512 [00:47<01:54,  1.85it/s]    cache-oblivious-strict/inputs_batch1 EXPORT:  58%|█████▊    | 299/512 [00:47<01:54,  1.85it/s]         cache-oblivious-strict/inputs_batch1 EXPORT:  59%|█████▊    | 300/512 [00:48<01:50,  1.92it/s]    cache-oblivious-rt-strict/inputs EXPORT:  59%|█████▊    | 300/512 [00:48<01:50,  1.92it/s]        cache-oblivious-rt-strict/inputs EXPORT:  59%|█████▉    | 301/512 [00:48<01:50,  1.91it/s]    cache-oblivious-rt-strict/inputs2 EXPORT:  59%|█████▉    | 301/512 [00:48<01:50,  1.91it/s]    cache-oblivious-rt-strict/inputs2 EXPORT:  59%|█████▉    | 302/512 [00:49<01:50,  1.90it/s]    cache-oblivious-rt-strict/inputs_empty_cache EXPORT:  59%|█████▉    | 302/512 [00:49<01:50,  1.90it/s]    cache-oblivious-rt-strict/inputs_empty_cache EXPORT:  59%|█████▉    | 303/512 [00:49<01:48,  1.93it/s]    cache-oblivious-rt-strict/inputs_batch1 EXPORT:  59%|█████▉    | 303/512 [00:49<01:48,  1.93it/s]         cache-oblivious-rt-strict/inputs_batch1 EXPORT:  59%|█████▉    | 304/512 [00:50<01:55,  1.80it/s]    cache-oblivious:auto-strict/inputs EXPORT:  59%|█████▉    | 304/512 [00:50<01:55,  1.80it/s]         cache-oblivious:auto-strict/inputs EXPORT:  60%|█████▉    | 305/512 [00:50<01:54,  1.81it/s]    cache-oblivious:auto-strict/inputs2 EXPORT:  60%|█████▉    | 305/512 [00:50<01:54,  1.81it/s]    cache-oblivious:auto-strict/inputs2 EXPORT:  60%|█████▉    | 306/512 [00:51<01:58,  1.75it/s]    cache-oblivious:auto-strict/inputs_empty_cache EXPORT:  60%|█████▉    | 306/512 [00:51<01:58,  1.75it/s]    cache-oblivious:auto-strict/inputs_empty_cache EXPORT:  60%|█████▉    | 307/512 [00:51<01:51,  1.84it/s]    cache-oblivious:auto-strict/inputs_batch1 EXPORT:  60%|█████▉    | 307/512 [00:51<01:51,  1.84it/s]         cache-oblivious:auto-strict/inputs_batch1 EXPORT:  60%|██████    | 308/512 [00:52<01:48,  1.88it/s]    cache-oblivious:auto-rt-strict/inputs EXPORT:  60%|██████    | 308/512 [00:52<01:48,  1.88it/s]        cache-oblivious:auto-rt-strict/inputs EXPORT:  60%|██████    | 309/512 [00:52<01:45,  1.93it/s]    cache-oblivious:auto-rt-strict/inputs2 EXPORT:  60%|██████    | 309/512 [00:52<01:45,  1.93it/s]    cache-oblivious:auto-rt-strict/inputs2 EXPORT:  61%|██████    | 310/512 [00:53<01:44,  1.93it/s]    cache-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  61%|██████    | 310/512 [00:53<01:44,  1.93it/s]    cache-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  61%|██████    | 311/512 [00:53<01:44,  1.92it/s]    cache-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  61%|██████    | 311/512 [00:53<01:44,  1.92it/s]         cache-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  61%|██████    | 312/512 [00:54<01:42,  1.94it/s]    cache-oblivious:half-strict/inputs EXPORT:  61%|██████    | 312/512 [00:54<01:42,  1.94it/s]              cache-oblivious:half-strict/inputs EXPORT:  61%|██████    | 313/512 [00:54<01:40,  1.97it/s]    cache-oblivious:half-strict/inputs2 EXPORT:  61%|██████    | 313/512 [00:54<01:40,  1.97it/s]    cache-oblivious:half-strict/inputs2 EXPORT:  61%|██████▏   | 314/512 [00:55<01:42,  1.93it/s]    cache-oblivious:half-strict/inputs_empty_cache EXPORT:  61%|██████▏   | 314/512 [00:55<01:42,  1.93it/s]    cache-oblivious:half-strict/inputs_empty_cache EXPORT:  62%|██████▏   | 315/512 [00:56<01:39,  1.98it/s]    cache-oblivious:half-strict/inputs_batch1 EXPORT:  62%|██████▏   | 315/512 [00:56<01:39,  1.98it/s]         cache-oblivious:half-strict/inputs_batch1 EXPORT:  62%|██████▏   | 316/512 [00:56<01:39,  1.97it/s]    cache-oblivious:half-rt-strict/inputs EXPORT:  62%|██████▏   | 316/512 [00:56<01:39,  1.97it/s]        cache-oblivious:half-rt-strict/inputs EXPORT:  62%|██████▏   | 317/512 [00:57<01:39,  1.97it/s]    cache-oblivious:half-rt-strict/inputs2 EXPORT:  62%|██████▏   | 317/512 [00:57<01:39,  1.97it/s]    cache-oblivious:half-rt-strict/inputs2 EXPORT:  62%|██████▏   | 318/512 [00:57<01:39,  1.95it/s]    cache-oblivious:half-rt-strict/inputs_empty_cache EXPORT:  62%|██████▏   | 318/512 [00:57<01:39,  1.95it/s]    cache-oblivious:half-rt-strict/inputs_empty_cache EXPORT:  62%|██████▏   | 319/512 [00:58<01:42,  1.88it/s]    cache-oblivious:half-rt-strict/inputs_batch1 EXPORT:  62%|██████▏   | 319/512 [00:58<01:42,  1.88it/s]         cache-oblivious:half-rt-strict/inputs_batch1 EXPORT:  62%|██████▎   | 320/512 [00:58<01:45,  1.82it/s]    cache-cache_patch:all/inputs EXPORT:  62%|██████▎   | 320/512 [00:58<01:45,  1.82it/s]                    cache-cache_patch:all/inputs VALIDATE:  62%|██████▎   | 320/512 [01:00<01:45,  1.82it/s]    cache-cache_patch:all/inputs VALIDATE:  63%|██████▎   | 321/512 [01:01<03:33,  1.12s/it]    cache-cache_patch:all/inputs2 EXPORT:  63%|██████▎   | 321/512 [01:01<03:33,  1.12s/it]     cache-cache_patch:all/inputs2 VALIDATE:  63%|██████▎   | 321/512 [01:02<03:33,  1.12s/it]    cache-cache_patch:all/inputs2 VALIDATE:  63%|██████▎   | 322/512 [01:02<03:45,  1.19s/it]    cache-cache_patch:all/inputs_empty_cache EXPORT:  63%|██████▎   | 322/512 [01:02<03:45,  1.19s/it]    cache-cache_patch:all/inputs_empty_cache VALIDATE:  63%|██████▎   | 322/512 [01:06<03:45,  1.19s/it]    cache-cache_patch:all/inputs_empty_cache VALIDATE:  63%|██████▎   | 323/512 [01:06<06:06,  1.94s/it]    cache-cache_patch:all/inputs_batch1 EXPORT:  63%|██████▎   | 323/512 [01:06<06:06,  1.94s/it]           cache-cache_patch:all/inputs_batch1 VALIDATE:  63%|██████▎   | 323/512 [01:07<06:06,  1.94s/it]    cache-cache_patch:all/inputs_batch1 VALIDATE:  63%|██████▎   | 324/512 [01:07<05:45,  1.84s/it]    cache-cache_patch:all-rt/inputs EXPORT:  63%|██████▎   | 324/512 [01:07<05:45,  1.84s/it]          cache-cache_patch:all-rt/inputs VALIDATE:  63%|██████▎   | 324/512 [01:09<05:45,  1.84s/it]    cache-cache_patch:all-rt/inputs VALIDATE:  63%|██████▎   | 325/512 [01:09<05:42,  1.83s/it]    cache-cache_patch:all-rt/inputs2 EXPORT:  63%|██████▎   | 325/512 [01:09<05:42,  1.83s/it]     cache-cache_patch:all-rt/inputs2 VALIDATE:  63%|██████▎   | 325/512 [01:11<05:42,  1.83s/it]    cache-cache_patch:all-rt/inputs2 VALIDATE:  64%|██████▎   | 326/512 [01:11<05:37,  1.81s/it]    cache-cache_patch:all-rt/inputs_empty_cache EXPORT:  64%|██████▎   | 326/512 [01:11<05:37,  1.81s/it]    cache-cache_patch:all-rt/inputs_empty_cache VALIDATE:  64%|██████▎   | 326/512 [01:13<05:37,  1.81s/it]    cache-cache_patch:all-rt/inputs_empty_cache VALIDATE:  64%|██████▍   | 327/512 [01:13<05:43,  1.86s/it]    cache-cache_patch:all-rt/inputs_batch1 EXPORT:  64%|██████▍   | 327/512 [01:13<05:43,  1.86s/it]           cache-cache_patch:all-rt/inputs_batch1 VALIDATE:  64%|██████▍   | 327/512 [01:14<05:43,  1.86s/it]    cache-cache_patch:all-rt/inputs_batch1 VALIDATE:  64%|██████▍   | 328/512 [01:15<05:42,  1.86s/it]    cache-cache_patch:all-oblivious/inputs EXPORT:  64%|██████▍   | 328/512 [01:15<05:42,  1.86s/it]      cache-cache_patch:all-oblivious/inputs VALIDATE:  64%|██████▍   | 328/512 [01:17<05:42,  1.86s/it]    cache-cache_patch:all-oblivious/inputs VALIDATE:  64%|██████▍   | 329/512 [01:17<05:44,  1.88s/it]    cache-cache_patch:all-oblivious/inputs2 EXPORT:  64%|██████▍   | 329/512 [01:17<05:44,  1.88s/it]     cache-cache_patch:all-oblivious/inputs2 VALIDATE:  64%|██████▍   | 329/512 [01:18<05:44,  1.88s/it]    cache-cache_patch:all-oblivious/inputs2 VALIDATE:  64%|██████▍   | 330/512 [01:19<05:41,  1.88s/it]    cache-cache_patch:all-oblivious/inputs_empty_cache EXPORT:  64%|██████▍   | 330/512 [01:19<05:41,  1.88s/it]    cache-cache_patch:all-oblivious/inputs_empty_cache VALIDATE:  64%|██████▍   | 330/512 [01:20<05:41,  1.88s/it]    cache-cache_patch:all-oblivious/inputs_empty_cache VALIDATE:  65%|██████▍   | 331/512 [01:20<05:33,  1.84s/it]    cache-cache_patch:all-oblivious/inputs_batch1 EXPORT:  65%|██████▍   | 331/512 [01:20<05:33,  1.84s/it]           cache-cache_patch:all-oblivious/inputs_batch1 VALIDATE:  65%|██████▍   | 331/512 [01:22<05:33,  1.84s/it]    cache-cache_patch:all-oblivious/inputs_batch1 VALIDATE:  65%|██████▍   | 332/512 [01:22<05:30,  1.84s/it]    cache-cache_patch:all-oblivious-rt/inputs EXPORT:  65%|██████▍   | 332/512 [01:22<05:30,  1.84s/it]          cache-cache_patch:all-oblivious-rt/inputs VALIDATE:  65%|██████▍   | 332/512 [01:24<05:30,  1.84s/it]    cache-cache_patch:all-oblivious-rt/inputs VALIDATE:  65%|██████▌   | 333/512 [01:24<05:37,  1.88s/it]    cache-cache_patch:all-oblivious-rt/inputs2 EXPORT:  65%|██████▌   | 333/512 [01:24<05:37,  1.88s/it]     cache-cache_patch:all-oblivious-rt/inputs2 VALIDATE:  65%|██████▌   | 333/512 [01:26<05:37,  1.88s/it]    cache-cache_patch:all-oblivious-rt/inputs2 VALIDATE:  65%|██████▌   | 334/512 [01:26<05:59,  2.02s/it]    cache-cache_patch:all-oblivious-rt/inputs_empty_cache EXPORT:  65%|██████▌   | 334/512 [01:26<05:59,  2.02s/it]    cache-cache_patch:all-oblivious-rt/inputs_empty_cache VALIDATE:  65%|██████▌   | 334/512 [01:28<05:59,  2.02s/it]    cache-cache_patch:all-oblivious-rt/inputs_empty_cache VALIDATE:  65%|██████▌   | 335/512 [01:28<05:55,  2.01s/it]    cache-cache_patch:all-oblivious-rt/inputs_batch1 EXPORT:  65%|██████▌   | 335/512 [01:28<05:55,  2.01s/it]           cache-cache_patch:all-oblivious-rt/inputs_batch1 VALIDATE:  65%|██████▌   | 335/512 [01:30<05:55,  2.01s/it]    cache-cache_patch:all-oblivious-rt/inputs_batch1 VALIDATE:  66%|██████▌   | 336/512 [01:30<05:37,  1.92s/it]    cache-cache_patch:all-oblivious:auto/inputs EXPORT:  66%|██████▌   | 336/512 [01:30<05:37,  1.92s/it]           cache-cache_patch:all-oblivious:auto/inputs VALIDATE:  66%|██████▌   | 336/512 [01:32<05:37,  1.92s/it]    cache-cache_patch:all-oblivious:auto/inputs VALIDATE:  66%|██████▌   | 337/512 [01:32<05:21,  1.84s/it]    cache-cache_patch:all-oblivious:auto/inputs2 EXPORT:  66%|██████▌   | 337/512 [01:32<05:21,  1.84s/it]     cache-cache_patch:all-oblivious:auto/inputs2 VALIDATE:  66%|██████▌   | 337/512 [01:33<05:21,  1.84s/it]    cache-cache_patch:all-oblivious:auto/inputs2 VALIDATE:  66%|██████▌   | 338/512 [01:33<05:09,  1.78s/it]    cache-cache_patch:all-oblivious:auto/inputs_empty_cache EXPORT:  66%|██████▌   | 338/512 [01:33<05:09,  1.78s/it]    cache-cache_patch:all-oblivious:auto/inputs_empty_cache VALIDATE:  66%|██████▌   | 338/512 [01:34<05:09,  1.78s/it]    cache-cache_patch:all-oblivious:auto/inputs_empty_cache VALIDATE:  66%|██████▌   | 339/512 [01:35<04:32,  1.58s/it]    cache-cache_patch:all-oblivious:auto/inputs_batch1 EXPORT:  66%|██████▌   | 339/512 [01:35<04:32,  1.58s/it]           cache-cache_patch:all-oblivious:auto/inputs_batch1 VALIDATE:  66%|██████▌   | 339/512 [01:36<04:32,  1.58s/it]    cache-cache_patch:all-oblivious:auto/inputs_batch1 VALIDATE:  66%|██████▋   | 340/512 [01:36<04:44,  1.65s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs EXPORT:  66%|██████▋   | 340/512 [01:36<04:44,  1.65s/it]          cache-cache_patch:all-oblivious:auto-rt/inputs VALIDATE:  66%|██████▋   | 340/512 [01:38<04:44,  1.65s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs VALIDATE:  67%|██████▋   | 341/512 [01:38<04:46,  1.67s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs2 EXPORT:  67%|██████▋   | 341/512 [01:38<04:46,  1.67s/it]     cache-cache_patch:all-oblivious:auto-rt/inputs2 VALIDATE:  67%|██████▋   | 341/512 [01:40<04:46,  1.67s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs2 VALIDATE:  67%|██████▋   | 342/512 [01:40<04:46,  1.68s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs_empty_cache EXPORT:  67%|██████▋   | 342/512 [01:40<04:46,  1.68s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs_empty_cache VALIDATE:  67%|██████▋   | 342/512 [01:41<04:46,  1.68s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs_empty_cache VALIDATE:  67%|██████▋   | 343/512 [01:41<04:44,  1.69s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs_batch1 EXPORT:  67%|██████▋   | 343/512 [01:41<04:44,  1.69s/it]           cache-cache_patch:all-oblivious:auto-rt/inputs_batch1 VALIDATE:  67%|██████▋   | 343/512 [01:43<04:44,  1.69s/it]    cache-cache_patch:all-oblivious:auto-rt/inputs_batch1 VALIDATE:  67%|██████▋   | 344/512 [01:43<04:47,  1.71s/it]    cache-cache_patch:all-oblivious:half/inputs EXPORT:  67%|██████▋   | 344/512 [01:43<04:47,  1.71s/it]                cache-cache_patch:all-oblivious:half/inputs VALIDATE:  67%|██████▋   | 344/512 [01:45<04:47,  1.71s/it]    cache-cache_patch:all-oblivious:half/inputs VALIDATE:  67%|██████▋   | 345/512 [01:45<04:47,  1.72s/it]    cache-cache_patch:all-oblivious:half/inputs2 EXPORT:  67%|██████▋   | 345/512 [01:45<04:47,  1.72s/it]     cache-cache_patch:all-oblivious:half/inputs2 VALIDATE:  67%|██████▋   | 345/512 [01:47<04:47,  1.72s/it]    cache-cache_patch:all-oblivious:half/inputs2 VALIDATE:  68%|██████▊   | 346/512 [01:47<04:57,  1.79s/it]    cache-cache_patch:all-oblivious:half/inputs_empty_cache EXPORT:  68%|██████▊   | 346/512 [01:47<04:57,  1.79s/it]    cache-cache_patch:all-oblivious:half/inputs_empty_cache VALIDATE:  68%|██████▊   | 346/512 [01:48<04:57,  1.79s/it]    cache-cache_patch:all-oblivious:half/inputs_empty_cache VALIDATE:  68%|██████▊   | 347/512 [01:49<04:48,  1.75s/it]    cache-cache_patch:all-oblivious:half/inputs_batch1 EXPORT:  68%|██████▊   | 347/512 [01:49<04:48,  1.75s/it]           cache-cache_patch:all-oblivious:half/inputs_batch1 VALIDATE:  68%|██████▊   | 347/512 [01:50<04:48,  1.75s/it]    cache-cache_patch:all-oblivious:half/inputs_batch1 VALIDATE:  68%|██████▊   | 348/512 [01:50<04:52,  1.78s/it]    cache-cache_patch:all-oblivious:half-rt/inputs EXPORT:  68%|██████▊   | 348/512 [01:50<04:52,  1.78s/it]          cache-cache_patch:all-oblivious:half-rt/inputs VALIDATE:  68%|██████▊   | 348/512 [01:55<04:52,  1.78s/it]    cache-cache_patch:all-oblivious:half-rt/inputs VALIDATE:  68%|██████▊   | 349/512 [01:56<07:37,  2.80s/it]    cache-cache_patch:all-oblivious:half-rt/inputs2 EXPORT:  68%|██████▊   | 349/512 [01:56<07:37,  2.80s/it]     cache-cache_patch:all-oblivious:half-rt/inputs2 VALIDATE:  68%|██████▊   | 349/512 [01:57<07:37,  2.80s/it]    cache-cache_patch:all-oblivious:half-rt/inputs2 VALIDATE:  68%|██████▊   | 350/512 [01:57<06:43,  2.49s/it]    cache-cache_patch:all-oblivious:half-rt/inputs_empty_cache EXPORT:  68%|██████▊   | 350/512 [01:57<06:43,  2.49s/it]    cache-cache_patch:all-oblivious:half-rt/inputs_empty_cache VALIDATE:  68%|██████▊   | 350/512 [01:59<06:43,  2.49s/it]    cache-cache_patch:all-oblivious:half-rt/inputs_empty_cache VALIDATE:  69%|██████▊   | 351/512 [01:59<06:10,  2.30s/it]    cache-cache_patch:all-oblivious:half-rt/inputs_batch1 EXPORT:  69%|██████▊   | 351/512 [01:59<06:10,  2.30s/it]           cache-cache_patch:all-oblivious:half-rt/inputs_batch1 VALIDATE:  69%|██████▊   | 351/512 [02:01<06:10,  2.30s/it]    cache-cache_patch:all-oblivious:half-rt/inputs_batch1 VALIDATE:  69%|██████▉   | 352/512 [02:01<05:39,  2.12s/it]    cache-cache_patch:all-strict/inputs EXPORT:  69%|██████▉   | 352/512 [02:01<05:39,  2.12s/it]                        cache-cache_patch:all-strict/inputs EXPORT:  69%|██████▉   | 353/512 [02:03<05:09,  1.95s/it]    cache-cache_patch:all-strict/inputs2 EXPORT:  69%|██████▉   | 353/512 [02:03<05:09,  1.95s/it]    cache-cache_patch:all-strict/inputs2 EXPORT:  69%|██████▉   | 354/512 [02:04<04:42,  1.79s/it]    cache-cache_patch:all-strict/inputs_empty_cache EXPORT:  69%|██████▉   | 354/512 [02:04<04:42,  1.79s/it]    cache-cache_patch:all-strict/inputs_empty_cache EXPORT:  69%|██████▉   | 355/512 [02:05<04:21,  1.67s/it]    cache-cache_patch:all-strict/inputs_batch1 EXPORT:  69%|██████▉   | 355/512 [02:05<04:21,  1.67s/it]         cache-cache_patch:all-strict/inputs_batch1 EXPORT:  70%|██████▉   | 356/512 [02:08<05:15,  2.02s/it]    cache-cache_patch:all-rt-strict/inputs EXPORT:  70%|██████▉   | 356/512 [02:08<05:15,  2.02s/it]        cache-cache_patch:all-rt-strict/inputs EXPORT:  70%|██████▉   | 357/512 [02:12<06:42,  2.60s/it]    cache-cache_patch:all-rt-strict/inputs2 EXPORT:  70%|██████▉   | 357/512 [02:12<06:42,  2.60s/it]    cache-cache_patch:all-rt-strict/inputs2 EXPORT:  70%|██████▉   | 358/512 [02:16<07:36,  2.97s/it]    cache-cache_patch:all-rt-strict/inputs_empty_cache EXPORT:  70%|██████▉   | 358/512 [02:16<07:36,  2.97s/it]    cache-cache_patch:all-rt-strict/inputs_empty_cache EXPORT:  70%|███████   | 359/512 [02:20<08:19,  3.26s/it]    cache-cache_patch:all-rt-strict/inputs_batch1 EXPORT:  70%|███████   | 359/512 [02:20<08:19,  3.26s/it]         cache-cache_patch:all-rt-strict/inputs_batch1 EXPORT:  70%|███████   | 360/512 [02:24<08:42,  3.44s/it]    cache-cache_patch:all-oblivious-strict/inputs EXPORT:  70%|███████   | 360/512 [02:24<08:42,  3.44s/it]    cache-cache_patch:all-oblivious-strict/inputs EXPORT:  71%|███████   | 361/512 [02:25<07:07,  2.83s/it]    cache-cache_patch:all-oblivious-strict/inputs2 EXPORT:  71%|███████   | 361/512 [02:25<07:07,  2.83s/it]    cache-cache_patch:all-oblivious-strict/inputs2 EXPORT:  71%|███████   | 362/512 [02:27<06:10,  2.47s/it]    cache-cache_patch:all-oblivious-strict/inputs_empty_cache EXPORT:  71%|███████   | 362/512 [02:27<06:10,  2.47s/it]    cache-cache_patch:all-oblivious-strict/inputs_empty_cache EXPORT:  71%|███████   | 363/512 [02:28<05:21,  2.16s/it]    cache-cache_patch:all-oblivious-strict/inputs_batch1 EXPORT:  71%|███████   | 363/512 [02:28<05:21,  2.16s/it]         cache-cache_patch:all-oblivious-strict/inputs_batch1 EXPORT:  71%|███████   | 364/512 [02:33<06:59,  2.83s/it]    cache-cache_patch:all-oblivious-rt-strict/inputs EXPORT:  71%|███████   | 364/512 [02:33<06:59,  2.83s/it]        cache-cache_patch:all-oblivious-rt-strict/inputs EXPORT:  71%|███████▏  | 365/512 [02:37<07:58,  3.26s/it]    cache-cache_patch:all-oblivious-rt-strict/inputs2 EXPORT:  71%|███████▏  | 365/512 [02:37<07:58,  3.26s/it]    cache-cache_patch:all-oblivious-rt-strict/inputs2 EXPORT:  71%|███████▏  | 366/512 [02:40<08:11,  3.36s/it]    cache-cache_patch:all-oblivious-rt-strict/inputs_empty_cache EXPORT:  71%|███████▏  | 366/512 [02:40<08:11,  3.36s/it]    cache-cache_patch:all-oblivious-rt-strict/inputs_empty_cache EXPORT:  72%|███████▏  | 367/512 [02:45<08:44,  3.62s/it]    cache-cache_patch:all-oblivious-rt-strict/inputs_batch1 EXPORT:  72%|███████▏  | 367/512 [02:45<08:44,  3.62s/it]         cache-cache_patch:all-oblivious-rt-strict/inputs_batch1 EXPORT:  72%|███████▏  | 368/512 [02:49<08:59,  3.75s/it]    cache-cache_patch:all-oblivious:auto-strict/inputs EXPORT:  72%|███████▏  | 368/512 [02:49<08:59,  3.75s/it]         cache-cache_patch:all-oblivious:auto-strict/inputs EXPORT:  72%|███████▏  | 369/512 [02:53<09:04,  3.81s/it]    cache-cache_patch:all-oblivious:auto-strict/inputs2 EXPORT:  72%|███████▏  | 369/512 [02:53<09:04,  3.81s/it]    cache-cache_patch:all-oblivious:auto-strict/inputs2 EXPORT:  72%|███████▏  | 370/512 [02:54<07:25,  3.14s/it]    cache-cache_patch:all-oblivious:auto-strict/inputs_empty_cache EXPORT:  72%|███████▏  | 370/512 [02:54<07:25,  3.14s/it]    cache-cache_patch:all-oblivious:auto-strict/inputs_empty_cache EXPORT:  72%|███████▏  | 371/512 [02:56<06:09,  2.62s/it]    cache-cache_patch:all-oblivious:auto-strict/inputs_batch1 EXPORT:  72%|███████▏  | 371/512 [02:56<06:09,  2.62s/it]         cache-cache_patch:all-oblivious:auto-strict/inputs_batch1 EXPORT:  73%|███████▎  | 372/512 [02:59<06:17,  2.70s/it]    cache-cache_patch:all-oblivious:auto-rt-strict/inputs EXPORT:  73%|███████▎  | 372/512 [02:59<06:17,  2.70s/it]        cache-cache_patch:all-oblivious:auto-rt-strict/inputs EXPORT:  73%|███████▎  | 373/512 [03:01<06:15,  2.70s/it]    cache-cache_patch:all-oblivious:auto-rt-strict/inputs2 EXPORT:  73%|███████▎  | 373/512 [03:01<06:15,  2.70s/it]    cache-cache_patch:all-oblivious:auto-rt-strict/inputs2 EXPORT:  73%|███████▎  | 374/512 [03:04<06:13,  2.71s/it]    cache-cache_patch:all-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  73%|███████▎  | 374/512 [03:04<06:13,  2.71s/it]    cache-cache_patch:all-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  73%|███████▎  | 375/512 [03:07<06:10,  2.71s/it]    cache-cache_patch:all-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  73%|███████▎  | 375/512 [03:07<06:10,  2.71s/it]         cache-cache_patch:all-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  73%|███████▎  | 376/512 [03:09<05:37,  2.48s/it]    cache-cache_patch:all-oblivious:half-strict/inputs EXPORT:  73%|███████▎  | 376/512 [03:09<05:37,  2.48s/it]              cache-cache_patch:all-oblivious:half-strict/inputs EXPORT:  74%|███████▎  | 377/512 [03:10<04:51,  2.16s/it]    cache-cache_patch:all-oblivious:half-strict/inputs2 EXPORT:  74%|███████▎  | 377/512 [03:10<04:51,  2.16s/it]    cache-cache_patch:all-oblivious:half-strict/inputs2 EXPORT:  74%|███████▍  | 378/512 [03:11<04:16,  1.92s/it]    cache-cache_patch:all-oblivious:half-strict/inputs_empty_cache EXPORT:  74%|███████▍  | 378/512 [03:11<04:16,  1.92s/it]    cache-cache_patch:all-oblivious:half-strict/inputs_empty_cache EXPORT:  74%|███████▍  | 379/512 [03:13<03:54,  1.76s/it]    cache-cache_patch:all-oblivious:half-strict/inputs_batch1 EXPORT:  74%|███████▍  | 379/512 [03:13<03:54,  1.76s/it]         cache-cache_patch:all-oblivious:half-strict/inputs_batch1 EXPORT:  74%|███████▍  | 380/512 [03:15<04:28,  2.04s/it]    cache-cache_patch:all-oblivious:half-rt-strict/inputs EXPORT:  74%|███████▍  | 380/512 [03:15<04:28,  2.04s/it]        cache-cache_patch:all-oblivious:half-rt-strict/inputs EXPORT:  74%|███████▍  | 381/512 [03:19<05:18,  2.43s/it]    cache-cache_patch:all-oblivious:half-rt-strict/inputs2 EXPORT:  74%|███████▍  | 381/512 [03:19<05:18,  2.43s/it]    cache-cache_patch:all-oblivious:half-rt-strict/inputs2 EXPORT:  75%|███████▍  | 382/512 [03:21<05:23,  2.49s/it]    cache-cache_patch:all-oblivious:half-rt-strict/inputs_empty_cache EXPORT:  75%|███████▍  | 382/512 [03:21<05:23,  2.49s/it]    cache-cache_patch:all-oblivious:half-rt-strict/inputs_empty_cache EXPORT:  75%|███████▍  | 383/512 [03:24<05:20,  2.49s/it]    cache-cache_patch:all-oblivious:half-rt-strict/inputs_batch1 EXPORT:  75%|███████▍  | 383/512 [03:24<05:20,  2.49s/it]         cache-cache_patch:all-oblivious:half-rt-strict/inputs_batch1 EXPORT:  75%|███████▌  | 384/512 [03:26<05:11,  2.43s/it]    cache-cache_patch:torch/inputs EXPORT:  75%|███████▌  | 384/512 [03:26<05:11,  2.43s/it]                              


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch/inputs EXPORT:  75%|███████▌  | 385/512 [03:27<03:48,  1.80s/it]    cache-cache_patch:torch/inputs2 EXPORT:  75%|███████▌  | 385/512 [03:27<03:48,  1.80s/it]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch/inputs2 EXPORT:  75%|███████▌  | 386/512 [03:27<02:49,  1.35s/it]    cache-cache_patch:torch/inputs_empty_cache EXPORT:  75%|███████▌  | 386/512 [03:27<02:49,  1.35s/it]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch/inputs_empty_cache EXPORT:  76%|███████▌  | 387/512 [03:27<02:07,  1.02s/it]    cache-cache_patch:torch/inputs_batch1 EXPORT:  76%|███████▌  | 387/512 [03:27<02:07,  1.02s/it]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch/inputs_batch1 EXPORT:  76%|███████▌  | 388/512 [03:27<01:38,  1.25it/s]    cache-cache_patch:torch-rt/inputs EXPORT:  76%|███████▌  | 388/512 [03:27<01:38,  1.25it/s]    


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs EXPORT:  76%|███████▌  | 389/512 [03:28<01:18,  1.56it/s]    cache-cache_patch:torch-rt/inputs2 EXPORT:  76%|███████▌  | 389/512 [03:28<01:18,  1.56it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs2 EXPORT:  76%|███████▌  | 390/512 [03:28<01:04,  1.89it/s]    cache-cache_patch:torch-rt/inputs_empty_cache EXPORT:  76%|███████▌  | 390/512 [03:28<01:04,  1.89it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs_empty_cache EXPORT:  76%|███████▋  | 391/512 [03:28<00:54,  2.20it/s]    cache-cache_patch:torch-rt/inputs_batch1 EXPORT:  76%|███████▋  | 391/512 [03:28<00:54,  2.20it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-rt/inputs_batch1 EXPORT:  77%|███████▋  | 392/512 [03:28<00:47,  2.51it/s]    cache-cache_patch:torch-oblivious/inputs EXPORT:  77%|███████▋  | 392/512 [03:28<00:47,  2.51it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs EXPORT:  77%|███████▋  | 393/512 [03:29<00:42,  2.81it/s]    cache-cache_patch:torch-oblivious/inputs2 EXPORT:  77%|███████▋  | 393/512 [03:29<00:42,  2.81it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs2 EXPORT:  77%|███████▋  | 394/512 [03:29<00:38,  3.05it/s]    cache-cache_patch:torch-oblivious/inputs_empty_cache EXPORT:  77%|███████▋  | 394/512 [03:29<00:38,  3.05it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs_empty_cache EXPORT:  77%|███████▋  | 395/512 [03:29<00:37,  3.12it/s]    cache-cache_patch:torch-oblivious/inputs_batch1 EXPORT:  77%|███████▋  | 395/512 [03:29<00:37,  3.12it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious/inputs_batch1 EXPORT:  77%|███████▋  | 396/512 [03:30<00:36,  3.20it/s]    cache-cache_patch:torch-oblivious-rt/inputs EXPORT:  77%|███████▋  | 396/512 [03:30<00:36,  3.20it/s]    


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs EXPORT:  78%|███████▊  | 397/512 [03:30<00:34,  3.33it/s]    cache-cache_patch:torch-oblivious-rt/inputs2 EXPORT:  78%|███████▊  | 397/512 [03:30<00:34,  3.33it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs2 EXPORT:  78%|███████▊  | 398/512 [03:30<00:34,  3.27it/s]    cache-cache_patch:torch-oblivious-rt/inputs_empty_cache EXPORT:  78%|███████▊  | 398/512 [03:30<00:34,  3.27it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs_empty_cache EXPORT:  78%|███████▊  | 399/512 [03:30<00:33,  3.35it/s]    cache-cache_patch:torch-oblivious-rt/inputs_batch1 EXPORT:  78%|███████▊  | 399/512 [03:30<00:33,  3.35it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious-rt/inputs_batch1 EXPORT:  78%|███████▊  | 400/512 [03:31<00:32,  3.45it/s]    cache-cache_patch:torch-oblivious:auto/inputs EXPORT:  78%|███████▊  | 400/512 [03:31<00:32,  3.45it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto/inputs EXPORT:  78%|███████▊  | 401/512 [03:31<00:30,  3.59it/s]    cache-cache_patch:torch-oblivious:auto/inputs2 EXPORT:  78%|███████▊  | 401/512 [03:31<00:30,  3.59it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto/inputs2 EXPORT:  79%|███████▊  | 402/512 [03:31<00:30,  3.63it/s]    cache-cache_patch:torch-oblivious:auto/inputs_empty_cache EXPORT:  79%|███████▊  | 402/512 [03:31<00:30,  3.63it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto/inputs_empty_cache EXPORT:  79%|███████▊  | 403/512 [03:32<00:30,  3.63it/s]    cache-cache_patch:torch-oblivious:auto/inputs_batch1 EXPORT:  79%|███████▊  | 403/512 [03:32<00:30,  3.63it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto/inputs_batch1 EXPORT:  79%|███████▉  | 404/512 [03:32<00:30,  3.58it/s]    cache-cache_patch:torch-oblivious:auto-rt/inputs EXPORT:  79%|███████▉  | 404/512 [03:32<00:30,  3.58it/s]    


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto-rt/inputs EXPORT:  79%|███████▉  | 405/512 [03:32<00:30,  3.49it/s]    cache-cache_patch:torch-oblivious:auto-rt/inputs2 EXPORT:  79%|███████▉  | 405/512 [03:32<00:30,  3.49it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto-rt/inputs2 EXPORT:  79%|███████▉  | 406/512 [03:32<00:30,  3.45it/s]    cache-cache_patch:torch-oblivious:auto-rt/inputs_empty_cache EXPORT:  79%|███████▉  | 406/512 [03:32<00:30,  3.45it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto-rt/inputs_empty_cache EXPORT:  79%|███████▉  | 407/512 [03:33<00:33,  3.13it/s]    cache-cache_patch:torch-oblivious:auto-rt/inputs_batch1 EXPORT:  79%|███████▉  | 407/512 [03:33<00:33,  3.13it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:auto-rt/inputs_batch1 EXPORT:  80%|███████▉  | 408/512 [03:33<00:33,  3.07it/s]    cache-cache_patch:torch-oblivious:half/inputs EXPORT:  80%|███████▉  | 408/512 [03:33<00:33,  3.07it/s]          


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half/inputs EXPORT:  80%|███████▉  | 409/512 [03:34<00:37,  2.71it/s]    cache-cache_patch:torch-oblivious:half/inputs2 EXPORT:  80%|███████▉  | 409/512 [03:34<00:37,  2.71it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half/inputs2 EXPORT:  80%|████████  | 410/512 [03:34<00:35,  2.87it/s]    cache-cache_patch:torch-oblivious:half/inputs_empty_cache EXPORT:  80%|████████  | 410/512 [03:34<00:35,  2.87it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half/inputs_empty_cache EXPORT:  80%|████████  | 411/512 [03:34<00:34,  2.96it/s]    cache-cache_patch:torch-oblivious:half/inputs_batch1 EXPORT:  80%|████████  | 411/512 [03:34<00:34,  2.96it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half/inputs_batch1 EXPORT:  80%|████████  | 412/512 [03:35<00:32,  3.09it/s]    cache-cache_patch:torch-oblivious:half-rt/inputs EXPORT:  80%|████████  | 412/512 [03:35<00:32,  3.09it/s]    


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half-rt/inputs EXPORT:  81%|████████  | 413/512 [03:38<02:01,  1.23s/it]    cache-cache_patch:torch-oblivious:half-rt/inputs2 EXPORT:  81%|████████  | 413/512 [03:38<02:01,  1.23s/it]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half-rt/inputs2 EXPORT:  81%|████████  | 414/512 [03:38<01:32,  1.06it/s]    cache-cache_patch:torch-oblivious:half-rt/inputs_empty_cache EXPORT:  81%|████████  | 414/512 [03:38<01:32,  1.06it/s]


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half-rt/inputs_empty_cache EXPORT:  81%|████████  | 415/512 [03:38<01:12,  1.34it/s]    cache-cache_patch:torch-oblivious:half-rt/inputs_batch1 EXPORT:  81%|████████  | 415/512 [03:38<01:12,  1.34it/s]     


    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    



    def forward(self, arg0_1: "f32[32000, 192]", arg1_1: "f32[192, 192]", arg2_1: "f32[96, 192]", arg3_1: "f32[96, 192]", arg4_1: "f32[192, 192]", arg5_1: "f32[1024, 192]", arg6_1: "f32[1024, 192]", arg7_1: "f32[192, 1024]", arg8_1: "f32[192]", arg9_1: "f32[192]", arg10_1: "f32[192]", arg11_1: "f32[32000, 192]", arg12_1: "f32[48]", arg13_1: "i64[s72, s70]", arg14_1: "i64[s43, s53]", arg15_1: "i64[s44, s9]", arg16_1: "f32[s23, 1, s31, 96]", arg17_1: "f32[s4, 1, s11, 96]"):
        # No stacktrace found for following nodes
        _tensor_constant0: "f32[0]" = self._tensor_constant0
        lift_fresh_copy: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        detach_: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None
        _tensor_constant1: "f32[0]" = self._tensor_constant1
        lift_fresh_copy_1: "f32[0]" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        detach__1: "f32[0]" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None
        cat: "f32[s23, 1, s31, 96]" = torch.ops.aten.cat.default([detach_, arg16_1], -2);  detach_ = arg16_1 = None
        cat_1: "f32[s4, 1, s11, 96]" = torch.ops.aten.cat.default([detach__1, arg17_1], -2);  detach__1 = arg17_1 = cat_1 = None
    
         # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
        embedding: "f32[s72, s70, 192]" = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        sym_numel_default: "Sym(96*s23*s31)" = torch.ops.aten.sym_numel.default(cat)
        eq: "Sym(False)" = sym_numel_default == 0;  eq = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        sym_size_int: "Sym(s31)" = torch.ops.aten.sym_size.int(cat, 2);  cat = None
        sym_size_int_1: "Sym(s70)" = torch.ops.aten.sym_size.int(arg13_1, 1)
        add: "Sym(s31 + s70)" = sym_size_int + sym_size_int_1;  sym_size_int_1 = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
        arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int, add, device = device(type='cpu'), pin_memory = False);  add = None
    
         # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
        to: "b8[s43, s53]" = torch.ops.aten.to.device(arg14_1, device(type='cpu'), torch.bool);  to = None
        eq_1: "Sym(False)" = sym_numel_default == 0;  sym_numel_default = eq_1 = None
        sym_size_int_2: "Sym(s70)" = torch.ops.aten.sym_size.int(arange, 0)
        add_1: "Sym(s31 + s70)" = sym_size_int + sym_size_int_2;  sym_size_int = None
        add_2: "Sym(s31 + s70)" = add_1 + 0
        sym_size_int_3: "Sym(s53)" = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
        sub: "Sym(s31 - s53 + s70)" = add_2 - sym_size_int_3;  add_2 = None
        gt: "Sym(s31 - s53 + s70 > 0)" = sub > 0;  sub = gt = None
        gt_1: "Sym(s53 > s31 + s70)" = sym_size_int_3 > add_1;  sym_size_int_3 = gt_1 = None
        arange_1: "i64[s31 + s70]" = torch.ops.aten.arange.default(add_1, device = device(type='cpu'), pin_memory = False);  add_1 = None
        add_: "i64[s31 + s70]" = torch.ops.aten.add_.Tensor(arange_1, 0)
        sym_size_int_4: "Sym(s72)" = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
        arange_2: "i64[s72]" = torch.ops.aten.arange.default(sym_size_int_4, device = device(type='cpu'), pin_memory = False);  sym_size_int_4 = None
        arange_3: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
        movedim: "i64[s72]" = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
        unsqueeze: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
        sym_size_int_5: "Sym(s72)" = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
        expand: "i64[s72, 1]" = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
        unsqueeze_1: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
        expand_1: "i64[s72, s70]" = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
        unsqueeze_2: "i64[1, s31 + s70]" = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
        sym_size_int_6: "Sym(s31 + s70)" = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
        expand_2: "i64[s72, s31 + s70]" = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None
    
    cache-cache_patch:torch-oblivious:half-rt/inputs_batch1 EXPORT:  81%|████████▏ | 416/512 [03:39<00:58,  1.65it/s]    cache-cache_patch:torch-strict/inputs EXPORT:  81%|████████▏ | 416/512 [03:39<00:58,  1.65it/s]                      cache-cache_patch:torch-strict/inputs EXPORT:  81%|████████▏ | 417/512 [03:39<00:47,  2.01it/s]    cache-cache_patch:torch-strict/inputs2 EXPORT:  81%|████████▏ | 417/512 [03:39<00:47,  2.01it/s]    cache-cache_patch:torch-strict/inputs2 EXPORT:  82%|████████▏ | 418/512 [03:39<00:39,  2.38it/s]    cache-cache_patch:torch-strict/inputs_empty_cache EXPORT:  82%|████████▏ | 418/512 [03:39<00:39,  2.38it/s]    cache-cache_patch:torch-strict/inputs_empty_cache EXPORT:  82%|████████▏ | 419/512 [03:39<00:32,  2.82it/s]    cache-cache_patch:torch-strict/inputs_batch1 EXPORT:  82%|████████▏ | 419/512 [03:39<00:32,  2.82it/s]         cache-cache_patch:torch-strict/inputs_batch1 EXPORT:  82%|████████▏ | 420/512 [03:40<00:27,  3.36it/s]    cache-cache_patch:torch-rt-strict/inputs EXPORT:  82%|████████▏ | 420/512 [03:40<00:27,  3.36it/s]        cache-cache_patch:torch-rt-strict/inputs EXPORT:  82%|████████▏ | 421/512 [03:40<00:24,  3.79it/s]    cache-cache_patch:torch-rt-strict/inputs2 EXPORT:  82%|████████▏ | 421/512 [03:40<00:24,  3.79it/s]    cache-cache_patch:torch-rt-strict/inputs2 EXPORT:  82%|████████▏ | 422/512 [03:40<00:23,  3.90it/s]    cache-cache_patch:torch-rt-strict/inputs_empty_cache EXPORT:  82%|████████▏ | 422/512 [03:40<00:23,  3.90it/s]    cache-cache_patch:torch-rt-strict/inputs_empty_cache EXPORT:  83%|████████▎ | 423/512 [03:40<00:20,  4.40it/s]    cache-cache_patch:torch-rt-strict/inputs_batch1 EXPORT:  83%|████████▎ | 423/512 [03:40<00:20,  4.40it/s]         cache-cache_patch:torch-oblivious-strict/inputs EXPORT:  83%|████████▎ | 423/512 [03:40<00:20,  4.40it/s]    cache-cache_patch:torch-oblivious-strict/inputs2 EXPORT:  83%|████████▎ | 423/512 [03:40<00:20,  4.40it/s]    cache-cache_patch:torch-oblivious-strict/inputs_empty_cache EXPORT:  83%|████████▎ | 423/512 [03:40<00:20,  4.40it/s]    cache-cache_patch:torch-oblivious-strict/inputs_batch1 EXPORT:  83%|████████▎ | 423/512 [03:40<00:20,  4.40it/s]         cache-cache_patch:torch-oblivious-strict/inputs_batch1 EXPORT:  84%|████████▎ | 428/512 [03:40<00:08,  9.40it/s]    cache-cache_patch:torch-oblivious-rt-strict/inputs EXPORT:  84%|████████▎ | 428/512 [03:40<00:08,  9.40it/s]        cache-cache_patch:torch-oblivious-rt-strict/inputs EXPORT:  84%|████████▍ | 429/512 [03:41<00:09,  8.30it/s]    cache-cache_patch:torch-oblivious-rt-strict/inputs2 EXPORT:  84%|████████▍ | 429/512 [03:41<00:09,  8.30it/s]    cache-cache_patch:torch-oblivious-rt-strict/inputs2 EXPORT:  84%|████████▍ | 430/512 [03:41<00:10,  7.52it/s]    cache-cache_patch:torch-oblivious-rt-strict/inputs_empty_cache EXPORT:  84%|████████▍ | 430/512 [03:41<00:10,  7.52it/s]    cache-cache_patch:torch-oblivious-rt-strict/inputs_empty_cache EXPORT:  84%|████████▍ | 431/512 [03:41<00:12,  6.73it/s]    cache-cache_patch:torch-oblivious-rt-strict/inputs_batch1 EXPORT:  84%|████████▍ | 431/512 [03:41<00:12,  6.73it/s]         cache-cache_patch:torch-oblivious-rt-strict/inputs_batch1 EXPORT:  84%|████████▍ | 432/512 [03:41<00:13,  6.15it/s]    cache-cache_patch:torch-oblivious:auto-strict/inputs EXPORT:  84%|████████▍ | 432/512 [03:41<00:13,  6.15it/s]         cache-cache_patch:torch-oblivious:auto-strict/inputs EXPORT:  85%|████████▍ | 433/512 [03:41<00:12,  6.22it/s]    cache-cache_patch:torch-oblivious:auto-strict/inputs2 EXPORT:  85%|████████▍ | 433/512 [03:41<00:12,  6.22it/s]    cache-cache_patch:torch-oblivious:auto-strict/inputs2 EXPORT:  85%|████████▍ | 434/512 [03:42<00:13,  5.71it/s]    cache-cache_patch:torch-oblivious:auto-strict/inputs_empty_cache EXPORT:  85%|████████▍ | 434/512 [03:42<00:13,  5.71it/s]    cache-cache_patch:torch-oblivious:auto-strict/inputs_empty_cache EXPORT:  85%|████████▍ | 435/512 [03:42<00:13,  5.57it/s]    cache-cache_patch:torch-oblivious:auto-strict/inputs_batch1 EXPORT:  85%|████████▍ | 435/512 [03:42<00:13,  5.57it/s]         cache-cache_patch:torch-oblivious:auto-strict/inputs_batch1 EXPORT:  85%|████████▌ | 436/512 [03:42<00:14,  5.13it/s]    cache-cache_patch:torch-oblivious:auto-rt-strict/inputs EXPORT:  85%|████████▌ | 436/512 [03:42<00:14,  5.13it/s]        cache-cache_patch:torch-oblivious:auto-rt-strict/inputs EXPORT:  85%|████████▌ | 437/512 [03:42<00:14,  5.03it/s]    cache-cache_patch:torch-oblivious:auto-rt-strict/inputs2 EXPORT:  85%|████████▌ | 437/512 [03:42<00:14,  5.03it/s]    cache-cache_patch:torch-oblivious:auto-rt-strict/inputs2 EXPORT:  86%|████████▌ | 438/512 [03:42<00:16,  4.37it/s]    cache-cache_patch:torch-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  86%|████████▌ | 438/512 [03:42<00:16,  4.37it/s]    cache-cache_patch:torch-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  86%|████████▌ | 439/512 [03:43<00:16,  4.47it/s]    cache-cache_patch:torch-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  86%|████████▌ | 439/512 [03:43<00:16,  4.47it/s]         cache-cache_patch:torch-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  86%|████████▌ | 440/512 [03:43<00:16,  4.32it/s]    cache-cache_patch:torch-oblivious:half-strict/inputs EXPORT:  86%|████████▌ | 440/512 [03:43<00:16,  4.32it/s]              cache-cache_patch:torch-oblivious:half-strict/inputs EXPORT:  86%|████████▌ | 441/512 [03:43<00:15,  4.63it/s]    cache-cache_patch:torch-oblivious:half-strict/inputs2 EXPORT:  86%|████████▌ | 441/512 [03:43<00:15,  4.63it/s]    cache-cache_patch:torch-oblivious:half-strict/inputs2 EXPORT:  86%|████████▋ | 442/512 [03:43<00:14,  4.79it/s]    cache-cache_patch:torch-oblivious:half-strict/inputs_empty_cache EXPORT:  86%|████████▋ | 442/512 [03:43<00:14,  4.79it/s]    cache-cache_patch:torch-oblivious:half-strict/inputs_empty_cache EXPORT:  87%|████████▋ | 443/512 [03:44<00:14,  4.76it/s]    cache-cache_patch:torch-oblivious:half-strict/inputs_batch1 EXPORT:  87%|████████▋ | 443/512 [03:44<00:14,  4.76it/s]         cache-cache_patch:torch-oblivious:half-strict/inputs_batch1 EXPORT:  87%|████████▋ | 444/512 [03:44<00:14,  4.85it/s]    cache-cache_patch:torch-oblivious:half-rt-strict/inputs EXPORT:  87%|████████▋ | 444/512 [03:44<00:14,  4.85it/s]        cache-cache_patch:torch-oblivious:half-rt-strict/inputs EXPORT:  87%|████████▋ | 445/512 [03:44<00:14,  4.72it/s]    cache-cache_patch:torch-oblivious:half-rt-strict/inputs2 EXPORT:  87%|████████▋ | 445/512 [03:44<00:14,  4.72it/s]    cache-cache_patch:torch-oblivious:half-rt-strict/inputs2 EXPORT:  87%|████████▋ | 446/512 [03:44<00:13,  4.78it/s]    cache-cache_patch:torch-oblivious:half-rt-strict/inputs_empty_cache EXPORT:  87%|████████▋ | 446/512 [03:44<00:13,  4.78it/s]    cache-cache_patch:torch-oblivious:half-rt-strict/inputs_empty_cache EXPORT:  87%|████████▋ | 447/512 [03:44<00:14,  4.56it/s]    cache-cache_patch:torch-oblivious:half-rt-strict/inputs_batch1 EXPORT:  87%|████████▋ | 447/512 [03:44<00:14,  4.56it/s]         cache-cache_patch:torch-oblivious:half-rt-strict/inputs_batch1 EXPORT:  88%|████████▊ | 448/512 [03:45<00:13,  4.87it/s]    cache-cache_patch:transformers/inputs EXPORT:  88%|████████▊ | 448/512 [03:45<00:13,  4.87it/s]                             cache-cache_patch:transformers/inputs VALIDATE:  88%|████████▊ | 448/512 [03:46<00:13,  4.87it/s]    cache-cache_patch:transformers/inputs VALIDATE:  88%|████████▊ | 449/512 [03:46<00:40,  1.55it/s]    cache-cache_patch:transformers/inputs2 EXPORT:  88%|████████▊ | 449/512 [03:46<00:40,  1.55it/s]     cache-cache_patch:transformers/inputs2 VALIDATE:  88%|████████▊ | 449/512 [03:48<00:40,  1.55it/s]    cache-cache_patch:transformers/inputs2 VALIDATE:  88%|████████▊ | 450/512 [03:48<00:58,  1.06it/s]    cache-cache_patch:transformers/inputs_empty_cache EXPORT:  88%|████████▊ | 450/512 [03:48<00:58,  1.06it/s]    cache-cache_patch:transformers/inputs_empty_cache VALIDATE:  88%|████████▊ | 450/512 [03:50<00:58,  1.06it/s]    cache-cache_patch:transformers/inputs_empty_cache VALIDATE:  88%|████████▊ | 451/512 [03:50<01:24,  1.38s/it]    cache-cache_patch:transformers/inputs_batch1 EXPORT:  88%|████████▊ | 451/512 [03:50<01:24,  1.38s/it]           cache-cache_patch:transformers/inputs_batch1 EXPORT:  88%|████████▊ | 452/512 [03:51<01:15,  1.27s/it]    cache-cache_patch:transformers-rt/inputs EXPORT:  88%|████████▊ | 452/512 [03:51<01:15,  1.27s/it]        cache-cache_patch:transformers-rt/inputs VALIDATE:  88%|████████▊ | 452/512 [03:53<01:15,  1.27s/it]    cache-cache_patch:transformers-rt/inputs VALIDATE:  88%|████████▊ | 453/512 [03:53<01:23,  1.41s/it]    cache-cache_patch:transformers-rt/inputs2 EXPORT:  88%|████████▊ | 453/512 [03:53<01:23,  1.41s/it]     cache-cache_patch:transformers-rt/inputs2 VALIDATE:  88%|████████▊ | 453/512 [03:55<01:23,  1.41s/it]    cache-cache_patch:transformers-rt/inputs2 VALIDATE:  89%|████████▊ | 454/512 [03:55<01:37,  1.68s/it]    cache-cache_patch:transformers-rt/inputs_empty_cache EXPORT:  89%|████████▊ | 454/512 [03:55<01:37,  1.68s/it]    cache-cache_patch:transformers-rt/inputs_empty_cache VALIDATE:  89%|████████▊ | 454/512 [03:57<01:37,  1.68s/it]    cache-cache_patch:transformers-rt/inputs_empty_cache VALIDATE:  89%|████████▉ | 455/512 [03:57<01:39,  1.74s/it]    cache-cache_patch:transformers-rt/inputs_batch1 EXPORT:  89%|████████▉ | 455/512 [03:57<01:39,  1.74s/it]           cache-cache_patch:transformers-rt/inputs_batch1 EXPORT:  89%|████████▉ | 456/512 [03:58<01:25,  1.52s/it]    cache-cache_patch:transformers-oblivious/inputs EXPORT:  89%|████████▉ | 456/512 [03:58<01:25,  1.52s/it]    cache-cache_patch:transformers-oblivious/inputs VALIDATE:  89%|████████▉ | 456/512 [04:00<01:25,  1.52s/it]    cache-cache_patch:transformers-oblivious/inputs VALIDATE:  89%|████████▉ | 457/512 [04:00<01:23,  1.52s/it]    cache-cache_patch:transformers-oblivious/inputs2 EXPORT:  89%|████████▉ | 457/512 [04:00<01:23,  1.52s/it]     cache-cache_patch:transformers-oblivious/inputs2 VALIDATE:  89%|████████▉ | 457/512 [04:01<01:23,  1.52s/it]    cache-cache_patch:transformers-oblivious/inputs2 VALIDATE:  89%|████████▉ | 458/512 [04:01<01:21,  1.51s/it]    cache-cache_patch:transformers-oblivious/inputs_empty_cache EXPORT:  89%|████████▉ | 458/512 [04:01<01:21,  1.51s/it]    cache-cache_patch:transformers-oblivious/inputs_empty_cache VALIDATE:  89%|████████▉ | 458/512 [04:03<01:21,  1.51s/it]    cache-cache_patch:transformers-oblivious/inputs_empty_cache VALIDATE:  90%|████████▉ | 459/512 [04:03<01:19,  1.49s/it]    cache-cache_patch:transformers-oblivious/inputs_batch1 EXPORT:  90%|████████▉ | 459/512 [04:03<01:19,  1.49s/it]           cache-cache_patch:transformers-oblivious/inputs_batch1 VALIDATE:  90%|████████▉ | 459/512 [04:04<01:19,  1.49s/it]    cache-cache_patch:transformers-oblivious/inputs_batch1 VALIDATE:  90%|████████▉ | 460/512 [04:04<01:21,  1.57s/it]    cache-cache_patch:transformers-oblivious-rt/inputs EXPORT:  90%|████████▉ | 460/512 [04:04<01:21,  1.57s/it]          cache-cache_patch:transformers-oblivious-rt/inputs VALIDATE:  90%|████████▉ | 460/512 [04:06<01:21,  1.57s/it]    cache-cache_patch:transformers-oblivious-rt/inputs VALIDATE:  90%|█████████ | 461/512 [04:06<01:17,  1.53s/it]    cache-cache_patch:transformers-oblivious-rt/inputs2 EXPORT:  90%|█████████ | 461/512 [04:06<01:17,  1.53s/it]     cache-cache_patch:transformers-oblivious-rt/inputs2 VALIDATE:  90%|█████████ | 461/512 [04:07<01:17,  1.53s/it]    cache-cache_patch:transformers-oblivious-rt/inputs2 VALIDATE:  90%|█████████ | 462/512 [04:07<01:14,  1.50s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache EXPORT:  90%|█████████ | 462/512 [04:07<01:14,  1.50s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache VALIDATE:  90%|█████████ | 462/512 [04:09<01:14,  1.50s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache VALIDATE:  90%|█████████ | 463/512 [04:09<01:13,  1.49s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_batch1 EXPORT:  90%|█████████ | 463/512 [04:09<01:13,  1.49s/it]           cache-cache_patch:transformers-oblivious-rt/inputs_batch1 VALIDATE:  90%|█████████ | 463/512 [04:10<01:13,  1.49s/it]    cache-cache_patch:transformers-oblivious-rt/inputs_batch1 VALIDATE:  91%|█████████ | 464/512 [04:10<01:10,  1.48s/it]    cache-cache_patch:transformers-oblivious:auto/inputs EXPORT:  91%|█████████ | 464/512 [04:10<01:10,  1.48s/it]           cache-cache_patch:transformers-oblivious:auto/inputs VALIDATE:  91%|█████████ | 464/512 [04:12<01:10,  1.48s/it]    cache-cache_patch:transformers-oblivious:auto/inputs VALIDATE:  91%|█████████ | 465/512 [04:12<01:10,  1.49s/it]    cache-cache_patch:transformers-oblivious:auto/inputs2 EXPORT:  91%|█████████ | 465/512 [04:12<01:10,  1.49s/it]     cache-cache_patch:transformers-oblivious:auto/inputs2 VALIDATE:  91%|█████████ | 465/512 [04:12<01:10,  1.49s/it]    cache-cache_patch:transformers-oblivious:auto/inputs2 VALIDATE:  91%|█████████ | 466/512 [04:13<01:00,  1.32s/it]    cache-cache_patch:transformers-oblivious:auto/inputs_empty_cache EXPORT:  91%|█████████ | 466/512 [04:13<01:00,  1.32s/it]    cache-cache_patch:transformers-oblivious:auto/inputs_empty_cache VALIDATE:  91%|█████████ | 466/512 [04:14<01:00,  1.32s/it]    cache-cache_patch:transformers-oblivious:auto/inputs_empty_cache VALIDATE:  91%|█████████ | 467/512 [04:14<01:00,  1.34s/it]    cache-cache_patch:transformers-oblivious:auto/inputs_batch1 EXPORT:  91%|█████████ | 467/512 [04:14<01:00,  1.34s/it]           cache-cache_patch:transformers-oblivious:auto/inputs_batch1 VALIDATE:  91%|█████████ | 467/512 [04:15<01:00,  1.34s/it]    cache-cache_patch:transformers-oblivious:auto/inputs_batch1 VALIDATE:  91%|█████████▏| 468/512 [04:15<01:00,  1.38s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs EXPORT:  91%|█████████▏| 468/512 [04:15<01:00,  1.38s/it]          cache-cache_patch:transformers-oblivious:auto-rt/inputs VALIDATE:  91%|█████████▏| 468/512 [04:17<01:00,  1.38s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs VALIDATE:  92%|█████████▏| 469/512 [04:17<01:01,  1.43s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs2 EXPORT:  92%|█████████▏| 469/512 [04:17<01:01,  1.43s/it]     cache-cache_patch:transformers-oblivious:auto-rt/inputs2 VALIDATE:  92%|█████████▏| 469/512 [04:18<01:01,  1.43s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs2 VALIDATE:  92%|█████████▏| 470/512 [04:19<01:02,  1.48s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs_empty_cache EXPORT:  92%|█████████▏| 470/512 [04:19<01:02,  1.48s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs_empty_cache VALIDATE:  92%|█████████▏| 470/512 [04:20<01:02,  1.48s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs_empty_cache VALIDATE:  92%|█████████▏| 471/512 [04:20<01:01,  1.50s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs_batch1 EXPORT:  92%|█████████▏| 471/512 [04:20<01:01,  1.50s/it]           cache-cache_patch:transformers-oblivious:auto-rt/inputs_batch1 VALIDATE:  92%|█████████▏| 471/512 [04:22<01:01,  1.50s/it]    cache-cache_patch:transformers-oblivious:auto-rt/inputs_batch1 VALIDATE:  92%|█████████▏| 472/512 [04:22<01:02,  1.55s/it]    cache-cache_patch:transformers-oblivious:half/inputs EXPORT:  92%|█████████▏| 472/512 [04:22<01:02,  1.55s/it]                cache-cache_patch:transformers-oblivious:half/inputs VALIDATE:  92%|█████████▏| 472/512 [04:26<01:02,  1.55s/it]    cache-cache_patch:transformers-oblivious:half/inputs VALIDATE:  92%|█████████▏| 473/512 [04:27<01:38,  2.51s/it]    cache-cache_patch:transformers-oblivious:half/inputs2 EXPORT:  92%|█████████▏| 473/512 [04:27<01:38,  2.51s/it]     cache-cache_patch:transformers-oblivious:half/inputs2 VALIDATE:  92%|█████████▏| 473/512 [04:28<01:38,  2.51s/it]    cache-cache_patch:transformers-oblivious:half/inputs2 VALIDATE:  93%|█████████▎| 474/512 [04:29<01:29,  2.35s/it]    cache-cache_patch:transformers-oblivious:half/inputs_empty_cache EXPORT:  93%|█████████▎| 474/512 [04:29<01:29,  2.35s/it]    cache-cache_patch:transformers-oblivious:half/inputs_empty_cache VALIDATE:  93%|█████████▎| 474/512 [04:30<01:29,  2.35s/it]    cache-cache_patch:transformers-oblivious:half/inputs_empty_cache VALIDATE:  93%|█████████▎| 475/512 [04:30<01:18,  2.13s/it]    cache-cache_patch:transformers-oblivious:half/inputs_batch1 EXPORT:  93%|█████████▎| 475/512 [04:30<01:18,  2.13s/it]           cache-cache_patch:transformers-oblivious:half/inputs_batch1 VALIDATE:  93%|█████████▎| 475/512 [04:32<01:18,  2.13s/it]    cache-cache_patch:transformers-oblivious:half/inputs_batch1 VALIDATE:  93%|█████████▎| 476/512 [04:32<01:15,  2.10s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs EXPORT:  93%|█████████▎| 476/512 [04:32<01:15,  2.10s/it]          cache-cache_patch:transformers-oblivious:half-rt/inputs VALIDATE:  93%|█████████▎| 476/512 [04:34<01:15,  2.10s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs VALIDATE:  93%|█████████▎| 477/512 [04:34<01:09,  2.00s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs2 EXPORT:  93%|█████████▎| 477/512 [04:34<01:09,  2.00s/it]     cache-cache_patch:transformers-oblivious:half-rt/inputs2 VALIDATE:  93%|█████████▎| 477/512 [04:36<01:09,  2.00s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs2 VALIDATE:  93%|█████████▎| 478/512 [04:36<01:08,  2.01s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs_empty_cache EXPORT:  93%|█████████▎| 478/512 [04:36<01:08,  2.01s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs_empty_cache VALIDATE:  93%|█████████▎| 478/512 [04:37<01:08,  2.01s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs_empty_cache VALIDATE:  94%|█████████▎| 479/512 [04:38<01:01,  1.87s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs_batch1 EXPORT:  94%|█████████▎| 479/512 [04:38<01:01,  1.87s/it]           cache-cache_patch:transformers-oblivious:half-rt/inputs_batch1 VALIDATE:  94%|█████████▎| 479/512 [04:39<01:01,  1.87s/it]    cache-cache_patch:transformers-oblivious:half-rt/inputs_batch1 VALIDATE:  94%|█████████▍| 480/512 [04:39<00:57,  1.79s/it]    cache-cache_patch:transformers-strict/inputs EXPORT:  94%|█████████▍| 480/512 [04:39<00:57,  1.79s/it]                        cache-cache_patch:transformers-strict/inputs EXPORT:  94%|█████████▍| 481/512 [04:41<00:53,  1.72s/it]    cache-cache_patch:transformers-strict/inputs2 EXPORT:  94%|█████████▍| 481/512 [04:41<00:53,  1.72s/it]    cache-cache_patch:transformers-strict/inputs2 EXPORT:  94%|█████████▍| 482/512 [04:42<00:49,  1.66s/it]    cache-cache_patch:transformers-strict/inputs_empty_cache EXPORT:  94%|█████████▍| 482/512 [04:42<00:49,  1.66s/it]    cache-cache_patch:transformers-strict/inputs_empty_cache EXPORT:  94%|█████████▍| 483/512 [04:43<00:40,  1.39s/it]    cache-cache_patch:transformers-strict/inputs_batch1 EXPORT:  94%|█████████▍| 483/512 [04:43<00:40,  1.39s/it]         cache-cache_patch:transformers-strict/inputs_batch1 EXPORT:  95%|█████████▍| 484/512 [04:45<00:46,  1.65s/it]    cache-cache_patch:transformers-rt-strict/inputs EXPORT:  95%|█████████▍| 484/512 [04:45<00:46,  1.65s/it]        cache-cache_patch:transformers-rt-strict/inputs EXPORT:  95%|█████████▍| 485/512 [04:48<00:53,  1.98s/it]    cache-cache_patch:transformers-rt-strict/inputs2 EXPORT:  95%|█████████▍| 485/512 [04:48<00:53,  1.98s/it]    cache-cache_patch:transformers-rt-strict/inputs2 EXPORT:  95%|█████████▍| 486/512 [04:51<00:56,  2.15s/it]    cache-cache_patch:transformers-rt-strict/inputs_empty_cache EXPORT:  95%|█████████▍| 486/512 [04:51<00:56,  2.15s/it]    cache-cache_patch:transformers-rt-strict/inputs_empty_cache EXPORT:  95%|█████████▌| 487/512 [04:53<00:56,  2.26s/it]    cache-cache_patch:transformers-rt-strict/inputs_batch1 EXPORT:  95%|█████████▌| 487/512 [04:53<00:56,  2.26s/it]         cache-cache_patch:transformers-rt-strict/inputs_batch1 EXPORT:  95%|█████████▌| 488/512 [04:56<00:55,  2.33s/it]    cache-cache_patch:transformers-oblivious-strict/inputs EXPORT:  95%|█████████▌| 488/512 [04:56<00:55,  2.33s/it]    cache-cache_patch:transformers-oblivious-strict/inputs EXPORT:  96%|█████████▌| 489/512 [04:57<00:48,  2.12s/it]    cache-cache_patch:transformers-oblivious-strict/inputs2 EXPORT:  96%|█████████▌| 489/512 [04:57<00:48,  2.12s/it]    cache-cache_patch:transformers-oblivious-strict/inputs2 EXPORT:  96%|█████████▌| 490/512 [04:58<00:41,  1.87s/it]    cache-cache_patch:transformers-oblivious-strict/inputs_empty_cache EXPORT:  96%|█████████▌| 490/512 [04:58<00:41,  1.87s/it]    cache-cache_patch:transformers-oblivious-strict/inputs_empty_cache EXPORT:  96%|█████████▌| 491/512 [05:00<00:34,  1.66s/it]    cache-cache_patch:transformers-oblivious-strict/inputs_batch1 EXPORT:  96%|█████████▌| 491/512 [05:00<00:34,  1.66s/it]         cache-cache_patch:transformers-oblivious-strict/inputs_batch1 EXPORT:  96%|█████████▌| 492/512 [05:04<00:46,  2.33s/it]    cache-cache_patch:transformers-oblivious-rt-strict/inputs EXPORT:  96%|█████████▌| 492/512 [05:04<00:46,  2.33s/it]        cache-cache_patch:transformers-oblivious-rt-strict/inputs EXPORT:  96%|█████████▋| 493/512 [05:06<00:45,  2.42s/it]    cache-cache_patch:transformers-oblivious-rt-strict/inputs2 EXPORT:  96%|█████████▋| 493/512 [05:06<00:45,  2.42s/it]    cache-cache_patch:transformers-oblivious-rt-strict/inputs2 EXPORT:  96%|█████████▋| 494/512 [05:09<00:43,  2.41s/it]    cache-cache_patch:transformers-oblivious-rt-strict/inputs_empty_cache EXPORT:  96%|█████████▋| 494/512 [05:09<00:43,  2.41s/it]    cache-cache_patch:transformers-oblivious-rt-strict/inputs_empty_cache EXPORT:  97%|█████████▋| 495/512 [05:12<00:47,  2.78s/it]    cache-cache_patch:transformers-oblivious-rt-strict/inputs_batch1 EXPORT:  97%|█████████▋| 495/512 [05:12<00:47,  2.78s/it]         cache-cache_patch:transformers-oblivious-rt-strict/inputs_batch1 EXPORT:  97%|█████████▋| 496/512 [05:15<00:44,  2.79s/it]    cache-cache_patch:transformers-oblivious:auto-strict/inputs EXPORT:  97%|█████████▋| 496/512 [05:15<00:44,  2.79s/it]         cache-cache_patch:transformers-oblivious:auto-strict/inputs EXPORT:  97%|█████████▋| 497/512 [05:16<00:35,  2.37s/it]    cache-cache_patch:transformers-oblivious:auto-strict/inputs2 EXPORT:  97%|█████████▋| 497/512 [05:16<00:35,  2.37s/it]    cache-cache_patch:transformers-oblivious:auto-strict/inputs2 EXPORT:  97%|█████████▋| 498/512 [05:18<00:28,  2.04s/it]    cache-cache_patch:transformers-oblivious:auto-strict/inputs_empty_cache EXPORT:  97%|█████████▋| 498/512 [05:18<00:28,  2.04s/it]    cache-cache_patch:transformers-oblivious:auto-strict/inputs_empty_cache EXPORT:  97%|█████████▋| 499/512 [05:19<00:22,  1.75s/it]    cache-cache_patch:transformers-oblivious:auto-strict/inputs_batch1 EXPORT:  97%|█████████▋| 499/512 [05:19<00:22,  1.75s/it]         cache-cache_patch:transformers-oblivious:auto-strict/inputs_batch1 EXPORT:  98%|█████████▊| 500/512 [05:21<00:22,  1.89s/it]    cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs EXPORT:  98%|█████████▊| 500/512 [05:21<00:22,  1.89s/it]        cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs EXPORT:  98%|█████████▊| 501/512 [05:24<00:22,  2.09s/it]    cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs2 EXPORT:  98%|█████████▊| 501/512 [05:24<00:22,  2.09s/it]    cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs2 EXPORT:  98%|█████████▊| 502/512 [05:29<00:31,  3.17s/it]    cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  98%|█████████▊| 502/512 [05:29<00:31,  3.17s/it]    cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs_empty_cache EXPORT:  98%|█████████▊| 503/512 [05:32<00:26,  2.98s/it]    cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  98%|█████████▊| 503/512 [05:32<00:26,  2.98s/it]         cache-cache_patch:transformers-oblivious:auto-rt-strict/inputs_batch1 EXPORT:  98%|█████████▊| 504/512 [05:34<00:22,  2.76s/it]    cache-cache_patch:transformers-oblivious:half-strict/inputs EXPORT:  98%|█████████▊| 504/512 [05:34<00:22,  2.76s/it]              cache-cache_patch:transformers-oblivious:half-strict/inputs EXPORT:  99%|█████████▊| 505/512 [05:35<00:16,  2.31s/it]    cache-cache_patch:transformers-oblivious:half-strict/inputs2 EXPORT:  99%|█████████▊| 505/512 [05:35<00:16,  2.31s/it]    cache-cache_patch:transformers-oblivious:half-strict/inputs2 EXPORT:  99%|█████████▉| 506/512 [05:36<00:11,  1.96s/it]    cache-cache_patch:transformers-oblivious:half-strict/inputs_empty_cache EXPORT:  99%|█████████▉| 506/512 [05:36<00:11,  1.96s/it]    cache-cache_patch:transformers-oblivious:half-strict/inputs_empty_cache EXPORT:  99%|█████████▉| 507/512 [05:38<00:08,  1.74s/it]    cache-cache_patch:transformers-oblivious:half-strict/inputs_batch1 EXPORT:  99%|█████████▉| 507/512 [05:38<00:08,  1.74s/it]         cache-cache_patch:transformers-oblivious:half-strict/inputs_batch1 EXPORT:  99%|█████████▉| 508/512 [05:40<00:07,  1.94s/it]    cache-cache_patch:transformers-oblivious:half-rt-strict/inputs EXPORT:  99%|█████████▉| 508/512 [05:40<00:07,  1.94s/it]        cache-cache_patch:transformers-oblivious:half-rt-strict/inputs EXPORT:  99%|█████████▉| 509/512 [05:42<00:06,  2.06s/it]    cache-cache_patch:transformers-oblivious:half-rt-strict/inputs2 EXPORT:  99%|█████████▉| 509/512 [05:42<00:06,  2.06s/it]    cache-cache_patch:transformers-oblivious:half-rt-strict/inputs2 EXPORT: 100%|█████████▉| 510/512 [05:45<00:04,  2.19s/it]    cache-cache_patch:transformers-oblivious:half-rt-strict/inputs_empty_cache EXPORT: 100%|█████████▉| 510/512 [05:45<00:04,  2.19s/it]    cache-cache_patch:transformers-oblivious:half-rt-strict/inputs_empty_cache EXPORT: 100%|█████████▉| 511/512 [05:47<00:02,  2.05s/it]    cache-cache_patch:transformers-oblivious:half-rt-strict/inputs_batch1 EXPORT: 100%|█████████▉| 511/512 [05:47<00:02,  2.05s/it]         cache-cache_patch:transformers-oblivious:half-rt-strict/inputs_batch1 EXPORT: 100%|██████████| 512/512 [05:49<00:00,  2.12s/it]    cache-cache_patch:transformers-oblivious:half-rt-strict/inputs_batch1 EXPORT: 100%|██████████| 512/512 [05:49<00:00,  1.47it/s]




.. GENERATED FROM PYTHON SOURCE LINES 234-235

Let's save the results.

.. GENERATED FROM PYTHON SOURCE LINES 235-240

.. code-block:: Python


    df = pandas.DataFrame(results)
    df.to_excel("plot_export_tiny_llm_dim01_onnx.xlsx")
    df






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>cache</th>
          <th>cache_patch</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>strict</th>
          <th>export_with</th>
          <th>EXPORT</th>
          <th>ERR-EXPORT</th>
          <th>run_with</th>
          <th>WORKS</th>
          <th>ERR-RUN</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123134303003584)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs2</td>
          <td>0</td>
          <td>8*s72 (123134304933536)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>8*s72 (123134442474288)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>8*s31 + 8*s70 (123134082711232)is not tracked ...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>0</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123134303786752)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>501</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>0</td>
          <td>1</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>502</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>503</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs2</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>504</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>505</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    <p>506 rows × 11 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 241-246

.. code-block:: Python


    no_export = df[df.EXPORT == 0]
    no_export.to_excel("plot_export_tiny_llm_dim01_onnx_custom.no_export.xlsx")
    no_export






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>cache</th>
          <th>cache_patch</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>strict</th>
          <th>export_with</th>
          <th>EXPORT</th>
          <th>ERR-EXPORT</th>
          <th>run_with</th>
          <th>WORKS</th>
          <th>ERR-RUN</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123134303003584)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs2</td>
          <td>0</td>
          <td>8*s72 (123134304933536)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>8*s72 (123134442474288)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>8*s31 + 8*s70 (123134082711232)is not tracked ...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>0</td>
          <td>inputs</td>
          <td>0</td>
          <td>8*s72 (123134303786752)is not tracked with pro...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>501</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>0</td>
          <td>1</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>502</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>503</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs2</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>504</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_empty_cache</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>505</th>
          <td>1</td>
          <td>transformers</td>
          <td>half</td>
          <td>1</td>
          <td>1</td>
          <td>inputs_batch1</td>
          <td>0</td>
          <td>Found the following conflicts between user-spe...</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    <p>258 rows × 11 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 247-248

The validation failures.

.. GENERATED FROM PYTHON SOURCE LINES 248-257

.. code-block:: Python


    invalid = df[(df.EXPORT == 1) & (df.WORKS == 0)].pivot(
        index=["cache", "cache_patch", "strict", "oblivious", "rt", "export_with"],
        columns=["run_with"],
        values=["WORKS", "ERR-RUN"],
    )
    invalid.to_excel("plot_export_tiny_llm_dim01_onnx_custom.invalid.xlsx")
    invalid






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead tr th {
            text-align: left;
        }

        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th colspan="3" halign="left">WORKS</th>
          <th colspan="3" halign="left">ERR-RUN</th>
        </tr>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th>run_with</th>
          <th>inputs</th>
          <th>inputs2</th>
          <th>inputs_empty_cache</th>
          <th>inputs</th>
          <th>inputs2</th>
          <th>inputs_empty_cache</th>
        </tr>
        <tr>
          <th>cache</th>
          <th>cache_patch</th>
          <th>strict</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>export_with</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>1</th>
          <th>all</th>
          <th>0</th>
          <th>0</th>
          <th>0</th>
          <th>inputs_batch1</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : N...</td>
          <td>[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : N...</td>
          <td>[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : N...</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 258-268

.. code-block:: Python


    success = df[(df.EXPORT == 1) & (df.WORKS == 1)].pivot(
        index=["cache", "cache_patch", "strict", "oblivious", "rt", "export_with"],
        columns=["run_with"],
        values=["WORKS"],
    )
    success.to_excel("plot_export_tiny_llm_dim01_onnx_custom.success.xlsx")
    success







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead tr th {
            text-align: left;
        }

        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th colspan="4" halign="left">WORKS</th>
        </tr>
        <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th>run_with</th>
          <th>inputs</th>
          <th>inputs2</th>
          <th>inputs_batch1</th>
          <th>inputs_empty_cache</th>
        </tr>
        <tr>
          <th>cache</th>
          <th>cache_patch</th>
          <th>strict</th>
          <th>oblivious</th>
          <th>rt</th>
          <th>export_with</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="11" valign="top">1</th>
          <th rowspan="5" valign="top">all</th>
          <th rowspan="5" valign="top">0</th>
          <th rowspan="5" valign="top">0</th>
          <th rowspan="4" valign="top">0</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>1.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>1</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>...</th>
          <th>...</th>
          <th>...</th>
          <th>...</th>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th rowspan="5" valign="top">transformers</th>
          <th rowspan="5" valign="top">0</th>
          <th rowspan="5" valign="top">half</th>
          <th>0</th>
          <th>inputs_empty_cache</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th rowspan="4" valign="top">1</th>
          <th>inputs</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs2</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs_batch1</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>inputs_empty_cache</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
      </tbody>
    </table>
    <p>62 rows × 4 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 269-271

If you have any error, then look at example
:ref:`l-plot-tiny-llm-export-patched`.

.. GENERATED FROM PYTHON SOURCE LINES 271-273

.. code-block:: Python


    doc.plot_legend("Tiny-LLM\nexport with\ndimension in {0,1}", "to_onnx", "tomato")



.. image-sg:: /auto_examples/images/sphx_glr_plot_export_tiny_llm_dim01_onnx_custom_001.png
   :alt: plot export tiny llm dim01 onnx custom
   :srcset: /auto_examples/images/sphx_glr_plot_export_tiny_llm_dim01_onnx_custom_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (5 minutes 51.138 seconds)


.. _sphx_glr_download_auto_examples_plot_export_tiny_llm_dim01_onnx_custom.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_tiny_llm_dim01_onnx_custom.ipynb <plot_export_tiny_llm_dim01_onnx_custom.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_tiny_llm_dim01_onnx_custom.py <plot_export_tiny_llm_dim01_onnx_custom.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_tiny_llm_dim01_onnx_custom.zip <plot_export_tiny_llm_dim01_onnx_custom.zip>`


.. include:: plot_export_tiny_llm_dim01_onnx_custom.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
