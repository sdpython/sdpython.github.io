
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_technical/plot_parallelized_reduction.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_technical_plot_parallelized_reduction.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_technical_plot_parallelized_reduction.py:


Reproducible Parallelized Reduction is difficult
================================================

A reduction is a frequent operation in neural network. It appears in layer normalization,
softmax. Because of the float precision, the result of the computation
changes based on the order of the elements. The following examples show the variation
based on different hypothesis on the vector distribution.
We consider a vector :math:`X = (x_1, ..., x_n)`.
It computes the average:

.. math::

    mean(X) = \frac{\sum_{i=1}^n x_i}{n}

Or the normalization of the vector:

.. math::

    norm(X)_i = \frac{ X_i  - \mathbb{E}X}{ \sqrt{ \mathbb{V}X}}

We draw 128 random permutation of X. The average or mean should not change.
And the normalized vector should have the same value. In the first case, we compute
the difference between the highest and the lowest values obtained for the average.
In the second case, we look for the maximum difference between the original normalized
vector and the permuted one (both sorted).

The computation code
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 31-115

.. code-block:: Python


    import itertools
    from tqdm import tqdm
    import numpy as np
    import pandas

    DATA = []


    def str_dtype(dtype):
        """Displays numpy dtype in a nicer way."""
        if dtype == np.float64:
            return "fp64"
        if dtype == np.float32:
            return "fp32"
        if dtype == np.float16:
            return "fp16"
        raise ValueError(f"Unexpected value {dtype}")


    def layer_norm(a, eps=1e-6):
        """
        Normalized the vector a.
        The computation is done in float32 or float64.
        """
        ctype = np.float32 if a.dtype == np.float16 else a.dtype
        a32 = a.astype(ctype)
        m = a32.mean(axis=-1, keepdims=True)
        c = a32 - m
        va = np.sqrt((c * c).mean(axis=-1, keepdims=True))
        va += eps
        return (c / va).astype(a.dtype)


    def compute(values, fct):
        """
        Compare the results of function ``fct`` on a sample.
        Loops over multiple sizes, dtypes. Tries 128 times.
        """

        def make_value(base, value):
            if value.size > 1:
                return np.abs(np.sort(base) - np.sort(value)).max()
            return value

        sizes = [2, 4, 8, 16, 512, 1024, 2048, 4096, 8192]
        dtypes = [np.float64, np.float32, np.float16]
        N = list(range(128))
        exps = list(itertools.product(sizes, dtypes, N))
        data = []
        ech = None
        for size, dtype, n in tqdm(exps):
            if n == 0:
                ech = values[:size].astype(dtype)
                base = fct(ech)
                assert base.dtype == ech.dtype
                obs = dict(
                    n=n, size=size, dtype=str_dtype(ech.dtype), value=make_value(base, fct(ech))
                )
                data.append(obs)

            if n == 1:
                new_ech = np.sort(ech)
            elif n == 2:
                new_ech = np.sort(ech)[::-1]
            else:
                new_ech = np.random.permutation(ech)
            assert new_ech.dtype == ech.dtype
            assert new_ech.shape == ech.shape
            obs = dict(
                n=n + 1,
                size=size,
                dtype=str_dtype(new_ech.dtype),
                value=make_value(base, fct(new_ech)),
            )
            data.append(obs)

        df = pandas.DataFrame(data)
        agg = df.drop("n", axis=1).groupby(["dtype", "size"], as_index=False).agg(["min", "max"])
        agg["value", "delta"] = agg["value", "max"] - agg["value", "min"]
        piv = agg.pivot(index="size", columns="dtype", values=("value", "delta"))
        return piv









.. GENERATED FROM PYTHON SOURCE LINES 116-121

Normal Law
++++++++++

Let's see what it returns an on random sample following a normal law.
First the average.

.. GENERATED FROM PYTHON SOURCE LINES 121-127

.. code-block:: Python


    values = np.random.randn(4096)
    mean = compute(values, lambda x: np.mean(x).astype(x.dtype))
    mean["name"] = "normal"
    print(mean)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     83%|████████▎ | 2864/3456 [00:00<00:00, 28627.13it/s]    100%|██████████| 3456/3456 [00:00<00:00, 21464.25it/s]
    dtype  fp16          fp32          fp64    name
    size                                           
    2       0.0  0.000000e+00  0.000000e+00  normal
    4       0.0  1.192093e-07  2.220446e-16  normal
    8       0.0  1.192093e-07  1.110223e-16  normal
    16      0.0  8.940697e-08  1.110223e-16  normal
    512     0.0  1.862645e-08  7.632783e-17  normal
    1024    0.0  2.235174e-08  5.551115e-17  normal
    2048    0.0  1.490116e-08  2.775558e-17  normal
    4096    0.0  5.960464e-08  1.110223e-16  normal
    8192    0.0  5.960464e-08  1.110223e-16  normal




.. GENERATED FROM PYTHON SOURCE LINES 128-129

Then the layer normalization.

.. GENERATED FROM PYTHON SOURCE LINES 129-135

.. code-block:: Python


    ln = compute(values, layer_norm)
    ln["name"] = "normal"
    DATA.append(ln.reset_index(drop=True).max(axis=0))
    print(ln)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     65%|██████▍   | 2240/3456 [00:00<00:00, 22343.39it/s]    100%|██████████| 3456/3456 [00:00<00:00, 6913.98it/s] 
    dtype          fp16          fp32          fp64    name
    size                                                   
    2      0.000000e+00  0.000000e+00  0.000000e+00  normal
    4      0.000000e+00  2.384186e-07  4.440892e-16  normal
    8      0.000000e+00  2.384186e-07  2.220446e-16  normal
    16     0.000000e+00  2.384186e-07  4.440892e-16  normal
    512    5.960464e-08  2.384186e-07  6.661338e-16  normal
    1024   9.765625e-04  2.384186e-07  4.440892e-16  normal
    2048   9.765625e-04  1.192093e-07  4.440892e-16  normal
    4096   7.629395e-06  2.384186e-07  4.440892e-16  normal
    8192   7.629395e-06  2.384186e-07  4.440892e-16  normal




.. GENERATED FROM PYTHON SOURCE LINES 136-140

Fixed values
++++++++++++

We try a fixed vector with one very high value and all the others are small.

.. GENERATED FROM PYTHON SOURCE LINES 140-153

.. code-block:: Python


    values[:] = -1e-4
    values[::128] = 100
    mean = compute(values, lambda x: np.mean(x).astype(x.dtype))
    mean["name"] = "fixed"
    print(mean)


    ln = compute(values, layer_norm)
    ln["name"] = "fixed"
    DATA.append(ln.reset_index(drop=True).max(axis=0))
    print(ln)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3456 [00:00<?, ?it/s]     88%|████████▊ | 3042/3456 [00:00<00:00, 30406.98it/s]    100%|██████████| 3456/3456 [00:00<00:00, 25203.85it/s]
    dtype  fp16          fp32          fp64   name
    size                                          
    2       0.0  0.000000e+00  0.000000e+00  fixed
    4       0.0  0.000000e+00  3.552714e-15  fixed
    8       0.0  0.000000e+00  0.000000e+00  fixed
    16      0.0  0.000000e+00  0.000000e+00  fixed
    512     0.0  2.980232e-07  5.551115e-16  fixed
    1024    0.0  2.384186e-07  4.440892e-16  fixed
    2048    0.0  4.768372e-07  4.440892e-16  fixed
    4096    0.0  2.980232e-07  9.992007e-16  fixed
    8192    0.0  2.980232e-07  9.992007e-16  fixed
      0%|          | 0/3456 [00:00<?, ?it/s]     76%|███████▌  | 2623/3456 [00:00<00:00, 26222.84it/s]    100%|██████████| 3456/3456 [00:00<00:00, 10175.88it/s]
    dtype  fp16      fp32          fp64   name
    size                                      
    2       0.0  0.000000  0.000000e+00  fixed
    4       0.0  0.000000  2.220446e-16  fixed
    8       0.0  0.000000  0.000000e+00  fixed
    16      0.0  0.000000  0.000000e+00  fixed
    512     0.0  0.000003  1.776357e-15  fixed
    1024    0.0  0.000002  1.776357e-15  fixed
    2048    0.0  0.000004  7.105427e-15  fixed
    4096    0.0  0.000002  1.776357e-15  fixed
    8192    0.0  0.000002  1.776357e-15  fixed




.. GENERATED FROM PYTHON SOURCE LINES 154-158

Pareto Distribution
+++++++++++++++++++

A law with a long tail.

.. GENERATED FROM PYTHON SOURCE LINES 158-172

.. code-block:: Python


    values = np.random.pareto(1, (4096,))
    print(values)

    mean = compute(values, lambda x: np.mean(x).astype(x.dtype))
    mean["name"] = "normal"
    print(mean)


    ln = compute(values, layer_norm)
    ln["name"] = "pareto"
    DATA.append(ln.reset_index(drop=True).max(axis=0))
    print(ln)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [6.74765115 1.46723716 1.78061809 ... 2.843406   0.23612517 2.31461451]
      0%|          | 0/3456 [00:00<?, ?it/s]     85%|████████▌ | 2939/3456 [00:00<00:00, 29353.15it/s]    100%|██████████| 3456/3456 [00:00<00:00, 20033.16it/s]
    dtype  fp16          fp32          fp64    name
    size                                           
    2       0.0  0.000000e+00  0.000000e+00  normal
    4       0.0  2.384186e-07  4.440892e-16  normal
    8       0.0  9.536743e-07  3.552714e-15  normal
    16      0.0  1.430511e-06  2.664535e-15  normal
    512     0.0  1.907349e-06  3.552714e-15  normal
    1024    0.0  3.814697e-06  7.105427e-15  normal
    2048    0.0  5.722046e-06  1.065814e-14  normal
    4096    0.0  3.814697e-06  5.329071e-15  normal
    8192    0.0  3.814697e-06  5.329071e-15  normal
      0%|          | 0/3456 [00:00<?, ?it/s]     65%|██████▍   | 2236/3456 [00:00<00:00, 22344.68it/s]    100%|██████████| 3456/3456 [00:00<00:00, 6662.11it/s] 
    dtype      fp16          fp32          fp64    name
    size                                               
    2      0.000000  0.000000e+00  0.000000e+00  pareto
    4      0.000000  3.576279e-07  4.440892e-16  pareto
    8      0.000000  2.384186e-07  4.440892e-16  pareto
    16     0.000000  4.768372e-07  8.881784e-16  pareto
    512    0.000000  4.768372e-07  3.552714e-15  pareto
    1024   0.000061  9.536743e-06  1.065814e-14  pareto
    2048   0.000031  1.144409e-05  1.421085e-14  pareto
    4096   0.000031  1.144409e-05  1.421085e-14  pareto
    8192   0.000031  7.629395e-06  1.421085e-14  pareto




.. GENERATED FROM PYTHON SOURCE LINES 173-177

Summary
+++++++

We consider the maximum difference obtained for any sample size.

.. GENERATED FROM PYTHON SOURCE LINES 177-182

.. code-block:: Python


    print(DATA)
    df = pandas.DataFrame(DATA).set_index("name")
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [dtype
    fp16    0.000977
    fp32         0.0
    fp64         0.0
    name      normal
    dtype: object, dtype
    fp16         0.0
    fp32    0.000004
    fp64         0.0
    name       fixed
    dtype: object, dtype
    fp16    0.000061
    fp32    0.000011
    fp64         0.0
    name      pareto
    dtype: object]
    dtype       fp16          fp32          fp64
    name                                        
    normal  0.000977  2.384186e-07  6.661338e-16
    fixed   0.000000  3.814697e-06  7.105427e-15
    pareto  0.000061  1.144409e-05  1.421085e-14




.. GENERATED FROM PYTHON SOURCE LINES 183-184

Visually.

.. GENERATED FROM PYTHON SOURCE LINES 184-189

.. code-block:: Python


    ax = df.plot.bar(logy=True)
    fig = ax.get_figure()
    fig.savefig("plot_parallelized_reduction.png")




.. image-sg:: /auto_technical/images/sphx_glr_plot_parallelized_reduction_001.png
   :alt: plot parallelized reduction
   :srcset: /auto_technical/images/sphx_glr_plot_parallelized_reduction_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 190-199

In a deep neural network
++++++++++++++++++++++++

Some of the vector have 500 values, 16x32x1024x1024. A layer normalization
does 16x32x1024 ~ 2M reductions, over 20 layers.
When a deep neural network is computed with a difference code,
doing a different parallelization (GPU/CPU for example),
the order of the reduction may change and therefore,
some errors will appear and propagate.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 2.189 seconds)


.. _sphx_glr_download_auto_technical_plot_parallelized_reduction.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_parallelized_reduction.ipynb <plot_parallelized_reduction.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_parallelized_reduction.py <plot_parallelized_reduction.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_parallelized_reduction.zip <plot_parallelized_reduction.zip>`


.. include:: plot_parallelized_reduction.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
