<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints" href="plot_export_with_auto.html" /><link rel="prev" title="Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)" href="plot_export_tiny_llm.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Test the export on untrained models - onnx-diagnostic 0.5.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a7360d90" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">onnx-diagnostic 0.5.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">onnx-diagnostic 0.5.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API of onnx_diagnostic</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of API of onnx_diagnostic</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/export/index.html">onnx_diagnostic.export</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.export</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/export/dynamic_shapes.html">onnx_diagnostic.export.dynamic_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/validate.html">onnx_diagnostic.export.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/helpers/index.html">onnx_diagnostic.helpers</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.helpers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/args_helper.html">onnx_diagnostic.helpers.args_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/bench_run.html">onnx_diagnostic.helpers.bench_run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/cache_helper.html">onnx_diagnostic.helpers.cache_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/config_helper.html">onnx_diagnostic.helpers.config_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/helper.html">onnx_diagnostic.helpers.helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/memory_peak.html">onnx_diagnostic.helpers.memory_peak</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/onnx_helper.html">onnx_diagnostic.helpers.onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/ort_session.html">onnx_diagnostic.helpers.ort_session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/rt_helper.html">onnx_diagnostic.helpers.rt_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_test_helper.html">onnx_diagnostic.helpers.torch_test_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">onnx_diagnostic.reference</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">onnx_diagnostic.reference.ops</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">onnx_diagnostic.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">onnx_diagnostic.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">onnx_diagnostic.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">onnx_diagnostic.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">onnx_diagnostic.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">onnx_diagnostic.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">onnx_diagnostic.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">onnx_diagnostic.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">onnx_diagnostic.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">onnx_diagnostic.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">onnx_diagnostic.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">onnx_diagnostic.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">onnx_diagnostic.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">onnx_diagnostic.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">onnx_diagnostic.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">onnx_diagnostic.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">onnx_diagnostic.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">onnx_diagnostic.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">onnx_diagnostic.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">onnx_diagnostic.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">onnx_diagnostic.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">onnx_diagnostic.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">onnx_diagnostic.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">onnx_diagnostic.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">onnx_diagnostic.reference.quantized_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">onnx_diagnostic.reference.ort_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/tasks/index.html">onnx_diagnostic.tasks</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.tasks</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/automatic_speech_recognition.html">onnx_diagnostic.tasks.automatic_speech_recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/fill_mask.html">onnx_diagnostic.tasks.fill_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/feature_extraction.html">onnx_diagnostic.tasks.feature_extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_classification.html">onnx_diagnostic.tasks.image_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_text_to_text.html">onnx_diagnostic.export.image_text_to_text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/mixture_of_expert.html">onnx_diagnostic.tasks.mixture_of_expert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/object_detection.html">onnx_diagnostic.tasks.object_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/sentence_similarity.html">onnx_diagnostic.tasks.sentence_similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_classification.html">onnx_diagnostic.tasks.text_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_generation.html">onnx_diagnostic.tasks.text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text2text_generation.html">onnx_diagnostic.tasks.text2text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/zero_shot_image_classification.html">onnx_diagnostic.tasks.zero_shot_image_classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_export_patches/index.html">onnx_diagnostic.torch_export_patches</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/patches/index.html">onnx_diagnostic.torch_export_patches.patches</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches.patches</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_torch.html">onnx_diagnostic.torch_export_patches.patches.patch_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_transformers.html">onnx_diagnostic.torch_export_patches.patches.patch_transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_inputs.html">onnx_diagnostic.torch_export_patches.patch_inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module.html">onnx_diagnostic.torch_export_patches.patch_module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">onnx_diagnostic.torch_models</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_models</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_models/hghub/index.html">onnx_diagnostic.torch_models.hghub</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_models.hghub</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_api.html">onnx_diagnostic.torch_models.hghub.hub_api</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_data.html">onnx_diagnostic.torch_models.hghub.hub_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/model_inputs.html">onnx_diagnostic.torch_models.hghub.model_inputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llms.html">onnx_diagnostic.torch_models.llms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/test_helper.html">onnx_diagnostic.torch_models.test_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_onnx/index.html">onnx_diagnostic.torch_onnx</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_onnx</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs.html">onnx_diagnostic.torch_onnx.sbs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">onnx_diagnostic.ext_test_case</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cmds/index.html">Command Lines</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of Command Lines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cmds/config.html">-m onnx_diagnostic config … prints the config for a model id</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/validate.html">-m onnx_diagnostic validate … validate a model id</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Examples Gallery</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of Examples Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_args_kwargs.html">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_patched.html">Export Tiny-LLM with patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_cond.html">Export a model with a control flow (If)</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_phi2.html">Export microsoft/phi-2</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_dynamic_cache.html">Export with DynamicCache and dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_locate_issue.html">Find and fix an export issue due to dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_model_extract.html">Find where a model is failing by running submodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_reference_evaluator.html">Intermediate results with (ONNX) ReferenceEvaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_onnxruntime_evaluator.html">Intermediate results with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm.html">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Test the export on untrained models</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_auto.html">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_recipes/index.html">Common Export Issues</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of Common Export Issues</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_max.html">Cannot export <code class="docutils literal notranslate"><span class="pre">torch.sym_max(x.shape[0],</span> <span class="pre">y.shape[0])</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_python_int.html">Do not use python int with dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_nonzero.html">Half certain nonzero</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_examples/plot_export_hub_codellama.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-export-hub-codellama-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="test-the-export-on-untrained-models">
<span id="l-plot-export-hub-codellama"></span><span id="sphx-glr-auto-examples-plot-export-hub-codellama-py"></span><h1>Test the export on untrained models<a class="headerlink" href="#test-the-export-on-untrained-models" title="Link to this heading">¶</a></h1>
<p>Checking the exporter on a whole model takes time as it is
usually big but we can create a smaller version with
the same architecture. Then fix export issues on such a
small model is faster.</p>
<section id="codellama-codellama-7b-python-hf">
<h2>codellama/CodeLlama-7b-Python-hf<a class="headerlink" href="#codellama-codellama-7b-python-hf" title="Link to this heading">¶</a></h2>
<p>Let’s grab some information about this model.
This reuses <a class="reference external" href="https://github.com/huggingface/huggingface_hub">huggingface_hub</a> API.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic</span> <span class="kn">import</span> <span class="n">doc</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.helpers</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_models.hghub</span> <span class="kn">import</span> <span class="p">(</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><span class="n">get_untrained_model_with_inputs</span></a></a><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_models.hghub.hub_api</span> <span class="kn">import</span> <span class="p">(</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_model_info</span></a></a><span class="p">,</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_pretrained_config</span></a></a><span class="p">,</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">task_from_id</span></a></a><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_export_patches</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_export_patches.patch_inputs</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a>

<a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a> <span class="o">=</span> <span class="s2">&quot;codellama/CodeLlama-7b-Python-hf&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;info&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" title="onnx_diagnostic.torch_models.hghub.hub_api.get_model_info" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_model_info</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>info ModelInfo(id=&#39;codellama/CodeLlama-7b-Python-hf&#39;, author=&#39;codellama&#39;, sha=&#39;d4178f5d2eead875e627ec487b23679266319b7f&#39;, created_at=datetime.datetime(2023, 8, 24, 16, 31, 28, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 4, 12, 14, 16, 26, tzinfo=datetime.timezone.utc), private=False, disabled=False, downloads=25203, downloads_all_time=None, gated=False, gguf=None, inference=None, inference_provider_mapping=None, likes=141, library_name=&#39;transformers&#39;, tags=[&#39;transformers&#39;, &#39;pytorch&#39;, &#39;safetensors&#39;, &#39;llama&#39;, &#39;text-generation&#39;, &#39;llama-2&#39;, &#39;code&#39;, &#39;arxiv:2308.12950&#39;, &#39;license:llama2&#39;, &#39;autotrain_compatible&#39;, &#39;text-generation-inference&#39;, &#39;endpoints_compatible&#39;, &#39;region:us&#39;], pipeline_tag=&#39;text-generation&#39;, mask_token=None, card_data={&#39;base_model&#39;: None, &#39;datasets&#39;: None, &#39;eval_results&#39;: None, &#39;language&#39;: [&#39;code&#39;], &#39;library_name&#39;: None, &#39;license&#39;: &#39;llama2&#39;, &#39;license_name&#39;: None, &#39;license_link&#39;: None, &#39;metrics&#39;: None, &#39;model_name&#39;: None, &#39;pipeline_tag&#39;: &#39;text-generation&#39;, &#39;tags&#39;: [&#39;llama-2&#39;]}, widget_data=None, model_index=None, config={&#39;architectures&#39;: [&#39;LlamaForCausalLM&#39;], &#39;model_type&#39;: &#39;llama&#39;, &#39;tokenizer_config&#39;: {&#39;bos_token&#39;: {&#39;__type&#39;: &#39;AddedToken&#39;, &#39;content&#39;: &#39;&lt;s&gt;&#39;, &#39;lstrip&#39;: False, &#39;normalized&#39;: True, &#39;rstrip&#39;: False, &#39;single_word&#39;: False}, &#39;eos_token&#39;: {&#39;__type&#39;: &#39;AddedToken&#39;, &#39;content&#39;: &#39;&lt;/s&gt;&#39;, &#39;lstrip&#39;: False, &#39;normalized&#39;: True, &#39;rstrip&#39;: False, &#39;single_word&#39;: False}, &#39;pad_token&#39;: None, &#39;unk_token&#39;: {&#39;__type&#39;: &#39;AddedToken&#39;, &#39;content&#39;: &#39;&lt;unk&gt;&#39;, &#39;lstrip&#39;: False, &#39;normalized&#39;: True, &#39;rstrip&#39;: False, &#39;single_word&#39;: False}}}, transformers_info=TransformersInfo(auto_model=&#39;AutoModelForCausalLM&#39;, custom_class=None, pipeline_tag=&#39;text-generation&#39;, processor=&#39;AutoTokenizer&#39;), trending_score=None, siblings=[RepoSibling(rfilename=&#39;.gitattributes&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;LICENSE&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;README.md&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;USE_POLICY.md&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;config.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;generation_config.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;model-00001-of-00002.safetensors&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;model-00002-of-00002.safetensors&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;model.safetensors.index.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model-00001-of-00003.bin&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model-00002-of-00003.bin&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model-00003-of-00003.bin&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;pytorch_model.bin.index.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;special_tokens_map.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;tokenizer.json&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;tokenizer.model&#39;, size=None, blob_id=None, lfs=None), RepoSibling(rfilename=&#39;tokenizer_config.json&#39;, size=None, blob_id=None, lfs=None)], spaces=[&#39;bigcode/bigcode-models-leaderboard&#39;, &#39;Intel/low_bit_open_llm_leaderboard&#39;, &#39;BAAI/open_cn_llm_leaderboard&#39;, &#39;qiantong-xu/toolbench-leaderboard&#39;, &#39;gsaivinay/open_llm_leaderboard&#39;, &#39;EvanTHU/MotionLLM&#39;, &#39;GTBench/GTBench&#39;, &#39;Vikhrmodels/small-shlepa-lb&#39;, &#39;kz-transformers/kaz-llm-lb&#39;, &#39;Vikhrmodels/DOoM-lb&#39;, &#39;felixz/open_llm_leaderboard&#39;, &#39;HemaAM/GPT_train_on_LLaMa&#39;, &#39;21world/bigcode-models-leaderboard&#39;, &#39;OPTML-Group/UnlearnCanvas-Benchmark&#39;, &#39;whackthejacker/CodeTuneStudio&#39;, &#39;anantgupta129/LitGPT-Pythia-160M&#39;, &#39;BAAI/open_flageval_vlm_leaderboard&#39;, &#39;neubla/neubla-llm-evaluation-board&#39;, &#39;PrarthanaTS/tsai-gpt-from-scratch&#39;, &#39;MadhurGarg/TSAIGPTRedPajama&#39;, &#39;RaviNaik/ERA-SESSION22&#39;, &#39;theangkko/codellama-CodeLlama-7b-Python-hf&#39;, &#39;rodrigomasini/data_only_open_llm_leaderboard&#39;, &#39;Docfile/open_llm_leaderboard&#39;, &#39;Sijuade/GPTNEXTWORD&#39;, &#39;VDebugger/VDebugger-generalist-for-VQA&#39;, &#39;Temuzin64/code_helper&#39;, &#39;piyushgrover/MiniGPT_S22&#39;, &#39;supra-e-acc/Pythia-160M-text-generate&#39;, &#39;venkyyuvy/GPT_redpajama&#39;, &#39;mkthoma/GPT_From_Scratch&#39;, &#39;VarunSivamani/GPT-From-Scratch&#39;, &#39;sanjanatule/GPTNext&#39;, &#39;RashiAgarwal/TSAIGPTRedPajama&#39;, &#39;neuralorbs/DialogGen&#39;, &#39;GunaKoppula/ERA-Session-22&#39;, &#39;Navyabhat/ERAV1-Session-22&#39;, &#39;Vaish2705/ERA_S22&#39;, &#39;smothiki/open_llm_leaderboard&#39;, &#39;aemonge/codellama-CodeLlama-7b-Python-hf&#39;, &#39;sid92/codellama-CodeLlama-7b-Python-hf&#39;, &#39;poubellearman/codellama-CodeLlama-7b-Python-hf&#39;, &#39;IPF/codellama-CodeLlama-7b-Python-hf&#39;, &#39;Chris4K/codellama-CodeLlama-7b-Python-hf&#39;, &#39;CuriosityPdf/codellama-CodeLlama-7b-Python-hf&#39;, &#39;shreefhamed/codellama-CodeLlama-7b-Python-hf&#39;, &#39;markl11/codellama-CodeLlama-7b-Python-hf&#39;, &#39;0x1668/open_llm_leaderboard&#39;, &#39;rpratl/codellama-CodeLlama-7b-Python-hf&#39;, &#39;pngwn/open_llm_leaderboard-check&#39;, &#39;asir0z/open_llm_leaderboard&#39;, &#39;LovelySweet/codellama-CodeLlama-7b-Python-hf&#39;, &#39;kbmlcoding/open_llm_leaderboard_free&#39;, &#39;ToletiSri/TSAI_S22&#39;, &#39;aichampions/open_llm_leaderboard&#39;, &#39;Adeco/open_llm_leaderboard&#39;, &#39;anirudh937/open_llm_leaderboard&#39;, &#39;smothiki/open_llm_leaderboard2&#39;, &#39;mjalg/IFEvalTR&#39;, &#39;lastsamuraii/LitGPT-Pythia-160M&#39;, &#39;atlasas/bigcode-models-leaderboard&#39;, &#39;Wazahat/Pyco&#39;], safetensors=SafeTensorsInfo(parameters={&#39;BF16&#39;: 6738415616}, total=6738415616), security_repo_status=None, xet_enabled=None)
</pre></div>
</div>
<p>The configuration.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" title="onnx_diagnostic.torch_models.hghub.hub_api.get_pretrained_config" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">get_pretrained_config</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>config LlamaConfig {
  &quot;architectures&quot;: [
    &quot;LlamaForCausalLM&quot;
  ],
  &quot;attention_bias&quot;: false,
  &quot;attention_dropout&quot;: 0.0,
  &quot;bos_token_id&quot;: 1,
  &quot;eos_token_id&quot;: 2,
  &quot;head_dim&quot;: 128,
  &quot;hidden_act&quot;: &quot;silu&quot;,
  &quot;hidden_size&quot;: 4096,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 11008,
  &quot;max_position_embeddings&quot;: 16384,
  &quot;mlp_bias&quot;: false,
  &quot;model_type&quot;: &quot;llama&quot;,
  &quot;num_attention_heads&quot;: 32,
  &quot;num_hidden_layers&quot;: 32,
  &quot;num_key_value_heads&quot;: 32,
  &quot;pretraining_tp&quot;: 1,
  &quot;rms_norm_eps&quot;: 1e-05,
  &quot;rope_scaling&quot;: null,
  &quot;rope_theta&quot;: 1000000,
  &quot;tie_word_embeddings&quot;: false,
  &quot;torch_dtype&quot;: &quot;bfloat16&quot;,
  &quot;transformers_version&quot;: &quot;4.52.0.dev0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 32000
}
</pre></div>
</div>
<p>The task determines the set of inputs which needs
to be created for this input.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;task&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/hub_api.html#onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" title="onnx_diagnostic.torch_models.hghub.hub_api.task_from_id" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub-hub_api sphx-glr-backref-type-py-function"><span class="n">task_from_id</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>task text-generation
</pre></div>
</div>
</section>
<section id="untrained-model">
<h2>Untrained model<a class="headerlink" href="#untrained-model" title="Link to this heading">¶</a></h2>
<p>The function <a class="reference internal" href="../api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_untrained_model_with_inputs</span></code></a>.
It loads the pretrained configuration, extracts the task associated
to the model and them creates random inputs and dynamic shapes
for <a class="reference external" href="https://docs.pytorch.org/docs/main/export.html#torch.export.export" title="(in PyTorch vmain (2.8.0a0+gitd197228 ))"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><span class="n">get_untrained_model_with_inputs</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_id</span></a></a><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;model size:&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;size&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of weights:&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;n_weights&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;fields:&quot;</span><span class="p">,</span> <span class="nb">set</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[get_untrained_model_with_inputs] model_id=&#39;codellama/CodeLlama-7b-Python-hf&#39;
[get_untrained_model_with_inputs] use preinstalled &#39;codellama/CodeLlama-7b-Python-hf&#39;
[get_untrained_model_with_inputs] architecture=&#39;LlamaForCausalLM&#39;
[get_untrained_model_with_inputs] cls=&#39;LlamaConfig&#39;
[get_untrained_model_with_inputs] task=&#39;text-generation&#39;
[get_untrained_model_with_inputs] use fct=&lt;function get_inputs at 0x7f321f774540&gt;
model size: 410532864
number of weights: 102633216
fields: {&#39;input_kwargs&#39;, &#39;size&#39;, &#39;inputs&#39;, &#39;model_kwargs&#39;, &#39;n_weights&#39;, &#39;dynamic_shapes&#39;, &#39;configuration&#39;, &#39;model&#39;, &#39;task&#39;}
</pre></div>
</div>
<p>Inputs</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs:&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>inputs: dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x128,T1s2x32x30x128], value_cache=#2[T1s2x32x30x128,T1s2x32x30x128]))
</pre></div>
</div>
<p>Dynamic Shapes</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dynamic shapes:&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/pprint.html#pprint.pformat" title="pprint.pformat" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pformat" title="pprint.pformat" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span><span class="o">.</span><span class="n">pformat</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dynamic shapes: {&#39;attention_mask&#39;: {0: Dim(&#39;batch&#39;, min=1, max=1024), 1: &#39;cache+seq&#39;},
 &#39;input_ids&#39;: {0: Dim(&#39;batch&#39;, min=1, max=1024), 1: &#39;seq_length&#39;},
 &#39;past_key_values&#39;: [[{0: Dim(&#39;batch&#39;, min=1, max=1024), 2: &#39;cache_length&#39;},
                      {0: Dim(&#39;batch&#39;, min=1, max=1024), 2: &#39;cache_length&#39;}],
                     [{0: Dim(&#39;batch&#39;, min=1, max=1024), 2: &#39;cache_length&#39;},
                      {0: Dim(&#39;batch&#39;, min=1, max=1024), 2: &#39;cache_length&#39;}]],
 &#39;position_ids&#39;: {0: Dim(&#39;batch&#39;, min=1, max=1024), 1: &#39;cache+seq&#39;}}
</pre></div>
</div>
<p>Let’s check the model runs. We still needs to
copy the inputs before using the models, the cache
is usually modified inplace.
Expected outputs can be used later to compute
discrepancies.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs_copy</span></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span>
<span class="n">expected_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs_copy</span></a></a><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;outputs:&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a><span class="p">(</span><span class="n">expected_outputs</span><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>outputs: CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#2[T1s2x32x33x128,T1s2x32x33x128], value_cache=#2[T1s2x32x33x128,T1s2x32x33x128]))
</pre></div>
</div>
<p>It works.</p>
</section>
<section id="export">
<h2>Export<a class="headerlink" href="#export" title="Link to this heading">¶</a></h2>
<p>The model uses <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.cache_utils.DynamicCache</span></code>.
It still requires patches to be exportable (control flow).
See <a class="reference internal" href="../api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.torch_export_patches.torch_export_patches()</span></code></a></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a><span class="p">(</span><span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><span class="n">f</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]),</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">]),</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_embed_tokens_weight: &quot;f32[32000, 768]&quot;, p_model_layers_0_self_attn_q_proj_weight: &quot;f32[4096, 768]&quot;, p_model_layers_0_self_attn_k_proj_weight: &quot;f32[4096, 768]&quot;, p_model_layers_0_self_attn_v_proj_weight: &quot;f32[4096, 768]&quot;, p_model_layers_0_self_attn_o_proj_weight: &quot;f32[768, 4096]&quot;, p_model_layers_0_mlp_gate_proj_weight: &quot;f32[6144, 768]&quot;, p_model_layers_0_mlp_up_proj_weight: &quot;f32[6144, 768]&quot;, p_model_layers_0_mlp_down_proj_weight: &quot;f32[768, 6144]&quot;, p_model_layers_0_input_layernorm_weight: &quot;f32[768]&quot;, p_model_layers_0_post_attention_layernorm_weight: &quot;f32[768]&quot;, p_model_layers_1_self_attn_q_proj_weight: &quot;f32[4096, 768]&quot;, p_model_layers_1_self_attn_k_proj_weight: &quot;f32[4096, 768]&quot;, p_model_layers_1_self_attn_v_proj_weight: &quot;f32[4096, 768]&quot;, p_model_layers_1_self_attn_o_proj_weight: &quot;f32[768, 4096]&quot;, p_model_layers_1_mlp_gate_proj_weight: &quot;f32[6144, 768]&quot;, p_model_layers_1_mlp_up_proj_weight: &quot;f32[6144, 768]&quot;, p_model_layers_1_mlp_down_proj_weight: &quot;f32[768, 6144]&quot;, p_model_layers_1_input_layernorm_weight: &quot;f32[768]&quot;, p_model_layers_1_post_attention_layernorm_weight: &quot;f32[768]&quot;, p_model_norm_weight: &quot;f32[768]&quot;, p_lm_head_weight: &quot;f32[32000, 768]&quot;, b_model_rotary_emb_inv_freq: &quot;f32[64]&quot;, input_ids: &quot;i64[s14, s2]&quot;, attention_mask: &quot;i64[s14, s2 + s67]&quot;, position_ids: &quot;i64[s14, s2]&quot;, past_key_values_key_cache_0: &quot;f32[s14, 32, s67, 128]&quot;, past_key_values_key_cache_1: &quot;f32[s14, 32, s67, 128]&quot;, past_key_values_value_cache_0: &quot;f32[s14, 32, s96, 128]&quot;, past_key_values_value_cache_1: &quot;f32[s14, 32, s81, 128]&quot;):
             #
            sym_size_int_21: &quot;Sym(s14)&quot; = torch.ops.aten.sym_size.int(input_ids, 0)
            sym_size_int_22: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(input_ids, 1)
            sym_size_int_23: &quot;Sym(s67)&quot; = torch.ops.aten.sym_size.int(past_key_values_key_cache_0, 2)

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(
            embedding: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:542 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            add: &quot;Sym(s2 + s67)&quot; = sym_size_int_23 + sym_size_int_22

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:541 in forward, code: cache_position = torch.arange(
            arange: &quot;i64[s2]&quot; = torch.ops.aten.arange.start(sym_size_int_23, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_23 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:548 in forward, code: causal_mask = self._update_causal_mask(
            full: &quot;f32[s2, s2 + s67]&quot; = torch.ops.aten.full.default([sym_size_int_22, add], -3.4028234663852886e+38, dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
            triu: &quot;f32[s2, s2 + s67]&quot; = torch.ops.aten.triu.default(full, 1);  full = None
            arange_1: &quot;i64[s2 + s67]&quot; = torch.ops.aten.arange.default(add, device = device(type=&#39;cpu&#39;), pin_memory = False)
            reshape: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
            gt: &quot;b8[s2, s2 + s67]&quot; = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
            mul_: &quot;f32[s2, s2 + s67]&quot; = torch.ops.aten.mul_.Tensor(triu, gt);  triu = gt = None
            unsqueeze: &quot;f32[1, s2, s2 + s67]&quot; = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
            unsqueeze_1: &quot;f32[1, 1, s2, s2 + s67]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze, 1);  unsqueeze = None
            slice_1: &quot;f32[1, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_1, 2, 0, 9223372036854775807);  unsqueeze_1 = None
            slice_2: &quot;f32[1, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_1, 3, 0, 9223372036854775807);  slice_1 = None
            expand: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.expand.default(slice_2, [sym_size_int_21, 1, -1, -1]);  slice_2 = None
            clone: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.clone.default(expand);  expand = None
            slice_3: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(clone)
            slice_4: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_3, 1);  slice_3 = None
            slice_5: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_4, 2);  slice_4 = None
            slice_6: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_5, 3, None, add);  slice_5 = None
            slice_7: &quot;i64[s14, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(attention_mask, 0, 0, 9223372036854775807);  attention_mask = None
            unsqueeze_2: &quot;i64[s14, 1, s2 + s67]&quot; = torch.ops.aten.unsqueeze.default(slice_7, 1);  slice_7 = None
            unsqueeze_3: &quot;i64[s14, 1, 1, s2 + s67]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_2, 2);  unsqueeze_2 = None
            slice_8: &quot;i64[s14, 1, 1, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_3, 3, 0, 9223372036854775807);  unsqueeze_3 = None
            _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(slice_8, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default = None
            to: &quot;i64[s14, 1, 1, s2 + s67]&quot; = torch.ops.aten.to.dtype_layout(slice_8, dtype = torch.int64, layout = torch.strided, device = device(type=&#39;cpu&#39;));  slice_8 = None
            add_2: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.add.Tensor(slice_6, to);  slice_6 = to = None
            eq_6: &quot;b8[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.eq.Scalar(add_2, 0);  add_2 = None
            slice_9: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(clone)
            slice_10: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_9, 1);  slice_9 = None
            slice_11: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_10, 2);  slice_10 = None
            slice_12: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_11, 3, None, add);  slice_11 = None
            masked_fill: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.masked_fill.Scalar(slice_12, eq_6, -3.4028234663852886e+38);  slice_12 = eq_6 = None
            slice_13: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
            slice_14: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_13, 1, 0, 9223372036854775807);  slice_13 = None
            slice_15: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_14, 2, 0, 9223372036854775807);  slice_14 = None
            copy_: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.copy_.default(slice_15, masked_fill);  slice_15 = masked_fill = copy_ = None

            # No stacktrace found for following nodes
            submod_3 = self.submod_1
            wrap_with_set_grad_enabled = torch.ops.higher_order.wrap_with_set_grad_enabled(False, submod_3, b_model_rotary_emb_inv_freq, sym_size_int_21, position_ids);  submod_3 = b_model_rotary_emb_inv_freq = position_ids = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:125 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
            to_6: &quot;f32[s14, s2, 128]&quot; = wrap_with_set_grad_enabled[0]
            to_7: &quot;f32[s14, s2, 128]&quot; = wrap_with_set_grad_enabled[1];  wrap_with_set_grad_enabled = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
            to_8: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:83 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_8, 2)
            mean: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:84 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_3: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.rsqrt.default(add_3);  add_3 = None
            mul_2: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(to_8, rsqrt);  rsqrt = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:85 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
            to_9: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
            mul_3: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_9);  p_model_layers_0_input_layernorm_weight = to_9 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:254 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.view.default(linear, [sym_size_int_21, sym_size_int_22, -1, 128]);  linear = None
            transpose_1: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.transpose.int(view, 1, 2);  view = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_1: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:255 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_1: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.view.default(linear_1, [sym_size_int_21, sym_size_int_22, -1, 128]);  linear_1 = None
            transpose_2: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_2: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:256 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_2: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.view.default(linear_2, [sym_size_int_21, sym_size_int_22, -1, 128]);  linear_2 = None
            transpose_3: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:259 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_7: &quot;f32[s14, 1, s2, 128]&quot; = torch.ops.aten.unsqueeze.default(to_6, 1)
            unsqueeze_8: &quot;f32[s14, 1, s2, 128]&quot; = torch.ops.aten.unsqueeze.default(to_7, 1)
            mul_4: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_7)
            slice_19: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 64)
            slice_20: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 64, 9223372036854775807);  transpose_1 = None
            neg: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.neg.default(slice_20);  slice_20 = None
            cat_1: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.cat.default([neg, slice_19], -1);  neg = slice_19 = None
            mul_5: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_8);  cat_1 = None
            add_4: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            mul_6: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_7);  unsqueeze_7 = None
            slice_21: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 64)
            slice_22: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 64, 9223372036854775807);  transpose_2 = None
            neg_1: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.neg.default(slice_22);  slice_22 = None
            cat_2: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.cat.default([neg_1, slice_21], -1);  neg_1 = slice_21 = None
            mul_7: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_8);  cat_2 = unsqueeze_8 = None
            add_5: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_3: &quot;f32[s14, 32, s2 + s67, 128]&quot; = torch.ops.aten.cat.default([past_key_values_key_cache_0, add_5], -2);  past_key_values_key_cache_0 = add_5 = None
            cat_4: &quot;f32[s14, 32, s2 + s96, 128]&quot; = torch.ops.aten.cat.default([past_key_values_value_cache_0, transpose_3], -2);  past_key_values_value_cache_0 = transpose_3 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:277 in forward, code: attn_output, attn_weights = attention_interface(
            slice_23: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(clone)
            slice_24: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_23, 1);  slice_23 = None
            slice_25: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_24, 2);  slice_24 = None
            slice_26: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_25, 3, None, add);  slice_25 = None
            contiguous: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.contiguous.default(add_4);  add_4 = None
            scaled_dot_product_attention: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.scaled_dot_product_attention.default(contiguous, cat_3, cat_4, slice_26, scale = 0.08838834764831845);  contiguous = slice_26 = None
            transpose_4: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None
            contiguous_1: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.contiguous.default(transpose_4);  transpose_4 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:288 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_1: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.reshape.default(contiguous_1, [sym_size_int_21, sym_size_int_22, -1]);  contiguous_1 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_3: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.linear.default(reshape_1, p_model_layers_0_self_attn_o_proj_weight);  reshape_1 = p_model_layers_0_self_attn_o_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
            add_7: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.add.Tensor(to_8, linear_3);  to_8 = linear_3 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
            to_10: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:83 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
            mean_1: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:84 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_8: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
            mul_8: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(to_10, rsqrt_1);  rsqrt_1 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:85 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_8, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
            to_11: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(mul_8, torch.float32);  mul_8 = None
            mul_9: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_11);  p_model_layers_0_post_attention_layernorm_weight = to_11 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_4: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.linear.default(mul_9, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/activation.py:434 in forward, code: return F.silu(input, inplace=self.inplace)
            silu: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.silu.default(linear_4);  linear_4 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_5: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.linear.default(mul_9, p_model_layers_0_mlp_up_proj_weight);  mul_9 = p_model_layers_0_mlp_up_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:174 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_10: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_6: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.linear.default(mul_10, p_model_layers_0_mlp_down_proj_weight);  mul_10 = p_model_layers_0_mlp_down_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
            add_9: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.add.Tensor(to_10, linear_6);  to_10 = linear_6 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_9, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
            to_12: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(add_9, torch.float32);  add_9 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:83 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
            mean_2: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:84 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_10: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
            mul_11: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(to_12, rsqrt_2);  rsqrt_2 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:85 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_11, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
            to_13: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(mul_11, torch.float32);  mul_11 = None
            mul_12: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_1_input_layernorm_weight, to_13);  p_model_layers_1_input_layernorm_weight = to_13 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_7: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.linear.default(mul_12, p_model_layers_1_self_attn_q_proj_weight);  p_model_layers_1_self_attn_q_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:254 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_3: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.view.default(linear_7, [sym_size_int_21, sym_size_int_22, -1, 128]);  linear_7 = None
            transpose_5: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_8: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.linear.default(mul_12, p_model_layers_1_self_attn_k_proj_weight);  p_model_layers_1_self_attn_k_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:255 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_4: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.view.default(linear_8, [sym_size_int_21, sym_size_int_22, -1, 128]);  linear_8 = None
            transpose_6: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_9: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.linear.default(mul_12, p_model_layers_1_self_attn_v_proj_weight);  mul_12 = p_model_layers_1_self_attn_v_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:256 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_5: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.view.default(linear_9, [sym_size_int_21, sym_size_int_22, -1, 128]);  linear_9 = None
            transpose_7: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:259 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_9: &quot;f32[s14, 1, s2, 128]&quot; = torch.ops.aten.unsqueeze.default(to_6, 1);  to_6 = None
            unsqueeze_10: &quot;f32[s14, 1, s2, 128]&quot; = torch.ops.aten.unsqueeze.default(to_7, 1);  to_7 = None
            mul_13: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_5, unsqueeze_9)
            slice_27: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_5, 3, 0, 64)
            slice_28: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_5, 3, 64, 9223372036854775807);  transpose_5 = None
            neg_2: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.neg.default(slice_28);  slice_28 = None
            cat_5: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.cat.default([neg_2, slice_27], -1);  neg_2 = slice_27 = None
            mul_14: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(cat_5, unsqueeze_10);  cat_5 = None
            add_11: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.add.Tensor(mul_13, mul_14);  mul_13 = mul_14 = None
            mul_15: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(transpose_6, unsqueeze_9);  unsqueeze_9 = None
            slice_29: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_6, 3, 0, 64)
            slice_30: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.slice.Tensor(transpose_6, 3, 64, 9223372036854775807);  transpose_6 = None
            neg_3: &quot;f32[s14, 32, s2, 64]&quot; = torch.ops.aten.neg.default(slice_30);  slice_30 = None
            cat_6: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.cat.default([neg_3, slice_29], -1);  neg_3 = slice_29 = None
            mul_16: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.mul.Tensor(cat_6, unsqueeze_10);  cat_6 = unsqueeze_10 = None
            add_12: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.add.Tensor(mul_15, mul_16);  mul_15 = mul_16 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_7: &quot;f32[s14, 32, s2 + s67, 128]&quot; = torch.ops.aten.cat.default([past_key_values_key_cache_1, add_12], -2);  past_key_values_key_cache_1 = add_12 = None
            cat_8: &quot;f32[s14, 32, s2 + s81, 128]&quot; = torch.ops.aten.cat.default([past_key_values_value_cache_1, transpose_7], -2);  past_key_values_value_cache_1 = transpose_7 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:277 in forward, code: attn_output, attn_weights = attention_interface(
            slice_31: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(clone);  clone = None
            slice_32: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_31, 1);  slice_31 = None
            slice_33: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_32, 2);  slice_32 = None
            slice_34: &quot;f32[s14, 1, s2, s2 + s67]&quot; = torch.ops.aten.slice.Tensor(slice_33, 3, None, add);  slice_33 = add = None
            contiguous_2: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.contiguous.default(add_11);  add_11 = None
            scaled_dot_product_attention_1: &quot;f32[s14, 32, s2, 128]&quot; = torch.ops.aten.scaled_dot_product_attention.default(contiguous_2, cat_7, cat_8, slice_34, scale = 0.08838834764831845);  contiguous_2 = slice_34 = None
            transpose_8: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None
            contiguous_3: &quot;f32[s14, s2, 32, 128]&quot; = torch.ops.aten.contiguous.default(transpose_8);  transpose_8 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:288 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: &quot;f32[s14, s2, 4096]&quot; = torch.ops.aten.reshape.default(contiguous_3, [sym_size_int_21, sym_size_int_22, -1]);  contiguous_3 = sym_size_int_21 = sym_size_int_22 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_10: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.linear.default(reshape_2, p_model_layers_1_self_attn_o_proj_weight);  reshape_2 = p_model_layers_1_self_attn_o_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
            add_13: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.add.Tensor(to_12, linear_10);  to_12 = linear_10 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_13, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
            to_14: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(add_13, torch.float32);  add_13 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:83 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_4: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
            mean_3: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:84 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_14: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.add.Tensor(mean_3, 1e-05);  mean_3 = None
            rsqrt_3: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
            mul_17: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(to_14, rsqrt_3);  rsqrt_3 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:85 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_17, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
            to_15: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(mul_17, torch.float32);  mul_17 = None
            mul_18: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_1_post_attention_layernorm_weight, to_15);  p_model_layers_1_post_attention_layernorm_weight = to_15 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_11: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_1_mlp_gate_proj_weight);  p_model_layers_1_mlp_gate_proj_weight = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/activation.py:434 in forward, code: return F.silu(input, inplace=self.inplace)
            silu_1: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.silu.default(linear_11);  linear_11 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_12: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_1_mlp_up_proj_weight);  mul_18 = p_model_layers_1_mlp_up_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:174 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_19: &quot;f32[s14, s2, 6144]&quot; = torch.ops.aten.mul.Tensor(silu_1, linear_12);  silu_1 = linear_12 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_13: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.linear.default(mul_19, p_model_layers_1_mlp_down_proj_weight);  mul_19 = p_model_layers_1_mlp_down_proj_weight = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
            add_15: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.add.Tensor(to_14, linear_13);  to_14 = linear_13 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_16 = torch.ops.aten._assert_tensor_metadata.default(add_15, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_16 = None
            to_16: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(add_15, torch.float32);  add_15 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:83 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_5: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_16, 2)
            mean_4: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.mean.dim(pow_5, [-1], True);  pow_5 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:84 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_16: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.add.Tensor(mean_4, 1e-05);  mean_4 = None
            rsqrt_4: &quot;f32[s14, s2, 1]&quot; = torch.ops.aten.rsqrt.default(add_16);  add_16 = None
            mul_20: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(to_16, rsqrt_4);  to_16 = rsqrt_4 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:85 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_17 = torch.ops.aten._assert_tensor_metadata.default(mul_20, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_17 = None
            to_17: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.to.dtype(mul_20, torch.float32);  mul_20 = None
            mul_21: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_17);  p_model_norm_weight = to_17 = None

             # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:825 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
            slice_35: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.slice.Tensor(mul_21);  mul_21 = None
            slice_36: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.slice.Tensor(slice_35, 1, 0);  slice_35 = None
            slice_37: &quot;f32[s14, s2, 768]&quot; = torch.ops.aten.slice.Tensor(slice_36, 2);  slice_36 = None

             # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_14: &quot;f32[s14, s2, 32000]&quot; = torch.ops.aten.linear.default(slice_37, p_lm_head_weight);  slice_37 = p_lm_head_weight = None
            return (linear_14, cat_3, cat_7, cat_4, cat_8)

        class submod_1(torch.nn.Module):
            def forward(self, b_model_rotary_emb_inv_freq: &quot;f32[64]&quot;, sym_size_int_21: &quot;Sym(s14)&quot;, position_ids: &quot;i64[s14, s2]&quot;):
                 # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:115 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
                unsqueeze_4: &quot;f32[1, 64]&quot; = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
                slice_16: &quot;f32[1, 64]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_4, 1, 0, 9223372036854775807);  unsqueeze_4 = None
                unsqueeze_5: &quot;f32[1, 64, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_16, 2);  slice_16 = None
                _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_5, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
                to_1: &quot;f32[1, 64, 1]&quot; = torch.ops.aten.to.dtype(unsqueeze_5, torch.float32);  unsqueeze_5 = None
                expand_1: &quot;f32[s14, 64, 1]&quot; = torch.ops.aten.expand.default(to_1, [sym_size_int_21, -1, 1]);  to_1 = sym_size_int_21 = None
                _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
                to_2: &quot;f32[s14, 64, 1]&quot; = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  expand_1 = None

                 # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:116 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
                slice_17: &quot;i64[s14, s2]&quot; = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
                unsqueeze_6: &quot;i64[s14, 1, s2]&quot; = torch.ops.aten.unsqueeze.default(slice_17, 1);  slice_17 = None
                slice_18: &quot;i64[s14, 1, s2]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_6, 2, 0, 9223372036854775807);  unsqueeze_6 = None
                _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(slice_18, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
                to_3: &quot;f32[s14, 1, s2]&quot; = torch.ops.aten.to.dtype(slice_18, torch.float32);  slice_18 = None

                # No stacktrace found for following nodes
                submod_3 = self.submod_1
                wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast(&#39;cpu&#39;, torch.bfloat16, False, False, submod_3, to_2, to_3);  submod_3 = to_2 = to_3 = None

                 # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:122 in forward, code: cos = emb.cos() * self.attention_scaling
                mul: &quot;f32[s14, s2, 128]&quot; = wrap_with_autocast[0]

                 # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:123 in forward, code: sin = emb.sin() * self.attention_scaling
                mul_1: &quot;f32[s14, s2, 128]&quot; = wrap_with_autocast[1];  wrap_with_autocast = None

                 # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:125 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
                _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                to_6: &quot;f32[s14, s2, 128]&quot; = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
                _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                to_7: &quot;f32[s14, s2, 128]&quot; = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
                return (to_6, to_7)

            class submod_1(torch.nn.Module):
                def forward(self, to_2: &quot;f32[s14, 64, 1]&quot;, to_3: &quot;f32[s14, 1, s2]&quot;):
                     # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:120 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                    _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(to_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
                    to_4: &quot;f32[s14, 64, 1]&quot; = torch.ops.aten.to.dtype(to_2, torch.float32);  to_2 = None
                    _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(to_3, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
                    to_5: &quot;f32[s14, 1, s2]&quot; = torch.ops.aten.to.dtype(to_3, torch.float32);  to_3 = None
                    matmul: &quot;f32[s14, 64, s2]&quot; = torch.ops.aten.matmul.default(to_4, to_5);  to_4 = to_5 = None
                    transpose: &quot;f32[s14, s2, 64]&quot; = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None

                     # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:121 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                    cat: &quot;f32[s14, s2, 128]&quot; = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None

                     # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:122 in forward, code: cos = emb.cos() * self.attention_scaling
                    cos: &quot;f32[s14, s2, 128]&quot; = torch.ops.aten.cos.default(cat)
                    mul: &quot;f32[s14, s2, 128]&quot; = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None

                     # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:123 in forward, code: sin = emb.sin() * self.attention_scaling
                    sin: &quot;f32[s14, s2, 128]&quot; = torch.ops.aten.sin.default(cat);  cat = None
                    mul_1: &quot;f32[s14, s2, 128]&quot; = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                    return (mul, mul_1)

Graph signature:
    # inputs
    p_model_embed_tokens_weight: PARAMETER target=&#39;model.embed_tokens.weight&#39;
    p_model_layers_0_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.q_proj.weight&#39;
    p_model_layers_0_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.k_proj.weight&#39;
    p_model_layers_0_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.v_proj.weight&#39;
    p_model_layers_0_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.o_proj.weight&#39;
    p_model_layers_0_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.gate_proj.weight&#39;
    p_model_layers_0_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.up_proj.weight&#39;
    p_model_layers_0_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.down_proj.weight&#39;
    p_model_layers_0_input_layernorm_weight: PARAMETER target=&#39;model.layers.0.input_layernorm.weight&#39;
    p_model_layers_0_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.0.post_attention_layernorm.weight&#39;
    p_model_layers_1_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.q_proj.weight&#39;
    p_model_layers_1_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.k_proj.weight&#39;
    p_model_layers_1_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.v_proj.weight&#39;
    p_model_layers_1_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.o_proj.weight&#39;
    p_model_layers_1_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.gate_proj.weight&#39;
    p_model_layers_1_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.up_proj.weight&#39;
    p_model_layers_1_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.down_proj.weight&#39;
    p_model_layers_1_input_layernorm_weight: PARAMETER target=&#39;model.layers.1.input_layernorm.weight&#39;
    p_model_layers_1_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.1.post_attention_layernorm.weight&#39;
    p_model_norm_weight: PARAMETER target=&#39;model.norm.weight&#39;
    p_lm_head_weight: PARAMETER target=&#39;lm_head.weight&#39;
    b_model_rotary_emb_inv_freq: BUFFER target=&#39;model.rotary_emb.inv_freq&#39; persistent=False
    input_ids: USER_INPUT
    attention_mask: USER_INPUT
    position_ids: USER_INPUT
    past_key_values_key_cache_0: USER_INPUT
    past_key_values_key_cache_1: USER_INPUT
    past_key_values_value_cache_0: USER_INPUT
    past_key_values_value_cache_1: USER_INPUT

    # outputs
    linear_14: USER_OUTPUT
    cat_3: USER_OUTPUT
    cat_7: USER_OUTPUT
    cat_4: USER_OUTPUT
    cat_8: USER_OUTPUT

Range constraints: {s14: VR[1, 1024], s2: VR[2, int_oo], s2 + s67: VR[4, int_oo], s67: VR[2, int_oo], s96: VR[2, int_oo], s81: VR[2, int_oo]}
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span><span class="o">.</span><span class="n">plot_legend</span><span class="p">(</span>
    <span class="s2">&quot;untrained</span><span class="se">\n</span><span class="s2">codellama/</span><span class="se">\n</span><span class="s2">CodeLlama-7b-Python-hf&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.export.export&quot;</span><span class="p">,</span> <span class="s2">&quot;tomato&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_export_hub_codellama_001.png" srcset="../_images/sphx_glr_plot_export_hub_codellama_001.png" alt="plot export hub codellama" class = "sphx-glr-single-img"/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 2.224 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-export-hub-codellama-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/ddd9c1f67aff5f3035228e4ae64ecf6d/plot_export_hub_codellama.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_export_hub_codellama.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/a4876826f2dd4d37a228cdab8e0fe187/plot_export_hub_codellama.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_export_hub_codellama.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b6c0f575e324bf8b84d99b1a8df56381/plot_export_hub_codellama.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_export_hub_codellama.zip</span></code></a></p>
</div>
</div>
<p class="rubric">Related examples</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This function exports an smaller untrained model with the same architecture. It is faster than the pretrained model. When this works, the untrained model can be replaced by the trained one."><img alt="" src="../_images/sphx_glr_plot_export_tiny_phi2_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_phi2.html#sphx-glr-auto-examples-plot-export-tiny-phi2-py"><span class="std std-ref">Export microsoft/phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export microsoft/phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Inputs are always dynamic with LLMs that is why dynamic shapes needs to be specified when a LLM is exported with torch.export.export. Most of the examples on HuggingFace use method transformers.GenerationMixin.generate but we only want to export the model and its method forward."><img alt="" src="../_images/sphx_glr_plot_export_tiny_llm_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_llm.html#sphx-glr-auto-examples-plot-export-tiny-llm-py"><span class="std std-ref">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Many models from transformers cannot be converted because the implementation uses cache classes. Let&#x27;s see how to get around that. We focus on the model arnir0/Tiny-LLM. To avoid downloading any weights, we write a function creating a random model based on the same architecture. This continues example l-plot-tiny-llm-export."><img alt="" src="../_images/sphx_glr_plot_export_tiny_llm_patched_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_llm_patched.html#sphx-glr-auto-examples-plot-export-tiny-llm-patched-py"><span class="std std-ref">Export Tiny-LLM with patches</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export Tiny-LLM with patches</div>
</div></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="plot_export_with_auto.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_export_tiny_llm.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Test the export on untrained models</a><ul>
<li><a class="reference internal" href="#codellama-codellama-7b-python-hf">codellama/CodeLlama-7b-Python-hf</a></li>
<li><a class="reference internal" href="#untrained-model">Untrained model</a></li>
<li><a class="reference internal" href="#export">Export</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=e4e9a439"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    </body>
</html>