
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_with_dynamic_cache.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_with_dynamic_cache.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_with_dynamic_cache.py:


.. _l-plot-export-with-dynamic-shape:

===========================================
Export with DynamicCache and dynamic shapes
===========================================

Every LLMs implemented in :epkg:`transformers` use cache.
One of the most used is :class:`transformers.cache_utils.DynamicCache`.
The cache size is dynamic to cope with the growing context.
The example shows a tool which determines the dynamic shapes
for :func:`torch.export.export` based on a set of valid inputs.

DynamicCache
============

:func:`torch.export.export` serializes caches and any custom class
if these serialization functions are provided with is the case for
:class:`transformers.cache_utils.DynamicCache` and ``transformers>=4.50``.
The dynamic shapes must be provided following the serialized form.

.. GENERATED FROM PYTHON SOURCE LINES 22-60

.. code-block:: Python


    import pprint
    import torch
    from onnx_diagnostic import doc
    from onnx_diagnostic.ext_test_case import has_transformers
    from onnx_diagnostic.helpers import string_type
    from onnx_diagnostic.helpers.cache_helper import (
        flatten_unflatten_for_dynamic_shapes,
        make_dynamic_cache,
    )
    from onnx_diagnostic.export import ModelInputs
    from onnx_diagnostic.torch_export_patches import torch_export_patches


    class Model(torch.nn.Module):
        def forward(self, cache, z):
            return (
                z
                + cache.key_cache[0]
                + cache.key_cache[1]
                + cache.value_cache[0]
                + cache.value_cache[1]
            )


    model = Model()

    n_layers = 2
    bsize, nheads, slen, dim = 2, 4, 3, 7
    cache = make_dynamic_cache(
        [
            (torch.randn(bsize, nheads, slen, dim), torch.randn(bsize, nheads, slen, dim))
            for i in range(n_layers)
        ]
    )
    z = torch.randn((1, 1, 1, 7))
    model(cache, z)  # to check it works.





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[[[-1.6451,  4.6769,  2.4954,  1.9903,  0.1196,  1.3573, -4.1147],
              [ 1.0941,  2.5750, -2.9095,  2.2536,  3.6761,  1.4008, -2.9188],
              [ 1.0428,  2.7288,  3.4837,  2.1204,  1.8898,  2.2318,  0.9838]],

             [[-1.6884,  3.2351, -2.3128,  4.5125,  1.8170,  0.0840, -3.0889],
              [-0.5594, -3.1297, -2.1675,  2.7615, -0.0141,  1.8741, -5.6661],
              [-2.6808,  3.7571,  1.1419,  2.4061,  0.6211,  4.6024, -3.0085]],

             [[-1.3009, -0.6404, -1.0626,  4.8081,  0.3766,  4.3647, -5.1730],
              [-1.8378,  0.5357,  2.9958, -2.0931,  0.5253,  3.6843, -4.1240],
              [-1.0651,  1.1607, -0.9425,  2.2837,  2.8387, -0.4201, -3.5133]],

             [[-2.7222,  1.5465,  1.8049, -0.4755,  0.6099,  2.4579, -2.7255],
              [-4.4873,  1.3548,  1.0025,  1.2130,  0.7810, -0.0106, -3.3306],
              [ 1.0511, -2.3566, -0.5066, -1.6400, -4.5910,  3.8744, -4.0520]]],


            [[[ 0.2850,  2.7183,  3.2517,  1.2717,  2.8724,  0.7991, -3.1614],
              [ 1.1191, -0.8127, -2.9343, -1.4724, -1.5950,  0.5661, -6.4547],
              [-0.0330,  2.9147, -0.5010,  2.7049,  0.6924, -1.0156, -5.0240]],

             [[ 3.1593,  3.7994, -1.6015,  3.7717,  1.6989,  2.1202, -1.9168],
              [-2.0209,  1.7215,  0.5721, -0.7653, -1.0193,  1.6929, -5.8299],
              [ 0.0947, -2.9404,  2.3487,  1.5296, -2.0119,  3.3447, -6.1521]],

             [[-0.4662,  0.6649,  1.2774, -0.1387, -0.4308,  2.0942, -3.0129],
              [-0.7947, -1.6511,  2.1483, -1.0035, -3.3719, -0.3709,  1.1247],
              [-1.3259,  1.4665, -2.9379, -3.0904,  3.0782,  6.3679, -5.4120]],

             [[-1.1213, -0.9122,  1.0147, -1.1665, -2.0338, -0.6723, -4.7350],
              [-2.8199, -1.3486,  1.0302,  3.8690,  0.4719,  1.7988, -0.6435],
              [ 0.7368, -0.7030,  5.8169, -0.4665, -1.5257, -4.3173, -0.8349]]]])



.. GENERATED FROM PYTHON SOURCE LINES 61-62

The cache looks like this:

.. GENERATED FROM PYTHON SOURCE LINES 62-66

.. code-block:: Python


    print(string_type(cache, with_shape=True))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    DynamicCache(key_cache=#2[T1s2x4x3x7,T1s2x4x3x7], value_cache=#2[T1s2x4x3x7,T1s2x4x3x7])




.. GENERATED FROM PYTHON SOURCE LINES 67-82

.. code-block:: Python


    cache2 = make_dynamic_cache(
        [
            (
                torch.randn(bsize + 1, nheads, slen + 1, dim + 1),
                torch.randn(bsize + 1, nheads, slen + 1, dim + 1),
            )
            for i in range(n_layers)
        ]
    )
    inputs = [
        (cache, z),
        (cache2, torch.randn((1, 1, 1, 8))),
    ]








.. GENERATED FROM PYTHON SOURCE LINES 83-84

And the second set of inputs looks like:

.. GENERATED FROM PYTHON SOURCE LINES 84-86

.. code-block:: Python

    print(string_type(inputs[1], with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (DynamicCache(key_cache=#2[T1s3x4x4x8,T1s3x4x4x8], value_cache=#2[T1s3x4x4x8,T1s3x4x4x8]),T1s1x1x1x8)




.. GENERATED FROM PYTHON SOURCE LINES 87-92

Guess the dynamic shapes
========================

The following tool can be used to guess the dynamic shapes
the way :func:`torch.export.export` expects them.

.. GENERATED FROM PYTHON SOURCE LINES 92-98

.. code-block:: Python


    mi = ModelInputs(Model(), inputs)
    ds = mi.guess_dynamic_shapes()

    pprint.pprint(ds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (([[{0: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True)},
        {0: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True)}],
       [{0: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True)},
        {0: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                     min=None,
                     max=None,
                     _factory=True)}]],
      {3: _DimHint(type=<_DimHintType.DYNAMIC: 3>,
                   min=None,
                   max=None,
                   _factory=True)}),
     {})




.. GENERATED FROM PYTHON SOURCE LINES 99-105

And finally the export.
The export is simple if ``transformers>=4.50``, otherwise,
transformers needs to be patched.
:func:`onnx_diagnostic.torch_export_patches.torch_export_patches`
registers functions to serialize ``DynamicCache``. This one is modified to make
the shape inference implemented in :epkg:`torch` happy.

.. GENERATED FROM PYTHON SOURCE LINES 105-124

.. code-block:: Python


    if has_transformers("4.50"):
        ep = torch.export.export(model, inputs[0], dynamic_shapes=ds[0], strict=False)
    else:
        with torch_export_patches(patch_transformers=True) as modificator:
            ep = torch.export.export(
                model, modificator(inputs[0]), dynamic_shapes=ds[0], strict=False
            )
    print(ep)

    # Do we need to guess?
    # ++++++++++++++++++++
    #
    # Function :func:`onnx_diagnostic.helpers.string_type` is using
    # the serialization functions to print out the DynamicCache the was
    # :func:`torch.export.export` expects them.

    print(string_type(cache, with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, cache_key_cache_0: "f32[s26, 4, s28, s1]", cache_key_cache_1: "f32[s26, 4, s28, s1]", cache_value_cache_0: "f32[s26, 4, s28, s1]", cache_value_cache_1: "f32[s26, 4, s28, s1]", z: "f32[1, 1, 1, s1]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_cache.py:39 in forward, code: z
                add: "f32[s26, 4, s28, s1]" = torch.ops.aten.add.Tensor(z, cache_key_cache_0);  z = cache_key_cache_0 = None
                add_1: "f32[s26, 4, s28, s1]" = torch.ops.aten.add.Tensor(add, cache_key_cache_1);  add = cache_key_cache_1 = None
                add_2: "f32[s26, 4, s28, s1]" = torch.ops.aten.add.Tensor(add_1, cache_value_cache_0);  add_1 = cache_value_cache_0 = None
                add_3: "f32[s26, 4, s28, s1]" = torch.ops.aten.add.Tensor(add_2, cache_value_cache_1);  add_2 = cache_value_cache_1 = None
                return (add_3,)
            
    Graph signature: 
        # inputs
        cache_key_cache_0: USER_INPUT
        cache_key_cache_1: USER_INPUT
        cache_value_cache_0: USER_INPUT
        cache_value_cache_1: USER_INPUT
        z: USER_INPUT
    
        # outputs
        add_3: USER_OUTPUT
    
    Range constraints: {s26: VR[2, int_oo], s28: VR[2, int_oo], s1: VR[2, int_oo]}

    DynamicCache(key_cache=#2[T1s2x4x3x7,T1s2x4x3x7], value_cache=#2[T1s2x4x3x7,T1s2x4x3x7])




.. GENERATED FROM PYTHON SOURCE LINES 125-129

You can also use function
:func:`onnx_diagnostic.helpers.cache_helper.flatten_unflatten_for_dynamic_shapes`
to show a DynamicCache restructured the way :func:`torch.export.export` expects
it to be without the custom class.

.. GENERATED FROM PYTHON SOURCE LINES 129-132

.. code-block:: Python


    print(string_type(flatten_unflatten_for_dynamic_shapes(cache), with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    #2[#2[T1s2x4x3x7,T1s2x4x3x7],#2[T1s2x4x3x7,T1s2x4x3x7]]




.. GENERATED FROM PYTHON SOURCE LINES 133-135

This code works for any custom class if it was registered
with :func:`torch.utils._pytree.register_pytree_node`.

.. GENERATED FROM PYTHON SOURCE LINES 135-138

.. code-block:: Python



    doc.plot_legend("dynamic shapes\nfor DynamicCache", "torch.export.export", "tomato")



.. image-sg:: /auto_examples/images/sphx_glr_plot_export_with_dynamic_cache_001.png
   :alt: plot export with dynamic cache
   :srcset: /auto_examples/images/sphx_glr_plot_export_with_dynamic_cache_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.167 seconds)


.. _sphx_glr_download_auto_examples_plot_export_with_dynamic_cache.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_with_dynamic_cache.ipynb <plot_export_with_dynamic_cache.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_with_dynamic_cache.py <plot_export_with_dynamic_cache.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_with_dynamic_cache.zip <plot_export_with_dynamic_cache.zip>`


.. include:: plot_export_with_dynamic_cache.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
