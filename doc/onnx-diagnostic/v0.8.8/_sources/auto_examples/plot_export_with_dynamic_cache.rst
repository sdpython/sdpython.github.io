
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_with_dynamic_cache.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_with_dynamic_cache.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_with_dynamic_cache.py:


.. _l-plot-export-with-dynamic-shape:

===================================================
Export with DynamicCache and guessed dynamic shapes
===================================================

Every LLMs implemented in :epkg:`transformers` use cache.
One of the most used is :class:`transformers.cache_utils.DynamicCache`.
The cache size is dynamic to cope with the growing context.
The example shows a tool which determines the dynamic shapes
for :func:`torch.export.export` based on a set of valid inputs.

DynamicCache
============

:func:`torch.export.export` serializes caches and any custom class
if these serialization functions are provided with is the case for
:class:`transformers.cache_utils.DynamicCache` and ``transformers>=4.50``.
The dynamic shapes must be provided following the serialized form.

.. GENERATED FROM PYTHON SOURCE LINES 22-61

.. code-block:: Python


    import pprint
    import torch
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers import string_type
    from onnx_diagnostic.helpers.cache_helper import (
        flatten_unflatten_for_dynamic_shapes,
        make_dynamic_cache,
        CacheKeyValue,
    )
    from onnx_diagnostic.export import ModelInputs
    from onnx_diagnostic.torch_export_patches import torch_export_patches


    class Model(torch.nn.Module):
        def forward(self, cache, z):
            cache = CacheKeyValue(cache)
            return (
                z
                + cache.key_cache[0]
                + cache.key_cache[1]
                + cache.value_cache[0]
                + cache.value_cache[1]
            )


    model = Model()

    n_layers = 2
    bsize, nheads, slen, dim = 2, 4, 3, 7
    cache = make_dynamic_cache(
        [
            (torch.randn(bsize, nheads, slen, dim), torch.randn(bsize, nheads, slen, dim))
            for i in range(n_layers)
        ]
    )
    z = torch.randn((1, 1, 1, 7))
    model(cache, z)  # to check it works.





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[[[-3.0480, -1.8073,  0.6495, -0.7381, -0.8798,  0.5898, -2.8004],
              [ 2.1842, -1.8548, -3.0003, -1.6655,  2.3879,  1.8654, -1.1088],
              [-1.7419, -2.0483, -2.1441,  1.7263,  4.8609, -0.9125, -1.6990]],

             [[-0.4475,  1.7122, -3.2443, -0.4818,  0.9170,  0.1613, -0.2010],
              [ 0.2781, -0.0750, -0.6615, -0.1611,  0.5996, -0.1890,  3.3484],
              [-0.7312, -1.6378,  0.4120,  1.5301, -2.1836, -0.8944, -3.0169]],

             [[ 3.9477, -0.5752, -2.4382,  2.6695, -2.8392,  0.0642, -0.9968],
              [-1.1000,  0.0556, -1.9869, -2.0721,  2.3018, -5.9606,  4.7270],
              [ 1.2055,  1.9346, -3.4212,  1.4522,  1.0991, -5.5324,  0.9484]],

             [[ 2.4892,  0.8174, -4.6810,  2.3765,  0.5757,  0.2363,  2.1117],
              [ 0.7241, -4.0972,  0.0493,  0.6650, -2.3097, -2.5479, -0.7381],
              [-0.1486, -0.8967, -0.6537,  1.8371,  1.6975, -3.0013,  2.1476]]],


            [[[-0.4016, -1.4695, -4.8510,  0.7859,  2.9136,  0.7801,  1.2864],
              [-1.4630, -1.6314,  0.2547,  5.5747,  2.5220, -3.9564,  1.0161],
              [ 0.9736, -0.3289, -1.5126,  0.4241,  1.9342,  2.9816,  0.6241]],

             [[-1.2355, -0.6681,  1.0512, -0.2258,  4.1040, -1.7015,  3.7405],
              [ 4.3105,  0.4773, -2.0226,  0.2613,  3.1922,  1.3685,  3.4226],
              [-3.1643, -2.3023, -1.7919,  2.6631, -1.6354, -4.4506, -0.5819]],

             [[-0.3613,  3.2567,  0.2353, -4.7029, -0.6312, -0.1357,  0.9231],
              [-0.2292, -1.0579, -1.3913, -0.5051, -0.3587, -0.4330, -0.1460],
              [ 0.4194, -4.6027, -3.1670,  0.8345,  0.5804, -1.5147, -1.8612]],

             [[-2.3072,  0.2388, -6.0574, -0.2454,  2.4291, -3.4830,  2.0750],
              [ 1.1975, -0.3019, -2.6884,  2.4735,  0.0962, -1.7582, -0.2080],
              [ 3.1556, -7.0135,  1.6012,  3.5653,  0.8449, -1.1759,  4.7246]]]])



.. GENERATED FROM PYTHON SOURCE LINES 62-63

The cache looks like this:

.. GENERATED FROM PYTHON SOURCE LINES 63-67

.. code-block:: Python


    print(string_type(cache, with_shape=True))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    DynamicCache(key_cache=#2[T1s2x4x3x7,T1s2x4x3x7], value_cache=#2[T1s2x4x3x7,T1s2x4x3x7])




.. GENERATED FROM PYTHON SOURCE LINES 68-83

.. code-block:: Python


    cache2 = make_dynamic_cache(
        [
            (
                torch.randn(bsize + 1, nheads, slen + 1, dim + 1),
                torch.randn(bsize + 1, nheads, slen + 1, dim + 1),
            )
            for i in range(n_layers)
        ]
    )
    inputs = [
        (cache, z),
        (cache2, torch.randn((1, 1, 1, 8))),
    ]








.. GENERATED FROM PYTHON SOURCE LINES 84-85

And the second set of inputs looks like:

.. GENERATED FROM PYTHON SOURCE LINES 85-87

.. code-block:: Python

    print(string_type(inputs[1], with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (DynamicCache(key_cache=#2[T1s3x4x4x8,T1s3x4x4x8], value_cache=#2[T1s3x4x4x8,T1s3x4x4x8]),T1s1x1x1x8)




.. GENERATED FROM PYTHON SOURCE LINES 88-95

.. _l-guess-dynamic-shapes-example:

Guess the dynamic shapes
========================

The following tool can be used to guess the dynamic shapes
the way :func:`torch.export.export` expects them.

.. GENERATED FROM PYTHON SOURCE LINES 95-101

.. code-block:: Python


    mi = ModelInputs(Model(), inputs)
    ds = mi.guess_dynamic_shapes()

    pprint.pprint(ds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (([{0: DimHint(DYNAMIC), 2: DimHint(DYNAMIC), 3: DimHint(DYNAMIC)},
       {0: DimHint(DYNAMIC), 2: DimHint(DYNAMIC), 3: DimHint(DYNAMIC)},
       {0: DimHint(DYNAMIC), 2: DimHint(DYNAMIC), 3: DimHint(DYNAMIC)},
       {0: DimHint(DYNAMIC), 2: DimHint(DYNAMIC), 3: DimHint(DYNAMIC)}],
      {3: DimHint(DYNAMIC)}),
     {})




.. GENERATED FROM PYTHON SOURCE LINES 102-108

And finally the export.
The export is simple if ``transformers>=4.50``, otherwise,
transformers needs to be patched.
:func:`onnx_diagnostic.torch_export_patches.torch_export_patches`
registers functions to serialize ``DynamicCache``. This one is modified to make
the shape inference implemented in :epkg:`torch` happy.

.. GENERATED FROM PYTHON SOURCE LINES 108-113

.. code-block:: Python


    with torch_export_patches(patch_transformers=True):
        ep = torch.export.export(model, inputs[0], dynamic_shapes=ds[0], strict=False)
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, cache_key_0: "f32[s50, 4, s15, s19]", cache_value_0: "f32[s50, 4, s15, s19]", cache_key_1: "f32[s50, 4, s15, s19]", cache_value_1: "f32[s50, 4, s15, s19]", z: "f32[1, 1, 1, s19]"):
                # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_cache.py:40 in forward, code: z
                add: "f32[s50, 4, s15, s19]" = torch.ops.aten.add.Tensor(z, cache_key_0);  z = cache_key_0 = None
                add_1: "f32[s50, 4, s15, s19]" = torch.ops.aten.add.Tensor(add, cache_key_1);  add = cache_key_1 = None
                add_2: "f32[s50, 4, s15, s19]" = torch.ops.aten.add.Tensor(add_1, cache_value_0);  add_1 = cache_value_0 = None
                add_3: "f32[s50, 4, s15, s19]" = torch.ops.aten.add.Tensor(add_2, cache_value_1);  add_2 = cache_value_1 = None
                return (add_3,)
            
    Graph signature: 
        # inputs
        cache_key_0: USER_INPUT
        cache_value_0: USER_INPUT
        cache_key_1: USER_INPUT
        cache_value_1: USER_INPUT
        z: USER_INPUT
    
        # outputs
        add_3: USER_OUTPUT
    
    Range constraints: {s50: VR[2, int_oo], s15: VR[2, int_oo], s19: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 114-119

Use string instead of DYNAMIC
+++++++++++++++++++++++++++++

ONNX exporter considers strings instead of DYNAMIC or AUTO
to give names to every dimension.

.. GENERATED FROM PYTHON SOURCE LINES 119-124

.. code-block:: Python


    dss = mi.guess_dynamic_shapes(auto="dim")
    pprint.pprint(dss)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (([{0: 'dim_0I_0o0', 2: 'dim_0I_0o2', 3: 'dim_0I_0o3'},
       {0: 'dim_0I_1o0', 2: 'dim_0I_1o2', 3: 'dim_0I_1o3'},
       {0: 'dim_0I_2o0', 2: 'dim_0I_2o2', 3: 'dim_0I_2o3'},
       {0: 'dim_0I_3o0', 2: 'dim_0I_3o2', 3: 'dim_0I_3o3'}],
      {3: 'dim_1I3'}),
     {})




.. GENERATED FROM PYTHON SOURCE LINES 125-131

Do we need to guess?
++++++++++++++++++++

Function :func:`onnx_diagnostic.helpers.string_type` is using
the serialization functions to print out the DynamicCache the was
:func:`torch.export.export` expects them.

.. GENERATED FROM PYTHON SOURCE LINES 131-134

.. code-block:: Python


    print(string_type(cache, with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    DynamicCache(key_cache=#2[T1s2x4x3x7,T1s2x4x3x7], value_cache=#2[T1s2x4x3x7,T1s2x4x3x7])




.. GENERATED FROM PYTHON SOURCE LINES 135-139

You can also use function
:func:`onnx_diagnostic.helpers.cache_helper.flatten_unflatten_for_dynamic_shapes`
to show a DynamicCache restructured the way :func:`torch.export.export` expects
it to be without the custom class.

.. GENERATED FROM PYTHON SOURCE LINES 139-142

.. code-block:: Python


    print(string_type(flatten_unflatten_for_dynamic_shapes(cache), with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    #4[T1s2x4x3x7,T1s2x4x3x7,T1s2x4x3x7,T1s2x4x3x7]




.. GENERATED FROM PYTHON SOURCE LINES 143-145

This code works for any custom class if it was registered
with :func:`torch.utils._pytree.register_pytree_node`.

.. GENERATED FROM PYTHON SOURCE LINES 145-148

.. code-block:: Python



    doc.plot_legend("dynamic shapes\nfor DynamicCache", "torch.export.export", "tomato")



.. image-sg:: /auto_examples/images/sphx_glr_plot_export_with_dynamic_cache_001.png
   :alt: plot export with dynamic cache
   :srcset: /auto_examples/images/sphx_glr_plot_export_with_dynamic_cache_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.204 seconds)


.. _sphx_glr_download_auto_examples_plot_export_with_dynamic_cache.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_with_dynamic_cache.ipynb <plot_export_with_dynamic_cache.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_with_dynamic_cache.py <plot_export_with_dynamic_cache.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_with_dynamic_cache.zip <plot_export_with_dynamic_cache.zip>`


.. include:: plot_export_with_dynamic_cache.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
