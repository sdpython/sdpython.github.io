{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export a model through method generate (with Tiny-LLM)\n\nThe main issue when exporting a LLM is the example on HuggingFace is\nbased on method generate but we only need to export the forward method.\nExample `l-plot-tiny-llm-export` gives details on how to guess\ndummy inputs and dynamic shapes to do so.\nLet's see how to simplify that.\n\n## Dummy Example\n\nLet's use the example provided on\n[arnir0/Tiny-LLM](https://huggingface.co/arnir0/Tiny-LLM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.export.api import method_to_onnx\n\n\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n\ndef generate_text(\n    prompt, model, tokenizer, max_length=50, temperature=1, top_k=50, top_p=0.95\n):\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        do_sample=True,\n    )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n    # Define your prompt\n\n\nprompt = \"Continue: it rains...\"\ngenerated_text = generate_text(prompt, model, tokenizer)\nprint(\"-----------------\")\nprint(generated_text)\nprint(\"-----------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace forward method\n\nWe now modify the model to export the model by replacing the forward method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"plot_export_tiny_llm_method_generate.onnx\"\nforward_replacement = method_to_onnx(\n    model,\n    method_name=\"forward\",\n    exporter=\"custom\",\n    filename=filename,\n    patch_kwargs=dict(patch_transformers=True),\n    verbose=1,\n    convert_after_n_calls=3,\n    skip_kwargs_names={\"kwargs\", \"use_cache\", \"return_dict\", \"inputs_embeds\"},\n    dynamic_shapes={\n        \"cache_position\": {0: \"total_sequence_length\"},\n        \"past_key_values\": [\n            {0: \"batch_size\", 2: \"past_sequence_length\"},\n            {0: \"batch_size\", 2: \"past_sequence_length\"},\n        ],\n        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lambda function cannot be skipped as\nforward_replacement is a module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"type(forward_replacement)={type(forward_replacement)}\")\nmodel.forward = lambda *args, **kwargs: forward_replacement(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's call generate again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "generated_text = generate_text(prompt, model, tokenizer)\nprint(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"Tiny-LLM\\nforward inputs\\through generate\", \"torch.export.export\", \"tomato\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}