{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Gemm or Matmul + Add\n\nOrder of computation matters. ``1 + 1e-20 - 1 != 1 - 1 + 1e-20`` if the\nprecision of the computation is taken into account.\nWhat an operator Gemm in :epkg:`onnxruntime`, the most simple\nway to represent a linear neural layer.\n\n## A model with many choices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cpuinfo\nimport numpy as np\nimport pandas\nimport matplotlib.pyplot as plt\nimport onnx\nimport onnx.helper as oh\nimport torch\nfrom onnx_diagnostic.helpers import max_diff\nfrom onnx_diagnostic.helpers.onnx_helper import pretty_onnx\nfrom onnx_diagnostic.reference import OnnxruntimeEvaluator\nfrom onnxruntime import (\n    InferenceSession,\n    SessionOptions,\n    __version__ as version_onnxruntime,\n    GraphOptimizationLevel,\n)\n\nprint(f\"onnxruntime version = {version_onnxruntime}\")\nprint(f\"cpu name = {cpuinfo.get_cpu_info()['brand_raw']}\")\nif torch.cuda.is_available():\n    print(f\"gpu name = {torch.cuda.get_device_name(0)}\")\n    print(f\"cuda version = {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The version is important. Numerical differences are observed\nwith onnxruntime<=1.22. Let's see how to make them happen.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model_gemm(itype: int) -> onnx.ModelProto:\n    return oh.make_model(\n        oh.make_graph(\n            [\n                oh.make_node(\"Gemm\", [\"A\", \"X\", \"B\"], [\"GemmOnly\"]),\n                oh.make_node(\"Gemm\", [\"A\", \"X\"], [\"gmm\"]),\n                oh.make_node(\"Add\", [\"gmm\", \"B\"], [\"GemmAdd\"]),\n                oh.make_node(\"MatMul\", [\"A\", \"X\"], [\"mm\"]),\n                oh.make_node(\"Add\", [\"mm\", \"B\"], [\"MatMulAdd\"]),\n                oh.make_node(\"FusedMatMul\", [\"A\", \"X\"], [\"fmm\"], domain=\"com.microsoft\"),\n                oh.make_node(\"Add\", [\"fmm\", \"B\"], [\"FusedMatMulAdd\"]),\n                oh.make_node(\"Cast\", [\"A\"], [\"Afloat\"], to=onnx.TensorProto.FLOAT),\n                oh.make_node(\"Cast\", [\"B\"], [\"Bfloat\"], to=onnx.TensorProto.FLOAT),\n                oh.make_node(\"Cast\", [\"X\"], [\"Xfloat\"], to=onnx.TensorProto.FLOAT),\n                oh.make_node(\"Gemm\", [\"Afloat\", \"Xfloat\"], [\"gmmfloat\"]),\n                oh.make_node(\"Add\", [\"gmmfloat\", \"Bfloat\"], [\"gemmaddfloat\"]),\n                oh.make_node(\"Cast\", [\"gemmaddfloat\"], [\"CastGemmAddCast\"], to=itype),\n                oh.make_node(\"Gemm\", [\"Afloat\", \"Xfloat\", \"Bfloat\"], [\"GemmOnlyfloat\"]),\n                oh.make_node(\"Cast\", [\"GemmOnlyfloat\"], [\"CastGemmOnlyCast\"], to=itype),\n            ],\n            \"test\",\n            [\n                oh.make_tensor_value_info(\"A\", itype, [\"a\", \"b\"]),\n                oh.make_tensor_value_info(\"X\", itype, [\"b\", \"c\"]),\n                oh.make_tensor_value_info(\"B\", itype, [\"c\"]),\n            ],\n            [\n                oh.make_tensor_value_info(\"GemmOnly\", itype, [\"a\", \"c\"]),\n                oh.make_tensor_value_info(\"GemmAdd\", itype, [\"a\", \"c\"]),\n                oh.make_tensor_value_info(\"FusedMatMulAdd\", itype, [\"a\", \"c\"]),\n                oh.make_tensor_value_info(\"MatMulAdd\", itype, [\"a\", \"c\"]),\n                oh.make_tensor_value_info(\"CastGemmAddCast\", itype, [\"a\", \"c\"]),\n                oh.make_tensor_value_info(\"CastGemmOnlyCast\", itype, [\"a\", \"c\"]),\n            ],\n        ),\n        opset_imports=[oh.make_opsetid(\"\", 22)],\n        ir_version=10,\n    )\n\n\ndef matrix_diff(tensors):\n    mat = np.zeros((len(tensors), len(tensors)), dtype=np.float32)\n    for i, t in enumerate(tensors):\n        for j in range(i + 1, len(tensors)):\n            mat[i, j] = max_diff(t, tensors[j])[\"abs\"]\n            mat[j, i] = mat[i, j]\n    return mat\n\n\nitype = onnx.TensorProto.FLOAT16\ndtype = np.float16\nmodel = make_model_gemm(itype)\n\nA = np.random.randn(1280, 256).astype(dtype)\nX = np.random.randn(256, 256).astype(dtype)\nB = np.random.randn(256).astype(dtype)\nfeeds = dict(A=A, X=X, B=B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We disable all the optimization made by onnxruntime to make\nthe computation follows what we want to verify.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opts = SessionOptions()\nopts.graph_optimization_level = GraphOptimizationLevel.ORT_DISABLE_ALL\nopts.optimized_model_filepath = \"plot_gemm_or_matmul.optimized.onnx\"\nsess = InferenceSession(model.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"])\nresults = [A @ X + B, *sess.run(None, feeds)]\ndiffs = matrix_diff(results)\n\nprint(diffs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = onnx.load(opts.optimized_model_filepath)\nprint(pretty_onnx(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems some cast were still inserted.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try with CUDA and float32 if it is available.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A = torch.randn((1280, 1280), dtype=torch.float32)\nX = torch.randn((1280, 1280), dtype=torch.float32)\nB = torch.randn((1280), dtype=torch.float32)\n\nfor itype, dtype, device in [\n    (onnx.TensorProto.FLOAT16, torch.float16, \"cpu\"),\n    (onnx.TensorProto.FLOAT, torch.float32, \"cpu\"),\n    (onnx.TensorProto.FLOAT16, torch.float16, \"cuda\"),\n    (onnx.TensorProto.FLOAT, torch.float32, \"cuda\"),\n]:\n    if device == \"cuda\" and not torch.cuda.is_available():\n        continue\n    a = A.to(dtype).to(device)\n    x = X.to(dtype).to(device)\n    b = B.to(dtype).to(device)\n    feeds = dict(A=a, X=x, B=b)\n    model = make_model_gemm(itype)\n\n    sess = OnnxruntimeEvaluator(model, whole=True)\n    results = sess.run(None, feeds)\n    diffs = matrix_diff(results)\n    print(f\"------ dtype={dtype}, device={device!r}\")\n    print(diffs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A weird bias\n\nIn the previous example, the coefficients of the bias\nare similar to the others coefficients. What if we make them\na lot higher.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A = A / A.max()\nX = X / X.max()\nB = (torch.arange(1280, dtype=torch.float32) + 1) / 1280 * 16\nlabels = [\"F.linear\", *[o.name for o in model.graph.output], \"a @ x + b\"]\nall_results = {}\n\nfor itype, dtype, device in [\n    (onnx.TensorProto.FLOAT, torch.float32, \"cpu\"),\n    (onnx.TensorProto.FLOAT16, torch.float16, \"cpu\"),\n    # missing implementation in onnxruntime\n    # (onnx.TensorProto.BFLOAT16, torch.bfloat16, \"cpu\"),\n    (onnx.TensorProto.FLOAT, torch.float32, \"cuda\"),\n    (onnx.TensorProto.FLOAT16, torch.float16, \"cuda\"),\n    (onnx.TensorProto.BFLOAT16, torch.bfloat16, \"cuda\"),\n]:\n    if device == \"cuda\" and not torch.cuda.is_available():\n        continue\n    a = A.to(dtype).to(device)\n    x = X.to(dtype).to(device)\n    b = B.to(dtype).to(device)\n    feeds = dict(A=a, X=x, B=b)\n    model = make_model_gemm(itype)\n\n    filename = f\"plot_gemm_or_matmul.{itype}.{device}.onnx\"\n    sess = OnnxruntimeEvaluator(\n        model,\n        whole=True,\n        graph_optimization_level=GraphOptimizationLevel.ORT_DISABLE_ALL,\n        optimized_model_filepath=filename,\n    )\n    results = [torch.nn.functional.linear(a, x.T, b), *sess.run(None, feeds), a @ x + b]\n    all_results[device, dtype] = results\n    has_cast = \"Cast\" in [n.op_type for n in onnx.load(filename).graph.node]\n    diffs = matrix_diff(results)\n    df = pandas.DataFrame(diffs, columns=labels, index=labels)\n    print(f\"------ has_cast={has_cast}, dtype={dtype}, device={device!r}, max(b)={b.max()}\")\n    print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cast is inserted on CPU because some kernel are not available for\nfloat16. Even though, we can see huge discrepancies happening.\n\n## bias value vs discrepancies\n\nLet's compare torch linear with GemmOnly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_figure_axis(all_results, i, j):\n    labs = labels[i], labels[j]\n    fig, ax = plt.subplots(len(all_results), 2, figsize=(12, 4 * len(all_results)))\n    for pos, ((device, dtype), results) in enumerate(all_results.items()):\n        m1, m2 = results[i], results[j]\n        diff = torch.abs(m1.to(torch.float32) - m2.to(torch.float32)).max(dim=0)[0]\n        print(f\"labels={labs}, {device}/{dtype}: max(diff)={diff.max()}\")\n        expand = 0.5 if diff.max() >= 1 else diff.max().detach().cpu() / 2\n        ax[pos, 0].plot(\n            B.tolist(), (diff.detach().cpu() + torch.rand(1280) * expand).tolist(), \".\"\n        )\n        ax[pos, 0].set_title(f\"{labs[0]}-{labs[1]} {device}/{dtype}\", fontsize=10)\n\n        corr = matrix_diff(results)\n        ax[pos, 1].imshow(corr, cmap=\"Wistia\", vmin=0, vmax=corr.max())\n        # ax[pos,1].colorbar(label=f'Discrepancies {device}/{dtype}')\n        ax[pos, 1].set_xticks(range(len(labels)), labels, rotation=45, ha=\"right\", fontsize=10)\n        ax[pos, 1].set_yticks(range(len(labels)), labels, fontsize=10)\n        ax[pos, 1].set_title(f\"max={diff.max():1.2g}\", fontsize=10)\n        for _i in range(corr.shape[0]):\n            for _j in range(corr.shape[1]):\n                ax[pos, 1].text(\n                    _j,\n                    _i,\n                    f\"{corr[_i, _j]:1.1g}\",\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"black\",\n                    fontsize=8,\n                )\n    fig.suptitle(\n        f\"Left column: discrepancies {labs[0]} VS {labs[1]}\\n\"\n        f\"Right column: max absolute error, across all configuration\\n\"\n        f\"white is good, orange is not\"\n    )\n    return fig, ax\n\n\nfig, ax = make_figure_axis(all_results, 0, 1)\nfig.tight_layout()\nfig.savefig(\"plot_gemm_or_matmul_add1.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare with ``A @ X + B``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = make_figure_axis(all_results, -1, 1)\nfig.tight_layout()\nfig.savefig(\"plot_gemm_or_matmul_add2.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discrepancies do not happen all the time but it is very likely to happen.\nThe use of Gemm with a bias not null should be used when torch is doing\nthe same and it seems to depend on the type as well.\nThe difference is even higher for bfloat16.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}