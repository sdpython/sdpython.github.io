<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Coverage of the Patches" href="patches_coverage.html" /><link rel="prev" title="Exported Programs with Dynamic Shapes" href="exported_program_dynamic.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Exported ONNX with Dynamic Shapes - onnx-diagnostic 0.7.5 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a7360d90" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">onnx-diagnostic 0.7.5 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">onnx-diagnostic 0.7.5 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="../patches.html">Patches Explained</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Patches Explained</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">Exporter Status</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Exporter Status</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="exported_program_dynamic.html">Exported Programs with Dynamic Shapes</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">Exported ONNX with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="patches_coverage.html">Coverage of the Patches</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API of onnx_diagnostic</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of API of onnx_diagnostic</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/export/index.html">onnx_diagnostic.export</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.export</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/export/dynamic_shapes.html">onnx_diagnostic.export.dynamic_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/shape_helper.html">onnx_diagnostic.export.shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/validate.html">onnx_diagnostic.export.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/helpers/index.html">onnx_diagnostic.helpers</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.helpers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/args_helper.html">onnx_diagnostic.helpers.args_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/bench_run.html">onnx_diagnostic.helpers.bench_run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/cache_helper.html">onnx_diagnostic.helpers.cache_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/config_helper.html">onnx_diagnostic.helpers.config_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/doc_helper.html">onnx_diagnostic.helpers.doc_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/graph_helper.html">onnx_diagnostic.helpers.graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/helper.html">onnx_diagnostic.helpers.helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/_log_helper.html">onnx_diagnostic.helpers._log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/log_helper.html">onnx_diagnostic.helpers.log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/memory_peak.html">onnx_diagnostic.helpers.memory_peak</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/mini_onnx_builder.html">onnx_diagnostic.helpers.mini_onnx_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/model_builder_helper.html">onnx_diagnostic.helpers.model_builder_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/onnx_helper.html">onnx_diagnostic.helpers.onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/ort_session.html">onnx_diagnostic.helpers.ort_session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/rt_helper.html">onnx_diagnostic.helpers.rt_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_helper.html">onnx_diagnostic.helpers.torch_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">onnx_diagnostic.reference</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">onnx_diagnostic.reference.ops</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">onnx_diagnostic.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">onnx_diagnostic.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">onnx_diagnostic.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">onnx_diagnostic.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">onnx_diagnostic.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">onnx_diagnostic.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">onnx_diagnostic.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">onnx_diagnostic.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">onnx_diagnostic.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">onnx_diagnostic.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">onnx_diagnostic.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">onnx_diagnostic.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">onnx_diagnostic.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">onnx_diagnostic.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">onnx_diagnostic.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">onnx_diagnostic.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">onnx_diagnostic.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">onnx_diagnostic.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">onnx_diagnostic.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">onnx_diagnostic.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">onnx_diagnostic.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">onnx_diagnostic.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">onnx_diagnostic.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/torch_ops/index.html">onnx_diagnostic.reference.torch_ops</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.reference.torch_ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/access_ops.html">onnx_diagnostic.reference.torch_ops.access_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/binary_ops.html">onnx_diagnostic.reference.torch_ops.binary_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/controlflow_ops.html">onnx_diagnostic.reference.torch_ops.controlflow_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/generator_ops.html">onnx_diagnostic.reference.torch_ops.generator_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/nn_ops.html">onnx_diagnostic.reference.torch_ops.nn_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/other_ops.html">onnx_diagnostic.reference.torch_ops.other_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/reduce_ops.html">onnx_diagnostic.reference.torch_ops.reduce_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/sequence_ops.html">onnx_diagnostic.reference.torch_ops.sequence_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/shape_ops.html">onnx_diagnostic.reference.torch_ops.shape_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/unary_ops.html">onnx_diagnostic.reference.torch_ops.unary_ops</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">onnx_diagnostic.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">onnx_diagnostic.reference.quantized_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">onnx_diagnostic.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/report_results_comparison.html">onnx_diagnostic.reference.report_results_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/torch_evaluator.html">onnx_diagnostic.reference.torch_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/tasks/index.html">onnx_diagnostic.tasks</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.tasks</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/automatic_speech_recognition.html">onnx_diagnostic.tasks.automatic_speech_recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/fill_mask.html">onnx_diagnostic.tasks.fill_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/feature_extraction.html">onnx_diagnostic.tasks.feature_extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_classification.html">onnx_diagnostic.tasks.image_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_text_to_text.html">onnx_diagnostic.export.image_text_to_text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/mixture_of_expert.html">onnx_diagnostic.tasks.mixture_of_expert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/object_detection.html">onnx_diagnostic.tasks.object_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/sentence_similarity.html">onnx_diagnostic.tasks.sentence_similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/summarization.html">onnx_diagnostic.tasks.summarization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_classification.html">onnx_diagnostic.tasks.text_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_generation.html">onnx_diagnostic.tasks.text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_to_image.html">onnx_diagnostic.tasks.text_to_image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text2text_generation.html">onnx_diagnostic.tasks.text2text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/zero_shot_image_classification.html">onnx_diagnostic.tasks.zero_shot_image_classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_export_patches/index.html">onnx_diagnostic.torch_export_patches</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/eval/index.html">onnx_diagnostic.torch_export_patches.eval</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches.eval</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/eval/model_cases.html">onnx_diagnostic.torch_export_patches.eval.model_cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_errors.html">onnx_diagnostic.torch_export_patches.onnx_export_errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_serialization.html">onnx_diagnostic.torch_export_patches.onnx_export_serialization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/patches/index.html">onnx_diagnostic.torch_export_patches.patches</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches.patches</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_torch.html">onnx_diagnostic.torch_export_patches.patches.patch_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_transformers.html">onnx_diagnostic.torch_export_patches.patches.patch_transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_expressions.html">onnx_diagnostic.torch_export_patches.patch_expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_inputs.html">onnx_diagnostic.torch_export_patches.patch_inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module.html">onnx_diagnostic.torch_export_patches.patch_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module_helper.html">onnx_diagnostic.torch_export_patches.patch_module_helper</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/serialization/index.html">onnx_diagnostic.torch_export_patches.serialization</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_export_patches.serialization</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/diffusers_impl.html">onnx_diagnostic.torch_export_patches.serialization.diffusers_impl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/transformers_impl.html">onnx_diagnostic.torch_export_patches.serialization.transformers_impl</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">onnx_diagnostic.torch_models</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_models</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_models/hghub/index.html">onnx_diagnostic.torch_models.hghub</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_models.hghub</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_api.html">onnx_diagnostic.torch_models.hghub.hub_api</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_data.html">onnx_diagnostic.torch_models.hghub.hub_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/model_inputs.html">onnx_diagnostic.torch_models.hghub.model_inputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llms.html">onnx_diagnostic.torch_models.llms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/validate.html">onnx_diagnostic.torch_models.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_onnx/index.html">onnx_diagnostic.torch_onnx</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of onnx_diagnostic.torch_onnx</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/runtime_info.html">onnx_diagnostic.torch_onnx.runtime_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs.html">onnx_diagnostic.torch_onnx.sbs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/api.html">onnx_diagnostic.api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">onnx_diagnostic.ext_test_case</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cmds/index.html">Command Lines</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of Command Lines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cmds/config.html">-m onnx_diagnostic config … prints the config for a model id</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/validate.html">-m onnx_diagnostic validate … validate a model id</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_examples/index.html">Examples Gallery</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of Examples Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_dump_intermediate_results.html">Dumps intermediate results of a torch model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_with_args_kwargs.html">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_llm_patched.html">Export Tiny-LLM with patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_phi2.html">Export microsoft/phi-2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_with_dynamic_cache.html">Export with DynamicCache and guessed dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_locate_issue.html">Find and fix an export issue due to dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_failing_model_extract.html">Find where a model is failing by running submodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_failing_reference_evaluator.html">Intermediate results with (ONNX) ReferenceEvaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_failing_onnxruntime_evaluator.html">Intermediate results with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_llm.html">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_hub_codellama.html">Test the export on untrained models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_recipes/index.html">Common Export Issues</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle navigation of Common Export Issues</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_dim1.html">0, 1, 2 for a Dynamic Dimension in the dummy example to export a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_what.html">Builds dynamic shapes from any input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_max.html">Cannot export <code class="docutils literal notranslate"><span class="pre">torch.sym_max(x.shape[0],</span> <span class="pre">y.shape[0])</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_python_int.html">Do not use python int with dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_cond.html">Export a model with a control flow (If)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_nonzero.html">Half certain nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_json.html">JSON returns list when the original dynamic shapes are list or tuple</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_with_dynamic.html">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_technical/index.html">Technical Details</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle navigation of Technical Details</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_layer_norm_discrepancies.html">LayerNormalization implementation cannot be exchanged</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_parallelized_reduction.html">Reproducible Parallelized Reduction is difficult</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/status/exporter_dynamic.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="exported-onnx-with-dynamic-shapes">
<h1>Exported ONNX with Dynamic Shapes<a class="headerlink" href="#exported-onnx-with-dynamic-shapes" title="Link to this heading">¶</a></h1>
<p>The following script shows the exported program for many short cases
and various l-plot-export-with-dynamic-shape to retrieve an ONNX model equivalent
to the original model.</p>
<div class="admonition">
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.helpers</span> <span class="kn">import</span> <span class="n">string_type</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.helpers.onnx_helper</span> <span class="kn">import</span> <span class="n">pretty_onnx</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_export_patches.eval</span> <span class="kn">import</span> <span class="n">discover</span><span class="p">,</span> <span class="n">run_exporter</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.ext_test_case</span> <span class="kn">import</span> <span class="n">unit_test_going</span>

<span class="n">cases</span> <span class="o">=</span> <span class="n">discover</span><span class="p">()</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;:ref:`Summary &lt;ledx-summary-exported-program&gt;`&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="n">sorted_cases</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">cases</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="k">if</span> <span class="n">unit_test_going</span><span class="p">():</span>
    <span class="n">sorted_cases</span> <span class="o">=</span> <span class="n">sorted_cases</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">cls_model</span> <span class="ow">in</span> <span class="n">sorted_cases</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;* :ref:`</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> &lt;ledx-model-case-export-</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&gt;`&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">()</span>

<span class="n">obs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">cls_model</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">cases</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;.. _ledx-model-case-export-</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;+++++++&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;.. code-block:: python&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getsource</span><span class="p">(</span><span class="n">cls_model</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">src</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span><span class="n">src</span><span class="p">),</span> <span class="s2">&quot;    &quot;</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    # code is missing&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">exporter</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;custom&quot;</span><span class="p">,</span> <span class="s2">&quot;dynamo-ir&quot;</span><span class="p">):</span>
        <span class="n">expname</span> <span class="o">=</span> <span class="n">exporter</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;export-&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">expname</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;+&quot;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">expname</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">run_exporter</span><span class="p">(</span><span class="n">exporter</span><span class="p">,</span> <span class="n">cls_model</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">case_ref</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;:ref:`</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> &lt;ledx-model-case-export-</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&gt;`&quot;</span>
        <span class="n">expo</span> <span class="o">=</span> <span class="n">exporter</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="s2">&quot;inputs&quot;</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;* **inputs:** ``</span><span class="si">{</span><span class="n">string_type</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">],</span><span class="w"> </span><span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="si">}</span><span class="s2">``&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;dynamic_shapes&quot;</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;* **shapes:** ``</span><span class="si">{</span><span class="n">string_type</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;dynamic_shapes&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">``&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;onx&quot;</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;.. code-block:: text&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="n">pretty_onnx</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;onx&quot;</span><span class="p">]),</span> <span class="s2">&quot;    &quot;</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="k">if</span> <span class="s2">&quot;error&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
                <span class="n">obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">case</span><span class="o">=</span><span class="n">case_ref</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">exporter</span><span class="o">=</span><span class="n">expo</span><span class="p">))</span>
        <span class="k">if</span> <span class="s2">&quot;error&quot;</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**FAILED**&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;.. code-block:: text&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="n">err</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;error&quot;</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">err</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="n">err</span><span class="p">,</span> <span class="s2">&quot;    &quot;</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    # no error found for the failure&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="n">obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">case</span><span class="o">=</span><span class="n">case_ref</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="s2">&quot;FAIL&quot;</span><span class="p">,</span> <span class="n">exporter</span><span class="o">=</span><span class="n">expo</span><span class="p">))</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;.. _ledx-summary-exported-program:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Summary&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;+++++++&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
<span class="n">piv</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">&quot;case&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&quot;exporter&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;error&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">piv</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">(</span><span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;rst&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>&gt;&gt;&gt;</p>
<p><a class="reference internal" href="#ledx-summary-exported-program"><span class="std std-ref">Summary</span></a></p>
<ul class="simple">
<li><p><a class="reference internal" href="#ledx-model-case-export-atenasstrided"><span class="std std-ref">AtenAsStrided</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-ateninterpolate"><span class="std std-ref">AtenInterpolate</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-atennonzero"><span class="std std-ref">AtenNonZero</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-atennonzerotuple"><span class="std std-ref">AtenNonZeroTuple</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-atenrollpos"><span class="std std-ref">AtenRollPos</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-atenrollrelu"><span class="std std-ref">AtenRollRelu</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-buildinisinstance"><span class="std std-ref">BuildInIsInstance</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-buildinlen"><span class="std std-ref">BuildInLen</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-complexpolar"><span class="std std-ref">ComplexPolar</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowcond"><span class="std std-ref">ControlFlowCond</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowcond2inputs"><span class="std std-ref">ControlFlowCond2Inputs</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowcond2outputs"><span class="std std-ref">ControlFlowCond2Outputs</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondconstant"><span class="std std-ref">ControlFlowCondConstant</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondidentity-153832"><span class="std std-ref">ControlFlowCondIdentity_153832</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondnestedmodule"><span class="std std-ref">ControlFlowCondNestedModule</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondnonzero"><span class="std std-ref">ControlFlowCondNonZero</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflownestcond"><span class="std std-ref">ControlFlowNestCond</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowscan"><span class="std std-ref">ControlFlowScan</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowscan2carried"><span class="std std-ref">ControlFlowScan2Carried</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowscancdist"><span class="std std-ref">ControlFlowScanCDist</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowscancdist2"><span class="std std-ref">ControlFlowScanCDist2</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowscancdistxy"><span class="std std-ref">ControlFlowScanCDistXY</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowscandecomposition-151564"><span class="std std-ref">ControlFlowScanDecomposition_151564</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-controlflowscaninplace-153705"><span class="std std-ref">ControlFlowScanInplace_153705</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-createfromshape"><span class="std std-ref">CreateFromShape</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-createfromshapethroughfunction"><span class="std std-ref">CreateFromShapeThroughFunction</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-croplastdimensionwithtensorcontent"><span class="std std-ref">CropLastDimensionWithTensorContent</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-croplastdimensionwithtensorshape"><span class="std std-ref">CropLastDimensionWithTensorShape</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplaceadd"><span class="std std-ref">InplaceAdd</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplaceadd2"><span class="std std-ref">InplaceAdd2</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplaceadd-mul"><span class="std std-ref">InplaceAdd_Mul</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplacecloneadd"><span class="std std-ref">InplaceCloneAdd_</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemellipsis-1"><span class="std std-ref">InplaceSetItemEllipsis_1</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemellipsis-2"><span class="std std-ref">InplaceSetItemEllipsis_2</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemmask"><span class="std std-ref">InplaceSetItemMask</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemsquare"><span class="std std-ref">InplaceSetItemSquare</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemsquareadd"><span class="std std-ref">InplaceSetItemSquareAdd</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemsquareadd2"><span class="std std-ref">InplaceSetItemSquareAdd2</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-signaturefloat1"><span class="std std-ref">SignatureFloat1</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-signatureint1"><span class="std std-ref">SignatureInt1</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-signatureint2"><span class="std std-ref">SignatureInt2</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-signaturelistfixedlength"><span class="std std-ref">SignatureListFixedLength</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-signaturelistfixedwithnone"><span class="std std-ref">SignatureListFixedWithNone</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-signaturelistvariablelength"><span class="std std-ref">SignatureListVariableLength</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-signatureshapeasindex"><span class="std std-ref">SignatureShapeAsIndex</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-typebfloat16"><span class="std std-ref">TypeBFloat16</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-vmap"><span class="std std-ref">Vmap</span></a></p></li>
<li><p><a class="reference internal" href="#ledx-model-case-export-vmappython"><span class="std std-ref">VmapPython</span></a></p></li>
</ul>
<section id="atenasstrided">
<span id="ledx-model-case-export-atenasstrided"></span><h2>AtenAsStrided<a class="headerlink" href="#atenasstrided" title="Link to this heading">¶</a></h2>
<section id="forward">
<h3>forward<a class="headerlink" href="#forward" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="custom">
<h3>custom<a class="headerlink" href="#custom" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>The implementation is still incorrect, x=&#39;x&#39;, shape=(&#39;batch&#39;, 2, 8, 8), size=[2, 2, 8, 4], stride=[128, 8, 16, 1], storage_offset=None
--DEBUG--
[GraphBuilder-VJC] Message starts, there are 0 initializers, 0 nodes, 1 inputs, 1 outputs.
--CONSTRAINTS--
    batch = {&#39;s77&#39;}
    s77 = {&#39;batch&#39;}
--SHAPE--
_dynamic_examples=
dynamic_objects=
   batch = &#39;batch&#39;
   s77 = &#39;s77&#39;
dynamic_objects_rev=
   &#39;batch&#39; = &lt;class &#39;list&#39;&gt;
     tuple
       &#39;batch&#39;
       ERR**: &lt;class &#39;torch.SymInt&#39;&gt;:&#39;batch&#39;
dynamic_dimensions_source={&#39;batch&#39;: [{&#39;axis&#39;: 0, &#39;input_name&#39;: &#39;x&#39;}]}
dynamic_dimensions_source_flat=[&#39;x&#39;]
output_dynamic_dimensions_source_flat=None
dynamic_alias={&#39;s77&#39;: &#39;batch&#39;}
dynamic_shapes={&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}
_known_shapes={&#39;x&#39;: (&#39;batch&#39;, 2, 8, 8)}
_known_types={&#39;x&#39;: 1}
_known_value_shape={}
_known_constants=[]
_known_ranks={}
--PARAMETERS--
_parameter_renaming=
--TORCH-USERS--
    as_strided -&gt; {output}
    x -&gt; {as_strided}
--TORCH-SHAPES--
    x: (&#39;run_node&#39;, (&#39;&#39;, (&#39;val&#39;, torch.float32, torch.Size([s77, 2, 8, 8])))) --- 1:4:(&#39;batch&#39;, 2, 8, 8):
    as_strided: (&#39;run_node&#39;, (&#39;&#39;, (&#39;val&#39;, torch.float32, torch.Size([2, 2, 8, 4])))) --- :::
--ONNX--
-- EXEPATH --
export-export_options=ExportOptions()
-- process.graph_module --
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;f32[s77, 2, 8, 8]&quot;):
             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:186 in forward, code: y = torch.as_strided(x, (2, 2, 8, 4), (128, 8, 16, 1))
            as_strided: &quot;f32[2, 2, 8, 4]&quot; = torch.ops.aten.as_strided.default(x, [2, 2, 8, 4], [128, 8, 16, 1]);  x = None
            return (as_strided,)
        
Graph signature: 
    # inputs
    x: USER_INPUT

    # outputs
    as_strided: USER_OUTPUT

Range constraints: {s77: VR[0, int_oo]}

-- process.graph_module.graph --
graph():
    %x : [num_users=1] = placeholder[target=x]
    %as_strided : [num_users=1] = call_function[target=torch.ops.aten.as_strided.default](args = (%x, [2, 2, 8, 4], [128, 8, 16, 1]), kwargs = {})
    return (as_strided,)
-- process.inputs_to_remove --
set()
-- process.progress --
node 1/3 target=aten.as_strided.default
-- 1 INPUTS
[GraphBuilder-VJC.make_tensor_input] x[1:batchx2x8x8]
-- 0 INITIALIZERS
-- 0 OUTPUTS
[GraphBuilder-VJC] Message completed, there are 0 initializers, 0 nodes, 1 inputs, 1 outputs.
</pre></div>
</div>
</section>
<section id="dynamo-ir">
<h3>dynamo-ir<a class="headerlink" href="#dynamo-ir" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s2x2x8x8,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,2,8,8] x) =&gt; (float[2,2,8,4] as_strided) 
   &lt;int64[4] val_0 =  {2,2,8,4}, int64[4] val_1 =  {128,8,16,1}, int64[1] neg_1 =  {-1}, int64[1] rank_tensor =  {4}, int64 indices =  {0}, int64 rank_0 =  {4}, int64 int64_1_cast =  {1}, float[1] tmp_14 =  {1}, int64[4] val_0, int64[4] val_1, int64[1] neg_1, int64[1] rank_tensor, int64 indices, int64 rank_0, int64 int64_1_cast, float[1] tmp_14, seq(float[]) one_seq, float[?] self_flatten&gt;
{
   [n4] one_seq = SequenceEmpty ()
   [n6_2] one_seq_16, indices_17 = Loop (rank_0, &quot;&quot;, one_seq, indices) &lt;body: graph = loop_body (int64 i, bool cond_in,  one_seq_1,  indices_2) =&gt; (bool cond_out,  one_seq_15,  indices_13) 
      &lt;int64 tmp, int64 j, int64[1] j_tensor, int64[1] size_dim_j, int64[?] size_after_j, int64[1] stride_dim_j, int64[?] tmp_6, int64[?] add_value, bool cond&gt;
{
      [n2_2] tmp = Sub (rank_0, i)
      [n5_2] j = Sub (tmp, int64_1_cast)
      [n6] j_tensor = Reshape (j, neg_1)
      [n7] size_dim_j = Gather &lt;axis: int = 0&gt; (val_0, j_tensor)
      [n8] size_after_j = Slice (val_0, j_tensor, rank_tensor)
      [n9] stride_dim_j = Gather &lt;axis: int = 0&gt; (val_1, j_tensor)
      [n10] indices_4 = Expand (indices_2, size_after_j)
      [n15] tmp_6 = Range (indices, size_dim_j, int64_1_cast)
      [n16] add_value = Mul (tmp_6, stride_dim_j)
      [n19] cond = Equal (i, indices)
      [n20] shape_11 = If (cond) &lt;then_branch: graph = thenGraph_39 () =&gt; (int64[1] shape) {
         [n0_3] shape = Identity (size_dim_j)
      }, else_branch: graph = elseGraph_39 () =&gt; (int64[] shape_10) 
         &lt;float[1] tmp_8&gt;
{
         [n0_4] ones = ConcatFromSequence &lt;axis: int = 0&gt; (one_seq_1)
         [n1_3] tmp_8 = Cast &lt;to: int = 1&gt; (size_dim_j)
         [n2_3] shape_9 = Concat &lt;axis: int = 0&gt; (tmp_8, ones)
         [n3_3] shape_10 = Cast &lt;to: int = 7&gt; (shape_9)
      }&gt;
      [n21] add_value_12 = Reshape (add_value, shape_11)
      [n22] indices_13 = Add (indices_4, add_value_12)
      [n24] one_seq_15 = SequenceInsert (one_seq_1, tmp_14)
      [n25] cond_out = Identity (cond_in)
   }&gt;
   [n8_2] self_flatten = Reshape (x, neg_1)
   [n10_2] storage_offset_cast = CastLike (indices, indices_17)
   [n11_2] indices_19 = Add (indices_17, storage_offset_cast)
   [n12_2] as_strided = Gather (self_flatten, indices_19)
}
</pre></div>
</div>
</section>
</section>
<section id="ateninterpolate">
<span id="ledx-model-case-export-ateninterpolate"></span><h2>AtenInterpolate<a class="headerlink" href="#ateninterpolate" title="Link to this heading">¶</a></h2>
<section id="id1">
<h3>forward<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
        <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>custom<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s2x2x3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s2x2x3x4[-2.2095751762390137,2.7057902812957764:A0.0801867587142624],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  _onx_concat_x::Shape:2: (&#39;batch&#39;, 2, 6, 8),
  x::Shape:2: (&#39;batch&#39;, 2),
}&quot;]
&gt;
experiment (float[batch,2,3,4] x) =&gt; (float[batch,2,6,8] output_0) 
   &lt;int64[2] init7_s2_6_8 =  {6,8}, float[batch,2,6,8] upsample_bilinear2d, int64[2] &quot;x::Shape:2&quot;&gt;
{
   [upsample_bicubic2d_vec] &quot;x::Shape:2&quot; = Shape &lt;end: int = 2, start: int = 0&gt; (x)
   [upsample_bicubic2d_vec2] &quot;_onx_concat_x::Shape:2&quot; = Concat &lt;axis: int = 0&gt; (&quot;x::Shape:2&quot;, init7_s2_6_8)
   [upsample_bicubic2d_vec3] output_0 = Resize &lt;coordinate_transformation_mode: string = &quot;pytorch_half_pixel&quot;, mode: string = &quot;linear&quot;, nearest_mode: string = &quot;floor&quot;&gt; (x, &quot;&quot;, &quot;&quot;, &quot;_onx_concat_x::Shape:2&quot;)
}
</pre></div>
</div>
</section>
<section id="id3">
<h3>dynamo-ir<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s2x2x3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,2,3,4] x) =&gt; (float[batch,2,6,8] upsample_bilinear2d) 
   &lt;float[4] val_0 =  {1,1,2,2}, float[4] val_0&gt;
{
   [node_upsample_bilinear2d] upsample_bilinear2d = Resize &lt;keep_aspect_ratio_policy: string = &quot;stretch&quot;, antialias: int = 0, extrapolation_value: float = 0, exclude_outside: int = 0, nearest_mode: string = &quot;floor&quot;, coordinate_transformation_mode: string = &quot;pytorch_half_pixel&quot;, cubic_coeff_a: float = -0.75, mode: string = &quot;linear&quot;&gt; (x, &quot;&quot;, val_0)
}
</pre></div>
</div>
</section>
</section>
<section id="atennonzero">
<span id="ledx-model-case-export-atennonzero"></span><h2>AtenNonZero<a class="headerlink" href="#atennonzero" title="Link to this heading">¶</a></h2>
<section id="id4">
<h3>forward<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="id5">
<h3>custom<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;0&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;0&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;2&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[-2.233980178833008,1.57475745677948:A0.18694991742571196],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;NEWDIM_nonzero&#39;: {&#39;u0&#39;},
 &#39;batch&#39;: {&#39;s77&#39;},
 &#39;s77&#39;: {&#39;batch&#39;},
 &#39;u0&#39;: {&#39;NEWDIM_nonzero&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  nonzero::Shape:1: (&#39;NEWDIM_nonzero&#39;,),
  sym_size_int_1: NEWDIM_nonzero,
}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (int64[NEWDIM_nonzero,2] output_0) 
   &lt;int64[2,NEWDIM_nonzero] _onx_nonzero_x, bool sym_constrain_range_for_size_default, int64 sym_size_int_1, int64[1] &quot;nonzero::Shape:1&quot;, int64[NEWDIM_nonzero,2] nonzero&gt;
{
   [nonzero] _onx_nonzero_x = NonZero (x)
   [nonzero2] output_0 = Transpose &lt;perm: ints = [1, 0]&gt; (_onx_nonzero_x)
}
</pre></div>
</div>
</section>
<section id="id6">
<h3>dynamo-ir<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (int64[u0,2] nonzero) 
   &lt;int64[2,?] val_0&gt;
{
   [node_NonZero_0] val_0 = NonZero (x)
   [node_nonzero] nonzero = Transpose &lt;perm: ints = [1, 0]&gt; (val_0)
}
</pre></div>
</div>
</section>
</section>
<section id="atennonzerotuple">
<span id="ledx-model-case-export-atennonzerotuple"></span><h2>AtenNonZeroTuple<a class="headerlink" href="#atennonzerotuple" title="Link to this heading">¶</a></h2>
<section id="id7">
<h3>forward<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>custom<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;8&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;4&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True, True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[-1.4644967317581177,1.1600340604782104:A-0.2688992905120055],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  getitem_2::Shape:1: (&#39;u0&#39;,),
  sym_size_int_1: u0,
}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (int64[u0] output_0, int64[u0] output_1) 
   &lt;int64[1] &quot;init7_s1_-1&quot; =  {-1}, int64[?,?] _onx_split_nonzero_x_1, int64[u0] &quot;nonzero_numpy#0&quot;, int64[2,NEWDIM_nonzero] _onx_nonzero_x, int64[1] &quot;getitem_2::Shape:1&quot;, bool sym_constrain_range_for_size_default, int64[u0] getitem_1, int64[u0] getitem_2, int64[u0] &quot;nonzero_numpy#1&quot;, int64 sym_size_int_1, int64[?,?] _onx_split_nonzero_x_0&gt;
{
   [nonzero_numpy] _onx_nonzero_x = NonZero (x)
   [nonzero_numpy2] _onx_split_nonzero_x_0, _onx_split_nonzero_x_1 = Split &lt;num_outputs: int = 2&gt; (_onx_nonzero_x)
   [nonzero_numpy3] output_0 = Reshape (_onx_split_nonzero_x_0, &quot;init7_s1_-1&quot;)
   [nonzero_numpy4] output_1 = Reshape (_onx_split_nonzero_x_1, &quot;init7_s1_-1&quot;)
}
</pre></div>
</div>
</section>
<section id="id9">
<h3>dynamo-ir<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (int64[u0] getitem, int64[u0] getitem_1) 
   &lt;int64[1] unbind_axis =  {1}, int64[1] unbind_axis, int64[2,?] val_0, int64[u0,2] nonzero, int64[u0,1] unbind_split_0, int64[u0,1] unbind_split_1&gt;
{
   [node_NonZero_0] val_0 = NonZero (x)
   [node_nonzero] nonzero = Transpose &lt;perm: ints = [1, 0]&gt; (val_0)
   [node_Split_4] unbind_split_0, unbind_split_1 = Split &lt;axis: int = 1, num_outputs: int = 2&gt; (nonzero)
   [node_Squeeze_6] getitem = Squeeze (unbind_split_0, unbind_axis)
   [node_Squeeze_7] getitem_1 = Squeeze (unbind_split_1, unbind_axis)
}
</pre></div>
</div>
</section>
</section>
<section id="atenrollpos">
<span id="ledx-model-case-export-atenrollpos"></span><h2>AtenRollPos<a class="headerlink" href="#atenrollpos" title="Link to this heading">¶</a></h2>
<section id="id10">
<h3>forward<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id11">
<h3>custom<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s2x3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;3&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;24&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s2x3x4[10.0,33.0:A21.5],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3,4] x) =&gt; (float[batch,3,4] output_0) 
   &lt;int64[1] &quot;init7_s1_-1&quot; =  {-1}, int64[1] init7_s1_4 =  {4}, int64[1] init7_s1_0 =  {0}, float[batch,3,4] roll, float[?,?,?] _onx_slice_x, float[?,?,?] _onx_slice_x2, float[batch,3,4] _onx_concat_slice_x&gt;
{
   [roll] _onx_slice_x = Slice (x, &quot;init7_s1_-1&quot;, init7_s1_4, &quot;init7_s1_-1&quot;)
   [roll2] _onx_slice_x2 = Slice (x, init7_s1_0, &quot;init7_s1_-1&quot;, &quot;init7_s1_-1&quot;)
   [roll3] output_0 = Concat &lt;axis: int = -1&gt; (_onx_slice_x, _onx_slice_x2)
}
</pre></div>
</div>
</section>
<section id="id12">
<h3>dynamo-ir<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s2x3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3,4] x) =&gt; (float[batch,3,4] roll) 
   &lt;int64[1] neg_1 =  {-1}, int64[1] shift_tensor =  {1}, int64[1] slice_length_3 =  {3}, int64[1] tmp_4 =  {0}, int64[1] neg_1, int64[1] shift_tensor, int64[1] slice_length_3, int64[1] tmp_4, float[batch,3,3] suffix, int64 tmp_5, int64[1] tmp_6, float[?,?,?] prefix&gt;
{
   [n9] suffix = Slice (x, tmp_4, slice_length_3, neg_1)
   [n10] tmp_5 = Size (x)
   [n11] tmp_6 = Reshape (tmp_5, neg_1)
   [n12] prefix = Slice (x, slice_length_3, tmp_6, neg_1)
   [n13] roll = Concat &lt;axis: int = -1&gt; (prefix, suffix)
}
</pre></div>
</div>
</section>
</section>
<section id="atenrollrelu">
<span id="ledx-model-case-export-atenrollrelu"></span><h2>AtenRollRelu<a class="headerlink" href="#atenrollrelu" title="Link to this heading">¶</a></h2>
<section id="id13">
<h3>forward<a class="headerlink" href="#id13" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="id14">
<h3>custom<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s2x3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;4&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;32&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;4&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s2x3x4[10.0,33.0:A21.5],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3,4] x) =&gt; (float[batch,3,4] output_0) 
   &lt;int64[1] init7_s1_1 =  {1}, int64[1] init7_s1_4 =  {4}, int64[1] &quot;init7_s1_-1&quot; =  {-1}, int64[1] init7_s1_0 =  {0}, float[batch,3,4] roll, float[?,?,?] _onx_slice_x, float[?,?,?] _onx_slice_x2, float[batch,3,4] _onx_concat_slice_x, float[batch,3,4] relu&gt;
{
   [roll] _onx_slice_x = Slice (x, init7_s1_1, init7_s1_4, &quot;init7_s1_-1&quot;)
   [roll2] _onx_slice_x2 = Slice (x, init7_s1_0, init7_s1_1, &quot;init7_s1_-1&quot;)
   [roll3] _onx_concat_slice_x = Concat &lt;axis: int = -1&gt; (_onx_slice_x, _onx_slice_x2)
   [relu] output_0 = Relu (_onx_concat_slice_x)
}
</pre></div>
</div>
</section>
<section id="id15">
<h3>dynamo-ir<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s2x3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3,4] x) =&gt; (float[batch,3,4] relu) 
   &lt;int64[1] neg_1 =  {-1}, int64[1] slice_length_3 =  {1}, int64[1] tmp_4 =  {0}, int64[1] neg_1, int64[1] slice_length_3, int64[1] tmp_4, float[batch,3,1] suffix, int64 tmp_5, int64[1] tmp_6, float[?,?,?] prefix, float[batch,3,4] roll&gt;
{
   [n9] suffix = Slice (x, tmp_4, slice_length_3, neg_1)
   [n10] tmp_5 = Size (x)
   [n11] tmp_6 = Reshape (tmp_5, neg_1)
   [n12] prefix = Slice (x, slice_length_3, tmp_6, neg_1)
   [n13] roll = Concat &lt;axis: int = -1&gt; (prefix, suffix)
   [node_relu] relu = Relu (roll)
}
</pre></div>
</div>
</section>
</section>
<section id="buildinisinstance">
<span id="ledx-model-case-export-buildinisinstance"></span><h2>BuildInIsInstance<a class="headerlink" href="#buildinisinstance" title="Link to this heading">¶</a></h2>
<section id="id16">
<h3>forward<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lx</span><span class="p">:</span> <span class="nb">list</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lx</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">lx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">+</span> <span class="n">t</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">+</span> <span class="n">lx</span>
</pre></div>
</div>
</section>
<section id="id17">
<h3>custom<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#2[T1s8x1,T1s8x2])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;4&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;28&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;6&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],#2[T1s4x1[10.0,13.0:A11.5],T1s4x2[10.0,17.0:A13.5]])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;lx&#39;: [{0: Dim(&#39;batch&#39;, min=0)}, {0: Dim(&#39;batch&#39;, min=0)}],
 &#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s50&#39;, &#39;s77&#39;, &#39;s53&#39;},
 &#39;s50&#39;: {&#39;s53&#39;, &#39;batch&#39;},
 &#39;s53&#39;: {&#39;batch&#39;},
 &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] output_0) 
   &lt;float[1] b_buff =  {0.5}, int64[1] init7_s1_1 =  {1}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {-0.520031,-0.425974,0.0522956}, float[1] &quot;linear.bias&quot; =  {0.120707}, float[batch,1] sub, float[batch,1] linear, float[batch,1] sum_1, float[1,3] p_linear_weight, int64[2] &quot;init7_s2_1_-1&quot;, float[batch,1] mul, float[batch,1] add, float[batch,1] _onx_matmul_x, float[batch,1] sigmoid, float[3,1] &quot;p_linear_weight::T10&quot;, float[1,3] &quot;linear.weight&quot;, float[1] p_linear_bias&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 1&gt; (lx_1, init7_s1_1)
   [mul_Tensor] mul = Mul (lx_0, sum_1)
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [sub_Tensor] sub = Sub (sigmoid, b_buff)
   [add_Tensor] output_0 = Add (sub, mul)
}
</pre></div>
</div>
</section>
<section id="id18">
<h3>dynamo-ir<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#2[T1s8x1,T1s8x2])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] add_15) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {0.0634959,-0.0883696,-0.391949}, float[1] &quot;linear.bias&quot; =  {-0.132326}, float[1] buff =  {0.5}, int64[1] val_6 =  {1}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, float[1] buff, int64[1] val_6, float[batch,1] sum_1, float[batch,1] mul_1, float[batch,1] linear, float[batch,1] sigmoid, float[batch,1] sub_4&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 1&gt; (lx_1, val_6)
   [node_mul_1] mul_1 = Mul (lx_0, sum_1)
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_sub_4] sub_4 = Sub (sigmoid, buff)
   [node_add_15] add_15 = Add (sub_4, mul_1)
}
</pre></div>
</div>
</section>
</section>
<section id="buildinlen">
<span id="ledx-model-case-export-buildinlen"></span><h2>BuildInLen<a class="headerlink" href="#buildinlen" title="Link to this heading">¶</a></h2>
<section id="id19">
<h3>forward<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lx</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">lx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lx</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">lx</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">+</span> <span class="n">t</span>
</pre></div>
</div>
</section>
<section id="id20">
<h3>custom<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#3[T1s8x1,T1s8x2,T1s8x3])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;4&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;28&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;6&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],#2[T1s4x1[10.0,13.0:A11.5],T1s4x2[10.0,17.0:A13.5]])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;lx&#39;: [{0: Dim(&#39;batch&#39;, min=0)}, {0: Dim(&#39;batch&#39;, min=0)}],
 &#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s50&#39;, &#39;s77&#39;, &#39;s53&#39;},
 &#39;s50&#39;: {&#39;s53&#39;, &#39;batch&#39;},
 &#39;s53&#39;: {&#39;batch&#39;},
 &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] output_0) 
   &lt;float[1] b_buff =  {0.5}, int64[1] init7_s1_1 =  {1}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {0.0266144,-0.387719,0.251595}, float[1] &quot;linear.bias&quot; =  {-0.0445831}, float[batch,1] sub, float[batch,1] linear, float[batch,1] sum_1, float[1,3] p_linear_weight, int64[2] &quot;init7_s2_1_-1&quot;, float[batch,1] mul, float[batch,1] add, float[batch,1] _onx_matmul_x, float[batch,1] sigmoid, float[3,1] &quot;p_linear_weight::T10&quot;, float[1,3] &quot;linear.weight&quot;, float[1] p_linear_bias&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 1&gt; (lx_1, init7_s1_1)
   [mul_Tensor] mul = Mul (lx_0, sum_1)
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [sub_Tensor] sub = Sub (sigmoid, b_buff)
   [add_Tensor] output_0 = Add (sub, mul)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>diff.1
</pre></div>
</div>
</section>
<section id="id21">
<h3>dynamo-ir<a class="headerlink" href="#id21" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#3[T1s8x1,T1s8x2,T1s8x3])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] add_15) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {-0.0815361,-0.23107,0.217732}, float[1] &quot;linear.bias&quot; =  {-0.365702}, float[1] buff =  {0.5}, int64[1] val_6 =  {1}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, float[1] buff, int64[1] val_6, float[batch,1] sum_1, float[batch,1] mul_1, float[batch,1] linear, float[batch,1] sigmoid, float[batch,1] sub_4&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 1&gt; (lx_1, val_6)
   [node_mul_1] mul_1 = Mul (lx_0, sum_1)
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_sub_4] sub_4 = Sub (sigmoid, buff)
   [node_add_15] add_15 = Add (sub_4, mul_1)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>diff.1
</pre></div>
</div>
</section>
</section>
<section id="complexpolar">
<span id="ledx-model-case-export-complexpolar"></span><h2>ComplexPolar<a class="headerlink" href="#complexpolar" title="Link to this heading">¶</a></h2>
<section id="id22">
<h3>forward<a class="headerlink" href="#id22" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">angle</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">polar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">angle</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id23">
<h3>custom<a class="headerlink" href="#id23" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x4,T1s4x4)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},angle:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;8&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;8&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x4[0.03763914108276367,0.9130630493164062:A0.4403964690864086],T1s4x4[0.002207815647125244,0.9957027435302734:A0.5276047624647617])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;angle&#39;: {0: Dim(&#39;batch&#39;, min=0)}, &#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;, &#39;s96&#39;}, &#39;s77&#39;: {&#39;batch&#39;}, &#39;s96&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x, float[batch,4] angle) =&gt; (complex64[batch,4] output_0) 
   &lt;complex64[1] init14_s1_ = ..., complex64[batch,4] &quot;x::C14&quot;, complex64[batch,4] polar, complex64[batch,4] &quot;_onx_sin_angle::C14&quot;, complex64[batch,4] &quot;_onx_cos_angle::C14&quot;, complex64[batch,4] &quot;_onx_add_cos_angle::C14&quot;, float[batch,4] _onx_cos_angle, float[batch,4] _onx_sin_angle, complex64[batch,4] &quot;_onx_mul_sin_angle::C14&quot;&gt;
{
   [polar] _onx_cos_angle = Cos (angle)
   [polar2] &quot;_onx_cos_angle::C14&quot; = Cast &lt;to: int = 14&gt; (_onx_cos_angle)
   [polar3] _onx_sin_angle = Sin (angle)
   [polar4] &quot;_onx_sin_angle::C14&quot; = Cast &lt;to: int = 14&gt; (_onx_sin_angle)
   [polar5] &quot;_onx_mul_sin_angle::C14&quot; = Mul (&quot;_onx_sin_angle::C14&quot;, init14_s1_)
   [polar6] &quot;x::C14&quot; = Cast &lt;to: int = 14&gt; (x)
   [polar7] &quot;_onx_add_cos_angle::C14&quot; = Add (&quot;_onx_cos_angle::C14&quot;, &quot;_onx_mul_sin_angle::C14&quot;)
   [polar8] output_0 = Mul (&quot;x::C14&quot;, &quot;_onx_add_cos_angle::C14&quot;)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Type Error: Type &#39;tensor(complex64)&#39; of input parameter (_onx_sin_angle::C14) of operator (Mul) in node (polar5) is invalid.
</pre></div>
</div>
</section>
<section id="id24">
<h3>dynamo-ir<a class="headerlink" href="#id24" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x4,T1s4x4)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},angle:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x, float[batch,4] angle) =&gt; (float[batch,4,2] polar) 
   &lt;int64[1] int64_m1_1d =  {-1}, int64[1] int64_m1_1d, float[batch,4] tmp, float[batch,4] tmp_0, float[batch,4,1] real, float[batch,4] tmp_1, float[batch,4] tmp_2, float[batch,4,1] imag&gt;
{
   [n0] tmp = Cos (angle)
   [n1] tmp_0 = Mul (x, tmp)
   [n3] real = Unsqueeze (tmp_0, int64_m1_1d)
   [n4] tmp_1 = Sin (angle)
   [n5] tmp_2 = Mul (x, tmp_1)
   [n7] imag = Unsqueeze (tmp_2, int64_m1_1d)
   [n8] polar = Concat &lt;axis: int = -1&gt; (real, imag)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>diff.0
</pre></div>
</div>
</section>
</section>
<section id="controlflowcond">
<span id="ledx-model-case-export-controlflowcond"></span><h2>ControlFlowCond<a class="headerlink" href="#controlflowcond" title="Link to this heading">¶</a></h2>
<section id="id25">
<h3>forward<a class="headerlink" href="#id25" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">true_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">false_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="id26">
<h3>custom<a class="headerlink" href="#id26" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;4&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s5x3[0.14236193895339966,0.9016755223274231:A0.4629640181859334],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x) =&gt; (float[batch,3] output_0) 
   &lt;float init1_s_ =  {0}, bool gt, float[1] &quot;init1_s_::RSh1&quot;, float[batch,3] getitem, int64[1] init7_s1_1, float sum_1, float[1] &quot;sum_1::RSh1&quot;&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 0&gt; (x)
   [gt_Scalar3] gt = Greater (sum_1, init1_s_)
   [cond] output_0 = If (gt) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [cos2] &quot;cond#0&quot; = Cos (x)
   }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [sin2] &quot;cond#0&quot; = Sin (x)
   }&gt;
}
</pre></div>
</div>
</section>
<section id="id27">
<h3>dynamo-ir<a class="headerlink" href="#id27" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x) =&gt; (float[batch,3] getitem) 
   &lt;float scalar_tensor_default =  {0}, float scalar_tensor_default, float sum_1, bool gt&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (x)
   [node_gt] gt = Greater (sum_1, scalar_tensor_default)
   [node_cond__0] getitem = If (gt) &lt;then_branch: graph = true_graph_0 () =&gt; (float[batch,3] sin_true_graph_0) {
      [node_sin] sin_true_graph_0 = Sin (x)
   }, else_branch: graph = false_graph_0 () =&gt; (float[batch,3] cos_false_graph_0) {
      [node_cos] cos_false_graph_0 = Cos (x)
   }&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowcond2inputs">
<span id="ledx-model-case-export-controlflowcond2inputs"></span><h2>ControlFlowCond2Inputs<a class="headerlink" href="#controlflowcond2inputs" title="Link to this heading">¶</a></h2>
<section id="id28">
<h3>forward<a class="headerlink" href="#id28" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">true_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">false_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="id29">
<h3>custom<a class="headerlink" href="#id29" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,T1s5x3)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},y:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;4&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True, True]&quot;, &quot;input_args&quot;: &quot;(T1s5x3[0.1351701021194458,0.8832821249961853:A0.4840537468592326],T1s5x3[0.09820079803466797,0.9622029066085815:A0.48026190996170043])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}, &#39;y&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;, &#39;s17&#39;}, &#39;s17&#39;: {&#39;batch&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x, float[batch,3] y) =&gt; (float[batch,3] output_0, float[batch,3] output_1) 
   &lt;float init1_s_ =  {0}, bool gt, float[1] &quot;init1_s_::RSh1&quot;, float[batch,3] getitem, int64[1] init7_s1_1, float sum_1, float[batch,3] getitem_1, float[1] &quot;sum_1::RSh1&quot;&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 0&gt; (x)
   [gt_Scalar3] gt = Greater (sum_1, init1_s_)
   [cond] output_0, output_1 = If (gt) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;,  &quot;cond#1&quot;) {
      [cos2] &quot;cond#0&quot; = Cos (x)
      [sin2] sin2 = Sin (x)
      [add_Tensor2] &quot;cond#1&quot; = Add (sin2, y)
   }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;,  &quot;cond#1&quot;) {
      [sin32] &quot;cond#0&quot; = Sin (x)
      [cos32] cos2 = Cos (x)
      [add_Tensor32] &quot;cond#1&quot; = Add (cos2, y)
   }&gt;
}
</pre></div>
</div>
</section>
<section id="id30">
<h3>dynamo-ir<a class="headerlink" href="#id30" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,T1s5x3)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},y:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x, float[batch,3] y) =&gt; (float[batch,3] getitem, float[batch,3] getitem_1) 
   &lt;float scalar_tensor_default =  {0}, float scalar_tensor_default, float sum_1, bool gt&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (x)
   [node_gt] gt = Greater (sum_1, scalar_tensor_default)
   [node_cond__1] getitem, getitem_1 = If (gt) &lt;then_branch: graph = true_graph_0 () =&gt; (float[batch,3] sin_true_graph_0, float[batch,3] add_12_true_graph_0) 
      &lt;float[batch,3] cos&gt;
{
      [node_sin] sin_true_graph_0 = Sin (x)
      [node_cos] cos = Cos (x)
      [node_add_12] add_12_true_graph_0 = Add (cos, y)
   }, else_branch: graph = false_graph_0 () =&gt; (float[batch,3] cos_false_graph_0, float[batch,3] add_12_false_graph_0) 
      &lt;float[batch,3] sin_2&gt;
{
      [node_cos_2] cos_false_graph_0 = Cos (x)
      [node_sin_2] sin_2 = Sin (x)
      [node_add_12_2] add_12_false_graph_0 = Add (sin_2, y)
   }&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowcond2outputs">
<span id="ledx-model-case-export-controlflowcond2outputs"></span><h2>ControlFlowCond2Outputs<a class="headerlink" href="#controlflowcond2outputs" title="Link to this heading">¶</a></h2>
<section id="id31">
<h3>forward<a class="headerlink" href="#id31" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">true_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">false_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="id32">
<h3>custom<a class="headerlink" href="#id32" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;4&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True, True]&quot;, &quot;input_args&quot;: &quot;(T1s5x3[0.005455434322357178,0.9612339735031128:A0.4906205932299296],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x) =&gt; (float[batch,3] output_0, float[batch,3] output_1) 
   &lt;float init1_s_ =  {0}, bool gt, float[1] &quot;init1_s_::RSh1&quot;, float[batch,3] getitem, int64[1] init7_s1_1, float sum_1, float[batch,3] getitem_1, float[1] &quot;sum_1::RSh1&quot;&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 0&gt; (x)
   [gt_Scalar3] gt = Greater (sum_1, init1_s_)
   [cond] output_0, output_1 = If (gt) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;,  &quot;cond#1&quot;) {
      [cos2] &quot;cond#0&quot; = Cos (x)
      [sin2] &quot;cond#1&quot; = Sin (x)
   }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;,  &quot;cond#1&quot;) {
      [sin32] &quot;cond#0&quot; = Sin (x)
      [cos32] &quot;cond#1&quot; = Cos (x)
   }&gt;
}
</pre></div>
</div>
</section>
<section id="id33">
<h3>dynamo-ir<a class="headerlink" href="#id33" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x) =&gt; (float[batch,3] getitem, float[batch,3] getitem_1) 
   &lt;float scalar_tensor_default =  {0}, float scalar_tensor_default, float sum_1, bool gt&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (x)
   [node_gt] gt = Greater (sum_1, scalar_tensor_default)
   [node_cond__1] getitem, getitem_1 = If (gt) &lt;then_branch: graph = true_graph_0 () =&gt; (float[batch,3] sin_true_graph_0, float[batch,3] cos_true_graph_0) {
      [node_sin] sin_true_graph_0 = Sin (x)
      [node_cos] cos_true_graph_0 = Cos (x)
   }, else_branch: graph = false_graph_0 () =&gt; (float[batch,3] cos_false_graph_0, float[batch,3] sin_false_graph_0) {
      [node_cos_2] cos_false_graph_0 = Cos (x)
      [node_sin_2] sin_false_graph_0 = Sin (x)
   }&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowcondconstant">
<span id="ledx-model-case-export-controlflowcondconstant"></span><h2>ControlFlowCondConstant<a class="headerlink" href="#controlflowcondconstant" title="Link to this heading">¶</a></h2>
<section id="id34">
<h3>forward<a class="headerlink" href="#id34" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">true_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">false_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="id35">
<h3>custom<a class="headerlink" href="#id35" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s1024x1024,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;4&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s1024x1024[1.1920928955078125e-07,0.9999991059303284:A0.49969165317679654],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,1024] x) =&gt; (float[batch,1024] output_0) 
   &lt;float init1_s_ =  {0}, bool gt, float[1] &quot;init1_s_::RSh1&quot;, float[batch,1024] getitem, int64[1] init7_s1_1, float sum_1, float[1] &quot;sum_1::RSh1&quot;&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 0&gt; (x)
   [gt_Scalar3] gt = Greater (sum_1, init1_s_)
   [cond] output_0 = If (gt) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [init2cst2] init7_s2_1_10242 = Constant &lt;value: tensor = int64[2] init7_s2_1_1024 {1,1024}&gt; ()
      [cos2] cos2 = Cos (x)
      [ones2] ones2 = ConstantOfShape &lt;value: tensor = float[1] {1}&gt; (init7_s2_1_10242)
      [add_Tensor2] &quot;cond#0&quot; = Add (cos2, ones2)
   }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [init2cst32] init7_s1_10242 = Constant &lt;value: tensor = int64[1] init7_s1_1024 {1024}&gt; ()
      [sin2] sin2 = Sin (x)
      [sym_size_int2] &quot;x::Shape:12&quot; = Shape &lt;end: int = 1, start: int = 0&gt; (x)
      [_mkshape_sym_size_int2] &quot;_onx_concat_sym_size_int::UnSq02&quot; = Concat &lt;axis: int = 0&gt; (&quot;x::Shape:12&quot;, init7_s1_10242)
      [ones32] ones32 = ConstantOfShape &lt;value: tensor = float[1] {1}&gt; (&quot;_onx_concat_sym_size_int::UnSq02&quot;)
      [sub_Tensor2] &quot;cond#0&quot; = Sub (sin2, ones32)
   }&gt;
}
</pre></div>
</div>
</section>
<section id="id36">
<h3>dynamo-ir<a class="headerlink" href="#id36" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s1024x1024,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,1024] x) =&gt; (float[batch,1024] getitem) 
   &lt;float scalar_tensor_default =  {0}, int64[1] val_1 =  {-1}, int64[1] val_3 =  {1024}, float val_7 =  {1}, float[1,1024] ones_2 =  {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1}, float scalar_tensor_default, int64[1] val_1, int64[1] val_3, float val_7, float[1,1024] ones_2, float sum_1, bool gt&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (x)
   [node_gt] gt = Greater (sum_1, scalar_tensor_default)
   [node_cond__0] getitem = If (gt) &lt;then_branch: graph = true_graph_0 () =&gt; (float[?,1024] sub_3_true_graph_0) 
      &lt;int64[1] val_0_2, int64 sym_size_int, float[batch,1024] sin, int64[1] val_2, int64[2] val_4, float[?,?] ones&gt;
{
      [node_Shape_0] val_0_2 = Shape &lt;end: int = 1, start: int = 0&gt; (x)
      [node_sym_size_int] sym_size_int = Squeeze (val_0_2)
      [node_sin] sin = Sin (x)
      [node_Reshape_2] val_2 = Reshape &lt;allowzero: int = 0&gt; (sym_size_int, val_1)
      [node_Concat_4] val_4 = Concat &lt;axis: int = 0&gt; (val_2, val_3)
      [node_ones] ones = Expand (val_7, val_4)
      [node_sub_3] sub_3_true_graph_0 = Sub (sin, ones)
   }, else_branch: graph = false_graph_0 () =&gt; (float[batch,1024] add_6_false_graph_0) 
      &lt;float[batch,1024] cos&gt;
{
      [node_cos] cos = Cos (x)
      [node_add_6] add_6_false_graph_0 = Add (cos, ones_2)
   }&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowcondidentity-153832">
<span id="ledx-model-case-export-controlflowcondidentity-153832"></span><h2>ControlFlowCondIdentity_153832<a class="headerlink" href="#controlflowcondidentity-153832" title="Link to this heading">¶</a></h2>
<section id="id37">
<h3>forward<a class="headerlink" href="#id37" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">branch_cond_then_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">branch_cond_else_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>  <span class="c1"># fails but succeeds with x.clone()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">branch_cond_then_1</span><span class="p">,</span> <span class="n">branch_cond_else_1</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="id38">
<h3>custom<a class="headerlink" href="#id38" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Cond doesn&#39;t work unless it is captured completely with torch.compile. Scroll up to find out what causes the graph break.

from user code:
   File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_higher_order_ops/cond.py&quot;, line 187, in _cond_op_wrapper
    return cond_op(*args, **kwargs)

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you&#39;re reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=&quot;+dynamo&quot;
</pre></div>
</div>
</section>
<section id="id39">
<h3>dynamo-ir<a class="headerlink" href="#id39" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to export the model with torch.export. [96mThis is step 1/3[0m of exporting the model to ONNX. Next steps:
- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.
- Debug `torch.export.export` and summit a PR to PyTorch.
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.

## Exception summary

&lt;class &#39;torch._dynamo.exc.Unsupported&#39;&gt;: Encountered aliasing during higher order op tracing
  Explanation: Higher order ops do not support aliasing. Found in cond
  Hint: Replace `return input` with `return input.clone()` to avoid aliasing.
  Hint: Consider using the debug context to change user code to avoid aliasing.
  Hint: Please open an issue.

  Developer debug context: Input-to-output aliasing detected at nodes l_args_3_0_ and l_args_3_0_ in
     graph():
        %l_args_3_0_ : torch._subclasses.fake_tensor.FakeTensor [num_users=1] = placeholder[target=l_args_3_0_]
        return (l_args_3_0_,)

⬆️
&lt;class &#39;torch._dynamo.exc.UncapturedHigherOrderOpError&#39;&gt;: Cond doesn&#39;t work unless it is captured completely with torch.compile. Scroll up to find out what causes the graph break.

from user code:
   File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_higher_order_ops/cond.py&quot;, line 187, in _cond_op_wrapper
    return cond_op(*args, **kwargs)

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you&#39;re reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=&quot;+dynamo&quot;


(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="controlflowcondnestedmodule">
<span id="ledx-model-case-export-controlflowcondnestedmodule"></span><h2>ControlFlowCondNestedModule<a class="headerlink" href="#controlflowcondnestedmodule" title="Link to this heading">¶</a></h2>
<section id="id40">
<h3>forward<a class="headerlink" href="#id40" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">true_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">false_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="id41">
<h3>custom<a class="headerlink" href="#id41" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T7s2,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions.0&quot; : 1, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;3&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T7s2[-1,2:A0.5],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (int64[batch] x) =&gt; (float[d_output_0_0] output_0) 
   &lt;int64 init7_s_0 =  {0}, float[1] weight =  {42}, float[1] &quot;submodule.weight&quot; =  {100}, bool gt, int64[1] &quot;init7_s_0::RSh1&quot;, float[1] p_submodule_weight, float[?] getitem, int64 sum_1, float[1] p_weight, int64[1] init7_s1_1, int64[1] &quot;sum_1::RSh1&quot;&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 0&gt; (x)
   [gt_Scalar3] gt = Greater (sum_1, init7_s_0)
   [cond] output_0 = If (gt) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [Opset2] &quot;x::C12&quot; = Cast &lt;to: int = 1&gt; (x)
      [sub_Tensor2] &quot;cond#0&quot; = Sub (&quot;x::C12&quot;, weight)
   }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [init2cst2] init7_s_1002 = Constant &lt;value: tensor = int64 init7_s_100 {100}&gt; ()
      [abs2] abs_12 = Abs (x)
      [sum22] sum_122 = ReduceSum &lt;keepdims: int = 0&gt; (abs_12)
      [gt_Scalar322] gt22 = Greater (sum_122, init7_s_1002)
      [cond22] &quot;cond#0&quot; = If (gt22) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
         [Opset32] &quot;x::C132&quot; = Cast &lt;to: int = 1&gt; (x)
         [div_Tensor2] &quot;cond#0&quot; = Div (&quot;x::C132&quot;, &quot;submodule.weight&quot;)
      }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
         [mul_Tensor3] &quot;x::C142&quot; = Cast &lt;to: int = 1&gt; (x)
         [mul_Tensor22] &quot;cond#0&quot; = Mul (&quot;x::C142&quot;, &quot;submodule.weight&quot;)
      }&gt;
   }&gt;
}
</pre></div>
</div>
</section>
<section id="id42">
<h3>dynamo-ir<a class="headerlink" href="#id42" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T7s2,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (int64[batch] x) =&gt; (float[batch] getitem) 
   &lt;float[1] weight =  {42}, float[1] &quot;submodule.weight&quot; =  {100}, int64 val_0 =  {0}, int64 val_0_2 =  {100}, float[1] weight, float[1] &quot;submodule.weight&quot;, int64 val_0, int64 val_0_2, int64 sum_1, bool gt&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (x)
   [node_gt] gt = Greater (sum_1, val_0)
   [node_cond__0] getitem = If (gt) &lt;then_branch: graph = true_graph_0 () =&gt; ( getitem_true_graph_0) 
      &lt;int64[batch] abs_1, int64 sum_1_2, bool gt_2&gt;
{
      [node_abs_1] abs_1 = Abs (x)
      [node_sum_1_2] sum_1_2 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (abs_1)
      [node_gt_2] gt_2 = Greater (sum_1_2, val_0_2)
      [node_cond__0_2] getitem_true_graph_0 = If (gt_2) &lt;then_branch: graph = true_graph_0__true_graph_0 () =&gt; (float[batch] mul_1_true_graph_0__true_graph_0) 
         &lt;float[batch] convert_element_type_default&gt;
{
         [node_convert_element_type_default] convert_element_type_default = Cast &lt;to: int = 1&gt; (x)
         [node_mul_1] mul_1_true_graph_0__true_graph_0 = Mul (convert_element_type_default, &quot;submodule.weight&quot;)
      }, else_branch: graph = true_graph_0__false_graph_0 () =&gt; (float[batch] div_true_graph_0__false_graph_0) 
         &lt;float[batch] convert_element_type_default_2&gt;
{
         [node_convert_element_type_default_2] convert_element_type_default_2 = Cast &lt;to: int = 1&gt; (x)
         [node_div] div_true_graph_0__false_graph_0 = Div (convert_element_type_default_2, &quot;submodule.weight&quot;)
      }&gt;
   }, else_branch: graph = false_graph_0 () =&gt; (float[batch] sub_1_false_graph_0) 
      &lt;float[batch] convert_element_type_default_3&gt;
{
      [node_convert_element_type_default_3] convert_element_type_default_3 = Cast &lt;to: int = 1&gt; (x)
      [node_sub_1] sub_1_false_graph_0 = Sub (convert_element_type_default_3, weight)
   }&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowcondnonzero">
<span id="ledx-model-case-export-controlflowcondnonzero"></span><h2>ControlFlowCondNonZero<a class="headerlink" href="#controlflowcondnonzero" title="Link to this heading">¶</a></h2>
<section id="id43">
<h3>forward<a class="headerlink" href="#id43" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">image_features</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">then_branch</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">image_features</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">condition</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_ids</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">input_ids</span> <span class="o">&gt;</span> <span class="o">-</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e9</span><span class="p">))</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">positions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">positions</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">else_branch</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">image_features</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
        <span class="n">image_features</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">then_branch</span><span class="p">,</span>
        <span class="n">else_branch</span><span class="p">,</span>
        <span class="p">[</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">image_features</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span>
</pre></div>
</div>
</section>
<section id="id44">
<h3>custom<a class="headerlink" href="#id44" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Expect operands to be a tuple of possibly nested dict/list/tuple that only consists of tensor leaves, but got [FakeTensor(..., size=(s72, 12), dtype=torch.int64), FakeTensor(..., size=(s28, s11)), 1025].
</pre></div>
</div>
</section>
<section id="id45">
<h3>dynamo-ir<a class="headerlink" href="#id45" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to export the model with torch.export. [96mThis is step 1/3[0m of exporting the model to ONNX. Next steps:
- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.
- Debug `torch.export.export` and summit a PR to PyTorch.
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.

## Exception summary

&lt;class &#39;RuntimeError&#39;&gt;: Expect operands to be a tuple of possibly nested dict/list/tuple that only consists of tensor leaves, but got [FakeTensor(..., size=(s72, 12), dtype=torch.int64), FakeTensor(..., size=(s28, s11)), 1025].

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="controlflownestcond">
<span id="ledx-model-case-export-controlflownestcond"></span><h2>ControlFlowNestCond<a class="headerlink" href="#controlflownestcond" title="Link to this heading">¶</a></h2>
<section id="id46">
<h3>forward<a class="headerlink" href="#id46" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">true_fn2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">true_fn1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">false_fn1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn1</span><span class="p">,</span> <span class="n">false_fn1</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">false_fn2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">x</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn2</span><span class="p">,</span> <span class="n">false_fn2</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="id47">
<h3>custom<a class="headerlink" href="#id47" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions.0&quot; : 1, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;4&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s5x3[0.16482001543045044,0.910514235496521:A0.4481788118680318],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x) =&gt; (float[batch,3] output_0) 
   &lt;float init1_s_ =  {0}, bool gt, float[1] &quot;init1_s_::RSh1&quot;, float[batch,3] getitem, int64[1] init7_s1_1, float sum_1, float[1] &quot;sum_1::RSh1&quot;&gt;
{
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 0&gt; (x)
   [gt_Scalar3] gt = Greater (sum_1, init1_s_)
   [cond] output_0 = If (gt) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [neg2] &quot;cond#0&quot; = Neg (x)
   }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
      [init2cst2] init1_s_22 = Constant &lt;value: tensor = float init1_s_ {0}&gt; ()
      [sum22] sum_122 = ReduceSum &lt;keepdims: int = 0&gt; (x)
      [lt_Scalar32] lt2 = Less (sum_122, init1_s_22)
      [cond22] &quot;cond#0&quot; = If (lt2) &lt;else_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
         [cos2] &quot;cond#0&quot; = Cos (x)
      }, then_branch: graph = experiment () =&gt; ( &quot;cond#0&quot;) {
         [sin2] &quot;cond#0&quot; = Sin (x)
      }&gt;
   }&gt;
}
</pre></div>
</div>
</section>
<section id="id48">
<h3>dynamo-ir<a class="headerlink" href="#id48" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x) =&gt; (float[batch,3] getitem) 
   &lt;float scalar_tensor_default =  {0}, float scalar_tensor_default, float sum_1, bool gt&gt;
{
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (x)
   [node_gt] gt = Greater (sum_1, scalar_tensor_default)
   [node_cond__0] getitem = If (gt) &lt;then_branch: graph = true_graph_0 () =&gt; ( getitem_true_graph_0) 
      &lt;float sum_1_2, bool lt&gt;
{
      [node_sum_1_2] sum_1_2 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (x)
      [node_lt] lt = Less (sum_1_2, scalar_tensor_default)
      [node_cond__0_2] getitem_true_graph_0 = If (lt) &lt;then_branch: graph = true_graph_0__true_graph_0 () =&gt; (float[batch,3] sin_true_graph_0__true_graph_0) {
         [node_sin] sin_true_graph_0__true_graph_0 = Sin (x)
      }, else_branch: graph = true_graph_0__false_graph_0 () =&gt; (float[batch,3] cos_true_graph_0__false_graph_0) {
         [node_cos] cos_true_graph_0__false_graph_0 = Cos (x)
      }&gt;
   }, else_branch: graph = false_graph_0 () =&gt; (float[batch,3] neg_false_graph_0) {
      [node_neg] neg_false_graph_0 = Neg (x)
   }&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowscan">
<span id="ledx-model-case-export-controlflowscan"></span><h2>ControlFlowScan<a class="headerlink" href="#controlflowscan" title="Link to this heading">¶</a></h2>
<section id="id49">
<h3>forward<a class="headerlink" href="#id49" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">carry</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">higher_order</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
        <span class="n">ControlFlowScan</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="p">[</span><span class="n">init</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">carry</span>
</pre></div>
</div>
</section>
<section id="id50">
<h3>custom<a class="headerlink" href="#id50" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;8&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;2&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x3[1.0,9.0:A5.0],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x) =&gt; (float[3] output_0) 
   &lt;int64[1] init7_s1_3 =  {3}, float[3] select, float[3] zeros_like, float[batch,3] getitem_1, float[3] getitem, float[batch,3] &quot;scan#1&quot;, int64 init7_s_0, float[3] &quot;scan#0&quot;&gt;
{
   [zeros_likeD2] zeros_like = ConstantOfShape &lt;value: tensor = float[1] {0}&gt; (init7_s1_3)
   [scan] output_0, &quot;scan#1&quot; = Scan (zeros_like, x) &lt;body: graph = experiment ( init_0_zeros_like,  scan_0_x) =&gt; ( output_0,  output_1) {
      [add_Tensor2] output_0 = Add (init_0_zeros_like, scan_0_x)
      [&quot;.output22&quot;] output_1 = Identity (output_0)
   }, num_scan_inputs: int = 1, scan_input_directions: ints = [0], scan_output_axes: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
</section>
<section id="id51">
<h3>dynamo-ir<a class="headerlink" href="#id51" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to decompose the FX graph for ONNX compatibility. [96mThis is step 2/3[0m of exporting the model to ONNX. Next steps:
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.
- Create an error report with `torch.onnx.export(..., report=True)`, and save the ExportedProgram as a pt2 file. Create an issue in the PyTorch GitHub repository against the [96m*onnx*[0m component. Attach the error report and the pt2 model.

## Exception summary

&lt;class &#39;RuntimeError&#39;&gt;: scan might be aliasing the input or the output!

While executing %scan : [num_users=2] = call_function[target=torch.ops.higher_order.scan](args = (%scan_combine_graph_0, [%zeros_like], [%x], ()), kwargs = {})
GraphModule: class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: &quot;f32[s77, 3][3, 1]&quot;; 

        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:386 in forward, code: init = torch.zeros_like(x[0])
        select: &quot;f32[3][1]&quot; = torch.ops.aten.select.int(x, 0, 0)
        zeros_like: &quot;f32[3][1]&quot; = torch.ops.aten.zeros_like.default(select, pin_memory = False);  select = None
    
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:387 in forward, code: carry, out = torch.ops.higher_order.scan(
        scan_combine_graph_0 = self.scan_combine_graph_0
        scan = torch.ops.higher_order.scan(scan_combine_graph_0, [zeros_like], [x], ());  scan_combine_graph_0 = zeros_like = x = None
        getitem: &quot;f32[3][1]&quot; = scan[0]
        getitem_1: &quot;f32[s77, 3][3, 1]&quot; = scan[1];  scan = getitem_1 = None
        return pytree.tree_unflatten((getitem,), self._out_spec)
    
    class scan_combine_graph_0(torch.nn.Module):
        def forward(self, carry_1: &quot;f32[3][1]&quot;, y_1: &quot;f32[3][1]&quot;):
             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:387 in forward, code: carry, out = torch.ops.higher_order.scan(
            add: &quot;f32[3][1]&quot; = torch.ops.aten.add.Tensor(carry_1, y_1);  carry_1 = y_1 = None
            return [add, add]
        

Original traceback:
File &quot;~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py&quot;, line 387, in forward
    carry, out = torch.ops.higher_order.scan(

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="controlflowscan2carried">
<span id="ledx-model-case-export-controlflowscan2carried"></span><h2>ControlFlowScan2Carried<a class="headerlink" href="#controlflowscan2carried" title="Link to this heading">¶</a></h2>
<section id="id52">
<h3>forward<a class="headerlink" href="#id52" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">init1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">init2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">carry1</span><span class="p">,</span> <span class="n">carry2</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">higher_order</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
        <span class="n">ControlFlowScan2Carried</span><span class="o">.</span><span class="n">add</span><span class="p">,</span>
        <span class="p">[</span><span class="n">init1</span><span class="p">,</span> <span class="n">init2</span><span class="p">],</span>
        <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">],</span>
        <span class="c1"># dim=0,  # 01/31/2025, not supported anymore</span>
        <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">carry1</span><span class="p">,</span> <span class="n">carry2</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span>
</pre></div>
</div>
</section>
<section id="id53">
<h3>custom<a class="headerlink" href="#id53" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;2&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;12&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;4&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True, True, True, True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[-1.0,9.0:A3.5],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[4] output_0, float[4] output_1, float[batch,4] output_2, float[batch,4] output_3) 
   &lt;int64[1] init7_s1_4 =  {4}, float[1] &quot;init1_s_::RSh1&quot; =  {2}, float init1_s_, int64[1] init7_s1_1, float[batch,4] &quot;scan#3&quot;, float[batch,4] mul, float[4] zeros_like, float[4] ones_like, float[4] getitem_1, float[4] &quot;scan#0&quot;, float[batch,4] &quot;scan#2&quot;, float[4] getitem, float[batch,4] getitem_2, float[4] select_1, float[batch,4] getitem_3, float[4] select, float[batch,4] _onx_mul_x, float[4] &quot;scan#1&quot;, int64 init7_s_0&gt;
{
   [zeros_likeD2] zeros_like = ConstantOfShape &lt;value: tensor = float[1] {0}&gt; (init7_s1_4)
   [ones_likeD2] ones_like = ConstantOfShape &lt;value: tensor = float[1] {1}&gt; (init7_s1_4)
   [mul_Tensor2] _onx_mul_x = Mul (x, &quot;init1_s_::RSh1&quot;)
   [scan] output_0, output_1, output_2, output_3 = Scan (zeros_like, ones_like, x, _onx_mul_x) &lt;body: graph = experiment ( init_0_zeros_like,  init_1_ones_like,  scan_0_x,  scan_1_mul) =&gt; ( output_0,  output_1,  output_2,  output_3) {
      [add_Tensor2] output_0 = Add (init_0_zeros_like, scan_0_x)
      [mul_Tensor42] output_1 = Mul (init_1_ones_like, scan_1_mul)
      [&quot;.output322&quot;] output_2 = Identity (output_0)
      [&quot;.output422&quot;] output_3 = Identity (output_1)
   }, num_scan_inputs: int = 2, scan_input_directions: ints = [0, 0], scan_output_axes: ints = [0, 0], scan_output_directions: ints = [0, 0]&gt;
}
</pre></div>
</div>
</section>
<section id="id54">
<h3>dynamo-ir<a class="headerlink" href="#id54" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to decompose the FX graph for ONNX compatibility. [96mThis is step 2/3[0m of exporting the model to ONNX. Next steps:
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.
- Create an error report with `torch.onnx.export(..., report=True)`, and save the ExportedProgram as a pt2 file. Create an issue in the PyTorch GitHub repository against the [96m*onnx*[0m component. Attach the error report and the pt2 model.

## Exception summary

&lt;class &#39;RuntimeError&#39;&gt;: scan might be aliasing the input or the output!

While executing %scan : [num_users=4] = call_function[target=torch.ops.higher_order.scan](args = (%scan_combine_graph_0, [%zeros_like, %ones_like], [%x, %mul], ()), kwargs = {})
GraphModule: class GraphModule(torch.nn.Module):
    def forward(self, x):
        x: &quot;f32[s77, 4][4, 1]&quot;; 

        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:404 in forward, code: init1 = torch.zeros_like(x[0])
        select: &quot;f32[4][1]&quot; = torch.ops.aten.select.int(x, 0, 0)
        zeros_like: &quot;f32[4][1]&quot; = torch.ops.aten.zeros_like.default(select, pin_memory = False);  select = None
    
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:405 in forward, code: init2 = torch.ones_like(x[0])
        select_1: &quot;f32[4][1]&quot; = torch.ops.aten.select.int(x, 0, 0)
        ones_like: &quot;f32[4][1]&quot; = torch.ops.aten.ones_like.default(select_1, pin_memory = False);  select_1 = None
    
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:409 in forward, code: [x, x * 2],
        mul: &quot;f32[s77, 4][4, 1]&quot; = torch.ops.aten.mul.Tensor(x, 2)
    
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:406 in forward, code: carry1, carry2, out1, out2 = torch.ops.higher_order.scan(
        scan_combine_graph_0 = self.scan_combine_graph_0
        scan = torch.ops.higher_order.scan(scan_combine_graph_0, [zeros_like, ones_like], [x, mul], ());  scan_combine_graph_0 = zeros_like = ones_like = x = mul = None
        getitem: &quot;f32[4][1]&quot; = scan[0]
        getitem_1: &quot;f32[4][1]&quot; = scan[1]
        getitem_2: &quot;f32[s77, 4][4, 1]&quot; = scan[2]
        getitem_3: &quot;f32[s77, 4][4, 1]&quot; = scan[3];  scan = None
        return pytree.tree_unflatten((getitem, getitem_1, getitem_2, getitem_3), self._out_spec)
    
    class scan_combine_graph_0(torch.nn.Module):
        def forward(self, carry1_1: &quot;f32[4][1]&quot;, carry2_1: &quot;f32[4][1]&quot;, y1_1: &quot;f32[4][1]&quot;, y2_1: &quot;f32[4][1]&quot;):
             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:406 in forward, code: carry1, carry2, out1, out2 = torch.ops.higher_order.scan(
            add: &quot;f32[4][1]&quot; = torch.ops.aten.add.Tensor(carry1_1, y1_1);  carry1_1 = y1_1 = None
            mul: &quot;f32[4][1]&quot; = torch.ops.aten.mul.Tensor(carry2_1, y2_1);  carry2_1 = y2_1 = None
            return [add, mul, add, mul]
        

Original traceback:
File &quot;~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py&quot;, line 406, in forward
    carry1, carry2, out1, out2 = torch.ops.higher_order.scan(

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="controlflowscancdist">
<span id="ledx-model-case-export-controlflowscancdist"></span><h2>ControlFlowScanCDist<a class="headerlink" href="#controlflowscancdist" title="Link to this heading">¶</a></h2>
<section id="id55">
<h3>forward<a class="headerlink" href="#id55" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">carry</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">higher_order</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
        <span class="n">ControlFlowScanCDist</span><span class="o">.</span><span class="n">dist</span><span class="p">,</span>
        <span class="p">[</span><span class="n">x</span><span class="p">],</span>
        <span class="p">[</span><span class="n">x</span><span class="p">],</span>
        <span class="c1"># dim=0,  # 01/31/2025, not supported anymore</span>
        <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</section>
<section id="id56">
<h3>custom<a class="headerlink" href="#id56" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;0&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;0&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;1&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[-1.0,9.0:A3.5],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[batch,batch] output_0) 
   &lt;float[batch,batch] getitem_1, float[batch,4] getitem, float[batch,batch] &quot;scan#1&quot;, float[batch,4] &quot;scan#0&quot;&gt;
{
   [scan] &quot;scan#0&quot;, output_0 = Scan (x, x) &lt;body: graph = experiment ( init_0_x,  scan_0_x) =&gt; ( output_0,  output_1) {
      [init2cst4] &quot;init7_s2_1_-12&quot; = Constant &lt;value: tensor = int64[2] &quot;init7_s2_1_-1&quot; {1,-1}&gt; ()
      [init2cst22] init7_s1_12 = Constant &lt;value: tensor = int64[1] init7_s1_1 {1}&gt; ()
      [init2cst32] init1_s_2 = Constant &lt;value: tensor = float init1_s_ {0.5}&gt; ()
      [reshape2] reshape2 = Reshape (scan_0_x, &quot;init7_s2_1_-12&quot;)
      [sub_Tensor2] sub2 = Sub (init_0_x, reshape2)
      [mul_Tensor2] mul2 = Mul (sub2, sub2)
      [sum2] sum_12 = ReduceSum &lt;keepdims: int = 0&gt; (mul2, init7_s1_12)
      [pow_Tensor_Scalar2] output_1 = Pow (sum_12, init1_s_2)
      [&quot;.output22&quot;] output_0 = Identity (init_0_x)
   }, num_scan_inputs: int = 1, scan_input_directions: ints = [0], scan_output_axes: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
</section>
<section id="id57">
<h3>dynamo-ir<a class="headerlink" href="#id57" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (float[batch,batch] getitem_1) 
   &lt;int64[2] val_1 =  {1,-1}, int64[1] val_5 =  {1}, float val_6 =  {0.5}, int64[2] val_1, int64[1] val_5, float val_6, float[batch,4] scan__0&gt;
{
   [node_scan__1] scan__0, getitem_1 = Scan (x, x) &lt;body: graph = scan_combine_graph_0 (float[s77,4] x_scan_combine_graph_0__subgraph_in, float[4] x_scan_combine_graph_0__subgraph_in) =&gt; (float[s77,4] clone_scan_combine_graph_0, float[s77] pow_1_scan_combine_graph_0) 
      &lt;float[1,4] view, float[s77,4] sub_1, float[s77,4] mul_4, float[s77] sum_1&gt;
{
      [node_view] view = Reshape &lt;allowzero: int = 1&gt; (x_scan_combine_graph_0__subgraph_in, val_1)
      [node_sub_1] sub_1 = Sub (x_scan_combine_graph_0__subgraph_in, view)
      [node_mul_4] mul_4 = Mul (sub_1, sub_1)
      [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (mul_4, val_5)
      [node_pow_1] pow_1_scan_combine_graph_0 = Pow (sum_1, val_6)
      [node_clone] clone_scan_combine_graph_0 = Identity (x_scan_combine_graph_0__subgraph_in)
   }, num_scan_inputs: int = 1, scan_input_directions: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ONNXRuntimeError] : 1 : FAIL : Error: Duplicate definition-site for (x_scan_combine_graph_0__subgraph_in).
</pre></div>
</div>
</section>
</section>
<section id="controlflowscancdist2">
<span id="ledx-model-case-export-controlflowscancdist2"></span><h2>ControlFlowScanCDist2<a class="headerlink" href="#controlflowscancdist2" title="Link to this heading">¶</a></h2>
<section id="id58">
<h3>forward<a class="headerlink" href="#id58" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">higher_order</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
        <span class="n">ControlFlowScanCDist2</span><span class="o">.</span><span class="n">dist</span><span class="p">,</span>
        <span class="p">[</span><span class="n">z</span><span class="p">],</span>
        <span class="p">[</span><span class="n">x</span><span class="p">],</span>
        <span class="c1"># dim=0,  # 01/31/2025, not supported anymore</span>
        <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id59">
<h3>custom<a class="headerlink" href="#id59" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;4&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;2&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[-1.0,9.0:A3.5],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[batch,batch] output_0) 
   &lt;float[1] c_lifted_tensor_0 =  {0}, float[1] detach_, float[1] lift_fresh_copy, float[batch,4] hidden_input_scan_0_clone, float[batch,4] clone, float[batch,batch] getitem_1, float[1] getitem, float[batch,batch] &quot;scan#1&quot;, float[1] &quot;scan#0&quot;&gt;
{
   [_DONOTREMOVE_Scan_hidden_input_0] hidden_input_scan_0_clone = Identity (x)
   [scan] &quot;scan#0&quot;, output_0 = Scan (c_lifted_tensor_0, x) &lt;body: graph = experiment ( init_0_detach_,  scan_0_x) =&gt; ( output_0,  output_1) {
      [init2cst3] &quot;init7_s2_1_-12&quot; = Constant &lt;value: tensor = int64[2] &quot;init7_s2_1_-1&quot; {1,-1}&gt; ()
      [init2cst22] init7_s1_12 = Constant &lt;value: tensor = int64[1] init7_s1_1 {1}&gt; ()
      [reshape2] reshape2 = Reshape (scan_0_x, &quot;init7_s2_1_-12&quot;)
      [sub_Tensor2] sub2 = Sub (hidden_input_scan_0_clone, reshape2)
      [mul_Tensor2] mul2 = Mul (sub2, sub2)
      [sum2] sum_12 = ReduceSum &lt;keepdims: int = 0&gt; (mul2, init7_s1_12)
      [sqrt2] output_1 = Sqrt (sum_12)
      [&quot;.output22&quot;] output_0 = Identity (init_0_detach_)
   }, num_scan_inputs: int = 1, scan_input_directions: ints = [0], scan_output_axes: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
</section>
<section id="id60">
<h3>dynamo-ir<a class="headerlink" href="#id60" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s3x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (float[batch,batch] getitem_1) 
   &lt;float[1] clone =  {0}, int64[2] val_1 =  {1,-1}, int64[1] val_5 =  {1}, float[1] clone, int64[2] val_1, int64[1] val_5, float[1] scan__0&gt;
{
   [node_scan__1] scan__0, getitem_1 = Scan (clone, x) &lt;body: graph = scan_combine_graph_0 (float[1] clone_scan_combine_graph_0__subgraph_in, float[4] x_scan_combine_graph_0__subgraph_in) =&gt; (float[1] clone_scan_combine_graph_0, float[batch] sqrt_scan_combine_graph_0) 
      &lt;float[1,4] view, float[batch,4] sub_1, float[batch,4] mul_4, float[batch] sum_1&gt;
{
      [node_view] view = Reshape &lt;allowzero: int = 1&gt; (x_scan_combine_graph_0__subgraph_in, val_1)
      [node_sub_1] sub_1 = Sub (x, view)
      [node_mul_4] mul_4 = Mul (sub_1, sub_1)
      [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (mul_4, val_5)
      [node_sqrt] sqrt_scan_combine_graph_0 = Sqrt (sum_1)
      [node_clone_2] clone_scan_combine_graph_0 = Identity (clone_scan_combine_graph_0__subgraph_in)
   }, num_scan_inputs: int = 1, scan_input_directions: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowscancdistxy">
<span id="ledx-model-case-export-controlflowscancdistxy"></span><h2>ControlFlowScanCDistXY<a class="headerlink" href="#controlflowscancdistxy" title="Link to this heading">¶</a></h2>
<section id="id61">
<h3>forward<a class="headerlink" href="#id61" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">carry</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">higher_order</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
        <span class="n">ControlFlowScanCDistXY</span><span class="o">.</span><span class="n">dist</span><span class="p">,</span>
        <span class="p">[</span><span class="n">y</span><span class="p">],</span>
        <span class="p">[</span><span class="n">x</span><span class="p">],</span>
        <span class="c1"># dim=0,  # 01/31/2025, not supported anymore</span>
        <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</section>
<section id="id62">
<h3>custom<a class="headerlink" href="#id62" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,T1s5x4),(T1s13x14,T1s15x14)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(x_rows),1:Dim(dim)},y:{0:Dim(y_rows),1:Dim(dim)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;0&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;0&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;1&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[-1.6008905172348022,1.6169092655181885:A-0.24537086735169092],T1s5x4[-1.518800973892212,1.7033108472824097:A-0.07289859242737293])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;x_rows&#39;, min=0), 1: Dim(&#39;dim&#39;, min=0)},
 &#39;y&#39;: {0: Dim(&#39;y_rows&#39;, min=0), 1: Dim(&#39;dim&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;dim&#39;: {&#39;s94&#39;, &#39;s27&#39;},
 &#39;s17&#39;: {&#39;y_rows&#39;},
 &#39;s27&#39;: {&#39;dim&#39;},
 &#39;s77&#39;: {&#39;x_rows&#39;},
 &#39;s94&#39;: {&#39;dim&#39;},
 &#39;x_rows&#39;: {&#39;s77&#39;},
 &#39;y_rows&#39;: {&#39;s17&#39;}}&quot;]
&gt;
experiment (float[x_rows,dim] x, float[y_rows,dim] y) =&gt; (float[x_rows,y_rows] output_0) 
   &lt;float[x_rows,y_rows] getitem_1, float[y_rows,dim] getitem, float[x_rows,y_rows] &quot;scan#1&quot;, float[y_rows,dim] &quot;scan#0&quot;&gt;
{
   [scan] &quot;scan#0&quot;, output_0 = Scan (y, x) &lt;body: graph = experiment ( init_0_y,  scan_0_x) =&gt; ( output_0,  output_1) {
      [init2cst3] &quot;init7_s2_1_-12&quot; = Constant &lt;value: tensor = int64[2] &quot;init7_s2_1_-1&quot; {1,-1}&gt; ()
      [init2cst22] init7_s1_12 = Constant &lt;value: tensor = int64[1] init7_s1_1 {1}&gt; ()
      [reshape2] reshape2 = Reshape (scan_0_x, &quot;init7_s2_1_-12&quot;)
      [sub_Tensor2] sub2 = Sub (init_0_y, reshape2)
      [mul_Tensor2] mul2 = Mul (sub2, sub2)
      [sum2] sum_12 = ReduceSum &lt;keepdims: int = 0&gt; (mul2, init7_s1_12)
      [sqrt2] output_1 = Sqrt (sum_12)
      [&quot;.output22&quot;] output_0 = Identity (init_0_y)
   }, num_scan_inputs: int = 1, scan_input_directions: ints = [0], scan_output_axes: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
</section>
<section id="id63">
<h3>dynamo-ir<a class="headerlink" href="#id63" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,T1s5x4),(T1s13x14,T1s15x14)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(x_rows),1:Dim(dim)},y:{0:Dim(y_rows),1:Dim(dim)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[x_rows,dim] x, float[y_rows,dim] y) =&gt; (float[x_rows,y_rows] getitem_1) 
   &lt;int64[2] val_1_2 =  {1,-1}, int64[1] val_5 =  {1}, int64[2] val_1_2, int64[1] val_5, float[y_rows,dim] scan__0&gt;
{
   [node_scan__1] scan__0, getitem_1 = Scan (y, x) &lt;body: graph = scan_combine_graph_0 (float[s17,s27] y_scan_combine_graph_0__subgraph_in, float[s27] x_scan_combine_graph_0__subgraph_in) =&gt; (float[s17,s27] clone_scan_combine_graph_0, float[s17] sqrt_scan_combine_graph_0) 
      &lt;float[1,?] view, float[s17,?] sub_4, float[s17,?] mul_7, float[s17] sum_1&gt;
{
      [node_view] view = Reshape &lt;allowzero: int = 1&gt; (x_scan_combine_graph_0__subgraph_in, val_1_2)
      [node_sub_4] sub_4 = Sub (y_scan_combine_graph_0__subgraph_in, view)
      [node_mul_7] mul_7 = Mul (sub_4, sub_4)
      [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 0&gt; (mul_7, val_5)
      [node_sqrt] sqrt_scan_combine_graph_0 = Sqrt (sum_1)
      [node_clone] clone_scan_combine_graph_0 = Identity (y_scan_combine_graph_0__subgraph_in)
   }, num_scan_inputs: int = 1, scan_input_directions: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
</section>
</section>
<section id="controlflowscandecomposition-151564">
<span id="ledx-model-case-export-controlflowscandecomposition-151564"></span><h2>ControlFlowScanDecomposition_151564<a class="headerlink" href="#controlflowscandecomposition-151564" title="Link to this heading">¶</a></h2>
<section id="id64">
<h3>forward<a class="headerlink" href="#id64" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">position</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">select_when_exporting</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dummy_loop</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dummy_loop_with_scan</span><span class="p">)(</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">position</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id65">
<h3>custom<a class="headerlink" href="#id65" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s5x6,T7s5)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(images:{0:DYNAMIC,1:DYNAMIC},position:{0:DYNAMIC})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18, &quot;local_functions&quot; : 1],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;0&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;0&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;1&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s5x6[-1.6081827878952026,2.115975856781006:A0.06955699867103249],T7s5[1,5:A3.0])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;images&#39;: {0: Dim(&#39;DYN0&#39;, min=0), 1: Dim(&#39;DYN1&#39;, min=0)},
 &#39;position&#39;: {0: Dim(&#39;DYN2&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;DYN0&#39;: {&#39;s34&#39;},
 &#39;DYN1&#39;: {&#39;s90&#39;},
 &#39;DYN2&#39;: {&#39;s71&#39;},
 &#39;s34&#39;: {&#39;DYN0&#39;},
 &#39;s71&#39;: {&#39;DYN2&#39;},
 &#39;s90&#39;: {&#39;DYN1&#39;}}&quot;]
&gt;
experiment (float[batch,channel] images, int64[batch_1] position) =&gt; (float[batch,channel] output_0) 
   &lt;float[batch,channel] getitem, float[batch,channel] &quot;scan#0&quot;&gt;
{
   [scan] output_0 = Scan (images, position) &lt;body: graph = experiment ( scan_0_images,  scan_1_position) =&gt; ( output_0) {
      [init2cst2] init7_s1_02 = Constant &lt;value: tensor = int64[1] init7_s1_0 {0}&gt; ()
      [get_dynamic_dimension_a_item2] &quot;item::UnSq03&quot; = Unsqueeze (scan_1_position, init7_s1_02)
      [sym_size_int2] &quot;padded_1::Shape:12&quot; = Shape &lt;end: int = 1, start: int = 0&gt; (scan_0_images)
      [zerosA22] zeros2 = ConstantOfShape &lt;value: tensor = float[1] {0}&gt; (&quot;padded_1::Shape:12&quot;)
      [slide_Tensor2] slice_12 = Slice (scan_0_images, init7_s1_02, &quot;item::UnSq03&quot;, init7_s1_02)
      [setitem_rk022] &quot;zeros::Shape:2&quot; = Shape (zeros2)
      [setitem_rk032] _onx_slice_zeros2 = Slice (zeros2, &quot;item::UnSq03&quot;, &quot;zeros::Shape:2&quot;, init7_s1_02)
      [setitem_rk042] output_0 = Concat &lt;axis: int = 0&gt; (slice_12, _onx_slice_zeros2)
   }, num_scan_inputs: int = 2, scan_input_directions: ints = [0, 0], scan_output_axes: ints = [0], scan_output_directions: ints = [0]&gt;
}
</pre></div>
</div>
</section>
<section id="id66">
<h3>dynamo-ir<a class="headerlink" href="#id66" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to decompose the FX graph for ONNX compatibility. [96mThis is step 2/3[0m of exporting the model to ONNX. Next steps:
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.
- Create an error report with `torch.onnx.export(..., report=True)`, and save the ExportedProgram as a pt2 file. Create an issue in the PyTorch GitHub repository against the [96m*onnx*[0m component. Attach the error report and the pt2 model.

## Exception summary

&lt;class &#39;torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode&#39;&gt;: Could not guard on data-dependent expression u1 &lt; 0 (unhinted: u1 &lt; 0).  (Size-like symbols: none)

Caused by: (_decomp/decompositions.py:745 in slice_forward)
For more information, run with TORCH_LOGS=&quot;dynamic&quot;
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u1&quot;
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

While executing %scan : [num_users=1] = call_function[target=torch.ops.higher_order.scan](args = (%scan_combine_graph_0, [], [%images, %position], ()), kwargs = {})
GraphModule: class GraphModule(torch.nn.Module):
    def forward(self, images, position):
        images: &quot;f32[s34, s90][s90, 1]&quot;; position: &quot;i64[s71][1]&quot;; 

        images, position, = fx_pytree.tree_flatten_spec(([images, position], {}), self._in_spec)
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:568 in forward, code: return self.select_when_exporting(self.dummy_loop, self.dummy_loop_with_scan)(
        scan_combine_graph_0 = self.scan_combine_graph_0
        scan = torch.ops.higher_order.scan(scan_combine_graph_0, [], [images, position], ());  scan_combine_graph_0 = images = position = None
        getitem: &quot;f32[s34, s90][s90, 1]&quot; = scan[0];  scan = None
        return pytree.tree_unflatten((getitem,), self._out_spec)
    
    class scan_combine_graph_0(torch.nn.Module):
        def forward(self, padded_1: &quot;f32[s90][1]&quot;, p_1: &quot;i64[][]&quot;):
             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:568 in forward, code: return self.select_when_exporting(self.dummy_loop, self.dummy_loop_with_scan)(
            sym_size_int: &quot;Sym(s90)&quot; = torch.ops.aten.sym_size.int(padded_1, 0)
            zeros: &quot;f32[s90][1]&quot; = torch.ops.aten.zeros.default([sym_size_int], device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int = None
            item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(p_1);  p_1 = None
            slice_1: &quot;f32[u0][1]&quot; = torch.ops.aten.slice.Tensor(padded_1, 0, 0, item);  padded_1 = None
            slice_2: &quot;f32[u0][1]&quot; = torch.ops.aten.slice.Tensor(zeros, 0, 0, item);  item = None
            copy_: &quot;f32[u0][1]&quot; = torch.ops.aten.copy_.default(slice_2, slice_1);  slice_2 = slice_1 = copy_ = None
            return (zeros,)
        

Original traceback:
File &quot;~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py&quot;, line 568, in forward
    return self.select_when_exporting(self.dummy_loop, self.dummy_loop_with_scan)(

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="controlflowscaninplace-153705">
<span id="ledx-model-case-export-controlflowscaninplace-153705"></span><h2>ControlFlowScanInplace_153705<a class="headerlink" href="#controlflowscaninplace-153705" title="Link to this heading">¶</a></h2>
<section id="id67">
<h3>forward<a class="headerlink" href="#id67" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loop_body_1</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">iv</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">iv</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">z</span><span class="p">,</span> <span class="n">iv</span><span class="p">]</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">higher_order</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
        <span class="n">loop_body_1</span><span class="p">,</span> <span class="p">[</span><span class="n">z</span><span class="p">],</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)],</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id68">
<h3>custom<a class="headerlink" href="#id68" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got SymInt)
</pre></div>
</div>
</section>
<section id="id69">
<h3>dynamo-ir<a class="headerlink" href="#id69" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to decompose the FX graph for ONNX compatibility. [96mThis is step 2/3[0m of exporting the model to ONNX. Next steps:
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.
- Create an error report with `torch.onnx.export(..., report=True)`, and save the ExportedProgram as a pt2 file. Create an issue in the PyTorch GitHub repository against the [96m*onnx*[0m component. Attach the error report and the pt2 model.

## Exception summary

&lt;class &#39;RuntimeError&#39;&gt;: scan might be aliasing the input or the output!

While executing %scan : [num_users=2] = call_function[target=torch.ops.higher_order.scan](args = (%scan_combine_graph_0, [%empty], [%arange], (%x, %y)), kwargs = {})
GraphModule: class GraphModule(torch.nn.Module):
    def forward(self, x, y):
        x: &quot;f32[s77, s27][s27, 1]&quot;; y: &quot;f32[s17, s27][s27, 1]&quot;; 

        x, y, = fx_pytree.tree_flatten_spec(([x, y], {}), self._in_spec)
         # 
        sym_size_int_2: &quot;Sym(s77)&quot; = torch.ops.aten.sym_size.int(x, 0)
        sym_size_int_3: &quot;Sym(s27)&quot; = torch.ops.aten.sym_size.int(x, 1)
        sym_size_int_4: &quot;Sym(s17)&quot; = torch.ops.aten.sym_size.int(y, 0)
        sym_size_int_5: &quot;Sym(s27)&quot; = torch.ops.aten.sym_size.int(y, 1)
        eq: &quot;Sym(True)&quot; = sym_size_int_3 == sym_size_int_5;  sym_size_int_3 = sym_size_int_5 = None
        _assert_scalar_default = torch.ops.aten._assert_scalar.default(eq, &quot;Runtime assertion failed for expression Eq(s27, s94) on node &#39;eq&#39;&quot;);  eq = _assert_scalar_default = None
    
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:517 in forward, code: z = torch.empty((x.shape[0], y.shape[0]))
        empty: &quot;f32[s77, s17][s17, 1]&quot; = torch.ops.aten.empty.memory_format([sym_size_int_2, sym_size_int_4], device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:519 in forward, code: loop_body_1, [z], [torch.arange(x.shape[0], dtype=torch.int64)], [x, y]
        arange: &quot;i64[s77][1]&quot; = torch.ops.aten.arange.default(sym_size_int_2, dtype = torch.int64, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_2 = None
    
         # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:518 in forward, code: r = torch.ops.higher_order.scan(
        scan_combine_graph_0 = self.scan_combine_graph_0
        scan = torch.ops.higher_order.scan(scan_combine_graph_0, [empty], [arange], (x, y));  scan_combine_graph_0 = empty = arange = x = y = None
        getitem: &quot;f32[s77, s17][s17, 1]&quot; = scan[0]
        getitem_1: &quot;i64[s77][1]&quot; = scan[1];  scan = getitem_1 = None
        return pytree.tree_unflatten((getitem,), self._out_spec)
    
    class scan_combine_graph_0(torch.nn.Module):
        def forward(self, z_1: &quot;f32[s77, s17][s17, 1]&quot;, iv_1: &quot;i64[][]&quot;, x_1: &quot;f32[s77, s27][s27, 1]&quot;, y_1: &quot;f32[s17, s27][s27, 1]&quot;):
             # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py:518 in forward, code: r = torch.ops.higher_order.scan(
            clone: &quot;f32[s77, s17][s17, 1]&quot; = torch.ops.aten.clone.default(z_1);  z_1 = None
            select: &quot;f32[s27][1]&quot; = torch.ops.aten.select.int(x_1, 0, 0);  x_1 = None
            slice_1: &quot;f32[s27][1]&quot; = torch.ops.aten.slice.Tensor(select, 0, 0, 9223372036854775807);  select = None
            sub: &quot;f32[s17, s27][s27, 1]&quot; = torch.ops.aten.sub.Tensor(slice_1, y_1);  slice_1 = y_1 = None
            pow_1: &quot;f32[s17, s27][s27, 1]&quot; = torch.ops.aten.pow.Tensor_Scalar(sub, 2);  sub = None
            sum_1: &quot;f32[s17][1]&quot; = torch.ops.aten.sum.dim_IntList(pow_1, [-1]);  pow_1 = None
            select_1: &quot;f32[s17][1]&quot; = torch.ops.aten.select.int(clone, 0, 0)
            slice_2: &quot;f32[s17][1]&quot; = torch.ops.aten.slice.Tensor(select_1, 0, 0, 9223372036854775807);  select_1 = None
            copy_: &quot;f32[s17][1]&quot; = torch.ops.aten.copy_.default(slice_2, sum_1);  slice_2 = sum_1 = copy_ = None
            return [clone, iv_1]
        

Original traceback:
File &quot;~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py&quot;, line 518, in forward
    r = torch.ops.higher_order.scan(

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="createfromshape">
<span id="ledx-model-case-export-createfromshape"></span><h2>CreateFromShape<a class="headerlink" href="#createfromshape" title="Link to this heading">¶</a></h2>
<section id="id70">
<h3>forward<a class="headerlink" href="#id70" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="id71">
<h3>custom<a class="headerlink" href="#id71" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x4,),(T1s5x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(dx),1:Dim(dy)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;2&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;7&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x4[0.11289036273956299,0.8916060328483582:A0.5404852852225304],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;dx&#39;, min=0), 1: Dim(&#39;dy&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;dx&#39;: {&#39;s77&#39;}, &#39;dy&#39;: {&#39;s27&#39;}, &#39;s27&#39;: {&#39;dy&#39;}, &#39;s77&#39;: {&#39;dx&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  _onx_add_sym_size_int_3: dy+1,
  _onx_concat_sym_size_int_2::UnSq0: (&#39;dx&#39;, &#39;add&#39;),
  add::UnSq0: (&#39;add&#39;,),
  sym_size_int_2: dx,
  sym_size_int_2::UnSq0: (&#39;dx&#39;,),
  sym_size_int_3: dy,
  x::Shape1:2: (&#39;dy&#39;,),
  x::Shape:1: (&#39;dx&#39;,),
}&quot;]
&gt;
experiment (float[dx,dy] x) =&gt; (float[dx,dy+1] output_0) 
   &lt;int64 init7_s_1 =  {1}, int64[1] init7_s1_0 =  {0}, int64 sym_size_int_3, int64[1] &quot;sym_size_int_3::RSh1&quot;, int64[1] &quot;add::UnSq0&quot;, float[dx,dy+1] ones, int64 add, int64[1] &quot;x::Shape:1&quot;, int64[1] &quot;init7_s_1::RSh1&quot;, int64[1] init7_s1_1, int64[1] &quot;sym_size_int_2::UnSq0&quot;, int64 sym_size_int_2, int64[1] &quot;x::Shape1:2&quot;, int64 _onx_add_sym_size_int_3&gt;
{
   [sym_size_int] &quot;x::Shape:1&quot; = Shape &lt;end: int = 1, start: int = 0&gt; (x)
   [sym_size_int3] &quot;x::Shape1:2&quot; = Shape &lt;end: int = 2, start: int = 1&gt; (x)
   [sym_size_int4] sym_size_int_3 = Squeeze (&quot;x::Shape1:2&quot;)
   [add3] _onx_add_sym_size_int_3 = Add (sym_size_int_3, init7_s_1)
   [_mkshape1_add] &quot;add::UnSq0&quot; = Unsqueeze (_onx_add_sym_size_int_3, init7_s1_0)
   [_mkshape_add] &quot;_onx_concat_sym_size_int_2::UnSq0&quot; = Concat &lt;axis: int = 0&gt; (&quot;x::Shape:1&quot;, &quot;add::UnSq0&quot;)
   [ones] output_0 = ConstantOfShape &lt;value: tensor = float[1] {1}&gt; (&quot;_onx_concat_sym_size_int_2::UnSq0&quot;)
}
</pre></div>
</div>
</section>
<section id="id72">
<h3>dynamo-ir<a class="headerlink" href="#id72" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x4,),(T1s5x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(dx),1:Dim(dy)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[dx,dy] x) =&gt; (float[dx,dy + 1] ones) 
   &lt;int64 val_2 =  {1}, int64[1] val_5 =  {-1}, float val_10 =  {1}, int64 val_2, int64[1] val_5, float val_10, int64[1] val_0, int64[1] val_1, int64 sym_size_int_3, int64 add, int64[1] val_6, int64[2] val_7&gt;
{
   [node_Shape_0] val_0 = Shape &lt;end: int = 1, start: int = 0&gt; (x)
   [node_Shape_1] val_1 = Shape &lt;end: int = 2, start: int = 1&gt; (x)
   [node_sym_size_int_3] sym_size_int_3 = Squeeze (val_1)
   [node_add] add = Add (sym_size_int_3, val_2)
   [node_Reshape_6] val_6 = Reshape &lt;allowzero: int = 0&gt; (add, val_5)
   [node_Concat_7] val_7 = Concat &lt;axis: int = 0&gt; (val_0, val_6)
   [node_ones] ones = Expand (val_10, val_7)
}
</pre></div>
</div>
</section>
</section>
<section id="createfromshapethroughfunction">
<span id="ledx-model-case-export-createfromshapethroughfunction"></span><h2>CreateFromShapeThroughFunction<a class="headerlink" href="#createfromshapethroughfunction" title="Link to this heading">¶</a></h2>
<section id="id73">
<h3>forward<a class="headerlink" href="#id73" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">dy1</span> <span class="o">=</span> <span class="n">CreateFromShapeThroughFunction</span><span class="o">.</span><span class="n">add_one</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dy1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</section>
<section id="id74">
<h3>custom<a class="headerlink" href="#id74" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x4,),(T1s5x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(dx),1:Dim(dy)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;2&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;7&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x4[0.04374021291732788,0.9443026781082153:A0.41593707352876663],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;dx&#39;, min=0), 1: Dim(&#39;dy&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;dx&#39;: {&#39;s77&#39;}, &#39;dy&#39;: {&#39;s27&#39;}, &#39;s27&#39;: {&#39;dy&#39;}, &#39;s77&#39;: {&#39;dx&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  _onx_add_sym_size_int_3: dy+1,
  _onx_concat_sym_size_int_2::UnSq0: (&#39;dx&#39;, &#39;add&#39;),
  add::UnSq0: (&#39;add&#39;,),
  sym_size_int_2: dx,
  sym_size_int_2::UnSq0: (&#39;dx&#39;,),
  sym_size_int_3: dy,
  x::Shape1:2: (&#39;dy&#39;,),
  x::Shape:1: (&#39;dx&#39;,),
}&quot;]
&gt;
experiment (float[dx,dy] x) =&gt; (float[dx,dy+1] output_0) 
   &lt;int64 init7_s_1 =  {1}, int64[1] init7_s1_0 =  {0}, int64 sym_size_int_3, int64[1] &quot;sym_size_int_3::RSh1&quot;, int64[1] &quot;add::UnSq0&quot;, float[dx,dy+1] ones, int64 add, int64[1] &quot;x::Shape:1&quot;, int64[1] &quot;init7_s_1::RSh1&quot;, int64[1] init7_s1_1, int64[1] &quot;sym_size_int_2::UnSq0&quot;, int64 sym_size_int_2, int64[1] &quot;x::Shape1:2&quot;, int64 _onx_add_sym_size_int_3&gt;
{
   [sym_size_int] &quot;x::Shape:1&quot; = Shape &lt;end: int = 1, start: int = 0&gt; (x)
   [sym_size_int3] &quot;x::Shape1:2&quot; = Shape &lt;end: int = 2, start: int = 1&gt; (x)
   [sym_size_int4] sym_size_int_3 = Squeeze (&quot;x::Shape1:2&quot;)
   [add3] _onx_add_sym_size_int_3 = Add (sym_size_int_3, init7_s_1)
   [_mkshape1_add] &quot;add::UnSq0&quot; = Unsqueeze (_onx_add_sym_size_int_3, init7_s1_0)
   [_mkshape_add] &quot;_onx_concat_sym_size_int_2::UnSq0&quot; = Concat &lt;axis: int = 0&gt; (&quot;x::Shape:1&quot;, &quot;add::UnSq0&quot;)
   [ones] output_0 = ConstantOfShape &lt;value: tensor = float[1] {1}&gt; (&quot;_onx_concat_sym_size_int_2::UnSq0&quot;)
}
</pre></div>
</div>
</section>
<section id="id75">
<h3>dynamo-ir<a class="headerlink" href="#id75" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x4,),(T1s5x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(dx),1:Dim(dy)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[dx,dy] x) =&gt; (float[dx,dy + 1] ones) 
   &lt;int64 val_2 =  {1}, int64[1] val_5 =  {-1}, float val_10 =  {1}, int64 val_2, int64[1] val_5, float val_10, int64[1] val_0, int64[1] val_1, int64 sym_size_int_3, int64 add, int64[1] val_6, int64[2] val_7&gt;
{
   [node_Shape_0] val_0 = Shape &lt;end: int = 1, start: int = 0&gt; (x)
   [node_Shape_1] val_1 = Shape &lt;end: int = 2, start: int = 1&gt; (x)
   [node_sym_size_int_3] sym_size_int_3 = Squeeze (val_1)
   [node_add] add = Add (sym_size_int_3, val_2)
   [node_Reshape_6] val_6 = Reshape &lt;allowzero: int = 0&gt; (add, val_5)
   [node_Concat_7] val_7 = Concat &lt;axis: int = 0&gt; (val_0, val_6)
   [node_ones] ones = Expand (val_10, val_7)
}
</pre></div>
</div>
</section>
</section>
<section id="croplastdimensionwithtensorcontent">
<span id="ledx-model-case-export-croplastdimensionwithtensorcontent"></span><h2>CropLastDimensionWithTensorContent<a class="headerlink" href="#croplastdimensionwithtensorcontent" title="Link to this heading">¶</a></h2>
<section id="id76">
<h3>forward<a class="headerlink" href="#id76" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="id77">
<h3>custom<a class="headerlink" href="#id77" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)

Caused by: (_export/non_strict_utils.py:1066 in __torch_function__)
For more information, run with TORCH_LOGS=&quot;dynamic&quot;
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The following call raised this error:
  File &quot;~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/eval/model_cases.py&quot;, line 818, in forward
    return x[..., : shape[0]]


The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
</section>
<section id="id78">
<h3>dynamo-ir<a class="headerlink" href="#id78" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4x4,T7s1),(T1s6x4x4,T7s1)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},shape:{})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4,4] x, int64[1] shape) =&gt; (float[batch,4,2] slice_1) 
   &lt;int64[1] val_7 =  {0}, int64[1] val_10 =  {2}, int64[1] val_14 =  {1}, int64[1] val_7, int64[1] val_10, int64[1] val_14&gt;
{
   [node_slice_1] slice_1 = Slice (x, val_7, val_10, val_10, val_14)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>diff.1
</pre></div>
</div>
</section>
</section>
<section id="croplastdimensionwithtensorshape">
<span id="ledx-model-case-export-croplastdimensionwithtensorshape"></span><h2>CropLastDimensionWithTensorShape<a class="headerlink" href="#croplastdimensionwithtensorshape" title="Link to this heading">¶</a></h2>
<section id="id79">
<h3>forward<a class="headerlink" href="#id79" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="id80">
<h3>custom<a class="headerlink" href="#id80" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4x4,T1s2),(T1s6x4x4,T1s3)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},y:{0:Dim(crop)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;2&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;2&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4x4[0.007582306861877441,0.9870750308036804:A0.4917553812265396],T1s2[0.2851674556732178,0.9897225499153137:A0.6374450027942657])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}, &#39;y&#39;: {0: Dim(&#39;crop&#39;, min=1, max=3)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;crop&#39;: {&#39;s17&#39;}, &#39;s17&#39;: {&#39;crop&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  sym_size_int_2: crop,
  sym_size_int_2::UnSq0: (&#39;crop&#39;,),
  y::Shape:1: (&#39;crop&#39;,),
}&quot;]
&gt;
experiment (float[batch,4,4] x, float[crop] y) =&gt; (float[batch,4,crop] output_0) 
   &lt;int64[1] init7_s1_0 =  {0}, int64[1] init7_s1_2 =  {2}, int64[1] &quot;y::Shape:1&quot;, int64[1] &quot;sym_size_int_2::UnSq0&quot;, int64 sym_size_int_2, float[batch,4,crop] slice_1&gt;
{
   [sym_size_int] &quot;y::Shape:1&quot; = Shape &lt;end: int = 1, start: int = 0&gt; (y)
   [slide_Tensor] output_0 = Slice (x, init7_s1_0, &quot;y::Shape:1&quot;, init7_s1_2)
}
</pre></div>
</div>
</section>
<section id="id81">
<h3>dynamo-ir<a class="headerlink" href="#id81" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4x4,T1s2),(T1s6x4x4,T1s3)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},y:{0:Dim(crop)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4,4] x, float[crop] y) =&gt; (float[batch,4,crop] slice_1) 
   &lt;int64[1] val_1 =  {0}, int64[1] val_8 =  {2}, int64[1] val_9 =  {1}, int64[1] val_1, int64[1] val_8, int64[1] val_9, int64[1] val_0&gt;
{
   [node_Shape_0] val_0 = Shape &lt;end: int = 1, start: int = 0&gt; (y)
   [node_slice_1] slice_1 = Slice (x, val_1, val_0, val_8, val_9)
}
</pre></div>
</div>
</section>
</section>
<section id="inplaceadd">
<span id="ledx-model-case-export-inplaceadd"></span><h2>InplaceAdd<a class="headerlink" href="#inplaceadd" title="Link to this heading">¶</a></h2>
<section id="id82">
<h3>forward<a class="headerlink" href="#id82" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id83">
<h3>custom<a class="headerlink" href="#id83" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;1&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[6.050697326660156,6.9520487785339355:A6.48041836420695],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[batch,4] output_0) 
   &lt;float[1,4] c_bias =  {1,1,1,1}, float[batch,4] add_&gt;
{
   [add__Tensor] output_0 = Add (x, c_bias)
}
</pre></div>
</div>
</section>
<section id="id84">
<h3>dynamo-ir<a class="headerlink" href="#id84" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (float[batch,4] add_3) 
   &lt;float[1,4] bias =  {1,1,1,1}, float[1,4] bias&gt;
{
   [node_add_3] add_3 = Add (x, bias)
}
</pre></div>
</div>
</section>
</section>
<section id="inplaceadd2">
<span id="ledx-model-case-export-inplaceadd2"></span><h2>InplaceAdd2<a class="headerlink" href="#inplaceadd2" title="Link to this heading">¶</a></h2>
<section id="id85">
<h3>forward<a class="headerlink" href="#id85" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id86">
<h3>custom<a class="headerlink" href="#id86" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;1&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[8.027350425720215,8.97043228149414:A8.490078687667847],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[batch,4] output_0) 
   &lt;float[1,4] c_bias =  {1,1,1,1}, float[batch,4] add_&gt;
{
   [add__Tensor] output_0 = Add (x, c_bias)
}
</pre></div>
</div>
</section>
<section id="id87">
<h3>dynamo-ir<a class="headerlink" href="#id87" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (float[batch,4] add_3) 
   &lt;float[1,4] bias =  {1,1,1,1}, float[1,4] bias&gt;
{
   [node_add_3] add_3 = Add (x, bias)
}
</pre></div>
</div>
</section>
</section>
<section id="inplaceadd-mul">
<span id="ledx-model-case-export-inplaceadd-mul"></span><h2>InplaceAdd_Mul<a class="headerlink" href="#inplaceadd-mul" title="Link to this heading">¶</a></h2>
<section id="id88">
<h3>forward<a class="headerlink" href="#id88" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="id89">
<h3>custom<a class="headerlink" href="#id89" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;2&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;20&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;2&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[8.074901580810547,8.944803237915039:A8.603144884109497],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[batch,4] output_0) 
   &lt;float[1,4] c_bias =  {1,1,1,1}, float[1] &quot;init1_s_::RSh1&quot; =  {2}, float init1_s_, float[batch,4] mul, float[batch,4] add_, float[batch,4] _onx_mul_add_, int64[1] init7_s1_1&gt;
{
   [add__Tensor] add_ = Add (x, c_bias)
   [mul_Tensor2] output_0 = Mul (add_, &quot;init1_s_::RSh1&quot;)
}
</pre></div>
</div>
</section>
<section id="id90">
<h3>dynamo-ir<a class="headerlink" href="#id90" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (float[batch,4] mul_4) 
   &lt;float[1,4] bias =  {1,1,1,1}, float scalar_tensor_default =  {2}, float[1,4] bias, float scalar_tensor_default, float[batch,4] add_3&gt;
{
   [node_add_3] add_3 = Add (x, bias)
   [node_mul_4] mul_4 = Mul (add_3, scalar_tensor_default)
}
</pre></div>
</div>
</section>
</section>
<section id="inplacecloneadd">
<span id="ledx-model-case-export-inplacecloneadd"></span><h2><a class="reference internal" href="#inplacecloneadd">InplaceCloneAdd</a><a class="headerlink" href="#inplacecloneadd" title="Link to this heading">¶</a></h2>
<section id="id91">
<h3>forward<a class="headerlink" href="#id91" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">x</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id92">
<h3>custom<a class="headerlink" href="#id92" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;16&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;1&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s3x4[0.029643893241882324,0.9251223206520081:A0.43432919184366864],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[batch,4] output_0) 
   &lt;float[1,4] c_bias =  {1,1,1,1}, float[batch,4] clone, float[batch,4] add_&gt;
{
   [add__Tensor] output_0 = Add (x, c_bias)
}
</pre></div>
</div>
</section>
<section id="id93">
<h3>dynamo-ir<a class="headerlink" href="#id93" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s3x4,),(T1s5x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (float[batch,4] add_6) 
   &lt;float[1,4] bias =  {1,1,1,1}, float[1,4] bias&gt;
{
   [node_add_6] add_6 = Add (x, bias)
}
</pre></div>
</div>
</section>
</section>
<section id="inplacesetitemellipsis-1">
<span id="ledx-model-case-export-inplacesetitemellipsis-1"></span><h2>InplaceSetItemEllipsis_1<a class="headerlink" href="#inplacesetitemellipsis-1" title="Link to this heading">¶</a></h2>
<section id="id94">
<h3>forward<a class="headerlink" href="#id94" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">update</span><span class="p">):</span>
    <span class="n">copy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">copy</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">update</span>
    <span class="k">return</span> <span class="n">copy</span>
</pre></div>
</div>
</section>
<section id="id95">
<h3>custom<a class="headerlink" href="#id95" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>L[&#39;update&#39;].size()[0] = 8192 is not equal to L[&#39;index&#39;].size()[0] = 4

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
</section>
<section id="id96">
<h3>dynamo-ir<a class="headerlink" href="#id96" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to convert the exported program to an ONNX model. [96mThis is step 3/3[0m of exporting the model to ONNX. Next steps:
- If there is a missing ONNX function, implement it and register it to the registry.
- If there is an internal error during ONNX conversion, debug the error and summit a PR to PyTorch.
- Create an error report with `torch.onnx.export(..., report=True)`, and save the ExportedProgram as a pt2 file. Create an issue in the PyTorch GitHub repository against the [96m*onnx*[0m component. Attach the error report and the pt2 model.

## Exception summary

&lt;class &#39;TypeError&#39;&gt;: int() argument must be a string, a bytes-like object or a real number, not &#39;SymbolicDim&#39;
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.GraphConstructionError&#39;&gt;: Error processing Python constants for operator &#39;::Expand&#39;. named_inputs={&#39;input&#39;: SymbolicTensor(name=&#39;anonymous:124401587372112&#39;, producer=anonymous_node:124401593169488, index=0), &#39;shape&#39;: (SymbolicDim(s5), SymbolicDim(s32))}, named_attrs={}, opset=, op_signature=&#39;&#39;::Expand(input: T, shape: shape) -&gt; (T) where T=FLOAT | DOUBLE | UINT16 | COMPLEX128 | UINT32 | UINT8 | INT16 | INT32 | STRING | UINT64 | BFLOAT16 | INT64 | BOOL | FLOAT16 | INT8 | COMPLEX64, shape=INT64.
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.GraphConstructionError&#39;&gt;: Error calling operator &#39;Expand&#39; with args (SymbolicTensor(name=&#39;anonymous:124401587372112&#39;, producer=anonymous_node:124401593169488, index=0), (SymbolicDim(s5), SymbolicDim(s32))) and kwargs {}.
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.GraphConstructionError&#39;&gt;: Error when calling function &#39;TracedOnnxFunction(&lt;function aten_index_put at 0x71245c659800&gt;)&#39; with args &#39;[SymbolicTensor(name=&#39;clone&#39;, type=Tensor(FLOAT), shape=Shape([1, 8192, 4]), producer=&#39;node_clone&#39;, index=0), [None, None, SymbolicTensor(name=&#39;index&#39;, type=Tensor(INT64), shape=Shape([SymbolicDim(s91)]))], SymbolicTensor(name=&#39;update&#39;, type=Tensor(FLOAT), shape=Shape([SymbolicDim(s5), SymbolicDim(s32)]))]&#39; and kwargs &#39;{}&#39;
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.ConversionError&#39;&gt;: Error when translating node %index_put : [num_users=1] = call_function[target=torch.ops.aten.index_put.default](args = (%clone, [None, None, %index], %update), kwargs = {}). See the stack trace for more information.

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="inplacesetitemellipsis-2">
<span id="ledx-model-case-export-inplacesetitemellipsis-2"></span><h2>InplaceSetItemEllipsis_2<a class="headerlink" href="#inplacesetitemellipsis-2" title="Link to this heading">¶</a></h2>
<section id="id97">
<h3>forward<a class="headerlink" href="#id97" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">update</span><span class="p">):</span>
    <span class="n">copy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">copy</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">update</span>
    <span class="k">return</span> <span class="n">copy</span>
</pre></div>
</div>
</section>
<section id="id98">
<h3>custom<a class="headerlink" href="#id98" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>L[&#39;update&#39;].size()[0] = 8192 is not equal to L[&#39;index&#39;].size()[0] = 4

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
</section>
<section id="id99">
<h3>dynamo-ir<a class="headerlink" href="#id99" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to convert the exported program to an ONNX model. [96mThis is step 3/3[0m of exporting the model to ONNX. Next steps:
- If there is a missing ONNX function, implement it and register it to the registry.
- If there is an internal error during ONNX conversion, debug the error and summit a PR to PyTorch.
- Create an error report with `torch.onnx.export(..., report=True)`, and save the ExportedProgram as a pt2 file. Create an issue in the PyTorch GitHub repository against the [96m*onnx*[0m component. Attach the error report and the pt2 model.

## Exception summary

&lt;class &#39;TypeError&#39;&gt;: int() argument must be a string, a bytes-like object or a real number, not &#39;SymbolicDim&#39;
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.GraphConstructionError&#39;&gt;: Error processing Python constants for operator &#39;::Expand&#39;. named_inputs={&#39;input&#39;: SymbolicTensor(name=&#39;anonymous:124400879231024&#39;, producer=anonymous_node:124401593159120, index=0), &#39;shape&#39;: (SymbolicDim(s5), SymbolicDim(s32))}, named_attrs={}, opset=, op_signature=&#39;&#39;::Expand(input: T, shape: shape) -&gt; (T) where T=FLOAT | DOUBLE | UINT16 | COMPLEX128 | UINT32 | UINT8 | INT16 | INT32 | STRING | UINT64 | BFLOAT16 | INT64 | BOOL | FLOAT16 | INT8 | COMPLEX64, shape=INT64.
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.GraphConstructionError&#39;&gt;: Error calling operator &#39;Expand&#39; with args (SymbolicTensor(name=&#39;anonymous:124400879231024&#39;, producer=anonymous_node:124401593159120, index=0), (SymbolicDim(s5), SymbolicDim(s32))) and kwargs {}.
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.GraphConstructionError&#39;&gt;: Error when calling function &#39;TracedOnnxFunction(&lt;function aten_index_put at 0x71245c659800&gt;)&#39; with args &#39;[SymbolicTensor(name=&#39;clone&#39;, type=Tensor(FLOAT), shape=Shape([1, 8192, 6]), producer=&#39;node_clone&#39;, index=0), [None, None, SymbolicTensor(name=&#39;index&#39;, type=Tensor(INT64), shape=Shape([SymbolicDim(s91)]))], SymbolicTensor(name=&#39;update&#39;, type=Tensor(FLOAT), shape=Shape([SymbolicDim(s5), SymbolicDim(s32)]))]&#39; and kwargs &#39;{}&#39;
⬆️
&lt;class &#39;torch.onnx._internal.exporter._errors.ConversionError&#39;&gt;: Error when translating node %index_put : [num_users=1] = call_function[target=torch.ops.aten.index_put.default](args = (%clone, [None, None, %index], %update), kwargs = {}). See the stack trace for more information.

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="inplacesetitemmask">
<span id="ledx-model-case-export-inplacesetitemmask"></span><h2>InplaceSetItemMask<a class="headerlink" href="#inplacesetitemmask" title="Link to this heading">¶</a></h2>
<section id="id100">
<h3>forward<a class="headerlink" href="#id100" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
    <span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id101">
<h3>custom<a class="headerlink" href="#id101" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s2x3x3,),(T1s3x3x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;1&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;4&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;2&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s2x3x3[2.0,2.0:A2.0],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3,3] x) =&gt; (float[batch,3,3] output_0) 
   &lt;float c_lifted_tensor_0 =  {2}, float lift_fresh_copy, float[batch,3,3] index_put_, bool[batch,3,3] to&gt;
{
   [to_dtype] to = Cast &lt;to: int = 9&gt; (x)
   [index_put_1b__where] output_0 = Where (to, c_lifted_tensor_0, x)
}
</pre></div>
</div>
</section>
<section id="id102">
<h3>dynamo-ir<a class="headerlink" href="#id102" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s2x3x3,),(T1s3x3x3,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3,3] x) =&gt; (float[batch,3,3] index_put) 
   &lt;float clone =  {2}, float clone, bool[batch,3,3] _to_copy&gt;
{
   [node__to_copy] _to_copy = Cast &lt;to: int = 9&gt; (x)
   [node_index_put] index_put = Where (_to_copy, clone, x)
}
</pre></div>
</div>
</section>
</section>
<section id="inplacesetitemsquare">
<span id="ledx-model-case-export-inplacesetitemsquare"></span><h2>InplaceSetItemSquare<a class="headerlink" href="#inplacesetitemsquare" title="Link to this heading">¶</a></h2>
<section id="id103">
<h3>forward<a class="headerlink" href="#id103" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id104">
<h3>custom<a class="headerlink" href="#id104" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s5x5,),(T1s7x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;6&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;88&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;11&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[False, True]&quot;, &quot;input_args&quot;: &quot;(T1s5x5[0.003920555114746094,1.0:A0.6840395998954772],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  _onx_gather_x::Shape:: (&#39;batch&#39;,),
  slice_2::Shape:: (2, 3),
  x::Shape:: (&#39;batch&#39;, 5),
}&quot;]
&gt;
experiment (float[batch,5] x) =&gt; (float[batch,5] output_1) 
   &lt;int64[1] init7_s1_0 =  {0}, int64[1] init7_s1_2 =  {2}, int64[1] init7_s1_1 =  {1}, int64[2] &quot;init7_s2_-1_1&quot; =  {-1,1}, int64[3,1] &quot;init7_s3_0_1_2::RSh-1x1&quot; =  {0,1,2}, float[3,2] &quot;fill::T10&quot; =  {1,1,1,1,1,1}, float[2,5] slice_3, int64[2] &quot;slice_2::Shape:&quot;, float clone, int64[?] _onx_slice_range_init7_s1_0, int64[?,?] &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot;, int64[1] &quot;_onx_gather_x::Shape:&quot;, int64[?] _onx_range_init7_s1_0, int64[1] init7_s1_3, float[5,2] &quot;slice_3::T10&quot;, float c_lifted_tensor_0, int64[2] &quot;x::Shape:&quot;, float[batch,5] output_0, float[batch,5] slice_scatter_1, float[5,2] &quot;_onx_scatternd_slice_3::T10&quot;, int64[3] init7_s3_0_1_2, float[2,5] slice_scatter, float[2,3] fill, float[2,5] slice_1, float[2,3] slice_2&gt;
{
   [slide_Tensor3] slice_3 = Slice (x, init7_s1_0, init7_s1_2, init7_s1_0)
   [slice_scatter_static2] &quot;slice_3::T10&quot; = Transpose &lt;perm: ints = [1, 0]&gt; (slice_3)
   [slice_scatter_static4] &quot;_onx_scatternd_slice_3::T10&quot; = ScatterND (&quot;slice_3::T10&quot;, &quot;init7_s3_0_1_2::RSh-1x1&quot;, &quot;fill::T10&quot;)
   [slice_scatter_static5] slice_scatter = Transpose &lt;perm: ints = [1, 0]&gt; (&quot;_onx_scatternd_slice_3::T10&quot;)
   [slice_scatter_dynamic] &quot;x::Shape:&quot; = Shape (x)
   [slice_scatter_dynamic2] &quot;_onx_gather_x::Shape:&quot; = Gather (&quot;x::Shape:&quot;, init7_s1_0)
   [slice_scatter_dynamic3] _onx_range_init7_s1_0 = Range (init7_s1_0, &quot;_onx_gather_x::Shape:&quot;, init7_s1_1)
   [slice_scatter_dynamic4] _onx_slice_range_init7_s1_0 = Slice (_onx_range_init7_s1_0, init7_s1_0, init7_s1_2, init7_s1_0, init7_s1_1)
   [slice_scatter_dynamic5] &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot; = Reshape (_onx_slice_range_init7_s1_0, &quot;init7_s2_-1_1&quot;)
   [slice_scatter_dynamic6] output_0 = ScatterND (x, &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot;, slice_scatter)
   [&quot;.output2&quot;] output_1 = Identity (output_0)
}
</pre></div>
</div>
</section>
<section id="id105">
<h3>dynamo-ir<a class="headerlink" href="#id105" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s5x5,),(T1s7x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,5] x) =&gt; (float[batch,5] slice_scatter_1) 
   &lt;int64 val_0 =  {0}, int64 val_19 =  {1}, int64[1] val_26 =  {0}, int64[1] val_29 =  {2}, int64[1] val_33 =  {1}, int64[1] val_42 =  {-1}, int64[3,1] val_43 =  {0,1,2}, float[3,2] val_44 =  {1,1,1,1,1,1}, int64 val_0, int64 val_19, int64[1] val_26, int64[1] val_29, int64[1] val_33, int64[1] val_42, int64[3,1] val_43, float[3,2] val_44, float[2,5] slice_3, float[5,2] val_45, float[5,2] val_46, float[2,5] slice_scatter, int64[2] val_48, int64 val_49, int64[?] val_50, int64[?] val_54, int64[?,1] val_55&gt;
{
   [node_slice_3] slice_3 = Slice (x, val_26, val_29, val_26, val_33)
   [node_Transpose_45] val_45 = Transpose &lt;perm: ints = [1, 0]&gt; (slice_3)
   [node_ScatterND_46] val_46 = ScatterND &lt;reduction: string = &quot;none&quot;&gt; (val_45, val_43, val_44)
   [node_slice_scatter] slice_scatter = Transpose &lt;perm: ints = [1, 0]&gt; (val_46)
   [node_Shape_48] val_48 = Shape &lt;start: int = 0&gt; (x)
   [node_Gather_49] val_49 = Gather &lt;axis: int = 0&gt; (val_48, val_0)
   [node_Range_50] val_50 = Range (val_0, val_49, val_19)
   [node_Slice_54] val_54 = Slice (val_50, val_26, val_29, val_26, val_33)
   [node_Unsqueeze_55] val_55 = Unsqueeze (val_54, val_42)
   [node_slice_scatter_1] slice_scatter_1 = ScatterND &lt;reduction: string = &quot;none&quot;&gt; (x, val_55, slice_scatter)
}
</pre></div>
</div>
</section>
</section>
<section id="inplacesetitemsquareadd">
<span id="ledx-model-case-export-inplacesetitemsquareadd"></span><h2>InplaceSetItemSquareAdd<a class="headerlink" href="#inplacesetitemsquareadd" title="Link to this heading">¶</a></h2>
<section id="id106">
<h3>forward<a class="headerlink" href="#id106" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="id107">
<h3>custom<a class="headerlink" href="#id107" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s5x5,),(T1s7x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;7&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;92&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;11&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[False, True]&quot;, &quot;input_args&quot;: &quot;(T1s5x5[0.04627680778503418,1.0:A0.5518134546279907],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  _onx_gather_x::Shape:: (&#39;batch&#39;,),
  slice_2::Shape:: (2, 3),
  x::Shape:: (&#39;batch&#39;, 5),
}&quot;]
&gt;
experiment (float[batch,5] x) =&gt; (float[batch,5] output_1) 
   &lt;int64[1] init7_s1_0 =  {0}, int64[1] init7_s1_2 =  {2}, int64[1] init7_s1_1 =  {1}, int64[2] &quot;init7_s2_-1_1&quot; =  {-1,1}, int64[3,1] &quot;init7_s3_0_1_2::RSh-1x1&quot; =  {0,1,2}, float[3,2] &quot;fill::T10&quot; =  {1,1,1,1,1,1}, float[1] &quot;init1_s_::RSh1&quot; =  {2}, float init1_s_, float[2,5] slice_3, int64[2] &quot;slice_2::Shape:&quot;, float clone, int64[?] _onx_slice_range_init7_s1_0, int64[?,?] &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot;, int64[1] &quot;_onx_gather_x::Shape:&quot;, float[batch,5] add, int64[?] _onx_range_init7_s1_0, int64[1] init7_s1_3, float[5,2] &quot;slice_3::T10&quot;, float c_lifted_tensor_0, int64[2] &quot;x::Shape:&quot;, float[batch,5] output_0, float[batch,5] slice_scatter_1, float[5,2] &quot;_onx_scatternd_slice_3::T10&quot;, int64[3] init7_s3_0_1_2, float[2,5] slice_scatter, float[2,3] fill, float[2,5] slice_1, float[2,3] slice_2&gt;
{
   [slide_Tensor3] slice_3 = Slice (x, init7_s1_0, init7_s1_2, init7_s1_0)
   [slice_scatter_static2] &quot;slice_3::T10&quot; = Transpose &lt;perm: ints = [1, 0]&gt; (slice_3)
   [slice_scatter_static4] &quot;_onx_scatternd_slice_3::T10&quot; = ScatterND (&quot;slice_3::T10&quot;, &quot;init7_s3_0_1_2::RSh-1x1&quot;, &quot;fill::T10&quot;)
   [slice_scatter_static5] slice_scatter = Transpose &lt;perm: ints = [1, 0]&gt; (&quot;_onx_scatternd_slice_3::T10&quot;)
   [slice_scatter_dynamic] &quot;x::Shape:&quot; = Shape (x)
   [slice_scatter_dynamic2] &quot;_onx_gather_x::Shape:&quot; = Gather (&quot;x::Shape:&quot;, init7_s1_0)
   [slice_scatter_dynamic3] _onx_range_init7_s1_0 = Range (init7_s1_0, &quot;_onx_gather_x::Shape:&quot;, init7_s1_1)
   [slice_scatter_dynamic4] _onx_slice_range_init7_s1_0 = Slice (_onx_range_init7_s1_0, init7_s1_0, init7_s1_2, init7_s1_0, init7_s1_1)
   [slice_scatter_dynamic5] &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot; = Reshape (_onx_slice_range_init7_s1_0, &quot;init7_s2_-1_1&quot;)
   [slice_scatter_dynamic6] output_0 = ScatterND (x, &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot;, slice_scatter)
   [add_Tensor] output_1 = Add (output_0, &quot;init1_s_::RSh1&quot;)
}
</pre></div>
</div>
</section>
<section id="id108">
<h3>dynamo-ir<a class="headerlink" href="#id108" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s5x5,),(T1s7x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,5] x) =&gt; (float[batch,5] add) 
   &lt;int64 val_0 =  {0}, int64 val_19 =  {1}, int64[1] val_26 =  {0}, int64[1] val_29 =  {2}, int64[1] val_33 =  {1}, int64[1] val_42 =  {-1}, int64[3,1] val_43 =  {0,1,2}, float[3,2] val_44 =  {1,1,1,1,1,1}, float scalar_tensor_default =  {2}, int64 val_0, int64 val_19, int64[1] val_26, int64[1] val_29, int64[1] val_33, int64[1] val_42, int64[3,1] val_43, float[3,2] val_44, float scalar_tensor_default, float[2,5] slice_3, float[5,2] val_45, float[5,2] val_46, float[2,5] slice_scatter, int64[2] val_48, int64 val_49, int64[?] val_50, int64[?] val_54, int64[?,1] val_55, float[batch,5] slice_scatter_1&gt;
{
   [node_slice_3] slice_3 = Slice (x, val_26, val_29, val_26, val_33)
   [node_Transpose_45] val_45 = Transpose &lt;perm: ints = [1, 0]&gt; (slice_3)
   [node_ScatterND_46] val_46 = ScatterND &lt;reduction: string = &quot;none&quot;&gt; (val_45, val_43, val_44)
   [node_slice_scatter] slice_scatter = Transpose &lt;perm: ints = [1, 0]&gt; (val_46)
   [node_Shape_48] val_48 = Shape &lt;start: int = 0&gt; (x)
   [node_Gather_49] val_49 = Gather &lt;axis: int = 0&gt; (val_48, val_0)
   [node_Range_50] val_50 = Range (val_0, val_49, val_19)
   [node_Slice_54] val_54 = Slice (val_50, val_26, val_29, val_26, val_33)
   [node_Unsqueeze_55] val_55 = Unsqueeze (val_54, val_42)
   [node_slice_scatter_1] slice_scatter_1 = ScatterND &lt;reduction: string = &quot;none&quot;&gt; (x, val_55, slice_scatter)
   [node_add] add = Add (slice_scatter_1, scalar_tensor_default)
}
</pre></div>
</div>
</section>
</section>
<section id="inplacesetitemsquareadd2">
<span id="ledx-model-case-export-inplacesetitemsquareadd2"></span><h2>InplaceSetItemSquareAdd2<a class="headerlink" href="#inplacesetitemsquareadd2" title="Link to this heading">¶</a></h2>
<section id="id109">
<h3>forward<a class="headerlink" href="#id109" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">3</span>
</pre></div>
</div>
</section>
<section id="id110">
<h3>custom<a class="headerlink" href="#id110" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s5x5,),(T1s7x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;8&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;96&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;12&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[False, True, True]&quot;, &quot;input_args&quot;: &quot;(T1s5x5[0.04161882400512695,1.0:A0.5988937711715698],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  _onx_gather_x::Shape:: (&#39;batch&#39;,),
  slice_2::Shape:: (2, 3),
  x::Shape:: (&#39;batch&#39;, 5),
}&quot;]
&gt;
experiment (float[batch,5] x) =&gt; (float[batch,5] output_1, float[batch,5] output_2) 
   &lt;int64[1] init7_s1_0 =  {0}, int64[1] init7_s1_2 =  {2}, int64[1] init7_s1_1 =  {1}, int64[2] &quot;init7_s2_-1_1&quot; =  {-1,1}, int64[3,1] &quot;init7_s3_0_1_2::RSh-1x1&quot; =  {0,1,2}, float[3,2] &quot;fill::T10&quot; =  {1,1,1,1,1,1}, float[1] &quot;init1_s_::RSh1&quot; =  {2}, float[1] &quot;init1_s_2::RSh1&quot; =  {3}, float init1_s_, float[2,5] slice_3, int64[2] &quot;slice_2::Shape:&quot;, float clone, int64[?] _onx_slice_range_init7_s1_0, int64[?,?] &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot;, int64[1] &quot;_onx_gather_x::Shape:&quot;, float[batch,5] add, float[batch,5] add_4, int64[?] _onx_range_init7_s1_0, int64[1] init7_s1_3, float[5,2] &quot;slice_3::T10&quot;, float c_lifted_tensor_0, int64[2] &quot;x::Shape:&quot;, float[batch,5] output_0, float[batch,5] slice_scatter_1, float[5,2] &quot;_onx_scatternd_slice_3::T10&quot;, float init1_s_2, int64[3] init7_s3_0_1_2, float[2,5] slice_scatter, float[2,3] fill, float[2,5] slice_1, float[2,3] slice_2&gt;
{
   [slide_Tensor3] slice_3 = Slice (x, init7_s1_0, init7_s1_2, init7_s1_0)
   [slice_scatter_static2] &quot;slice_3::T10&quot; = Transpose &lt;perm: ints = [1, 0]&gt; (slice_3)
   [slice_scatter_static4] &quot;_onx_scatternd_slice_3::T10&quot; = ScatterND (&quot;slice_3::T10&quot;, &quot;init7_s3_0_1_2::RSh-1x1&quot;, &quot;fill::T10&quot;)
   [slice_scatter_static5] slice_scatter = Transpose &lt;perm: ints = [1, 0]&gt; (&quot;_onx_scatternd_slice_3::T10&quot;)
   [slice_scatter_dynamic] &quot;x::Shape:&quot; = Shape (x)
   [slice_scatter_dynamic2] &quot;_onx_gather_x::Shape:&quot; = Gather (&quot;x::Shape:&quot;, init7_s1_0)
   [slice_scatter_dynamic3] _onx_range_init7_s1_0 = Range (init7_s1_0, &quot;_onx_gather_x::Shape:&quot;, init7_s1_1)
   [slice_scatter_dynamic4] _onx_slice_range_init7_s1_0 = Slice (_onx_range_init7_s1_0, init7_s1_0, init7_s1_2, init7_s1_0, init7_s1_1)
   [slice_scatter_dynamic5] &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot; = Reshape (_onx_slice_range_init7_s1_0, &quot;init7_s2_-1_1&quot;)
   [slice_scatter_dynamic6] output_0 = ScatterND (x, &quot;_onx_slice_range_init7_s1_0::RSh-1x1&quot;, slice_scatter)
   [add_Tensor] output_1 = Add (output_0, &quot;init1_s_::RSh1&quot;)
   [add_Tensor2] output_2 = Add (output_0, &quot;init1_s_2::RSh1&quot;)
}
</pre></div>
</div>
</section>
<section id="id111">
<h3>dynamo-ir<a class="headerlink" href="#id111" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s5x5,),(T1s7x5,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,5] x) =&gt; (float[batch,5] add, float[batch,5] add_4) 
   &lt;int64 val_0 =  {0}, int64 val_19 =  {1}, int64[1] val_26 =  {0}, int64[1] val_29 =  {2}, int64[1] val_33 =  {1}, int64[1] val_42 =  {-1}, int64[3,1] val_43 =  {0,1,2}, float[3,2] val_44 =  {1,1,1,1,1,1}, float scalar_tensor_default =  {2}, float scalar_tensor_default_1 =  {3}, int64 val_0, int64 val_19, int64[1] val_26, int64[1] val_29, int64[1] val_33, int64[1] val_42, int64[3,1] val_43, float[3,2] val_44, float scalar_tensor_default, float scalar_tensor_default_1, float[2,5] slice_3, float[5,2] val_45, float[5,2] val_46, float[2,5] slice_scatter, int64[2] val_48, int64 val_49, int64[?] val_50, int64[?] val_54, int64[?,1] val_55, float[batch,5] slice_scatter_1&gt;
{
   [node_slice_3] slice_3 = Slice (x, val_26, val_29, val_26, val_33)
   [node_Transpose_45] val_45 = Transpose &lt;perm: ints = [1, 0]&gt; (slice_3)
   [node_ScatterND_46] val_46 = ScatterND &lt;reduction: string = &quot;none&quot;&gt; (val_45, val_43, val_44)
   [node_slice_scatter] slice_scatter = Transpose &lt;perm: ints = [1, 0]&gt; (val_46)
   [node_Shape_48] val_48 = Shape &lt;start: int = 0&gt; (x)
   [node_Gather_49] val_49 = Gather &lt;axis: int = 0&gt; (val_48, val_0)
   [node_Range_50] val_50 = Range (val_0, val_49, val_19)
   [node_Slice_54] val_54 = Slice (val_50, val_26, val_29, val_26, val_33)
   [node_Unsqueeze_55] val_55 = Unsqueeze (val_54, val_42)
   [node_slice_scatter_1] slice_scatter_1 = ScatterND &lt;reduction: string = &quot;none&quot;&gt; (x, val_55, slice_scatter)
   [node_add] add = Add (slice_scatter_1, scalar_tensor_default)
   [node_add_4] add_4 = Add (slice_scatter_1, scalar_tensor_default_1)
}
</pre></div>
</div>
</section>
</section>
<section id="signaturefloat1">
<span id="ledx-model-case-export-signaturefloat1"></span><h2>SignatureFloat1<a class="headerlink" href="#signaturefloat1" title="Link to this heading">¶</a></h2>
<section id="id112">
<h3>forward<a class="headerlink" href="#id112" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">*</span> <span class="n">alpha</span>
</pre></div>
</div>
</section>
<section id="id113">
<h3>custom<a class="headerlink" href="#id113" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,float),(T1s8x3,float)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">({0:Dim(batch)},None)</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;3&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;20&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],float=1.5)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;({0: Dim(&#39;batch&#39;, min=1, max=1024)}, None)&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x) =&gt; (float[batch,1] output_0) 
   &lt;float[1] mul =  {0.75}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {0.191434,-0.0494725,0.237434}, float[1] &quot;linear.bias&quot; =  {0.324541}, float init1_s_, float[batch,1] sub, float[batch,1] linear, float[1] _onx_mul_b_buff, int64[1] init7_s1_1, float[1,3] p_linear_weight, int64[2] &quot;init7_s2_1_-1&quot;, float[1] b_buff, float[batch,1] _onx_matmul_x, float[1] &quot;init1_s_::RSh1&quot;, float[batch,1] sigmoid, float[3,1] &quot;p_linear_weight::T10&quot;, float[1,3] &quot;linear.weight&quot;, float[1] p_linear_bias&gt;
{
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [sub_Tensor] output_0 = Sub (sigmoid, mul)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input mismatch, inputs[0]=(T1r2,float) but names=[&#39;x&#39;], model=SignatureFloat1, export=&#39;custom&#39;
</pre></div>
</div>
</section>
<section id="id114">
<h3>dynamo-ir<a class="headerlink" href="#id114" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,float),(T1s8x3,float)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">({0:Dim(batch)},None)</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[s77,3] x) =&gt; (float[s77,1] sub_2) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {-0.0768617,0.347785,-0.253902}, float[1] &quot;linear.bias&quot; =  {-0.468836}, float[1] mul_2 =  {0.75}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, float[1] mul_2, float[s77,1] linear, float[s77,1] sigmoid&gt;
{
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_sub_2] sub_2 = Sub (sigmoid, mul_2)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input mismatch, inputs[0]=(T1r2,float) but names=[&#39;x&#39;], model=SignatureFloat1, export=&#39;dynamo-ir&#39;
</pre></div>
</div>
</section>
</section>
<section id="signatureint1">
<span id="ledx-model-case-export-signatureint1"></span><h2>SignatureInt1<a class="headerlink" href="#signatureint1" title="Link to this heading">¶</a></h2>
<section id="id115">
<h3>forward<a class="headerlink" href="#id115" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">+</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id116">
<h3>custom<a class="headerlink" href="#id116" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,int),(T1s8x3,int)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">({0:Dim(batch)},None)</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;5&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;36&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;5&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],int=1)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;({0: Dim(&#39;batch&#39;, min=1, max=1024)}, None)&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x) =&gt; (float[batch,1] output_0) 
   &lt;float[1] b_buff =  {0.5}, int64[1] init7_s1_1 =  {1}, int64[1] init7_s1_2 =  {2}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {-0.231751,0.0259535,-0.306876}, float[1] &quot;linear.bias&quot; =  {-0.0971979}, float[batch,1] _onx_matmul_x, float[3,1] &quot;p_linear_weight::T10&quot;, float[batch,1] sub, float[1,3] &quot;linear.weight&quot;, float[batch,1] add, float[batch,1] linear, float[1] p_linear_bias, float[1,3] p_linear_weight, float[batch,1] slice_1, int64[2] &quot;init7_s2_1_-1&quot;, float[batch,1] sigmoid&gt;
{
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [sub_Tensor] sub = Sub (sigmoid, b_buff)
   [slide_Tensor] slice_1 = Slice (x, init7_s1_1, init7_s1_2, init7_s1_1)
   [add_Tensor] output_0 = Add (sub, slice_1)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input mismatch, inputs[0]=(T1r2,int) but names=[&#39;x&#39;], model=SignatureInt1, export=&#39;custom&#39;
</pre></div>
</div>
</section>
<section id="id117">
<h3>dynamo-ir<a class="headerlink" href="#id117" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,int),(T1s8x3,int)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">({0:Dim(batch)},None)</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[s77,3] x) =&gt; (float[s77,1] add_12) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {0.0720719,0.317282,-0.292143}, float[1] &quot;linear.bias&quot; =  {-0.431663}, float[1] buff =  {0.5}, int64[1] val_3 =  {1}, int64[1] val_7 =  {2}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, float[1] buff, int64[1] val_3, int64[1] val_7, float[s77,1] linear, float[s77,1] sigmoid, float[s77,1] sub_2, float[s77,1] slice_1&gt;
{
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_sub_2] sub_2 = Sub (sigmoid, buff)
   [node_slice_1] slice_1 = Slice (x, val_3, val_7, val_3, val_3)
   [node_add_12] add_12 = Add (sub_2, slice_1)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input mismatch, inputs[0]=(T1r2,int) but names=[&#39;x&#39;], model=SignatureInt1, export=&#39;dynamo-ir&#39;
</pre></div>
</div>
</section>
</section>
<section id="signatureint2">
<span id="ledx-model-case-export-signatureint2"></span><h2>SignatureInt2<a class="headerlink" href="#signatureint2" title="Link to this heading">¶</a></h2>
<section id="id118">
<h3>forward<a class="headerlink" href="#id118" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">+</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id119">
<h3>custom<a class="headerlink" href="#id119" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x3,int)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},i:None)</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;4&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;28&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;5&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],int=1)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;i&#39;: None, &#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x) =&gt; (float[batch,batch] output_0) 
   &lt;float[1] b_buff =  {0.5}, int64 init7_s_1 =  {1}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {-0.102946,0.259576,-0.121435}, float[1] &quot;linear.bias&quot; =  {-0.48489}, float[batch,1] _onx_matmul_x, float[3,1] &quot;p_linear_weight::T10&quot;, float[batch,1] sub, float[batch] select, float[1,3] &quot;linear.weight&quot;, float[batch,batch] add, float[batch,1] linear, float[1] p_linear_bias, float[1,3] p_linear_weight, int64[2] &quot;init7_s2_1_-1&quot;, float[batch,1] sigmoid&gt;
{
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [sub_Tensor] sub = Sub (sigmoid, b_buff)
   [select_int] select = Gather &lt;axis: int = 1&gt; (x, init7_s_1)
   [add_Tensor] output_0 = Add (sub, select)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input mismatch, inputs[0]=(T1r2,int) but names=[&#39;x&#39;], model=SignatureInt2, export=&#39;custom&#39;
</pre></div>
</div>
</section>
<section id="id120">
<h3>dynamo-ir<a class="headerlink" href="#id120" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x3,int)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},i:None)</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[s77,3] x) =&gt; (float[s77,s77] add_14) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {0.47722,-0.381471,-0.191322}, float[1] &quot;linear.bias&quot; =  {0.292383}, float[1] buff =  {0.5}, int64 val_12 =  {1}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, float[1] buff, int64 val_12, float[s77,1] linear, float[s77,1] sigmoid, float[s77,1] sub_2, float[s77] select&gt;
{
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_sub_2] sub_2 = Sub (sigmoid, buff)
   [node_select] select = Gather &lt;axis: int = 1&gt; (x, val_12)
   [node_add_14] add_14 = Add (sub_2, select)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input mismatch, inputs[0]=(T1r2,int) but names=[&#39;x&#39;], model=SignatureInt2, export=&#39;dynamo-ir&#39;
</pre></div>
</div>
</section>
</section>
<section id="signaturelistfixedlength">
<span id="ledx-model-case-export-signaturelistfixedlength"></span><h2>SignatureListFixedLength<a class="headerlink" href="#signaturelistfixedlength" title="Link to this heading">¶</a></h2>
<section id="id121">
<h3>forward<a class="headerlink" href="#id121" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lx</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">+</span> <span class="n">lx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id122">
<h3>custom<a class="headerlink" href="#id122" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#2[T1s8x1,T1s8x2])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;4&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;28&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;6&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],#2[T1s4x1[10.0,13.0:A11.5],T1s4x2[10.0,17.0:A13.5]])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;lx&#39;: [{0: Dim(&#39;batch&#39;, min=0)}, {0: Dim(&#39;batch&#39;, min=0)}],
 &#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s50&#39;, &#39;s77&#39;, &#39;s53&#39;},
 &#39;s50&#39;: {&#39;s53&#39;, &#39;batch&#39;},
 &#39;s53&#39;: {&#39;batch&#39;},
 &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] output_0) 
   &lt;float[1] b_buff =  {0.5}, int64[1] init7_s1_1 =  {1}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {-0.405453,-0.0646727,0.0489572}, float[1] &quot;linear.bias&quot; =  {-0.572107}, float[batch,1] sub, float[batch,1] linear, float[batch,1] sum_1, float[1,3] p_linear_weight, int64[2] &quot;init7_s2_1_-1&quot;, float[batch,1] mul, float[batch,1] add, float[batch,1] _onx_matmul_x, float[batch,1] sigmoid, float[3,1] &quot;p_linear_weight::T10&quot;, float[1,3] &quot;linear.weight&quot;, float[1] p_linear_bias&gt;
{
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [sub_Tensor] sub = Sub (sigmoid, b_buff)
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 1&gt; (lx_1, init7_s1_1)
   [mul_Tensor] mul = Mul (lx_0, sum_1)
   [add_Tensor] output_0 = Add (sub, mul)
}
</pre></div>
</div>
</section>
<section id="id123">
<h3>dynamo-ir<a class="headerlink" href="#id123" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#2[T1s8x1,T1s8x2])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] add_15) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {0.42753,-0.502124,0.529856}, float[1] &quot;linear.bias&quot; =  {0.557567}, float[1] buff =  {0.5}, int64[1] val_6 =  {1}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, float[1] buff, int64[1] val_6, float[batch,1] linear, float[batch,1] sigmoid, float[batch,1] sub_2, float[batch,1] sum_1, float[batch,1] mul_4&gt;
{
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_sub_2] sub_2 = Sub (sigmoid, buff)
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 1&gt; (lx_1, val_6)
   [node_mul_4] mul_4 = Mul (lx_0, sum_1)
   [node_add_15] add_15 = Add (sub_2, mul_4)
}
</pre></div>
</div>
</section>
</section>
<section id="signaturelistfixedwithnone">
<span id="ledx-model-case-export-signaturelistfixedwithnone"></span><h2>SignatureListFixedWithNone<a class="headerlink" href="#signaturelistfixedwithnone" title="Link to this heading">¶</a></h2>
<section id="id124">
<h3>forward<a class="headerlink" href="#id124" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lx</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">lx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">lx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">lx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">lx</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">lx</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id125">
<h3>custom<a class="headerlink" href="#id125" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Detected mismatch between the structure of `inputs` and `dynamic_shapes`: `inputs[&#39;lx&#39;]` has 3 elements, but `dynamic_shapes[&#39;lx&#39;]` has 2 elements
For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#dynamic-shapes-validation

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
</section>
<section id="id126">
<h3>dynamo-ir<a class="headerlink" href="#id126" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to export the model with torch.export. [96mThis is step 1/3[0m of exporting the model to ONNX. Next steps:
- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.
- Debug `torch.export.export` and summit a PR to PyTorch.
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.

## Exception summary

&lt;class &#39;torch._dynamo.exc.UserError&#39;&gt;: Detected mismatch between the structure of `inputs` and `dynamic_shapes`: `inputs[&#39;lx&#39;]` has 3 elements, but `dynamic_shapes[&#39;lx&#39;]` has 2 elements
For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#dynamic-shapes-validation

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="signaturelistvariablelength">
<span id="ledx-model-case-export-signaturelistvariablelength"></span><h2>SignatureListVariableLength<a class="headerlink" href="#signaturelistvariablelength" title="Link to this heading">¶</a></h2>
<section id="id127">
<h3>forward<a class="headerlink" href="#id127" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lx</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">lx</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buff</span> <span class="o">+</span> <span class="n">t</span>
</pre></div>
</div>
</section>
<section id="id128">
<h3>custom<a class="headerlink" href="#id128" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#3[T1s8x1,T1s8x2,T1s8x3])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;4&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;28&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;6&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],#2[T1s4x1[10.0,13.0:A11.5],T1s4x2[10.0,17.0:A13.5]])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;lx&#39;: [{0: Dim(&#39;batch&#39;, min=0)}, {0: Dim(&#39;batch&#39;, min=0)}],
 &#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s50&#39;, &#39;s77&#39;, &#39;s53&#39;},
 &#39;s50&#39;: {&#39;batch&#39;},
 &#39;s53&#39;: {&#39;batch&#39;},
 &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] output_0) 
   &lt;float[1] b_buff =  {0.5}, int64[1] init7_s1_1 =  {1}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {-0.186112,-0.327459,-0.355459}, float[1] &quot;linear.bias&quot; =  {0.253551}, float[batch,1] sub, float[batch,1] linear, float[batch,1] sum_1, float[1,3] p_linear_weight, int64[2] &quot;init7_s2_1_-1&quot;, float[batch,1] add, float[batch,1] _onx_matmul_x, float[batch,3] cat, float[batch,1] sigmoid, float[3,1] &quot;p_linear_weight::T10&quot;, float[1,3] &quot;linear.weight&quot;, float[1] p_linear_bias&gt;
{
   [cat] cat = Concat &lt;axis: int = 1&gt; (lx_0, lx_1)
   [sum] sum_1 = ReduceSum &lt;keepdims: int = 1&gt; (cat, init7_s1_1)
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [sub_Tensor] sub = Sub (sigmoid, b_buff)
   [add_Tensor] output_0 = Add (sub, sum_1)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>diff.1
</pre></div>
</div>
</section>
<section id="id129">
<h3>dynamo-ir<a class="headerlink" href="#id129" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#2[(T1s4x3,#2[T1s4x1,T1s4x2]),(T1s8x3,#3[T1s8x1,T1s8x2,T1s8x3])]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},lx:#2[{0:Dim(batch)},{0:Dim(batch)}])</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x, float[batch,1] lx_0, float[batch,2] lx_1) =&gt; (float[batch,1] add_15) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {0.314438,0.0142249,0.0971633}, float[1] &quot;linear.bias&quot; =  {0.437725}, float[1] buff =  {0.5}, int64[1] val_6 =  {1}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, float[1] buff, int64[1] val_6, float[batch,3] cat, float[batch,1] sum_1, float[batch,1] linear, float[batch,1] sigmoid, float[batch,1] sub_4&gt;
{
   [node_cat] cat = Concat &lt;axis: int = 1&gt; (lx_0, lx_1)
   [node_sum_1] sum_1 = ReduceSum &lt;noop_with_empty_axes: int = 0, keepdims: int = 1&gt; (cat, val_6)
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_sub_4] sub_4 = Sub (sigmoid, buff)
   [node_add_15] add_15 = Add (sub_4, sum_1)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>diff.1
</pre></div>
</div>
</section>
</section>
<section id="signatureshapeasindex">
<span id="ledx-model-case-export-signatureshapeasindex"></span><h2>SignatureShapeAsIndex<a class="headerlink" href="#signatureshapeasindex" title="Link to this heading">¶</a></h2>
<section id="id130">
<h3>forward<a class="headerlink" href="#id130" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">t</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="id131">
<h3>custom<a class="headerlink" href="#id131" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x3,T1s4x2)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},y:{0:Dim(batch),1:Dim(length)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;4&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;32&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;5&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x3[10.0,21.0:A15.5],T1s4x2[10.0,17.0:A13.5])&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0, max=1024)},
 &#39;y&#39;: {0: Dim(&#39;batch&#39;, min=0, max=1024), 1: Dim(&#39;length&#39;, min=0, max=2)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;, &#39;s17&#39;},
 &#39;length&#39;: {&#39;s94&#39;},
 &#39;s17&#39;: {&#39;batch&#39;},
 &#39;s77&#39;: {&#39;batch&#39;},
 &#39;s94&#39;: {&#39;length&#39;}}&quot;, &quot;_known_value_shapes&quot;: &quot;{
  sym_size_int_3: length,
  sym_size_int_3::UnSq0: (&#39;length&#39;,),
  y::Shape1:2: (&#39;length&#39;,),
}&quot;]
&gt;
experiment (float[batch,3] x, float[batch,length] y) =&gt; (float[batch,length] output_0) 
   &lt;int64[1] init7_s1_0 =  {0}, int64[1] init7_s1_1 =  {1}, float[1,3] &quot;GemmTransposePattern--p_linear_weight::T10&quot; =  {-0.392259,-0.561991,0.548349}, float[1] &quot;linear.bias&quot; =  {-0.199248}, int64 sym_size_int_3, int64[1] &quot;y::Shape1:2&quot;, float[batch,1] linear, float[1,3] p_linear_weight, int64[2] &quot;init7_s2_1_-1&quot;, float[1] b_buff, float[batch,3] add, float[batch,1] _onx_matmul_x, int64[1] &quot;sym_size_int_3::UnSq0&quot;, float[batch,1] sigmoid, float[3,1] &quot;p_linear_weight::T10&quot;, float[1,3] &quot;linear.weight&quot;, float[1] p_linear_bias, float[batch,length] slice_1&gt;
{
   [sym_size_int] &quot;y::Shape1:2&quot; = Shape &lt;end: int = 2, start: int = 1&gt; (y)
   [&quot;GemmTransposePattern--MatMulAddPattern--Opset2&quot;] linear = Gemm &lt;transB: int = 1&gt; (x, &quot;GemmTransposePattern--p_linear_weight::T10&quot;, &quot;linear.bias&quot;)
   [sigmoid] sigmoid = Sigmoid (linear)
   [add_Tensor] add = Add (sigmoid, x)
   [slide_Tensor] output_0 = Slice (add, init7_s1_0, &quot;y::Shape1:2&quot;, init7_s1_1)
}
</pre></div>
</div>
</section>
<section id="id132">
<h3>dynamo-ir<a class="headerlink" href="#id132" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x3,T1s4x2)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)},y:{0:Dim(batch),1:Dim(length)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,3] x, float[batch,length] y) =&gt; (float[batch,length] slice_1) 
   &lt;float[1,3] &quot;linear.weight&quot; =  {-0.243717,-0.0719953,0.490348}, float[1] &quot;linear.bias&quot; =  {0.497554}, int64[1] val_1 =  {0}, int64[1] val_8 =  {1}, float[1,3] &quot;linear.weight&quot;, float[1] &quot;linear.bias&quot;, int64[1] val_1, int64[1] val_8, int64[1] val_0, float[batch,1] linear, float[batch,1] sigmoid, float[batch,3] add_6&gt;
{
   [node_Shape_0] val_0 = Shape &lt;end: int = 2, start: int = 1&gt; (y)
   [node_linear] linear = Gemm &lt;beta: float = 1, transB: int = 1, alpha: float = 1, transA: int = 0&gt; (x, &quot;linear.weight&quot;, &quot;linear.bias&quot;)
   [node_sigmoid] sigmoid = Sigmoid (linear)
   [node_add_6] add_6 = Add (sigmoid, x)
   [node_slice_1] slice_1 = Slice (add_6, val_1, val_0, val_8, val_8)
}
</pre></div>
</div>
</section>
</section>
<section id="typebfloat16">
<span id="ledx-model-case-export-typebfloat16"></span><h2>TypeBFloat16<a class="headerlink" href="#typebfloat16" title="Link to this heading">¶</a></h2>
<section id="id133">
<h3>forward<a class="headerlink" href="#id133" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">xb</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">xb</span> <span class="o">+</span> <span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id134">
<h3>custom<a class="headerlink" href="#id134" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 8,
   opset_import: [&quot;&quot; : 18],
   metadata_props: [&quot;large_model&quot;: &quot;False&quot;, &quot;inline&quot;: &quot;True&quot;, &quot;external_threshold&quot;: &quot;1024&quot;, &quot;function_options&quot;: &quot;FunctionOptions()&quot;, &quot;optimize&quot;: &quot;True&quot;, &quot;n_initializers&quot;: &quot;0&quot;, &quot;n_large_initializers&quot;: &quot;0&quot;, &quot;size_initializers&quot;: &quot;0&quot;, &quot;size_large_initializers&quot;: &quot;0&quot;, &quot;n_nodes&quot;: &quot;3&quot;, &quot;n_nodes_other_domain&quot;: &quot;0&quot;, &quot;mask_outputs&quot;: &quot;[True]&quot;, &quot;input_args&quot;: &quot;(T1s4x4[0.10685443878173828,0.9290268421173096:A0.4994276612997055],)&quot;, &quot;input_kwargs&quot;: &quot;None&quot;, &quot;optimizations&quot;: &quot;OptimizationOptions(constant_folding={&#39;Cast&#39;, &#39;Transpose&#39;, &#39;Reshape&#39;, &#39;Concat&#39;}, patterns=[BatchNormalizationPattern(), BatchNormalizationTrainingPattern(), CastLayerNormalizationCastPattern(), CastPattern(), CastCastBinaryPattern(), CastOpCastPattern(), ClipClipPattern(), ComputationCastOpCastPattern(), ConcatGatherPattern(), ConvBiasNullPattern(), DropoutPattern(), ExpandPattern(), ExpandBroadcastPattern(), ExpandSwapPattern(), GeluPattern(), IdentityPattern(), LayerNormalizationPattern(), LayerNormalizationScalePattern(), LeakyReluPattern(), MulMulMulScalarPattern(), ReduceReshapePattern(), ReduceSumNormalizePattern(), ReshapePattern(), ReshapeMatMulReshapePattern(), Reshape2Of3Pattern(), ReshapeReshapeBinaryPattern(), MatMulAddPattern(), GemmTransposePattern(), MatMulReshape2Of3Pattern(), MulMulMatMulPattern(), ReshapeReshapePattern(), RotaryConcatPartPattern(), SameChildrenPattern(), SequenceConstructAtPattern(), SliceSlicePattern(), SlicesSplitPattern(), SoftmaxCrossEntropyLossCastPattern(), SplitConcatPattern(), SqueezeAddPattern(), SqueezeUnsqueezePattern(), Sub1MulPattern(), SwitchOrderBinaryPattern(), SwitchReshapeActivationPattern(), TransposeEqualReshapePattern(), TransposeMatMulPattern(), TransposeReshapeMatMulPattern(), TransposeReshapeTransposePattern(), TransposeTransposePattern(), UnsqueezeEqualPattern(), UnsqueezeUnsqueezePattern()])&quot;, &quot;dynamic_shapes&quot;: &quot;{&#39;x&#39;: {0: Dim(&#39;batch&#39;, min=0)}}&quot;, &quot;_discovered_shape_constraints&quot;: &quot;{&#39;batch&#39;: {&#39;s77&#39;}, &#39;s77&#39;: {&#39;batch&#39;}}&quot;]
&gt;
experiment (float[batch,4] x) =&gt; (float[batch,4] output_0) 
   &lt;bfloat16[batch,4] add, float[batch,4] &quot;add-x&quot;, bfloat16[batch,4] to, float[batch,4] to_1&gt;
{
   [&quot;CastCastBinaryPattern--add_Tensor&quot;] &quot;add-x&quot; = Add (x, x)
   [&quot;CastCastBinaryPattern--add_Tensor2&quot;] add = Cast &lt;to: int = 16&gt; (&quot;add-x&quot;)
   [to_dtype2] output_0 = Cast &lt;to: int = 1&gt; (add)
}
</pre></div>
</div>
</section>
<section id="id135">
<h3>dynamo-ir<a class="headerlink" href="#id135" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>inputs:</strong> <code class="docutils literal notranslate"><span class="pre">#1[(T1s4x4,)]</span></code></p></li>
<li><p><strong>shapes:</strong> <code class="docutils literal notranslate"><span class="pre">dict(x:{0:Dim(batch)})</span></code></p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;
   ir_version: 10,
   opset_import: [&quot;&quot; : 18],
   producer_name: &quot;pytorch&quot;,
   producer_version: &quot;2.9.0.dev20250711+cu126&quot;
&gt;
main_graph (float[batch,4] x) =&gt; (float[batch,4] _to_copy_1) 
   &lt;bfloat16[batch,4] _to_copy, bfloat16[batch,4] add_3&gt;
{
   [node__to_copy] _to_copy = Cast &lt;to: int = 16&gt; (x)
   [node_add_3] add_3 = Add (_to_copy, _to_copy)
   [node__to_copy_1] _to_copy_1 = Cast &lt;to: int = 1&gt; (add_3)
}
</pre></div>
</div>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(14) node with name &#39;node_add_3&#39;
</pre></div>
</div>
</section>
</section>
<section id="vmap">
<span id="ledx-model-case-export-vmap"></span><h2>Vmap<a class="headerlink" href="#vmap" title="Link to this heading">¶</a></h2>
<section id="id136">
<h3>forward<a class="headerlink" href="#id136" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># noqa: E731</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id137">
<h3>custom<a class="headerlink" href="#id137" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># no error found for the failure
</pre></div>
</div>
</section>
<section id="id138">
<h3>dynamo-ir<a class="headerlink" href="#id138" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to export the model with torch.export. [96mThis is step 1/3[0m of exporting the model to ONNX. Next steps:
- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.
- Debug `torch.export.export` and summit a PR to PyTorch.
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.

## Exception summary

&lt;class &#39;AssertionError&#39;&gt;: 

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
</section>
<section id="vmappython">
<span id="ledx-model-case-export-vmappython"></span><h2>VmapPython<a class="headerlink" href="#vmappython" title="Link to this heading">¶</a></h2>
<section id="id139">
<h3>forward<a class="headerlink" href="#id139" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># noqa: E731</span>
    <span class="k">return</span> <span class="n">patched_vmap</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id140">
<h3>custom<a class="headerlink" href="#id140" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>object of type &#39;Node&#39; has no len()
</pre></div>
</div>
</section>
<section id="id141">
<h3>dynamo-ir<a class="headerlink" href="#id141" title="Link to this heading">¶</a></h3>
<p><strong>FAILED</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Failed to export the model with torch.export. [96mThis is step 1/3[0m of exporting the model to ONNX. Next steps:
- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.
- Debug `torch.export.export` and summit a PR to PyTorch.
- Create an issue in the PyTorch GitHub repository against the [96m*torch.export*[0m component and attach the full error stack as well as reproduction scripts.

## Exception summary

&lt;class &#39;TypeError&#39;&gt;: object of type &#39;Node&#39; has no len()

(Refer to the full stack trace above for more information.)
</pre></div>
</div>
</section>
<section id="summary">
<span id="ledx-summary-exported-program"></span><h3>Summary<a class="headerlink" href="#summary" title="Link to this heading">¶</a></h3>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>case</p></th>
<th class="head"><p>custom</p></th>
<th class="head"><p>ir</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-atenasstrided"><span class="std std-ref">AtenAsStrided</span></a></p></td>
<td><p>FAIL</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-ateninterpolate"><span class="std std-ref">AtenInterpolate</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-atennonzero"><span class="std std-ref">AtenNonZero</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-atennonzerotuple"><span class="std std-ref">AtenNonZeroTuple</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-atenrollpos"><span class="std std-ref">AtenRollPos</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-atenrollrelu"><span class="std std-ref">AtenRollRelu</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-buildinisinstance"><span class="std std-ref">BuildInIsInstance</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-buildinlen"><span class="std std-ref">BuildInLen</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-complexpolar"><span class="std std-ref">ComplexPolar</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowcond"><span class="std std-ref">ControlFlowCond</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowcond2inputs"><span class="std std-ref">ControlFlowCond2Inputs</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowcond2outputs"><span class="std std-ref">ControlFlowCond2Outputs</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondconstant"><span class="std std-ref">ControlFlowCondConstant</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondidentity-153832"><span class="std std-ref">ControlFlowCondIdentity_153832</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondnestedmodule"><span class="std std-ref">ControlFlowCondNestedModule</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowcondnonzero"><span class="std std-ref">ControlFlowCondNonZero</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflownestcond"><span class="std std-ref">ControlFlowNestCond</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowscan"><span class="std std-ref">ControlFlowScan</span></a></p></td>
<td></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowscan2carried"><span class="std std-ref">ControlFlowScan2Carried</span></a></p></td>
<td></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowscancdist"><span class="std std-ref">ControlFlowScanCDist</span></a></p></td>
<td></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowscancdist2"><span class="std std-ref">ControlFlowScanCDist2</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowscancdistxy"><span class="std std-ref">ControlFlowScanCDistXY</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowscandecomposition-151564"><span class="std std-ref">ControlFlowScanDecomposition_151564</span></a></p></td>
<td></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-controlflowscaninplace-153705"><span class="std std-ref">ControlFlowScanInplace_153705</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-createfromshape"><span class="std std-ref">CreateFromShape</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-createfromshapethroughfunction"><span class="std std-ref">CreateFromShapeThroughFunction</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-croplastdimensionwithtensorcontent"><span class="std std-ref">CropLastDimensionWithTensorContent</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-croplastdimensionwithtensorshape"><span class="std std-ref">CropLastDimensionWithTensorShape</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-inplaceadd"><span class="std std-ref">InplaceAdd</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-inplaceadd2"><span class="std std-ref">InplaceAdd2</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-inplaceadd-mul"><span class="std std-ref">InplaceAdd_Mul</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-inplacecloneadd"><span class="std std-ref">InplaceCloneAdd_</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemellipsis-1"><span class="std std-ref">InplaceSetItemEllipsis_1</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemellipsis-2"><span class="std std-ref">InplaceSetItemEllipsis_2</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemmask"><span class="std std-ref">InplaceSetItemMask</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemsquare"><span class="std std-ref">InplaceSetItemSquare</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemsquareadd"><span class="std std-ref">InplaceSetItemSquareAdd</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-inplacesetitemsquareadd2"><span class="std std-ref">InplaceSetItemSquareAdd2</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-signaturefloat1"><span class="std std-ref">SignatureFloat1</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-signatureint1"><span class="std std-ref">SignatureInt1</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-signatureint2"><span class="std std-ref">SignatureInt2</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-signaturelistfixedlength"><span class="std std-ref">SignatureListFixedLength</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-signaturelistfixedwithnone"><span class="std std-ref">SignatureListFixedWithNone</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-signaturelistvariablelength"><span class="std std-ref">SignatureListVariableLength</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-signatureshapeasindex"><span class="std std-ref">SignatureShapeAsIndex</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-typebfloat16"><span class="std std-ref">TypeBFloat16</span></a></p></td>
<td></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ledx-model-case-export-vmap"><span class="std std-ref">Vmap</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ledx-model-case-export-vmappython"><span class="std std-ref">VmapPython</span></a></p></td>
<td><p>FAIL</p></td>
<td><p>FAIL</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="patches_coverage.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Coverage of the Patches</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="exported_program_dynamic.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Exported Programs with Dynamic Shapes</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=ffb1490a"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    </body>
</html>