{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export a LLM through method generate (with Tiny-LLM)\n\nThe main issue when exporting a LLM is the example on HuggingFace is\nbased on method generate but we only need to export the forward method.\nExample `l-plot-tiny-llm-export` gives details on how to guess\ndummy inputs and dynamic shapes to do so.\nLet's see how to simplify that.\n\n## Dummy Example\n\nLet's use the example provided on\n[arnir0/Tiny-LLM](https://huggingface.co/arnir0/Tiny-LLM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.export.api import method_to_onnx\n\n\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n\ndef generate_text(\n    prompt, model, tokenizer, max_length=50, temperature=1, top_k=50, top_p=0.95\n):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        do_sample=True,\n    )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n\n# Define your prompt\nprompt = \"Continue: it rains, what should I do?\"\ngenerated_text = generate_text(prompt, model, tokenizer)\nprint(\"-----------------\")\nprint(generated_text)\nprint(\"-----------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace forward method\n\nWe now modify the model to export the model by replacing the forward method.\nWe still call method ``generate`` but this one will call a different function\ncreated by :func:`onnx_diagnostic.export.api.method_to_onnx`.\nThis one captured the inputs of the forward method, 2 calls are needed or\nat least, 3 are recommended for LLMs as the first call does not contain any cache.\nIf the default settings do not work, ``skip_kwargs_names`` and ``dynamic_shapes``\ncan be changed to remove some undesired inputs or add more dynamic dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"plot_export_tiny_llm_method_generate.custom.onnx\"\nforward_replacement = method_to_onnx(\n    model,\n    method_name=\"forward\",  # default value\n    exporter=\"custom\",  # onnx-dynamo to use the official exporter\n    filename=filename,  # onnx file to create\n    patch_kwargs=dict(patch_transformers=True),  # patches before eporting\n    # to see the progress, it is recommended on the first try to see\n    # how to set ``skip_kwargs_names`` and ``dynamic_shapes`` if it is needed\n    verbose=1,\n    # triggers the ONNX conversion after 3 calls to forward method,\n    # the onnx version is triggered with the last one,\n    # the others are used to infer the dynamic shapes if they are not\n    # specified below\n    convert_after_n_calls=3,\n    # The input used in the example has a batch size equal to 1, all\n    # inputs going through method forward will have the same batch size.\n    # To force the dynamism of this dimension, we need to indicate\n    # which inputs have a batch size.\n    dynamic_batch_for={\"input_ids\", \"attention_mask\", \"past_key_values\"},\n    # Earlier versions of pytorch did not accept a dynamic batch size equal to 1,\n    # this last parameter can be added to expand some inputs if the batch size is 1.\n    # The exporter should work without.\n    expand_batch_for={\"input_ids\", \"attention_mask\", \"past_key_values\"},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dynamic shapes can be inferred from at least two calls to the forward method,\n3 is better for LLMs (first call is prefill, cache is missing),\nyou can see the inference results with ``verbose=1``.\nIf the value is not the expected one (to change the names for example),\nThey can be overwritten.\n\n```python\ndynamic_shapes={\n    \"cache_position\": {0: \"sequence_length\"},\n    \"past_key_values\": [\n        {0: \"batch_size\", 2: \"past_sequence_length\"},\n        {0: \"batch_size\", 2: \"past_sequence_length\"},\n    ],\n    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n    \"attention_mask\": {0: \"batch_size\", 1: \"total_sequence_length\"},\n}\n```\nFinally, we need to replace the forward method.\nAs ``forward_replacement`` is a module of type\n:class:`onnx_diagnostic.export.api.WrapperToExportMethodToOnnx`,\na lambda function must be used to avoid this one to be\nincluded as a submodule (and create an infinite loop).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"type(forward_replacement)={type(forward_replacement)}\")\nmodel.forward = lambda *args, **kwargs: forward_replacement(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's call generate again. The conversion is triggered after\n``convert_after_n_calls=3`` calls to the method forward,\nwhich exactly what the method generate is doing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "generated_text = generate_text(prompt, model, tokenizer)\nprint(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We finally need to check the discrepancies.\nThe exports produced an onnx file and dumped the input and output\nof the torch model. We now run the onnx model to check\nit produces the same results.\nIt is done after because the model may not hold twice in memory\n(torch and onnxruntime).\nverbose=2 shows more information about expected outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = forward_replacement.check_discrepancies(verbose=1)\ndf = pandas.DataFrame(data)\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimal script to export a LLM\n\nThe following lines are a condensed copy with less comments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# from HuggingFace\nprint(\"----------------\")\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n# to export into onnx\nforward_replacement = method_to_onnx(\n    model,\n    method_name=\"forward\",\n    exporter=\"onnx-dynamo\",\n    filename=\"plot_export_tiny_llm_method_generate.dynamo.onnx\",\n    patch_kwargs=dict(patch_transformers=True),\n    verbose=0,\n    convert_after_n_calls=3,\n    dynamic_batch_for={\"input_ids\", \"attention_mask\", \"past_key_values\"},\n)\nmodel.forward = lambda *args, **kwargs: forward_replacement(*args, **kwargs)\n\n# from HuggingFace again\nprompt = \"Continue: it rains, what should I do?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    max_length=100,\n    temperature=1,\n    top_k=50,\n    top_p=0.95,\n    do_sample=True,\n)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"prompt answer:\", generated_text)\n\n# to check discrepancies\ndata = forward_replacement.check_discrepancies()\ndf = pandas.DataFrame(data)\nprint(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.save_fig(doc.plot_dot(filename), f\"{filename}.png\", dpi=400)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}