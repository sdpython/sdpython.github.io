
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_technical/plot_layer_norm_discrepancies.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_technical_plot_layer_norm_discrepancies.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_technical_plot_layer_norm_discrepancies.py:


LayerNormalization implementation cannot be exchanged
=====================================================

This example applies what was illustrated
:ref:`l-plot-parallelized-reduction`, reduction operations
are sensitive to parallelization.

Methodology
+++++++++++

We consider a simple model with a LayerNormalization followed by a MatMul.
Each operator can be run with :epkg:`onnxruntime` or :epkg:`pytorch`.
We compare the four combinations.

The model
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 19-65

.. code-block:: Python


    import itertools
    import numpy as np
    import pandas
    import onnx
    import onnx.helper as oh
    import onnxruntime
    import torch
    from onnx_diagnostic.doc import rotate_align, save_fig, plot_histogram, title, plot_dot
    from onnx_diagnostic.ext_test_case import unit_test_going
    from onnx_diagnostic.helpers import max_diff, string_diff, string_type
    from onnx_diagnostic.helpers.onnx_helper import onnx_dtype_name, onnx_dtype_to_np_dtype
    from onnx_diagnostic.helpers.torch_helper import onnx_dtype_to_torch_dtype
    from onnx_diagnostic.helpers.doc_helper import LayerNormalizationOrt, MatMulOrt
    from onnx_diagnostic.reference import TorchOnnxEvaluator

    TFLOAT = onnx.TensorProto.FLOAT
    TFLOAT16 = onnx.TensorProto.FLOAT16


    def get_model(itype: int = TFLOAT16):
        return oh.make_model(
            oh.make_graph(
                [
                    oh.make_node("LayerNormalization", ["X", "scale", "bias"], ["norm"], axis=-1),
                    oh.make_node("MatMul", ["norm", "weights"], ["mm"]),
                    oh.make_node("Add", ["mm", "bias2"], ["Z"]),
                ],
                "layer_norm_matmul_add",
                [
                    oh.make_tensor_value_info("X", itype, ["a", "b", "c"]),
                    oh.make_tensor_value_info("scale", itype, ["c"]),
                    oh.make_tensor_value_info("bias", itype, ["c"]),
                    oh.make_tensor_value_info("weights", itype, ["c", "c"]),
                    oh.make_tensor_value_info("bias2", itype, ["c"]),
                ],
                [oh.make_tensor_value_info("Z", itype, ["a", "b", "c"])],
            ),
            ir_version=9,
            opset_imports=[oh.make_opsetid("", 18)],
        )


    model = get_model()
    plot_dot(model)




.. image-sg:: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_001.png
   :alt: plot layer norm discrepancies
   :srcset: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 66-71

Let's compare two runtimes
++++++++++++++++++++++++++

That will be :epkg:`onnxruntime` and
:class:`onnx_diagnostic.reference.TorchOnnxEvaluator`.

.. GENERATED FROM PYTHON SOURCE LINES 71-143

.. code-block:: Python


    last_dim = 64 if unit_test_going() else 1152


    def make_feeds(last_dim: int):
        return {
            "X": (torch.rand((32, 1024, last_dim), dtype=torch.float16) - 0.5) * 120,
            "scale": torch.rand((last_dim,), dtype=torch.float16),
            "bias": torch.rand((last_dim,), dtype=torch.float16),
            "weights": torch.rand((last_dim, last_dim), dtype=torch.float16),
            "bias2": torch.rand((last_dim,), dtype=torch.float16),
        }


    def cast_feeds(itype, provider, feeds):
        ttype = onnx_dtype_to_torch_dtype(itype)
        np_dtype = onnx_dtype_to_np_dtype(itype)
        np_feeds = {k: v.detach().numpy() for k, v in feeds.items()}
        if provider == "CUDA":
            if not torch.cuda.is_available():
                return None, None
            tch_feeds = {k: v.to("cuda") for k, v in feeds.items()}
            ort_feeds = np_feeds
        else:
            tch_feeds = feeds.copy()
            tch_feeds["X"] = tch_feeds["X"][:2]  # too long otherwise
            ort_feeds = np_feeds.copy()
            ort_feeds["X"] = ort_feeds["X"][:2]
        tch_feeds = {k: v.to(ttype) for k, v in tch_feeds.items()}
        ort_feeds = {k: v.astype(np_dtype) for k, v in ort_feeds.items()}
        return tch_feeds, ort_feeds


    feeds = make_feeds(last_dim)
    kws = dict(with_shape=True, with_min_max=True, with_device=True)
    data = []
    baseline = {}

    for provider, itype in itertools.product(["CPU", "CUDA"], [TFLOAT, TFLOAT16]):
        tch_feeds, ort_feeds = cast_feeds(itype, provider, feeds)
        if tch_feeds is None:
            continue

        model = get_model(itype)
        print()
        print(f"-- running on {provider} with {onnx_dtype_name(itype)}")
        print("-- running with torch")
        torch_sess = TorchOnnxEvaluator(model, providers=[f"{provider}ExecutionProvider"])
        expected = torch_sess.run(None, tch_feeds)
        baseline[itype, provider, "torch"] = expected
        print(f"-- torch: {string_type(expected, **kws)}")

        print("-- running with ort")
        ort_sess = onnxruntime.InferenceSession(
            model.SerializeToString(), providers=[f"{provider}ExecutionProvider"]
        )
        got = ort_sess.run(None, ort_feeds)
        baseline[itype, provider, "ort"] = got
        print(f"-- ort: {string_type(got, **kws)}")
        diff = max_diff(expected, got, hist=True)
        print(f"-- diff {string_diff(diff)}")

        # memorize the data
        diff["dtype"] = onnx_dtype_name(itype)
        diff["provider"] = provider
        diff.update(diff["rep"])
        del diff["rep"]
        del diff["dnan"]
        del diff[">100.0"]
        del diff[">10.0"]
        data.append(diff)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    -- running on CPU with FLOAT
    -- running with torch
    -- torch: #1[CT1s2x1024x1152[233.81448364257812,327.09344482421875:A278.50962045335393]]
    -- running with ort
    -- ort: #1[A1s2x1024x1152[233.8144989013672,327.09344482421875:A278.5096203755885]]
    -- diff abs=0.000152587890625, rel=5.84550578309402e-07, n=2359296.0/#1478135>0.0-#1772>0.0001, dev=0

    -- running on CPU with FLOAT16
    -- running with torch
    -- torch: #1[CT10s2x1024x1152[233.875,327.25:A278.50880119535657]]
    -- running with ort
    -- ort: #1[A10s2x1024x1152[233.75,327.0:A278.50950892766315]]
    -- diff abs=0.25, rel=0.0009765586853176355, n=2359296.0/#594402>0.0-#594402>0.0001-#594402>0.001-#594402>0.01-#594402>0.1, dev=0

    -- running on CUDA with FLOAT
    -- running with torch
    -- torch: #1[GT1s32x1024x1152[231.80091857910156,327.2825012207031:A278.46603020625963]]
    -- running with ort
    -- ort: #1[A1s32x1024x1152[231.7963409423828,327.2726135253906:A278.46341761429073]]
    -- diff abs=0.01898193359375, rel=6.814625278993499e-05, n=37748736.0/#37646989>0.0-#37032459>0.0001-#31134445>0.001-#422970>0.01, dev=1

    -- running on CUDA with FLOAT16
    -- running with torch
    -- torch: #1[GT10s32x1024x1152[231.75,327.5:A278.46310534742145]]
    -- running with ort
    -- ort: #1[A10s32x1024x1152[231.75,327.5:A278.46310552954674]]
    -- diff abs=0.5, rel=0.0018115876391752205, n=37748736.0/#1527>0.0-#1527>0.0001-#1527>0.001-#1527>0.01-#1527>0.1, dev=1




.. GENERATED FROM PYTHON SOURCE LINES 144-147

.. code-block:: Python

    df = pandas.DataFrame(data).set_index(["provider", "dtype"])
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                           abs           rel            sum           n  dev      >0.0   >0.0001    >0.001   >0.01    >0.1  >1.0
    provider dtype                                                                                                              
    CPU      FLOAT    0.000153  5.845506e-07      56.950104   2359296.0    0   1478135      1772         0       0       0     0
             FLOAT16  0.250000  9.765587e-04  148055.750000   2359296.0    0    594402    594402    594402  594402  594402     0
    CUDA     FLOAT    0.018982  6.814625e-05  127651.713547  37748736.0    1  37646989  37032459  31134445  422970       0     0
             FLOAT16  0.500000  1.811588e-03     378.625000  37748736.0    1      1527      1527      1527    1527    1527     0




.. GENERATED FROM PYTHON SOURCE LINES 148-149

Visually.

.. GENERATED FROM PYTHON SOURCE LINES 149-157

.. code-block:: Python


    save_fig(
        rotate_align(
            df[["abs"]].plot.bar(title="Discrepancies ORT / torch for LayerNorm(X) @ W + B")
        ),
        "plot_layer_norm_discrepancies_1.png",
    )




.. image-sg:: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_002.png
   :alt: Discrepancies ORT / torch for LayerNorm(X) @ W + B
   :srcset: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 158-161

The discrepancies are significant on CUDA, higher for float16.
Let's see which operator is responsible for them,
*LayerNormalization* or *MatMul*.

.. GENERATED FROM PYTHON SOURCE LINES 163-165

Distribution of the results
+++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 165-169

.. code-block:: Python


    tensor = baseline[TFLOAT16, "CPU", "ort"][0].ravel().astype(np.float32)
    print(pandas.DataFrame({"expected": tensor}).describe())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

               expected
    count  2.359296e+06
    mean   2.785095e+02
    std    9.334977e+00
    min    2.337500e+02
    25%    2.722500e+02
    50%    2.785000e+02
    75%    2.847500e+02
    max    3.270000e+02




.. GENERATED FROM PYTHON SOURCE LINES 170-171

Histogram.

.. GENERATED FROM PYTHON SOURCE LINES 171-178

.. code-block:: Python


    save_fig(
        title(plot_histogram(tensor), "Distribution of the computed results"),
        "plot_layer_norm_discrepancies_hist.png",
    )





.. image-sg:: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_003.png
   :alt: Distribution of the computed results
   :srcset: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 179-183

The discrepancies come from?
++++++++++++++++++++++++++++

We mix torch and onnxruntime to execute the kernels.

.. GENERATED FROM PYTHON SOURCE LINES 183-226

.. code-block:: Python


    data = []

    for mod, provider, itype in itertools.product(
        ["ORT-ORT", "ORT-TORCH", "TORCH-ORT", "TORCH-TORCH"], ["CPU", "CUDA"], [TFLOAT, TFLOAT16]
    ):
        ttype = onnx_dtype_to_torch_dtype(itype)
        np_dtype = onnx_dtype_to_np_dtype(itype)
        tch_feeds, _ = cast_feeds(itype, provider, feeds)
        if tch_feeds is None:
            continue

        ker1, ker2 = mod.split("-")
        custom_kernels = (
            {("", "LayerNormalization"): LayerNormalizationOrt} if ker1 == "ORT" else {}
        ) | ({("", "MatMul"): MatMulOrt} if ker2 == "ORT" else {})

        model = get_model(itype)
        print()
        print(f"-- {mod} running on {provider} with {onnx_dtype_name(itype)}")
        sess = TorchOnnxEvaluator(
            model,
            custom_kernels=custom_kernels,
            providers=[f"{provider}ExecutionProvider"],
        )
        got = sess.run(None, tch_feeds)
        print(f"-- {mod}: {string_type(got, **kws)}")

        difft = max_diff(baseline[itype, provider, "torch"], got)
        print(f"-- diff with torch {string_diff(difft)}")
        diffo = max_diff(baseline[itype, provider, "ort"], got)
        print(f"-- diff with ort {string_diff(diffo)}")

        data.append(
            dict(
                model=mod,
                dtype=onnx_dtype_name(itype),
                provider=provider,
                diff_ort=diffo["abs"],
                diff_torch=difft["abs"],
            )
        )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    -- ORT-ORT running on CPU with FLOAT
    -- ORT-ORT: #1[CT1s2x1024x1152[233.8144989013672,327.09344482421875:A278.5096203755885]]
    -- diff with torch abs=0.000152587890625, rel=5.84550578309402e-07, n=2359296.0, dev=0
    -- diff with ort abs=0, rel=0, dev=0

    -- ORT-ORT running on CPU with FLOAT16
    -- ORT-ORT: #1[CT10s2x1024x1152[233.875,327.25:A278.50879096984863]]
    -- diff with torch abs=0.5, rel=0.001801795308845734, n=2359296.0, dev=0
    -- diff with ort abs=0.25, rel=0.0009765586853176355, n=2359296.0, dev=0

    -- ORT-ORT running on CUDA with FLOAT
    -- ORT-ORT: #1[GT1s32x1024x1152[231.7963409423828,327.2726135253906:A278.46341761429073]]
    -- diff with torch abs=0.01898193359375, rel=6.814625278993499e-05, n=37748736.0, dev=0
    -- diff with ort abs=0, rel=0, dev=1

    -- ORT-ORT running on CUDA with FLOAT16
    -- ORT-ORT: #1[GT10s32x1024x1152[231.75,327.5:A278.46310552954674]]
    -- diff with torch abs=0.5, rel=0.0018115876391752205, n=37748736.0, dev=0
    -- diff with ort abs=0, rel=0, dev=1

    -- ORT-TORCH running on CPU with FLOAT
    -- ORT-TORCH: #1[CT1s2x1024x1152[233.81451416015625,327.09344482421875:A278.50962040281024]]
    -- diff with torch abs=0.0001220703125, rel=4.649866900907866e-07, n=2359296.0, dev=0
    -- diff with ort abs=0.000152587890625, rel=5.703125988565602e-07, n=2359296.0, dev=0

    -- ORT-TORCH running on CPU with FLOAT16
    -- ORT-TORCH: #1[CT10s2x1024x1152[233.875,327.25:A278.50879022810193]]
    -- diff with torch abs=0.5, rel=0.001801795308845734, n=2359296.0, dev=0
    -- diff with ort abs=0.25, rel=0.0009765586853176355, n=2359296.0, dev=0

    -- ORT-TORCH running on CUDA with FLOAT
    -- ORT-TORCH: #1[GT1s32x1024x1152[231.80091857910156,327.282470703125:A278.46603020667277]]
    -- diff with torch abs=0.000152587890625, rel=5.363249241263705e-07, n=37748736.0, dev=0
    -- diff with ort abs=0.01898193359375, rel=6.815089701819105e-05, n=37748736.0, dev=1

    -- ORT-TORCH running on CUDA with FLOAT16
    -- ORT-TORCH: #1[GT10s32x1024x1152[231.75,327.5:A278.46310552954674]]
    -- diff with torch abs=0.5, rel=0.0018115876391752205, n=37748736.0, dev=0
    -- diff with ort abs=0, rel=0, dev=1

    -- TORCH-ORT running on CPU with FLOAT
    -- TORCH-ORT: #1[CT1s2x1024x1152[233.81448364257812,327.09344482421875:A278.50962040870866]]
    -- diff with torch abs=0.000152587890625, rel=5.645968356365458e-07, n=2359296.0, dev=0
    -- diff with ort abs=0.0001220703125, rel=4.2071841428779394e-07, n=2359296.0, dev=0

    -- TORCH-ORT running on CPU with FLOAT16
    -- TORCH-ORT: #1[CT10s2x1024x1152[233.875,327.25:A278.50880347357855]]
    -- diff with torch abs=0.25, rel=0.0009699283416941157, n=2359296.0, dev=0
    -- diff with ort abs=0.25, rel=0.0009765586853176355, n=2359296.0, dev=0

    -- TORCH-ORT running on CUDA with FLOAT
    -- TORCH-ORT: #1[GT1s32x1024x1152[231.7963409423828,327.2726135253906:A278.4634175840652]]
    -- diff with torch abs=0.018463134765625, rel=6.555427086159538e-05, n=37748736.0, dev=0
    -- diff with ort abs=0.001953125, rel=7.584448297717083e-06, n=37748736.0, dev=1

    -- TORCH-ORT running on CUDA with FLOAT16
    -- TORCH-ORT: #1[GT10s32x1024x1152[231.75,327.5:A278.46310534742145]]
    -- diff with torch abs=0, rel=0, dev=0
    -- diff with ort abs=0.5, rel=0.0018148754450982032, n=37748736.0, dev=1

    -- TORCH-TORCH running on CPU with FLOAT
    -- TORCH-TORCH: #1[CT1s2x1024x1152[233.81448364257812,327.09344482421875:A278.50962045335393]]
    -- diff with torch abs=0, rel=0, dev=0
    -- diff with ort abs=0.000152587890625, rel=5.845502366102232e-07, n=2359296.0, dev=0

    -- TORCH-TORCH running on CPU with FLOAT16
    -- TORCH-TORCH: #1[CT10s2x1024x1152[233.875,327.25:A278.50880119535657]]
    -- diff with torch abs=0, rel=0, dev=0
    -- diff with ort abs=0.25, rel=0.0009765586853176355, n=2359296.0, dev=0

    -- TORCH-TORCH running on CUDA with FLOAT
    -- TORCH-TORCH: #1[GT1s32x1024x1152[231.80091857910156,327.2825012207031:A278.46603020625963]]
    -- diff with torch abs=0, rel=0, dev=0
    -- diff with ort abs=0.01898193359375, rel=6.815089701819105e-05, n=37748736.0, dev=1

    -- TORCH-TORCH running on CUDA with FLOAT16
    -- TORCH-TORCH: #1[GT10s32x1024x1152[231.75,327.5:A278.46310534742145]]
    -- diff with torch abs=0, rel=0, dev=0
    -- diff with ort abs=0.5, rel=0.0018148754450982032, n=37748736.0, dev=1




.. GENERATED FROM PYTHON SOURCE LINES 227-231

.. code-block:: Python

    df = pandas.DataFrame(data).set_index(["dtype", "provider", "model"])
    df = df.sort_index()
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                  diff_ort  diff_torch
    dtype   provider model                            
    FLOAT   CPU      ORT-ORT      0.000000    0.000153
                     ORT-TORCH    0.000153    0.000122
                     TORCH-ORT    0.000122    0.000153
                     TORCH-TORCH  0.000153    0.000000
            CUDA     ORT-ORT      0.000000    0.018982
                     ORT-TORCH    0.018982    0.000153
                     TORCH-ORT    0.001953    0.018463
                     TORCH-TORCH  0.018982    0.000000
    FLOAT16 CPU      ORT-ORT      0.250000    0.500000
                     ORT-TORCH    0.250000    0.500000
                     TORCH-ORT    0.250000    0.250000
                     TORCH-TORCH  0.250000    0.000000
            CUDA     ORT-ORT      0.000000    0.500000
                     ORT-TORCH    0.000000    0.500000
                     TORCH-ORT    0.500000    0.000000
                     TORCH-TORCH  0.500000    0.000000




.. GENERATED FROM PYTHON SOURCE LINES 232-233

Visually.

.. GENERATED FROM PYTHON SOURCE LINES 233-244

.. code-block:: Python


    save_fig(
        rotate_align(
            df[["diff_ort", "diff_torch"]].plot.bar(
                title="ORT/Torch or Torch/ORT for LayerNorm(X) @ W + B",
                figsize=(10, 4),
            )
        ),
        "plot_layer_norm_discrepancies_2.png",
    )




.. image-sg:: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_004.png
   :alt: ORT/Torch or Torch/ORT for LayerNorm(X) @ W + B
   :srcset: /auto_technical/images/sphx_glr_plot_layer_norm_discrepancies_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 245-251

Conclusion
++++++++++

:epkg:`torch` seems able to replicate the same results if the same computation
is run multiple times. :epkg:`onnxruntime` is only able to do that on CUDA.
With float16 and CUDA, LayerNormalization seems to introduce some discrepancies.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 53.702 seconds)


.. _sphx_glr_download_auto_technical_plot_layer_norm_discrepancies.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_layer_norm_discrepancies.ipynb <plot_layer_norm_discrepancies.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_layer_norm_discrepancies.py <plot_layer_norm_discrepancies.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_layer_norm_discrepancies.zip <plot_layer_norm_discrepancies.zip>`


.. include:: plot_layer_norm_discrepancies.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
