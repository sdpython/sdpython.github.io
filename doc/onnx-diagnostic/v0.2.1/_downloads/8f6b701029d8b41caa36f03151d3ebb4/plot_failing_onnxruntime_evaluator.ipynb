{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Intermediate results with onnxruntime\n\nExample `l-plot-failing-reference-evaluator` demonstrated\nhow to run a python runtime on a model but it may very slow sometimes\nand it could show some discrepancies if the only provider is not CPU.\nLet's use :class:`OnnxruntimeEvaluator <onnx_diagnostic.reference.OnnxruntimeEvaluator>`.\nIt splits the model into node and runs them independently until it succeeds\nor fails. This class converts every node into model based on the types\ndiscovered during the execution. It relies on :class:`InferenceSessionForTorch\n<onnx_diagnostic.ort_session.InferenceSessionForTorch>` or\n:class:`InferenceSessionForNumpy\n<onnx_diagnostic.ort_session.InferenceSessionForNumpy>`\nfor the execution. This example uses torch tensor and\nbfloat16.\n\n## A failing model\n\nThe issue here is a an operator ``Cast`` trying to convert a result\ninto a non-existing type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nimport onnx.helper as oh\nimport torch\nimport onnxruntime\nfrom onnx_diagnostic.ext_test_case import has_cuda\nfrom onnx_diagnostic.helpers import from_array_extended\nfrom onnx_diagnostic.reference import OnnxruntimeEvaluator\n\nTBFLOAT16 = onnx.TensorProto.BFLOAT16\n\nmodel = oh.make_model(\n    oh.make_graph(\n        [\n            oh.make_node(\"Mul\", [\"X\", \"Y\"], [\"xy\"], name=\"n0\"),\n            oh.make_node(\"Sigmoid\", [\"xy\"], [\"sy\"], name=\"n1\"),\n            oh.make_node(\"Add\", [\"sy\", \"one\"], [\"C\"], name=\"n2\"),\n            oh.make_node(\"Cast\", [\"C\"], [\"X999\"], to=999, name=\"failing\"),\n            oh.make_node(\"CastLike\", [\"X999\", \"Y\"], [\"Z\"], name=\"n4\"),\n        ],\n        \"-nd-\",\n        [\n            oh.make_tensor_value_info(\"X\", TBFLOAT16, [\"a\", \"b\", \"c\"]),\n            oh.make_tensor_value_info(\"Y\", TBFLOAT16, [\"a\", \"b\", \"c\"]),\n        ],\n        [oh.make_tensor_value_info(\"Z\", TBFLOAT16, [\"a\", \"b\", \"c\"])],\n        [from_array_extended(torch.tensor([1], dtype=torch.bfloat16), name=\"one\")],\n    ),\n    opset_imports=[oh.make_opsetid(\"\", 18)],\n    ir_version=9,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check it is failing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    onnxruntime.InferenceSession(model.SerializeToString(), providers=[\"CPUExecutionProvider\"])\nexcept onnxruntime.capi.onnxruntime_pybind11_state.Fail as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OnnxruntimeEvaluator\n\nThis class extends :class:`onnx.reference.ReferenceEvaluator`\nwith operators outside the standard but defined by :epkg:`onnxruntime`.\n`verbose=10` tells the class to print as much as possible,\n`verbose=0` prints nothing. Intermediate values for more or less verbosity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref = OnnxruntimeEvaluator(model, verbose=10)\nfeeds = dict(\n    X=torch.rand((3, 4), dtype=torch.bfloat16), Y=torch.rand((3, 4), dtype=torch.bfloat16)\n)\ntry:\n    ref.run(None, feeds)\nexcept Exception as e:\n    print(\"ERROR\", type(e), e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":epkg:`onnxruntime` may not support bfloat16 on CPU.\nSee :epkg:`onnxruntime kernels`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if has_cuda():\n    ref = OnnxruntimeEvaluator(model, providers=\"cuda\", verbose=10)\n    feeds = dict(\n        X=torch.rand((3, 4), dtype=torch.bfloat16), Y=torch.rand((3, 4), dtype=torch.bfloat16)\n    )\n    try:\n        ref.run(None, feeds)\n    except Exception as e:\n        print(\"ERROR\", type(e), e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see it run until it reaches `Cast` and stops.\nThe error message is not always obvious to interpret.\nIt gets improved every time from time to time.\nThis runtime is useful when it fails for a numerical reason.\nIt is possible to insert prints in the python code to print\nmore information or debug if needed.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}