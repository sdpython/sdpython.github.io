
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_with_dynamic_cache.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_with_dynamic_cache.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_with_dynamic_cache.py:


.. _l-plot-export-with-dynamic-shape:

===========================================
Export with DynamicCache and dynamic shapes
===========================================

Every LLMs implemented in :epkg:`transformers` use cache.
One of the most used is :class:`transformers.cache_utils.DynamicCache`.
The cache size is dynamic to cope with the growing context.
The example shows a tool which determines the dynamic shapes
for :func:`torch.export.export` based on a set of valid inputs.

Simple Examples
===============

We first look at examples playing positional and names parameters
to understand how :func:`torch.export.export` works.

args
++++

.. GENERATED FROM PYTHON SOURCE LINES 23-30

.. code-block:: Python


    import pprint
    import torch
    from onnx_diagnostic.cache_helpers import make_dynamic_cache
    from onnx_diagnostic.helpers import string_type
    from onnx_diagnostic.export import ModelInputs








.. GENERATED FROM PYTHON SOURCE LINES 31-33

We need addition import in case ``transformers<4.50``.
Exporting DynamicCache is not supported before that.

.. GENERATED FROM PYTHON SOURCE LINES 33-50

.. code-block:: Python

    from onnx_diagnostic.ext_test_case import has_transformers
    from onnx_diagnostic.torch_export_patches import bypass_export_some_errors


    class Model(torch.nn.Module):
        def forward(self, x, y):
            return x + y


    model = Model()
    x = torch.randn((5, 6))
    y = torch.randn((1, 6))
    model(x, y)  # to check it works

    ep = torch.export.export(model, (x, y))
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[5, 6]", y: "f32[1, 6]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_cache.py:39 in forward, code: return x + y
                add: "f32[5, 6]" = torch.ops.aten.add.Tensor(x, y);  x = y = None
                return (add,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
    
        # outputs
        add: USER_OUTPUT
    
    Range constraints: {}





.. GENERATED FROM PYTHON SOURCE LINES 51-56

As expected there is no dynamic shapes.
We use :class:`onnx_diagnostic.export.ModelInputs`
to define them from two set of valid inputs.
These inputs must have different value for the dynamic
dimensions.

.. GENERATED FROM PYTHON SOURCE LINES 56-62

.. code-block:: Python


    inputs = [(x, y), (torch.randn((7, 8)), torch.randn((1, 8)))]
    mi = ModelInputs(Model(), inputs)
    ds = mi.guess_dynamic_shapes()
    pprint.pprint(ds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (({0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
       1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)},
      {1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)}),
     {})




.. GENERATED FROM PYTHON SOURCE LINES 63-67

The function returns a tuple with two objects.
The first one for the positional arguments, the other one
for the named arguments. There is no named arguments. We
we used the first result to export.

.. GENERATED FROM PYTHON SOURCE LINES 67-71

.. code-block:: Python


    ep = torch.export.export(model, (x, y), dynamic_shapes=ds[0])
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s0, s1]", y: "f32[1, s1]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_cache.py:39 in forward, code: return x + y
                add: "f32[s0, s1]" = torch.ops.aten.add.Tensor(x, y);  x = y = None
                return (add,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
    
        # outputs
        add: USER_OUTPUT
    
    Range constraints: {s0: VR[2, int_oo], s1: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 72-76

kwargs
++++++

We do the same with named arguments.

.. GENERATED FROM PYTHON SOURCE LINES 76-88

.. code-block:: Python



    class Model(torch.nn.Module):
        def forward(self, x, y):
            return x + y


    model = Model()
    x = torch.randn((5, 6))
    y = torch.randn((1, 6))
    model(x=x, y=y)  # to check it works





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[-0.5369,  0.1690, -0.4195,  1.9897, -2.2059,  0.6071],
            [ 0.9976, -0.6973, -0.7654,  1.3082, -0.5183, -0.4533],
            [-0.2331,  1.8189, -0.7368, -0.5109, -1.4472, -0.7668],
            [-1.7635,  0.7107, -2.7969, -0.7911, -1.1228, -0.6222],
            [-2.2149, -0.5431,  0.5612, -0.8355, -2.1281, -0.2546]])



.. GENERATED FROM PYTHON SOURCE LINES 89-90

Two sets of valid inputs.

.. GENERATED FROM PYTHON SOURCE LINES 90-95

.. code-block:: Python

    inputs = [dict(x=x, y=y), dict(x=torch.randn((7, 8)), y=torch.randn((1, 8)))]
    mi = ModelInputs(Model(), inputs)
    ds = mi.guess_dynamic_shapes()
    pprint.pprint(ds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ((),
     {'x': {0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
            1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)},
      'y': {1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)}})




.. GENERATED FROM PYTHON SOURCE LINES 96-97

And we export.

.. GENERATED FROM PYTHON SOURCE LINES 97-100

.. code-block:: Python

    ep = torch.export.export(model, (), kwargs=dict(x=x, y=y), dynamic_shapes=ds[1])
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s0, s1]", y: "f32[1, s1]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_cache.py:80 in forward, code: return x + y
                add: "f32[s0, s1]" = torch.ops.aten.add.Tensor(x, y);  x = y = None
                return (add,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
    
        # outputs
        add: USER_OUTPUT
    
    Range constraints: {s0: VR[2, int_oo], s1: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 101-106

args and kwargs
+++++++++++++++

:func:`torch.export.export` does not like having dynami shapes
for both args and kwargs. We need to define them using one mechanism.

.. GENERATED FROM PYTHON SOURCE LINES 106-118

.. code-block:: Python



    class Model(torch.nn.Module):
        def forward(self, x, y):
            return x + y


    model = Model()
    x = torch.randn((5, 6))
    y = torch.randn((1, 6))
    model(x, y=y)  # to check it works





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[ 0.5945, -0.4268,  1.4064, -0.1249, -1.6105,  0.9724],
            [-0.6517,  0.6470,  2.0669, -0.1615, -1.0284, -2.1491],
            [ 0.6797,  0.2155,  0.4251,  1.6503, -2.0112, -0.1554],
            [ 0.7816,  0.6177, -0.4063,  0.5227, -0.0102,  0.9325],
            [-0.7742,  0.5255,  0.9321,  0.6020, -1.4119, -0.1311]])



.. GENERATED FROM PYTHON SOURCE LINES 119-120

Two sets of valid inputs with positional and names arguments.

.. GENERATED FROM PYTHON SOURCE LINES 120-126

.. code-block:: Python


    inputs = [((x,), dict(y=y)), ((torch.randn((7, 8)),), dict(y=torch.randn((1, 8))))]
    mi = ModelInputs(Model(), inputs)
    ds = mi.guess_dynamic_shapes()
    pprint.pprint(ds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (({0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
       1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)},),
     {'y': {1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)}})




.. GENERATED FROM PYTHON SOURCE LINES 127-131

This does not work with :func:`torch.export.export` so
we use a method to move the positional dynamic shapes to
named one. The method relies on the signature of the
forward method.

.. GENERATED FROM PYTHON SOURCE LINES 131-135

.. code-block:: Python


    new_args, new_kwargs, new_ds = mi.move_to_kwargs(*mi.inputs[0], ds)
    pprint.pprint(new_ds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ((),
     {'x': {0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
            1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)},
      'y': {1: _DimHint(type=<_DimHintType.DYNAMIC: 3>)}})




.. GENERATED FROM PYTHON SOURCE LINES 136-137

And we export.

.. GENERATED FROM PYTHON SOURCE LINES 137-141

.. code-block:: Python


    ep = torch.export.export(model, new_args, kwargs=new_kwargs, dynamic_shapes=new_ds[1])
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s0, s1]", y: "f32[1, s1]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_cache.py:110 in forward, code: return x + y
                add: "f32[s0, s1]" = torch.ops.aten.add.Tensor(x, y);  x = y = None
                return (add,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
    
        # outputs
        add: USER_OUTPUT
    
    Range constraints: {s0: VR[2, int_oo], s1: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 142-149

DynamicCache
============

:func:`torch.export.export` serializes caches and any custom class
if these serialization functions are provided with is the case for
:class:`transformers.cache_utils.DynamicCache` and ``transformers>=4.50``.
The dynamic shapes must be provided following the serialized form.

.. GENERATED FROM PYTHON SOURCE LINES 149-175

.. code-block:: Python



    class Model(torch.nn.Module):
        def forward(self, cache, z):
            return (
                z
                + cache.key_cache[0]
                + cache.key_cache[1]
                + cache.value_cache[0]
                + cache.value_cache[1]
            )


    model = Model()

    n_layers = 2
    bsize, nheads, slen, dim = 2, 4, 3, 7
    cache = make_dynamic_cache(
        [
            (torch.randn(bsize, nheads, slen, dim), torch.randn(bsize, nheads, slen, dim))
            for i in range(n_layers)
        ]
    )
    z = torch.randn((1, 1, 1, 7))
    model(cache, z)  # to check it works.





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[[[ 1.4855,  1.8191,  2.7826, -0.2851,  1.0561,  0.0314, -0.5804],
              [-2.0373,  3.2909,  3.4846, -0.1981,  0.6891,  0.5304,  0.0880],
              [ 0.7537, -0.8222,  1.4935,  0.4445, -1.2715,  0.2883, -0.7796]],

             [[-1.6897, -0.6526,  2.4505,  0.4204,  3.2128, -0.6853, -4.8082],
              [-2.3025, -0.7457, -0.5126,  0.7478, -1.1336,  3.4543, -0.1810],
              [ 3.8828, -3.1221,  4.4784, -1.9135,  3.3853,  1.6040,  2.7058]],

             [[ 2.4455,  0.2947, -1.3924, -1.5984, -2.9113,  0.3558, -2.4138],
              [ 2.3812,  2.1722,  0.8848, -1.2201, -2.4370, -2.3406,  0.7388],
              [-0.9314,  2.0401, -0.4906, -2.4672,  0.4342,  1.0382, -1.0126]],

             [[ 0.8948,  2.4408,  1.0158, -2.6894, -4.8829,  1.7201, -2.8761],
              [-0.5564, -0.4801, -0.0913, -1.9801, -2.9370, -0.9350,  1.7624],
              [-2.0936,  1.2497,  1.8590,  0.1348,  0.1190,  2.5224, -0.3983]]],


            [[[-2.7935, -2.5452,  0.9757,  1.0094, -0.2862, -0.6793,  3.5313],
              [ 3.3770, -1.4189,  1.5387,  1.8865,  0.0535,  1.5884,  0.6632],
              [ 1.8961, -2.0978, -0.3475, -0.4481,  3.7473, -0.2100, -3.2043]],

             [[ 2.2345,  0.7756,  3.6989,  0.0327,  2.2209,  3.5839, -3.2043],
              [ 1.7591, -2.9491,  2.3487,  0.3833, -1.3921,  2.9025, -1.5650],
              [ 0.2855, -3.4621, -0.2926,  0.2281, -2.1838,  0.0646, -1.4773]],

             [[ 0.6367, -1.1760,  5.6580, -0.8463, -1.3730,  2.0568, -0.5557],
              [-4.1214,  2.1411,  4.0721,  0.4259, -2.8677,  2.9855, -2.2240],
              [-2.9924, -1.7343,  1.9278,  2.9626,  1.2466,  1.6112, -1.1463]],

             [[-1.5169, -0.1487,  2.7478, -0.0715, -1.8546,  2.5310, -2.3762],
              [ 4.3467,  2.9935,  3.7172, -5.7153,  1.2875,  0.2507,  1.8847],
              [-1.6749, -1.1379, -1.1065,  0.5769, -3.4400, -2.5492, -3.9476]]]])



.. GENERATED FROM PYTHON SOURCE LINES 176-177

The cache looks like this:

.. GENERATED FROM PYTHON SOURCE LINES 177-181

.. code-block:: Python


    print(string_type(cache, with_shape=True))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    DynamicCache(key_cache=#2[T1s2x4x3x7,T1s2x4x3x7], value_cache=#2[T1s2x4x3x7,T1s2x4x3x7])




.. GENERATED FROM PYTHON SOURCE LINES 182-197

.. code-block:: Python


    cache2 = make_dynamic_cache(
        [
            (
                torch.randn(bsize + 1, nheads, slen + 1, dim + 1),
                torch.randn(bsize + 1, nheads, slen + 1, dim + 1),
            )
            for i in range(n_layers)
        ]
    )
    inputs = [
        (cache, z),
        (cache2, torch.randn((1, 1, 1, 8))),
    ]








.. GENERATED FROM PYTHON SOURCE LINES 198-199

And the first set of inputs looks like:

.. GENERATED FROM PYTHON SOURCE LINES 199-201

.. code-block:: Python

    print(string_type(inputs[0], with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (DynamicCache(key_cache=#2[T1s2x4x3x7,T1s2x4x3x7], value_cache=#2[T1s2x4x3x7,T1s2x4x3x7]),T1s1x1x1x7)




.. GENERATED FROM PYTHON SOURCE LINES 202-203

We can now compute the dynamic shapes.

.. GENERATED FROM PYTHON SOURCE LINES 203-208

.. code-block:: Python


    mi = ModelInputs(Model(), inputs)
    ds = mi.guess_dynamic_shapes()
    pprint.pprint(ds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (([[{0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>)},
        {0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>)}],
       [{0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>)},
        {0: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         2: _DimHint(type=<_DimHintType.DYNAMIC: 3>),
         3: _DimHint(type=<_DimHintType.DYNAMIC: 3>)}]],
      {3: _DimHint(type=<_DimHintType.DYNAMIC: 3>)}),
     {})




.. GENERATED FROM PYTHON SOURCE LINES 209-215

And finally the export.
The export is simple if ``transformers>=4.50``, otherwise,
transformers needs to be patched.
:func:`onnx_diagnostic.torch_export_patches.bypass_export_some_errors`
registers functions to serialize ``DynamicCache``. This one is modified to make
the shape inference implemented in :epkg:`torch` happy.

.. GENERATED FROM PYTHON SOURCE LINES 215-224

.. code-block:: Python


    if has_transformers("4.50"):
        ep = torch.export.export(model, inputs[0], dynamic_shapes=ds[0], strict=False)
    else:
        with bypass_export_some_errors(patch_transformers=True) as modificator:
            ep = torch.export.export(
                model, modificator(inputs[0]), dynamic_shapes=ds[0], strict=False
            )
    print(ep)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, cache_key_cache_0: "f32[s0, 4, s1, s11]", cache_key_cache_1: "f32[s0, 4, s1, s11]", cache_value_cache_0: "f32[s0, 4, s1, s11]", cache_value_cache_1: "f32[s0, 4, s1, s11]", z: "f32[1, 1, 1, s11]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_cache.py:154 in forward, code: z
                add: "f32[s0, 4, s1, s11]" = torch.ops.aten.add.Tensor(z, cache_key_cache_0);  z = cache_key_cache_0 = None
                add_1: "f32[s0, 4, s1, s11]" = torch.ops.aten.add.Tensor(add, cache_key_cache_1);  add = cache_key_cache_1 = None
                add_2: "f32[s0, 4, s1, s11]" = torch.ops.aten.add.Tensor(add_1, cache_value_cache_0);  add_1 = cache_value_cache_0 = None
                add_3: "f32[s0, 4, s1, s11]" = torch.ops.aten.add.Tensor(add_2, cache_value_cache_1);  add_2 = cache_value_cache_1 = None
                return (add_3,)
            
    Graph signature: 
        # inputs
        cache_key_cache_0: USER_INPUT
        cache_key_cache_1: USER_INPUT
        cache_value_cache_0: USER_INPUT
        cache_value_cache_1: USER_INPUT
        z: USER_INPUT
    
        # outputs
        add_3: USER_OUTPUT
    
    Range constraints: {s0: VR[2, int_oo], s1: VR[2, int_oo], s11: VR[2, int_oo]}






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.275 seconds)


.. _sphx_glr_download_auto_examples_plot_export_with_dynamic_cache.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_with_dynamic_cache.ipynb <plot_export_with_dynamic_cache.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_with_dynamic_cache.py <plot_export_with_dynamic_cache.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_with_dynamic_cache.zip <plot_export_with_dynamic_cache.zip>`


.. include:: plot_export_with_dynamic_cache.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
