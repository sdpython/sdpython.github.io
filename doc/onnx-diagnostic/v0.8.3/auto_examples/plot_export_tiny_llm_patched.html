<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="Export microsoft/phi-2" href="plot_export_tiny_phi2.html"><link rel="prev" title="Dynamic Shapes for *args, **kwargs" href="plot_export_with_args_kwargs.html">
        <link rel="prefetch" href="../_static/logo.png" as="image">

    <!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>Export Tiny-LLM with patches - onnx-diagnostic 0.8.3 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a7360d90" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">onnx-diagnostic 0.8.3 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">onnx-diagnostic 0.8.3 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../patches.html">Patches Explained</a><input aria-label="Toggle navigation of Patches Explained" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../status/index.html">Exporter Status</a><input aria-label="Toggle navigation of Exporter Status" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../status/exported_program_dynamic.html">Exported Programs with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/exporter_dynamic.html">Exported ONNX with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/patches_coverage.html">Coverage of the Patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/patches_diff.html">Patches Diff</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API of onnx_diagnostic</a><input aria-label="Toggle navigation of API of onnx_diagnostic" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/export/index.html">onnx_diagnostic.export</a><input aria-label="Toggle navigation of onnx_diagnostic.export" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/export/api.html">onnx_diagnostic.export.api</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/control_flow_onnx.html">onnx_diagnostic.export.control_flow_onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/dynamic_shapes.html">onnx_diagnostic.export.dynamic_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/onnx_plug.html">onnx_diagnostic.export.onnx_plug</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/shape_helper.html">onnx_diagnostic.export.shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/validate.html">onnx_diagnostic.export.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/helpers/index.html">onnx_diagnostic.helpers</a><input aria-label="Toggle navigation of onnx_diagnostic.helpers" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/args_helper.html">onnx_diagnostic.helpers.args_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/bench_run.html">onnx_diagnostic.helpers.bench_run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/cache_helper.html">onnx_diagnostic.helpers.cache_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/config_helper.html">onnx_diagnostic.helpers.config_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/doc_helper.html">onnx_diagnostic.helpers.doc_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/fake_tensor_helper.html">onnx_diagnostic.helpers.fake_tensor_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/graph_helper.html">onnx_diagnostic.helpers.graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/helper.html">onnx_diagnostic.helpers.helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/_log_helper.html">onnx_diagnostic.helpers._log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/log_helper.html">onnx_diagnostic.helpers.log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/memory_peak.html">onnx_diagnostic.helpers.memory_peak</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/mini_onnx_builder.html">onnx_diagnostic.helpers.mini_onnx_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/model_builder_helper.html">onnx_diagnostic.helpers.model_builder_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/onnx_helper.html">onnx_diagnostic.helpers.onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/ort_session.html">onnx_diagnostic.helpers.ort_session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/rt_helper.html">onnx_diagnostic.helpers.rt_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_fx_graph_helper.html">onnx_diagnostic.helpers.torch_fx_graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_helper.html">onnx_diagnostic.helpers.torch_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">onnx_diagnostic.reference</a><input aria-label="Toggle navigation of onnx_diagnostic.reference" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">onnx_diagnostic.reference.ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.ops" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">onnx_diagnostic.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">onnx_diagnostic.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">onnx_diagnostic.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">onnx_diagnostic.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">onnx_diagnostic.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">onnx_diagnostic.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">onnx_diagnostic.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">onnx_diagnostic.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">onnx_diagnostic.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">onnx_diagnostic.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">onnx_diagnostic.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">onnx_diagnostic.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">onnx_diagnostic.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">onnx_diagnostic.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">onnx_diagnostic.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">onnx_diagnostic.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">onnx_diagnostic.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">onnx_diagnostic.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">onnx_diagnostic.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">onnx_diagnostic.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">onnx_diagnostic.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">onnx_diagnostic.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">onnx_diagnostic.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/torch_ops/index.html">onnx_diagnostic.reference.torch_ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.torch_ops" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/access_ops.html">onnx_diagnostic.reference.torch_ops.access_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/binary_ops.html">onnx_diagnostic.reference.torch_ops.binary_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/controlflow_ops.html">onnx_diagnostic.reference.torch_ops.controlflow_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/generator_ops.html">onnx_diagnostic.reference.torch_ops.generator_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/nn_ops.html">onnx_diagnostic.reference.torch_ops.nn_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/other_ops.html">onnx_diagnostic.reference.torch_ops.other_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/reduce_ops.html">onnx_diagnostic.reference.torch_ops.reduce_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/sequence_ops.html">onnx_diagnostic.reference.torch_ops.sequence_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/shape_ops.html">onnx_diagnostic.reference.torch_ops.shape_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/unary_ops.html">onnx_diagnostic.reference.torch_ops.unary_ops</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">onnx_diagnostic.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">onnx_diagnostic.reference.quantized_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">onnx_diagnostic.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/report_results_comparison.html">onnx_diagnostic.reference.report_results_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/torch_evaluator.html">onnx_diagnostic.reference.torch_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/tasks/index.html">onnx_diagnostic.tasks</a><input aria-label="Toggle navigation of onnx_diagnostic.tasks" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/automatic_speech_recognition.html">onnx_diagnostic.tasks.automatic_speech_recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/fill_mask.html">onnx_diagnostic.tasks.fill_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/feature_extraction.html">onnx_diagnostic.tasks.feature_extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_classification.html">onnx_diagnostic.tasks.image_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_text_to_text.html">onnx_diagnostic.export.image_text_to_text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/mixture_of_expert.html">onnx_diagnostic.tasks.mixture_of_expert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/object_detection.html">onnx_diagnostic.tasks.object_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/sentence_similarity.html">onnx_diagnostic.tasks.sentence_similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/summarization.html">onnx_diagnostic.tasks.summarization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_classification.html">onnx_diagnostic.tasks.text_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_generation.html">onnx_diagnostic.tasks.text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_to_image.html">onnx_diagnostic.tasks.text_to_image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text2text_generation.html">onnx_diagnostic.tasks.text2text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/zero_shot_image_classification.html">onnx_diagnostic.tasks.zero_shot_image_classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_export_patches/index.html">onnx_diagnostic.torch_export_patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/eval/index.html">onnx_diagnostic.torch_export_patches.eval</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.eval" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/eval/model_cases.html">onnx_diagnostic.torch_export_patches.eval.model_cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_errors.html">onnx_diagnostic.torch_export_patches.onnx_export_errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_serialization.html">onnx_diagnostic.torch_export_patches.onnx_export_serialization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/patches/index.html">onnx_diagnostic.torch_export_patches.patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.patches" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_torch.html">onnx_diagnostic.torch_export_patches.patches.patch_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_transformers.html">onnx_diagnostic.torch_export_patches.patches.patch_transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_details.html">onnx_diagnostic.torch_export_patches.patch_details</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_expressions.html">onnx_diagnostic.torch_export_patches.patch_expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_inputs.html">onnx_diagnostic.torch_export_patches.patch_inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module.html">onnx_diagnostic.torch_export_patches.patch_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module_helper.html">onnx_diagnostic.torch_export_patches.patch_module_helper</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/serialization/index.html">onnx_diagnostic.torch_export_patches.serialization</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.serialization" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/diffusers_impl.html">onnx_diagnostic.torch_export_patches.serialization.diffusers_impl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/transformers_impl.html">onnx_diagnostic.torch_export_patches.serialization.transformers_impl</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">onnx_diagnostic.torch_models</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/code_sample.html">onnx_diagnostic.torch_models.code_sample</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_models/hghub/index.html">onnx_diagnostic.torch_models.hghub</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models.hghub" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_api.html">onnx_diagnostic.torch_models.hghub.hub_api</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_data.html">onnx_diagnostic.torch_models.hghub.hub_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/model_inputs.html">onnx_diagnostic.torch_models.hghub.model_inputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llms.html">onnx_diagnostic.torch_models.llms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/validate.html">onnx_diagnostic.torch_models.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_onnx/index.html">onnx_diagnostic.torch_onnx</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_onnx" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/runtime_info.html">onnx_diagnostic.torch_onnx.runtime_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs.html">onnx_diagnostic.torch_onnx.sbs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs_dataclasses.html">onnx_diagnostic.torch_onnx.sbs_dataclasses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/api.html">onnx_diagnostic.api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">onnx_diagnostic.ext_test_case</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cmds/index.html">Command Lines</a><input aria-label="Toggle navigation of Command Lines" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cmds/config.html">-m onnx_diagnostic config … prints the config for a model id</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/sbs.html">-m onnx_diagnostic sbs … runs a side-by-side torch/onnx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/validate.html">-m onnx_diagnostic validate … validate a model id</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Examples Gallery</a><input aria-label="Toggle navigation of Examples Gallery" checked="" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_dump_intermediate_results.html">Dumps intermediate results of a torch model</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_args_kwargs.html">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Export Tiny-LLM with patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_phi2.html">Export microsoft/phi-2</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_dynamic_cache.html">Export with DynamicCache and guessed dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx_custom.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX (custom)</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_locate_issue.html">Find and fix an export issue due to dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_model_extract.html">Find where a model is failing by running submodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_reference_evaluator.html">Intermediate results with (ONNX) ReferenceEvaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_onnxruntime_evaluator.html">Intermediate results with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm.html">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_hub_codellama.html">Test the export on untrained models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_recipes/index.html">Common Export Issues</a><input aria-label="Toggle navigation of Common Export Issues" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_dim1.html">0, 1, 2 for a Dynamic Dimension in the dummy example to export a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_what.html">Builds dynamic shapes from any input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_max.html">Cannot export <code class="docutils literal notranslate"><span class="pre">torch.sym_max(x.shape[0],</span> <span class="pre">y.shape[0])</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_python_int.html">Do not use python int with dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_cond.html">Export a model with a control flow (If)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_nonzero.html">Half certain nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_json.html">JSON returns list when the original dynamic shapes are list or tuple</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_with_dynamic.html">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_technical/index.html">Technical Details</a><input aria-label="Toggle navigation of Technical Details" class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_broadcast_export_issue.html">Dynamic Shapes and Broadcasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_generate.html">From a LLM to processing a prompt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_gemm_or_matmul_add.html">Gemm or Matmul + Add</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_layer_norm_discrepancies.html">LayerNormalization implementation cannot be exchanged</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_parallelized_reduction.html">Reproducible Parallelized Reduction is difficult</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_examples/plot_export_tiny_llm_patched.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-export-tiny-llm-patched-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="export-tiny-llm-with-patches">
<span id="l-plot-tiny-llm-export-patched"></span><span id="sphx-glr-auto-examples-plot-export-tiny-llm-patched-py"></span><h1>Export Tiny-LLM with patches<a class="headerlink" href="#export-tiny-llm-with-patches" title="Link to this heading">¶</a></h1>
<p>Many models from <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">transformers</a> cannot be converted because
the implementation uses cache classes. Let’s see how to get around that.
We focus on the model <a class="reference external" href="https://huggingface.co/arnir0/Tiny-LLM">arnir0/Tiny-LLM</a>.
To avoid downloading any weights, we write a function creating a
random model based on the same architecture.
This continues example <a class="reference internal" href="plot_export_tiny_llm.html#l-plot-tiny-llm-export"><span class="std std-ref">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</span></a>.</p>
<section id="errors">
<h2>Errors<a class="headerlink" href="#errors" title="Link to this heading">¶</a></h2>
<p>They depend on transformers version.</p>
<p><code class="docutils literal notranslate"><span class="pre">transformers&gt;=4.40,&lt;4.50</span></code> cannot serialize DynamicCache and cannot
map dynamic shapes to instances of DynamicCache. The following errors
would appear:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>torch._dynamo.exc.UserError: Cannot associate shape
    [[{0: &lt;class &#39;....batch&#39;&gt;, 2: &lt;class &#39;....cache_length&#39;&gt;}],
     [{0: &lt;class &#39;....batch&#39;&gt;, 2: &lt;class &#39;....cache_length&#39;&gt;}]]
    specified at `dynamic_shapes[&#39;past_key_values&#39;]`
    to non-tensor type &lt;class &#39;transformers.cache_utils.DynamicCache&#39;&gt;
    at `inputs[&#39;past_key_values&#39;]` (expected None)
For more information about this error,
see: https://docs.pytorch.org/docs/stable/generated/exportdb/index.html#dynamic-shapes-validation
</pre></div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">transformers==4.50</span></code>, it shows the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>torch._dynamo.exc.UserError: Constraints violated (batch)!
For more information, run with TORCH_LOGS=&quot;+dynamic&quot;.
    - Not all values of batch = L[&#39;args&#39;][1][&#39;input_ids&#39;].size()[0]
        in the specified range batch &lt;= 1024 are valid
        because batch was inferred to be a constant (2).
    - Not all values of batch = L[&#39;args&#39;][1][&#39;attention_mask&#39;].size()[0]
        in the specified range batch &lt;= 1024 are valid
        because batch was inferred to be a constant (2).
    - Not all values of batch = L[&#39;args&#39;][1][&#39;past_key_values&#39;][&#39;key_cache&#39;][0].size()[0]
        in the specified range batch &lt;= 1024 are valid
        because batch was inferred to be a constant (2).
    - Not all values of batch = L[&#39;args&#39;][1][&#39;past_key_values&#39;][&#39;value_cache&#39;][0].size()[0]
        in the specified range batch &lt;= 1024 are valid
        because batch was inferred to be a constant (2).
 Suggested fixes:
     batch = 2
</pre></div>
</div>
<p>However, this package implements a patch mechanism
with replaces the part causing these issues.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>restart after an export failure</p>
<p>If the export fails, it is better to start executing again,
or restart the kernel if you are in the notebook.
The export may leave <a class="reference external" href="https://docs.pytorch.org/docs/stable/torch.html">torch</a> in one unstable state.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pprint</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic</span><span class="w"> </span><span class="kn">import</span> <span class="n">doc</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.helpers.cache_helper</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">is_cache_dynamic_registered</span></a></a></a></a></a></a></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.helpers</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a></a></a></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a></a></a></a></a></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches.patch_inputs</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a></a></a></a></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_models.llms</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><span class="n">get_tiny_llm</span></a></a></a></a></a></a></a></a>


<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a></a></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><span class="n">get_tiny_llm</span></a></a></a></a></a></a></a></a><span class="p">()</span>
<span class="n">untrained_model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a></a></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a></a></a></a></a> <span class="o">=</span> <span class="p">(</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a></a></a></a></a><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a></a></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a></a></a></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">],</span>
<span class="p">)</span>

<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cloned_inputs</span></a></a></a></a></a></a></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span></a></a></a></a></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<p>Let’s show this inputs, this was inferred in
example <a class="reference internal" href="plot_export_tiny_llm.html#l-plot-tiny-llm-export"><span class="std std-ref">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</span></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a></a></a></a></a><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x30x96], value_cache=#1[T1s2x1x30x96]))
</pre></div>
</div>
<p>And the dynamic shapes</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span></a></a></a></a></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;attention_mask&#39;: {0: &#39;batch&#39;, 1: &#39;cache+seq&#39;},
 &#39;input_ids&#39;: {0: &#39;batch&#39;, 1: &#39;seq_length&#39;},
 &#39;past_key_values&#39;: [{0: &#39;batch&#39;, 2: &#39;cache_length&#39;},
                     {0: &#39;batch&#39;, 2: &#39;cache_length&#39;}],
 &#39;position_ids&#39;: {0: &#39;batch&#39;, 1: &#39;seq_length&#39;}}
</pre></div>
</div>
<p>Before exporting, we check <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.cache_utils.DynamicCache</span></code>
can serialized and deserialized otherwise <a class="reference external" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a>
fails.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-- DynamicCache registered: &quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">is_cache_dynamic_registered</span></a></a></a></a></a></a></a></a><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>-- DynamicCache registered:  True
</pre></div>
</div>
<p>If they are not registered, function
<a class="reference internal" href="../api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.torch_export_patches.torch_export_patches()</span></code></a>
should take care of it. Then we export.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a></a></a></a></a></a></a><span class="p">(</span><span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="k">as</span> <span class="n">modificator</span><span class="p">:</span>
    <span class="k">assert</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" title="onnx_diagnostic.helpers.cache_helper.is_cache_dynamic_registered" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">is_cache_dynamic_registered</span></a></a></a></a></a></a></a></a><span class="p">()</span>  <span class="c1"># it must be true here</span>
    <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a></a></a></a></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a></a></a></a></a></a></a><span class="p">(</span>
        <span class="n">untrained_model</span><span class="p">,</span>
        <span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><span class="n">modificator</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cloned_inputs</span></a></a></a></a></a></a></a></a><span class="p">),</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a></a></a></a></a><span class="o">=</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a></a></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a></a></a></a></a><span class="p">),</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># mandatory for torch==2.6</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It worked:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[torch_export_patches] patch_sympy=True
                     . patch_torch=True
                     . patch_transformers=True
                     . patch_diffusers=False
                     . catch_constraints=True
                     . stop_if_static=0
                     . patch=True
                     . custom_patches=None
[torch_export_patches] dump_rewriting=None
[torch_export_patches] replace torch.jit.isinstance, torch._dynamo.mark_static_address
[_fix_registration] BaseModelOutput is unregistered and registered first
[unregister_cache_serialization] unregistered BaseModelOutput
[register_class_serialization] ---------- register BaseModelOutput
[_fix_registration] BaseModelOutput done.
[register_class_serialization] ---------- register DynamicCache
[register_class_serialization] ---------- register HybridCache
[register_class_serialization] ---------- register EncoderDecoderCache
[register_class_serialization] ---------- register SlidingWindowCache
[register_class_serialization] ---------- register StaticCache
[register_class_serialization] ---------- register MambaCache
[register_class_serialization] already registered BaseModelOutput
[torch_export_patches] sympy.__version__=&#39;1.14.0&#39;
[torch_export_patches] patch sympy
[torch_export_patches] torch.__version__=&#39;2.10.0.dev20251106+cu130&#39;
[torch_export_patches] stop_if_static=0
[torch_export_patches] patch pytorch
[torch_export_patches] modifies shape constraints
[torch_export_patches] transformers.__version__=&#39;5.0.0.dev0&#39;
[torch_export_patches] patches transformers.masking_utils.eager_mask
[torch_export_patches] patches transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
[torch_export_patches] patches transformers.integrations.sdpa_attention.sdpa_attention_forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter:
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
[patch_module_or_classes] function: transformers.models.bart.modeling_bart.eager_attention_forward
[patch_module_or_classes] function: transformers.models.marian.modeling_marian.eager_attention_forward
[torch_export_patches] done patching
It worked:
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_embed_tokens_weight: &quot;f32[32000, 192]&quot;, p_model_layers_0_self_attn_q_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_self_attn_k_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_v_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_o_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_mlp_gate_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_up_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_down_proj_weight: &quot;f32[192, 1024]&quot;, p_model_layers_0_input_layernorm_weight: &quot;f32[192]&quot;, p_model_layers_0_post_attention_layernorm_weight: &quot;f32[192]&quot;, p_model_norm_weight: &quot;f32[192]&quot;, p_lm_head_weight: &quot;f32[32000, 192]&quot;, b_model_rotary_emb_inv_freq: &quot;f32[48]&quot;, input_ids: &quot;i64[s44, s70]&quot;, attention_mask: &quot;i64[s43, s53]&quot;, position_ids: &quot;i64[s44, s70]&quot;, past_key_values_key_0: &quot;f32[s44, 1, s45, 96]&quot;, past_key_values_value_0: &quot;f32[s44, 1, s21, 96]&quot;):
            # No stacktrace found for following nodes
            sym_size_int_16: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(input_ids, 1)
            sym_size_int_19: &quot;Sym(s44)&quot; = torch.ops.aten.sym_size.int(position_ids, 0)
            sym_size_int_22: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(past_key_values_key_0, 2)
            sym_size_int_24: &quot;Sym(s21)&quot; = torch.ops.aten.sym_size.int(past_key_values_value_0, 2)

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
            embedding: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            add: &quot;Sym(s45 + s70)&quot; = sym_size_int_22 + sym_size_int_16

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
            arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int_22, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_22 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
            _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(attention_mask, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default = None
            to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(attention_mask, device(type=&#39;cpu&#39;), torch.bool);  attention_mask = None
            arange_1: &quot;i64[s44]&quot; = torch.ops.aten.arange.default(sym_size_int_19, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add, device = device(type=&#39;cpu&#39;), pin_memory = False)
            add_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add.Tensor(arange_3, 0);  arange_3 = None
            slice_1: &quot;i64[s44]&quot; = torch.ops.aten.slice.Tensor(arange_1, 0, 0, 9223372036854775807);  arange_1 = None
            unsqueeze: &quot;i64[s44, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
            unsqueeze_1: &quot;i64[s44, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None
            unsqueeze_2: &quot;i64[s44, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None
            unsqueeze_3: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_2, 0);  arange_2 = None
            unsqueeze_4: &quot;i64[1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
            unsqueeze_5: &quot;i64[1, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = unsqueeze_5 = None
            unsqueeze_6: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
            unsqueeze_7: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_6, 1);  unsqueeze_6 = None
            slice_2: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
            unsqueeze_8: &quot;i64[1, 1, s70, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_2, 3);  slice_2 = None
            unsqueeze_9: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_3, 0);  add_3 = None
            unsqueeze_10: &quot;i64[1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_9, 1);  unsqueeze_9 = None
            unsqueeze_11: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_10, 2);  unsqueeze_10 = None
            slice_3: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
            new_ones: &quot;b8[]&quot; = torch.ops.aten.new_ones.default(unsqueeze_8, [], dtype = torch.bool, pin_memory = False)
            le_3: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.le.Tensor(slice_3, unsqueeze_8);  unsqueeze_8 = None
            _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(le_3, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
            to_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(le_3, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  le_3 = None
            and_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(new_ones, to_1);  new_ones = to_1 = None
            index: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.index.Tensor(to, [unsqueeze_2, slice_3]);  to = unsqueeze_2 = slice_3 = None
            _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(index, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
            to_2: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(index, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  index = None
            and_2: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(and_1, to_2);  and_1 = to_2 = None
            expand: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.expand.default(and_2, [sym_size_int_19, -1, sym_size_int_16, add]);  and_2 = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1549 in forward, code: inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
            unsqueeze_12: &quot;f32[1, 48]&quot; = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
            unsqueeze_13: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_12, 2);  unsqueeze_12 = None
            _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_13, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
            to_3: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.to.dtype(unsqueeze_13, torch.float32);  unsqueeze_13 = None
            expand_1: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.expand.default(to_3, [sym_size_int_19, -1, 1]);  to_3 = None
            _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
            to_4: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  expand_1 = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1551 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
            slice_4: &quot;i64[s44, s70]&quot; = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
            unsqueeze_14: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
            slice_5: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_14, 2, 0, 9223372036854775807);  unsqueeze_14 = None
            _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(slice_5, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
            to_5: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(slice_5, torch.float32);  slice_5 = None

            # No stacktrace found for following nodes
            submod_3 = self.submod_1
            wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast(&#39;cpu&#39;, torch.bfloat16, False, False, submod_3, to_4, to_5);  submod_3 = to_4 = to_5 = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
            mul: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[0]

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
            mul_1: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[1];  wrap_with_autocast = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1564 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
            _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
            to_8: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
            _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
            to_9: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
            to_10: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
            mean: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
            mul_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_10, rsqrt);  rsqrt = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
            to_11: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
            mul_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_11);  p_model_layers_0_input_layernorm_weight = to_11 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.view.default(linear, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear = None
            transpose_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.transpose.int(view, 1, 2);  view = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:265 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_1: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_1, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_1 = None
            transpose_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_2: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:266 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_2: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_2, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_2 = None
            transpose_3: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:269 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_15: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
            unsqueeze_16: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_9, 1);  to_9 = None
            mul_4: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_15)
            slice_6: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 48)
            slice_7: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 48, 9223372036854775807);  transpose_1 = None
            neg: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.neg.default(slice_7);  slice_7 = None
            cat_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
            mul_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_16);  cat_1 = None
            add_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            mul_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_15);  unsqueeze_15 = None
            slice_8: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 48)
            slice_9: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 48, 9223372036854775807);  transpose_2 = None
            neg_1: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.neg.default(slice_9);  slice_9 = None
            cat_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
            mul_7: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_16);  cat_2 = unsqueeze_16 = None
            add_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:274 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_3: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_key_0, add_6], -2);  past_key_values_key_0 = add_6 = None
            cat_4: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_value_0, transpose_3], -2);  past_key_values_value_0 = transpose_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:280 in forward, code: attn_output, attn_weights = attention_interface(
            slice_10: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_3, 0, 0, 9223372036854775807)
            unsqueeze_17: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_10, 2);  slice_10 = None
            slice_11: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_17, 3, 0, 9223372036854775807);  unsqueeze_17 = None
            expand_2: &quot;f32[s44, 1, 2, s45 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_11, [sym_size_int_19, 1, 2, add, 96]);  slice_11 = None
            reshape: &quot;f32[s44, 2, s45 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_2, [sym_size_int_19, 2, add, 96]);  expand_2 = None
            slice_12: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_4, 0, 0, 9223372036854775807)
            unsqueeze_18: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_12, 2);  slice_12 = None
            add_11: &quot;Sym(s21 + s70)&quot; = sym_size_int_24 + sym_size_int_16;  sym_size_int_24 = None
            slice_13: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_18, 3, 0, 9223372036854775807);  unsqueeze_18 = None
            expand_3: &quot;f32[s44, 1, 2, s21 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_13, [sym_size_int_19, 1, 2, add_11, 96]);  slice_13 = None
            reshape_1: &quot;f32[s44, 2, s21 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_3, [sym_size_int_19, 2, add_11, 96]);  expand_3 = add_11 = None
            slice_14: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(expand, 3, None, add);  expand = add = None
            scaled_dot_product_attention: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.scaled_dot_product_attention.default(add_5, reshape, reshape_1, slice_14, scale = 0.10206207261596575);  add_5 = reshape = reshape_1 = slice_14 = None
            transpose_4: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:291 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.reshape.default(transpose_4, [sym_size_int_19, sym_size_int_16, -1]);  transpose_4 = sym_size_int_19 = sym_size_int_16 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(reshape_2, p_model_layers_0_self_attn_o_proj_weight);  reshape_2 = p_model_layers_0_self_attn_o_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
            add_7: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_10, linear_3);  to_10 = linear_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
            to_12: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
            mean_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_8: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
            mul_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_12, rsqrt_1);  rsqrt_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_16, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
            to_13: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_16, torch.float32);  mul_16 = None
            mul_17: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_13);  p_model_layers_0_post_attention_layernorm_weight = to_13 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_4: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None

            # File: ~/github/transformers/src/transformers/activations.py:103 in forward, code: return nn.functional.silu(input)
            silu: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.silu.default(linear_4);  linear_4 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_5: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_up_proj_weight);  mul_17 = p_model_layers_0_mlp_up_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:184 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_18: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_6: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_0_mlp_down_proj_weight);  mul_18 = p_model_layers_0_mlp_down_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
            add_9: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_12, linear_6);  to_12 = linear_6 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_9, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
            to_14: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_9, torch.float32);  add_9 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
            mean_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_10: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
            mul_19: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_14, rsqrt_2);  to_14 = rsqrt_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_19, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
            to_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_19, torch.float32);  mul_19 = None
            mul_20: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_15);  p_model_norm_weight = to_15 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:500 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
            slice_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(mul_20, 0, 0, 9223372036854775807);  mul_20 = None
            slice_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(slice_15, 1, 0, 9223372036854775807);  slice_15 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_7: &quot;f32[s44, s70, 32000]&quot; = torch.ops.aten.linear.default(slice_16, p_lm_head_weight);  slice_16 = p_lm_head_weight = None
            return (linear_7, cat_3, cat_4)

        class submod_1(torch.nn.Module):
            def forward(self, to_4: &quot;f32[s44, 48, 1]&quot;, to_5: &quot;f32[s44, 1, s70]&quot;):
                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1559 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                to_6: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(to_5, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                to_7: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(to_5, torch.float32);  to_5 = None
                matmul: &quot;f32[s44, 48, s70]&quot; = torch.ops.aten.matmul.default(to_6, to_7);  to_6 = to_7 = None
                transpose: &quot;f32[s44, s70, 48]&quot; = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None

                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1560 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                cat: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None

                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
                cos: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cos.default(cat)
                mul: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None

                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
                sin: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.sin.default(cat);  cat = None
                mul_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                return (mul, mul_1)

Graph signature:
    # inputs
    p_model_embed_tokens_weight: PARAMETER target=&#39;model.embed_tokens.weight&#39;
    p_model_layers_0_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.q_proj.weight&#39;
    p_model_layers_0_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.k_proj.weight&#39;
    p_model_layers_0_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.v_proj.weight&#39;
    p_model_layers_0_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.o_proj.weight&#39;
    p_model_layers_0_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.gate_proj.weight&#39;
    p_model_layers_0_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.up_proj.weight&#39;
    p_model_layers_0_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.down_proj.weight&#39;
    p_model_layers_0_input_layernorm_weight: PARAMETER target=&#39;model.layers.0.input_layernorm.weight&#39;
    p_model_layers_0_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.0.post_attention_layernorm.weight&#39;
    p_model_norm_weight: PARAMETER target=&#39;model.norm.weight&#39;
    p_lm_head_weight: PARAMETER target=&#39;lm_head.weight&#39;
    b_model_rotary_emb_inv_freq: BUFFER target=&#39;model.rotary_emb.inv_freq&#39; persistent=False
    input_ids: USER_INPUT
    attention_mask: USER_INPUT
    position_ids: USER_INPUT
    past_key_values_key_0: USER_INPUT
    past_key_values_value_0: USER_INPUT

    # outputs
    linear_7: USER_OUTPUT
    cat_3: USER_OUTPUT
    cat_4: USER_OUTPUT

Range constraints: {s44: VR[2, int_oo], s70: VR[2, int_oo], s43: VR[2, int_oo], s53: VR[4, int_oo], s45: VR[2, int_oo], s21: VR[2, int_oo]}

[torch_export_patches] remove patches
[torch_export_patches] restored sympy functions
[torch_export_patches] restored pytorch functions
[torch_export_patches] restored shape constraints
[torch_export_patches] unpatches transformers
[torch_export_patches] restored transformers.masking_utils.eager_mask
[torch_export_patches] restored transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
[torch_export_patches] restored transformers.integrations.sdpa_attention.sdpa_attention_forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter:
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
[unpatch_module_or_classes] function transformers.models.bart.modeling_bart.eager_attention_forward
[unpatch_module_or_classes] function transformers.models.marian.modeling_marian.eager_attention_forward
</pre></div>
</div>
</section>
<section id="with-the-original-model">
<h2>With the original model<a class="headerlink" href="#with-the-original-model" title="Link to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">MODEL_NAME</span></a></a></a></a></a></a></a></a> <span class="o">=</span> <span class="s2">&quot;arnir0/Tiny-LLM&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">MODEL_NAME</span></a></a></a></a></a></a></a></a><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">MODEL_NAME</span></a></a></a></a></a></a></a></a><span class="p">)</span>

<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cloned_inputs</span></a></a></a></a></a></a></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span></a></a></a></a></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a></a></a></a></a><span class="p">)</span>

<span class="k">with</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a></a></a></a></a></a></a><span class="p">(</span><span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="k">as</span> <span class="n">modificator</span><span class="p">:</span>
    <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a></a></a></a></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a></a></a></a></a></a></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><span class="n">modificator</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cloned_inputs</span></a></a></a></a></a></a></a></a><span class="p">),</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a></a></a></a></a><span class="o">=</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a></a></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a></a></a></a></a><span class="p">),</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># mandatory for torch==2.6</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It worked:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[torch_export_patches] patch_sympy=True
                     . patch_torch=True
                     . patch_transformers=True
                     . patch_diffusers=False
                     . catch_constraints=True
                     . stop_if_static=0
                     . patch=True
                     . custom_patches=None
[torch_export_patches] dump_rewriting=None
[torch_export_patches] replace torch.jit.isinstance, torch._dynamo.mark_static_address
[_fix_registration] DynamicCache is unregistered and registered first
[unregister_cache_serialization] unregistered DynamicCache
[register_class_serialization] ---------- register DynamicCache
[_fix_registration] DynamicCache done.
[register_class_serialization] already registered DynamicCache
[register_class_serialization] already registered HybridCache
[register_class_serialization] already registered EncoderDecoderCache
[register_class_serialization] already registered SlidingWindowCache
[register_class_serialization] already registered StaticCache
[register_class_serialization] already registered MambaCache
[register_class_serialization] already registered BaseModelOutput
[torch_export_patches] sympy.__version__=&#39;1.14.0&#39;
[torch_export_patches] patch sympy
[torch_export_patches] torch.__version__=&#39;2.10.0.dev20251106+cu130&#39;
[torch_export_patches] stop_if_static=0
[torch_export_patches] patch pytorch
[torch_export_patches] modifies shape constraints
[torch_export_patches] transformers.__version__=&#39;5.0.0.dev0&#39;
[torch_export_patches] patches transformers.masking_utils.eager_mask
[torch_export_patches] patches transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
[torch_export_patches] patches transformers.integrations.sdpa_attention.sdpa_attention_forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter:
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
[patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
[patch_module_or_classes] function: transformers.models.bart.modeling_bart.eager_attention_forward
[patch_module_or_classes] function: transformers.models.marian.modeling_marian.eager_attention_forward
[torch_export_patches] done patching
It worked:
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_embed_tokens_weight: &quot;f32[32000, 192]&quot;, p_model_layers_0_self_attn_q_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_self_attn_k_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_v_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_o_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_mlp_gate_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_up_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_down_proj_weight: &quot;f32[192, 1024]&quot;, p_model_layers_0_input_layernorm_weight: &quot;f32[192]&quot;, p_model_layers_0_post_attention_layernorm_weight: &quot;f32[192]&quot;, p_model_norm_weight: &quot;f32[192]&quot;, p_lm_head_weight: &quot;f32[32000, 192]&quot;, b_model_rotary_emb_inv_freq: &quot;f32[48]&quot;, input_ids: &quot;i64[s44, s70]&quot;, attention_mask: &quot;i64[s43, s53]&quot;, position_ids: &quot;i64[s44, s70]&quot;, past_key_values_key_0: &quot;f32[s44, 1, s45, 96]&quot;, past_key_values_value_0: &quot;f32[s44, 1, s21, 96]&quot;):
            # No stacktrace found for following nodes
            sym_size_int_16: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(input_ids, 1)
            sym_size_int_19: &quot;Sym(s44)&quot; = torch.ops.aten.sym_size.int(position_ids, 0)
            sym_size_int_22: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(past_key_values_key_0, 2)
            sym_size_int_24: &quot;Sym(s21)&quot; = torch.ops.aten.sym_size.int(past_key_values_value_0, 2)

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
            embedding: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            add: &quot;Sym(s45 + s70)&quot; = sym_size_int_22 + sym_size_int_16

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
            arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int_22, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_22 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
            _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(attention_mask, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default = None
            to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(attention_mask, device(type=&#39;cpu&#39;), torch.bool);  attention_mask = None
            arange_1: &quot;i64[s44]&quot; = torch.ops.aten.arange.default(sym_size_int_19, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add, device = device(type=&#39;cpu&#39;), pin_memory = False)
            add_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add.Tensor(arange_3, 0);  arange_3 = None
            slice_1: &quot;i64[s44]&quot; = torch.ops.aten.slice.Tensor(arange_1, 0, 0, 9223372036854775807);  arange_1 = None
            unsqueeze: &quot;i64[s44, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
            unsqueeze_1: &quot;i64[s44, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None
            unsqueeze_2: &quot;i64[s44, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None
            unsqueeze_3: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_2, 0);  arange_2 = None
            unsqueeze_4: &quot;i64[1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
            unsqueeze_5: &quot;i64[1, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = unsqueeze_5 = None
            unsqueeze_6: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
            unsqueeze_7: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_6, 1);  unsqueeze_6 = None
            slice_2: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
            unsqueeze_8: &quot;i64[1, 1, s70, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_2, 3);  slice_2 = None
            unsqueeze_9: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_3, 0);  add_3 = None
            unsqueeze_10: &quot;i64[1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_9, 1);  unsqueeze_9 = None
            unsqueeze_11: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_10, 2);  unsqueeze_10 = None
            slice_3: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
            new_ones: &quot;b8[]&quot; = torch.ops.aten.new_ones.default(unsqueeze_8, [], dtype = torch.bool, pin_memory = False)
            le_3: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.le.Tensor(slice_3, unsqueeze_8);  unsqueeze_8 = None
            _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(le_3, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
            to_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(le_3, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  le_3 = None
            and_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(new_ones, to_1);  new_ones = to_1 = None
            index: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.index.Tensor(to, [unsqueeze_2, slice_3]);  to = unsqueeze_2 = slice_3 = None
            _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(index, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
            to_2: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(index, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  index = None
            and_2: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(and_1, to_2);  and_1 = to_2 = None
            expand: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.expand.default(and_2, [sym_size_int_19, -1, sym_size_int_16, add]);  and_2 = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1549 in forward, code: inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
            unsqueeze_12: &quot;f32[1, 48]&quot; = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
            unsqueeze_13: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_12, 2);  unsqueeze_12 = None
            _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_13, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
            to_3: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.to.dtype(unsqueeze_13, torch.float32);  unsqueeze_13 = None
            expand_1: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.expand.default(to_3, [sym_size_int_19, -1, 1]);  to_3 = None
            _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
            to_4: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  expand_1 = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1551 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
            slice_4: &quot;i64[s44, s70]&quot; = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
            unsqueeze_14: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
            slice_5: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_14, 2, 0, 9223372036854775807);  unsqueeze_14 = None
            _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(slice_5, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
            to_5: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(slice_5, torch.float32);  slice_5 = None

            # No stacktrace found for following nodes
            submod_3 = self.submod_1
            wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast(&#39;cpu&#39;, torch.bfloat16, False, False, submod_3, to_4, to_5);  submod_3 = to_4 = to_5 = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
            mul: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[0]

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
            mul_1: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[1];  wrap_with_autocast = None

            # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1564 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
            _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
            to_8: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
            _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
            to_9: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
            to_10: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
            mean: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
            mul_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_10, rsqrt);  rsqrt = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
            to_11: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
            mul_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_11);  p_model_layers_0_input_layernorm_weight = to_11 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.view.default(linear, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear = None
            transpose_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.transpose.int(view, 1, 2);  view = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:265 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_1: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_1, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_1 = None
            transpose_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_2: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:266 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_2: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_2, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_2 = None
            transpose_3: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:269 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_15: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
            unsqueeze_16: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_9, 1);  to_9 = None
            mul_4: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_15)
            slice_6: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 48)
            slice_7: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 48, 9223372036854775807);  transpose_1 = None
            neg: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.neg.default(slice_7);  slice_7 = None
            cat_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
            mul_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_16);  cat_1 = None
            add_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            mul_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_15);  unsqueeze_15 = None
            slice_8: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 48)
            slice_9: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 48, 9223372036854775807);  transpose_2 = None
            neg_1: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.neg.default(slice_9);  slice_9 = None
            cat_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
            mul_7: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_16);  cat_2 = unsqueeze_16 = None
            add_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:274 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_3: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_key_0, add_6], -2);  past_key_values_key_0 = add_6 = None
            cat_4: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_value_0, transpose_3], -2);  past_key_values_value_0 = transpose_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:280 in forward, code: attn_output, attn_weights = attention_interface(
            slice_10: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_3, 0, 0, 9223372036854775807)
            unsqueeze_17: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_10, 2);  slice_10 = None
            slice_11: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_17, 3, 0, 9223372036854775807);  unsqueeze_17 = None
            expand_2: &quot;f32[s44, 1, 2, s45 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_11, [sym_size_int_19, 1, 2, add, 96]);  slice_11 = None
            reshape: &quot;f32[s44, 2, s45 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_2, [sym_size_int_19, 2, add, 96]);  expand_2 = None
            slice_12: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_4, 0, 0, 9223372036854775807)
            unsqueeze_18: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_12, 2);  slice_12 = None
            add_11: &quot;Sym(s21 + s70)&quot; = sym_size_int_24 + sym_size_int_16;  sym_size_int_24 = None
            slice_13: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_18, 3, 0, 9223372036854775807);  unsqueeze_18 = None
            expand_3: &quot;f32[s44, 1, 2, s21 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_13, [sym_size_int_19, 1, 2, add_11, 96]);  slice_13 = None
            reshape_1: &quot;f32[s44, 2, s21 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_3, [sym_size_int_19, 2, add_11, 96]);  expand_3 = add_11 = None
            slice_14: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(expand, 3, None, add);  expand = add = None
            scaled_dot_product_attention: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.scaled_dot_product_attention.default(add_5, reshape, reshape_1, slice_14, scale = 0.10206207261596575);  add_5 = reshape = reshape_1 = slice_14 = None
            transpose_4: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:291 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.reshape.default(transpose_4, [sym_size_int_19, sym_size_int_16, -1]);  transpose_4 = sym_size_int_19 = sym_size_int_16 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(reshape_2, p_model_layers_0_self_attn_o_proj_weight);  reshape_2 = p_model_layers_0_self_attn_o_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
            add_7: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_10, linear_3);  to_10 = linear_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
            to_12: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
            mean_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_8: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
            mul_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_12, rsqrt_1);  rsqrt_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_16, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
            to_13: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_16, torch.float32);  mul_16 = None
            mul_17: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_13);  p_model_layers_0_post_attention_layernorm_weight = to_13 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_4: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None

            # File: ~/github/transformers/src/transformers/activations.py:103 in forward, code: return nn.functional.silu(input)
            silu: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.silu.default(linear_4);  linear_4 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_5: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_up_proj_weight);  mul_17 = p_model_layers_0_mlp_up_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:184 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_18: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_6: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_0_mlp_down_proj_weight);  mul_18 = p_model_layers_0_mlp_down_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
            add_9: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_12, linear_6);  to_12 = linear_6 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_9, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
            to_14: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_9, torch.float32);  add_9 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
            mean_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_10: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
            mul_19: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_14, rsqrt_2);  to_14 = rsqrt_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_19, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
            to_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_19, torch.float32);  mul_19 = None
            mul_20: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_15);  p_model_norm_weight = to_15 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:500 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
            slice_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(mul_20, 0, 0, 9223372036854775807);  mul_20 = None
            slice_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(slice_15, 1, 0, 9223372036854775807);  slice_15 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_7: &quot;f32[s44, s70, 32000]&quot; = torch.ops.aten.linear.default(slice_16, p_lm_head_weight);  slice_16 = p_lm_head_weight = None
            return (linear_7, cat_3, cat_4)

        class submod_1(torch.nn.Module):
            def forward(self, to_4: &quot;f32[s44, 48, 1]&quot;, to_5: &quot;f32[s44, 1, s70]&quot;):
                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1559 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                to_6: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(to_5, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                to_7: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(to_5, torch.float32);  to_5 = None
                matmul: &quot;f32[s44, 48, s70]&quot; = torch.ops.aten.matmul.default(to_6, to_7);  to_6 = to_7 = None
                transpose: &quot;f32[s44, s70, 48]&quot; = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None

                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1560 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                cat: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None

                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
                cos: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cos.default(cat)
                mul: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None

                # File: ~/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
                sin: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.sin.default(cat);  cat = None
                mul_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                return (mul, mul_1)

Graph signature:
    # inputs
    p_model_embed_tokens_weight: PARAMETER target=&#39;model.embed_tokens.weight&#39;
    p_model_layers_0_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.q_proj.weight&#39;
    p_model_layers_0_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.k_proj.weight&#39;
    p_model_layers_0_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.v_proj.weight&#39;
    p_model_layers_0_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.o_proj.weight&#39;
    p_model_layers_0_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.gate_proj.weight&#39;
    p_model_layers_0_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.up_proj.weight&#39;
    p_model_layers_0_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.down_proj.weight&#39;
    p_model_layers_0_input_layernorm_weight: PARAMETER target=&#39;model.layers.0.input_layernorm.weight&#39;
    p_model_layers_0_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.0.post_attention_layernorm.weight&#39;
    p_model_norm_weight: PARAMETER target=&#39;model.norm.weight&#39;
    p_lm_head_weight: PARAMETER target=&#39;lm_head.weight&#39;
    b_model_rotary_emb_inv_freq: BUFFER target=&#39;model.rotary_emb.inv_freq&#39; persistent=False
    input_ids: USER_INPUT
    attention_mask: USER_INPUT
    position_ids: USER_INPUT
    past_key_values_key_0: USER_INPUT
    past_key_values_value_0: USER_INPUT

    # outputs
    linear_7: USER_OUTPUT
    cat_3: USER_OUTPUT
    cat_4: USER_OUTPUT

Range constraints: {s44: VR[2, int_oo], s70: VR[2, int_oo], s43: VR[2, int_oo], s53: VR[4, int_oo], s45: VR[2, int_oo], s21: VR[2, int_oo]}

[torch_export_patches] remove patches
[torch_export_patches] restored sympy functions
[torch_export_patches] restored pytorch functions
[torch_export_patches] restored shape constraints
[torch_export_patches] unpatches transformers
[torch_export_patches] restored transformers.masking_utils.eager_mask
[torch_export_patches] restored transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
[torch_export_patches] restored transformers.integrations.sdpa_attention.sdpa_attention_forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter:
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
[unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
[unpatch_module_or_classes] function transformers.models.bart.modeling_bart.eager_attention_forward
[unpatch_module_or_classes] function transformers.models.marian.modeling_marian.eager_attention_forward
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span><span class="o">.</span><span class="n">plot_legend</span><span class="p">(</span><span class="s2">&quot;Tiny-LLM patched&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.export.export&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_export_tiny_llm_patched_001.png" srcset="../_images/sphx_glr_plot_export_tiny_llm_patched_001.png" alt="plot export tiny llm patched" class = "sphx-glr-single-img"/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 5.888 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-export-tiny-llm-patched-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/017a6b7f47e326b82633e3d0c9ea46f5/plot_export_tiny_llm_patched.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_export_tiny_llm_patched.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/ed9bb0f6d2e5225292dfdd0b1ea728bb/plot_export_tiny_llm_patched.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_export_tiny_llm_patched.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/a1928daec14415a5f181bbeead7fdbed/plot_export_tiny_llm_patched.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_export_tiny_llm_patched.zip</span></code></a></p>
</div>
</div>
<p class="rubric">Related examples</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Inputs are always dynamic with LLMs that is why dynamic shapes needs to be specified when a LLM is exported with torch.export.export. Most of the examples on HuggingFace use method transformers.GenerationMixin.generate but we only want to export the model and its method forward."><img alt="" src="../_images/sphx_glr_plot_export_tiny_llm_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_llm.html#sphx-glr-auto-examples-plot-export-tiny-llm-py"><span class="std std-ref">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This function exports an smaller untrained model with the same architecture. It is faster than the pretrained model. When this works, the untrained model can be replaced by the trained one."><img alt="" src="../_images/sphx_glr_plot_export_tiny_phi2_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_phi2.html#sphx-glr-auto-examples-plot-export-tiny-phi2-py"><span class="std std-ref">Export microsoft/phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export microsoft/phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="LLMs must be exported with dynamic shapes and it is common that a static dimension turns into a static ones. The error message from pytorch tells the user to define TORCH_LOGS=&quot;+dynamic&quot; but it shows a very long list of messages where we need to find the string range_refined_to_singleton and that does not really indicates where it comes from. The example shows how to tweak pytorch to get that information until it gets better."><img alt="" src="../_images/sphx_glr_plot_export_locate_issue_thumb.png" />
<p><a class="reference internal" href="plot_export_locate_issue.html#sphx-glr-auto-examples-plot-export-locate-issue-py"><span class="std std-ref">Find and fix an export issue due to dynamic shapes</span></a></p>
  <div class="sphx-glr-thumbnail-title">Find and fix an export issue due to dynamic shapes</div>
</div></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="plot_export_tiny_phi2.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Export microsoft/phi-2</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_export_with_args_kwargs.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Export Tiny-LLM with patches</a><ul>
<li><a class="reference internal" href="#errors">Errors</a></li>
<li><a class="reference internal" href="#with-the-original-model">With the original model</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=c1d87ce5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>