{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export LLM with dynamic shapes\n\nWe focus on the model\n[Tiny-LLM](https://huggingface.co/arnir0/Tiny-LLM).\nTo avoid downloading any weigths, we write a function creating a\nrandom model based on the same architecture.\n\n## Guess the cache dimension\n\nThe first step is to guess the dummy inputs.\nLet's use the true model for that.\nWe use the dummy example from the model page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nimport torch\nimport transformers\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.torch_models.llms import get_tiny_llm\n\n\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We rewrite the forward method to print the cache dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _forward_(*args, _f=None, **kwargs):\n    assert _f is not None\n    if not torch.compiler.is_exporting():\n        print(\"<-\", string_type((args, kwargs), with_shape=True, with_min_max=True))\n    res = _f(*args, **kwargs)\n    if not torch.compiler.is_exporting():\n        print(\"->\", string_type((args, kwargs), with_shape=True, with_min_max=True))\n    return res\n\n\nkeep_model_forward = model.forward\nmodel.forward = lambda *args, _f=keep_model_forward, **kwargs: _forward_(\n    *args, _f=_f, **kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prompt = \"Continue: it rains...\"\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n\noutputs = model.generate(\n    inputs, max_length=50, temperature=1, top_k=50, top_p=0.95, do_sample=True\n)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's restore the forward as it was.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.forward = keep_model_forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The model creation\n\nLet's create an untrained model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get the model, inputs and dynamic shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "experiment = get_tiny_llm()\nuntrained_model, inputs, dynamic_shapes = (\n    experiment[\"model\"],\n    experiment[\"inputs\"],\n    experiment[\"dynamic_shapes\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we run it, we make a copy of the inputs as the cache\nget modified by the execution. Then it is no longer valid\nassociated with the previous input_ids and mask.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cloned_inputs = copy.deepcopy(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"input type before\", string_type(inputs, with_shape=True))\n\nexpected_output = untrained_model(**inputs)\n\nprint(\"input type after-\", string_type(inputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The outputs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"result type\", string_type(expected_output, with_shape=True))\n\nep = torch.export.export(\n    untrained_model, (), kwargs=cloned_inputs, dynamic_shapes=dynamic_shapes\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works.\n\n## ExportedProgram\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    ep = torch.export.export(\n        untrained_model, (), kwargs=cloned_inputs, dynamic_shapes=dynamic_shapes\n    )\n    print(\"It worked:\")\n    print(ep)\nexcept Exception as e:\n    # To work, it needs at least PRs:\n    # * https://github.com/huggingface/transformers/pull/36311\n    # * https://github.com/huggingface/transformers/pull/36652\n    print(\"It failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Back to the original model\n\nLet's use the same dummy inputs but we use the downloaded model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    ep = torch.export.export(model, (), kwargs=cloned_inputs, dynamic_shapes=dynamic_shapes)\n    print(\"It worked:\")\n    print(ep)\nexcept Exception as e:\n    # To work, it needs at least PRs:\n    # * https://github.com/huggingface/transformers/pull/36311\n    # * https://github.com/huggingface/transformers/pull/36652\n    print(\"It failed:\", e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}