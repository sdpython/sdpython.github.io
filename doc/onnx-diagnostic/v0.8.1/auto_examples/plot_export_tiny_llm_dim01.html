<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="Export with dynamic dimensions in {0,1} into ONNX" href="plot_export_tiny_llm_dim01_onnx.html"><link rel="prev" title="Export with DynamicCache and guessed dynamic shapes" href="plot_export_with_dynamic_cache.html">
        <link rel="prefetch" href="../_static/logo.png" as="image">

    <!-- Generated with Sphinx 8.1.3 and Furo 2025.09.25 -->
        <title>Export with dynamic dimensions in {0,1} - onnx-diagnostic 0.8.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a7360d90" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">onnx-diagnostic 0.8.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">onnx-diagnostic 0.8.1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../patches.html">Patches Explained</a><input aria-label="Toggle navigation of Patches Explained" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../status/index.html">Exporter Status</a><input aria-label="Toggle navigation of Exporter Status" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../status/exported_program_dynamic.html">Exported Programs with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/exporter_dynamic.html">Exported ONNX with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/patches_coverage.html">Coverage of the Patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/patches_diff.html">Patches Diff</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API of onnx_diagnostic</a><input aria-label="Toggle navigation of API of onnx_diagnostic" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/export/index.html">onnx_diagnostic.export</a><input aria-label="Toggle navigation of onnx_diagnostic.export" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/export/api.html">onnx_diagnostic.export.api</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/dynamic_shapes.html">onnx_diagnostic.export.dynamic_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/shape_helper.html">onnx_diagnostic.export.shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/validate.html">onnx_diagnostic.export.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/helpers/index.html">onnx_diagnostic.helpers</a><input aria-label="Toggle navigation of onnx_diagnostic.helpers" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/args_helper.html">onnx_diagnostic.helpers.args_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/bench_run.html">onnx_diagnostic.helpers.bench_run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/cache_helper.html">onnx_diagnostic.helpers.cache_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/config_helper.html">onnx_diagnostic.helpers.config_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/doc_helper.html">onnx_diagnostic.helpers.doc_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/fake_tensor_helper.html">onnx_diagnostic.helpers.fake_tensor_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/graph_helper.html">onnx_diagnostic.helpers.graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/helper.html">onnx_diagnostic.helpers.helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/_log_helper.html">onnx_diagnostic.helpers._log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/log_helper.html">onnx_diagnostic.helpers.log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/memory_peak.html">onnx_diagnostic.helpers.memory_peak</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/mini_onnx_builder.html">onnx_diagnostic.helpers.mini_onnx_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/model_builder_helper.html">onnx_diagnostic.helpers.model_builder_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/onnx_helper.html">onnx_diagnostic.helpers.onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/ort_session.html">onnx_diagnostic.helpers.ort_session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/rt_helper.html">onnx_diagnostic.helpers.rt_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_helper.html">onnx_diagnostic.helpers.torch_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">onnx_diagnostic.reference</a><input aria-label="Toggle navigation of onnx_diagnostic.reference" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">onnx_diagnostic.reference.ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.ops" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">onnx_diagnostic.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">onnx_diagnostic.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">onnx_diagnostic.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">onnx_diagnostic.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">onnx_diagnostic.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">onnx_diagnostic.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">onnx_diagnostic.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">onnx_diagnostic.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">onnx_diagnostic.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">onnx_diagnostic.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">onnx_diagnostic.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">onnx_diagnostic.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">onnx_diagnostic.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">onnx_diagnostic.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">onnx_diagnostic.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">onnx_diagnostic.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">onnx_diagnostic.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">onnx_diagnostic.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">onnx_diagnostic.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">onnx_diagnostic.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">onnx_diagnostic.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">onnx_diagnostic.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">onnx_diagnostic.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/torch_ops/index.html">onnx_diagnostic.reference.torch_ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.torch_ops" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/access_ops.html">onnx_diagnostic.reference.torch_ops.access_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/binary_ops.html">onnx_diagnostic.reference.torch_ops.binary_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/controlflow_ops.html">onnx_diagnostic.reference.torch_ops.controlflow_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/generator_ops.html">onnx_diagnostic.reference.torch_ops.generator_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/nn_ops.html">onnx_diagnostic.reference.torch_ops.nn_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/other_ops.html">onnx_diagnostic.reference.torch_ops.other_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/reduce_ops.html">onnx_diagnostic.reference.torch_ops.reduce_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/sequence_ops.html">onnx_diagnostic.reference.torch_ops.sequence_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/shape_ops.html">onnx_diagnostic.reference.torch_ops.shape_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/unary_ops.html">onnx_diagnostic.reference.torch_ops.unary_ops</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">onnx_diagnostic.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">onnx_diagnostic.reference.quantized_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">onnx_diagnostic.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/report_results_comparison.html">onnx_diagnostic.reference.report_results_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/torch_evaluator.html">onnx_diagnostic.reference.torch_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/tasks/index.html">onnx_diagnostic.tasks</a><input aria-label="Toggle navigation of onnx_diagnostic.tasks" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/automatic_speech_recognition.html">onnx_diagnostic.tasks.automatic_speech_recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/fill_mask.html">onnx_diagnostic.tasks.fill_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/feature_extraction.html">onnx_diagnostic.tasks.feature_extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_classification.html">onnx_diagnostic.tasks.image_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_text_to_text.html">onnx_diagnostic.export.image_text_to_text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/mixture_of_expert.html">onnx_diagnostic.tasks.mixture_of_expert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/object_detection.html">onnx_diagnostic.tasks.object_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/sentence_similarity.html">onnx_diagnostic.tasks.sentence_similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/summarization.html">onnx_diagnostic.tasks.summarization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_classification.html">onnx_diagnostic.tasks.text_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_generation.html">onnx_diagnostic.tasks.text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_to_image.html">onnx_diagnostic.tasks.text_to_image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text2text_generation.html">onnx_diagnostic.tasks.text2text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/zero_shot_image_classification.html">onnx_diagnostic.tasks.zero_shot_image_classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_export_patches/index.html">onnx_diagnostic.torch_export_patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/eval/index.html">onnx_diagnostic.torch_export_patches.eval</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.eval" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/eval/model_cases.html">onnx_diagnostic.torch_export_patches.eval.model_cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_errors.html">onnx_diagnostic.torch_export_patches.onnx_export_errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_serialization.html">onnx_diagnostic.torch_export_patches.onnx_export_serialization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/patches/index.html">onnx_diagnostic.torch_export_patches.patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.patches" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_torch.html">onnx_diagnostic.torch_export_patches.patches.patch_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_transformers.html">onnx_diagnostic.torch_export_patches.patches.patch_transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_details.html">onnx_diagnostic.torch_export_patches.patch_details</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_expressions.html">onnx_diagnostic.torch_export_patches.patch_expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_inputs.html">onnx_diagnostic.torch_export_patches.patch_inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module.html">onnx_diagnostic.torch_export_patches.patch_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module_helper.html">onnx_diagnostic.torch_export_patches.patch_module_helper</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/serialization/index.html">onnx_diagnostic.torch_export_patches.serialization</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.serialization" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/diffusers_impl.html">onnx_diagnostic.torch_export_patches.serialization.diffusers_impl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/transformers_impl.html">onnx_diagnostic.torch_export_patches.serialization.transformers_impl</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">onnx_diagnostic.torch_models</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/code_sample.html">onnx_diagnostic.torch_models.code_sample</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_models/hghub/index.html">onnx_diagnostic.torch_models.hghub</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models.hghub" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_api.html">onnx_diagnostic.torch_models.hghub.hub_api</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_data.html">onnx_diagnostic.torch_models.hghub.hub_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/model_inputs.html">onnx_diagnostic.torch_models.hghub.model_inputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llms.html">onnx_diagnostic.torch_models.llms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/validate.html">onnx_diagnostic.torch_models.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_onnx/index.html">onnx_diagnostic.torch_onnx</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_onnx" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/runtime_info.html">onnx_diagnostic.torch_onnx.runtime_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs.html">onnx_diagnostic.torch_onnx.sbs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/api.html">onnx_diagnostic.api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">onnx_diagnostic.ext_test_case</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cmds/index.html">Command Lines</a><input aria-label="Toggle navigation of Command Lines" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cmds/config.html">-m onnx_diagnostic config … prints the config for a model id</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/validate.html">-m onnx_diagnostic validate … validate a model id</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Examples Gallery</a><input aria-label="Toggle navigation of Examples Gallery" checked="" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_dump_intermediate_results.html">Dumps intermediate results of a torch model</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_args_kwargs.html">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_patched.html">Export Tiny-LLM with patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_phi2.html">Export microsoft/phi-2</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_dynamic_cache.html">Export with DynamicCache and guessed dynamic shapes</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx_custom.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX (custom)</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_locate_issue.html">Find and fix an export issue due to dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_model_extract.html">Find where a model is failing by running submodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_reference_evaluator.html">Intermediate results with (ONNX) ReferenceEvaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_onnxruntime_evaluator.html">Intermediate results with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm.html">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_hub_codellama.html">Test the export on untrained models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_recipes/index.html">Common Export Issues</a><input aria-label="Toggle navigation of Common Export Issues" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_dim1.html">0, 1, 2 for a Dynamic Dimension in the dummy example to export a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_what.html">Builds dynamic shapes from any input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_max.html">Cannot export <code class="docutils literal notranslate"><span class="pre">torch.sym_max(x.shape[0],</span> <span class="pre">y.shape[0])</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_python_int.html">Do not use python int with dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_cond.html">Export a model with a control flow (If)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_nonzero.html">Half certain nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_json.html">JSON returns list when the original dynamic shapes are list or tuple</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_with_dynamic.html">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_technical/index.html">Technical Details</a><input aria-label="Toggle navigation of Technical Details" class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_broadcast_export_issue.html">Dynamic Shapes and Broadcasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_generate.html">From a LLM to processing a prompt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_layer_norm_discrepancies.html">LayerNormalization implementation cannot be exchanged</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_parallelized_reduction.html">Reproducible Parallelized Reduction is difficult</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_examples/plot_export_tiny_llm_dim01.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-export-tiny-llm-dim01-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="export-with-dynamic-dimensions-in-0-1">
<span id="l-plot-tiny-llm-export-dim01"></span><span id="sphx-glr-auto-examples-plot-export-tiny-llm-dim01-py"></span><h1>Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code><a class="headerlink" href="#export-with-dynamic-dimensions-in-0-1" title="Link to this heading">¶</a></h1>
<p>The first version of <a class="reference external" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a> did not support
any tensor with a dimension equal to 0, 1 if the dimension was expected
to be dynamic. The latest versions offers more options. Let’s check it works.
The experiments consists in exporting the model with different sets of inputs
and checking the exported models works with all set of inputs.</p>
<section id="available-input-sets">
<h2>Available input sets<a class="headerlink" href="#available-input-sets" title="Link to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic</span><span class="w"> </span><span class="kn">import</span> <span class="n">doc</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.helpers</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.max_diff" title="onnx_diagnostic.helpers.max_diff" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.max_diff" title="onnx_diagnostic.helpers.max_diff" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">max_diff</span></a></a><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.helpers.torch_helper</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><span class="n">torch_deepcopy</span></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_models.hghub.model_inputs</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><span class="n">get_untrained_model_with_inputs</span></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches.patch_inputs</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a><span class="p">,</span>
    <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">register_additional_serialization_functions</span></a></a><span class="p">,</span>
<span class="p">)</span>


<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/hghub/index.html#onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" title="onnx_diagnostic.torch_models.hghub.get_untrained_model_with_inputs" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-hghub sphx-glr-backref-type-py-function"><span class="n">get_untrained_model_with_inputs</span></a></a><span class="p">(</span><span class="s2">&quot;arnir0/Tiny-LLM&quot;</span><span class="p">,</span> <span class="n">add_second_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>The trained model can be obtained with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;arnir0/Tiny-LLM&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a> <span class="o">=</span> <span class="p">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">:</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a> <span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">)}</span>

<span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="si">:</span><span class="s2">20</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a><span class="p">,</span><span class="w"> </span><span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>inputs              : dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x30x96], value_cache=#1[T1s2x1x30x96]))
inputs2             : dict(input_ids:T7s3x4,attention_mask:T7s3x35,position_ids:T7s3x4,past_key_values:DynamicCache(key_cache=#1[T1s3x1x31x96], value_cache=#1[T1s3x1x31x96]))
inputs_empty_cache  : dict(input_ids:T7s2x3,attention_mask:T7s2x3,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x0x96], value_cache=#1[T1s2x1x0x96]))
inputs_batch1       : dict(input_ids:T7s1x3,attention_mask:T7s1x33,position_ids:T7s1x3,past_key_values:DynamicCache(key_cache=#1[T1s1x1x30x96], value_cache=#1[T1s1x1x30x96]))
</pre></div>
</div>
<p>The dynamic shapes are:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dynamic_shapes: </span><span class="si">{</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dynamic_shapes: dict(input_ids:{0:DYN(batch),1:DYN(seq_length)},attention_mask:{0:DYN(batch),1:DYN(cache+seq)},position_ids:{0:DYN(batch),1:DYN(seq_length)},past_key_values:#2[{0:DYN(batch),2:DYN(cache_length)},{0:DYN(batch),2:DYN(cache_length)}])
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dynamic_shapes: </span><span class="si">{</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dynamic_shapes: dict(input_ids:{0:DYNAMIC,1:DYNAMIC},attention_mask:{0:DYNAMIC,1:DYNAMIC},position_ids:{0:DYNAMIC,1:DYNAMIC},past_key_values:#2[{0:DYNAMIC,2:DYNAMIC},{0:DYNAMIC,2:DYNAMIC}])
</pre></div>
</div>
<p>Let’s check they all work and compute the expected values.
We use deepcopy because caches are usually modified inplace.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">expected</span></a></a> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">expected</span></a></a><span class="p">[</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><span class="n">torch_deepcopy</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="si">:</span><span class="s2">20</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">expected</span></a></a><span class="p">[</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">],</span><span class="w"> </span><span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>inputs              : CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s2x1x33x96], value_cache=#1[T1s2x1x33x96]))
inputs2             : CausalLMOutputWithPast(logits:T1s3x4x32000,past_key_values:DynamicCache(key_cache=#1[T1s3x1x35x96], value_cache=#1[T1s3x1x35x96]))
inputs_empty_cache  : CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s2x1x3x96], value_cache=#1[T1s2x1x3x96]))
inputs_batch1       : CausalLMOutputWithPast(logits:T1s1x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x33x96], value_cache=#1[T1s1x1x33x96]))
</pre></div>
</div>
</section>
<section id="export-with-options">
<h2>Export with options<a class="headerlink" href="#export-with-options" title="Link to this heading">¶</a></h2>
<p>We try to export with the following options:</p>
<ul class="simple">
<li><p>cache registration: register cache serialization with
<a class="reference internal" href="../api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.torch_export_patches.register_additional_serialization_functions()</span></code></a></p></li>
<li><p>oblivious: an option to remove some the exception raises by the exporter</p></li>
<li><p>rt: see <code class="docutils literal notranslate"><span class="pre">prefer_deferred_runtime_asserts_over_guards</span></code> in <a class="reference external" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a></p></li>
<li><p>cache_patch: patches the model before exporting with
<a class="reference internal" href="../api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.torch_export_patches.torch_export_patches()</span></code></a></p></li>
</ul>
<p>Some function first.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">export_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a> <span class="ow">and</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="p">:</span>
        <span class="k">with</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">register_additional_serialization_functions</span></a></a><span class="p">(</span><span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">export_model</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a>
            <span class="p">)</span>
    <span class="k">if</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="p">:</span>
        <span class="k">with</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">torch_export_patches</span></a></a><span class="p">(</span>
            <span class="n">patch_torch</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">patch_transformers</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;transformers&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="n">export_model</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a>
            <span class="p">)</span>
    <span class="k">if</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="n">backed_size_oblivious</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">export_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="p">)</span>
    <span class="k">return</span> <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="p">(),</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span>
        <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="p">,</span>
        <span class="n">prefer_deferred_runtime_asserts_over_guards</span><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">try_export_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">export_model</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span>
            <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="p">,</span>
            <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="p">,</span>
            <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="p">,</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="p">,</span>
            <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">e</span>


<span class="k">def</span><span class="w"> </span><span class="nf">validation</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">expected</span></a></a><span class="p">):</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">ep</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
    <span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">got</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="o">**</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><span class="n">torch_deepcopy</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a><span class="p">))</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">yield</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">,</span> <span class="n">e</span>
            <span class="k">continue</span>
        <span class="k">yield</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.max_diff" title="onnx_diagnostic.helpers.max_diff" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.max_diff" title="onnx_diagnostic.helpers.max_diff" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">max_diff</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">expected</span></a></a><span class="p">[</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">],</span> <span class="n">got</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-main-loop">
<h2>The main loop<a class="headerlink" href="#the-main-loop" title="Link to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">results</span></a></a> <span class="o">=</span> <span class="p">[]</span>

<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">possibilities</span></a></a> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)],</span> <span class="nb">list</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a><span class="p">)]</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">possibilities</span></a></a><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="s2">&quot;transformers&quot;</span><span class="p">]</span>
<span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><a href="https://docs.python.org/3/library/itertools.html#itertools.product" title="itertools.product" class="sphx-glr-backref-module-itertools sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/itertools.html#itertools.product" title="itertools.product" class="sphx-glr-backref-module-itertools sphx-glr-backref-type-py-function"><span class="n">itertools</span><span class="o">.</span><span class="n">product</span></a></a><span class="p">(</span><span class="o">*</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">possibilities</span></a></a><span class="p">)))</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
    <span class="k">for</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">if</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a> <span class="ow">and</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="p">:</span>
            <span class="c1"># patches include caches.</span>
            <span class="k">continue</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">kwargs</span></a></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cache_patch</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">strict</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">oblivious</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">rt</span></a></a>
        <span class="p">)</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">legend</span></a></a> <span class="o">=</span> <span class="s2">&quot;-&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="si">}</span><span class="s2">:</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">kwargs</span></a></a><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">v</span></a></a>
        <span class="p">)</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">legend</span></a></a> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">legend</span></a></a><span class="si">}</span><span class="s2">/</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">legend</span></a></a><span class="si">}</span><span class="s2"> EXPORT&quot;</span><span class="p">)</span>

        <span class="c1"># export</span>
        <span class="n">ep</span> <span class="o">=</span> <span class="n">try_export_model</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.torch_deepcopy" title="onnx_diagnostic.helpers.torch_helper.torch_deepcopy" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><span class="n">torch_deepcopy</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a><span class="p">[</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">]),</span> <span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">kwargs</span></a></a>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="ne">Exception</span><span class="p">):</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">obs</span></a></a> <span class="o">=</span> <span class="p">{</span>
                <span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">kwargs</span></a></a><span class="p">,</span>
                <span class="s2">&quot;export_with&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span>
                <span class="s2">&quot;EXPORT&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s2">&quot;ERR-EXPORT&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">ep</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
            <span class="p">}</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">results</span></a></a><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">obs</span></a></a><span class="p">)</span>
            <span class="k">continue</span>

        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">legend</span></a></a><span class="si">}</span><span class="s2"> VALIDATE&quot;</span><span class="p">)</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">common</span></a></a> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">kwargs</span></a></a><span class="p">,</span> <span class="s2">&quot;export_with&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a><span class="p">,</span> <span class="s2">&quot;EXPORT&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
        <span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inp</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res</span></a></a> <span class="ow">in</span> <span class="n">validation</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_sets</span></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">expected</span></a></a><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res</span></a></a><span class="p">,</span> <span class="ne">Exception</span><span class="p">):</span>
                <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">obs</span></a></a> <span class="o">=</span> <span class="p">{</span>
                    <span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">common</span></a></a><span class="p">,</span>
                    <span class="s2">&quot;run_with&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inp</span></a></a><span class="p">,</span>
                    <span class="s2">&quot;ERR-RUN&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res</span></a></a><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="s2">&quot;WORKS&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">obs</span></a></a> <span class="o">=</span> <span class="p">{</span>
                    <span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">common</span></a></a><span class="p">,</span>
                    <span class="s2">&quot;run_with&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inp</span></a></a><span class="p">,</span>
                    <span class="s2">&quot;WORKS&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="o">~</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ufunc.html#numpy.ufunc" title="numpy.ufunc" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ufunc.html#numpy.ufunc" title="numpy.ufunc" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">np</span><span class="o">.</span><span class="n">isnan</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res</span></a></a><span class="p">[</span><span class="s2">&quot;abs&quot;</span><span class="p">])</span> <span class="ow">and</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res</span></a></a><span class="p">[</span><span class="s2">&quot;abs&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">),</span>
                <span class="p">}</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">results</span></a></a><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">obs</span></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/256 [00:00&lt;?, ?it/s]
/inputs EXPORT:   0%|          | 0/256 [00:00&lt;?, ?it/s]
/inputs VALIDATE:   0%|          | 0/256 [00:00&lt;?, ?it/s]
/inputs2 EXPORT:   0%|          | 0/256 [00:00&lt;?, ?it/s]
/inputs2 VALIDATE:   0%|          | 0/256 [00:01&lt;?, ?it/s]
/inputs2 VALIDATE:   1%|          | 2/256 [00:01&lt;03:12,  1.32it/s]
/inputs_empty_cache EXPORT:   1%|          | 2/256 [00:01&lt;03:12,  1.32it/s]
/inputs_empty_cache EXPORT:   1%|          | 3/256 [00:02&lt;03:49,  1.10it/s]
/inputs_batch1 EXPORT:   1%|          | 3/256 [00:02&lt;03:49,  1.10it/s]
/inputs_batch1 EXPORT:   2%|▏         | 4/256 [00:03&lt;03:39,  1.15it/s]
rt/inputs EXPORT:   2%|▏         | 4/256 [00:03&lt;03:39,  1.15it/s]
rt/inputs VALIDATE:   2%|▏         | 4/256 [00:04&lt;03:39,  1.15it/s]
rt/inputs VALIDATE:   2%|▏         | 5/256 [00:04&lt;04:12,  1.00s/it]
rt/inputs2 EXPORT:   2%|▏         | 5/256 [00:04&lt;04:12,  1.00s/it]
rt/inputs2 VALIDATE:   2%|▏         | 5/256 [00:06&lt;04:12,  1.00s/it]
rt/inputs2 VALIDATE:   2%|▏         | 6/256 [00:06&lt;04:56,  1.19s/it]
rt/inputs_empty_cache EXPORT:   2%|▏         | 6/256 [00:06&lt;04:56,  1.19s/it]
rt/inputs_empty_cache EXPORT:   3%|▎         | 7/256 [00:07&lt;05:22,  1.29s/it]
rt/inputs_batch1 EXPORT:   3%|▎         | 7/256 [00:07&lt;05:22,  1.29s/it]
rt/inputs_batch1 EXPORT:   3%|▎         | 8/256 [00:08&lt;04:42,  1.14s/it]
oblivious/inputs EXPORT:   3%|▎         | 8/256 [00:08&lt;04:42,  1.14s/it]
oblivious/inputs VALIDATE:   3%|▎         | 8/256 [00:10&lt;04:42,  1.14s/it]
oblivious/inputs VALIDATE:   4%|▎         | 9/256 [00:10&lt;05:16,  1.28s/it]
oblivious/inputs2 EXPORT:   4%|▎         | 9/256 [00:10&lt;05:16,  1.28s/it]
oblivious/inputs2 VALIDATE:   4%|▎         | 9/256 [00:11&lt;05:16,  1.28s/it]
oblivious/inputs2 VALIDATE:   4%|▍         | 10/256 [00:11&lt;05:50,  1.43s/it]
oblivious/inputs_empty_cache EXPORT:   4%|▍         | 10/256 [00:11&lt;05:50,  1.43s/it]
oblivious/inputs_empty_cache VALIDATE:   4%|▍         | 10/256 [00:12&lt;05:50,  1.43s/it]
oblivious/inputs_empty_cache VALIDATE:   4%|▍         | 11/256 [00:13&lt;05:31,  1.35s/it]
oblivious/inputs_batch1 EXPORT:   4%|▍         | 11/256 [00:13&lt;05:31,  1.35s/it]
oblivious/inputs_batch1 VALIDATE:   4%|▍         | 11/256 [00:14&lt;05:31,  1.35s/it]
oblivious/inputs_batch1 VALIDATE:   5%|▍         | 12/256 [00:14&lt;05:50,  1.44s/it]
oblivious-rt/inputs EXPORT:   5%|▍         | 12/256 [00:14&lt;05:50,  1.44s/it]
oblivious-rt/inputs VALIDATE:   5%|▍         | 12/256 [00:15&lt;05:50,  1.44s/it]
oblivious-rt/inputs VALIDATE:   5%|▌         | 13/256 [00:15&lt;05:28,  1.35s/it]
oblivious-rt/inputs2 EXPORT:   5%|▌         | 13/256 [00:15&lt;05:28,  1.35s/it]
oblivious-rt/inputs2 VALIDATE:   5%|▌         | 13/256 [00:16&lt;05:28,  1.35s/it]
oblivious-rt/inputs2 VALIDATE:   5%|▌         | 14/256 [00:16&lt;05:08,  1.27s/it]
oblivious-rt/inputs_empty_cache EXPORT:   5%|▌         | 14/256 [00:16&lt;05:08,  1.27s/it]
oblivious-rt/inputs_empty_cache VALIDATE:   5%|▌         | 14/256 [00:18&lt;05:08,  1.27s/it]
oblivious-rt/inputs_empty_cache VALIDATE:   6%|▌         | 15/256 [00:18&lt;04:59,  1.24s/it]
oblivious-rt/inputs_batch1 EXPORT:   6%|▌         | 15/256 [00:18&lt;04:59,  1.24s/it]
oblivious-rt/inputs_batch1 VALIDATE:   6%|▌         | 15/256 [00:19&lt;04:59,  1.24s/it]
oblivious-rt/inputs_batch1 VALIDATE:   6%|▋         | 16/256 [00:19&lt;04:46,  1.19s/it]
strict/inputs EXPORT:   6%|▋         | 16/256 [00:19&lt;04:46,  1.19s/it]
strict/inputs EXPORT:   7%|▋         | 17/256 [00:20&lt;04:26,  1.11s/it]
strict/inputs2 EXPORT:   7%|▋         | 17/256 [00:20&lt;04:26,  1.11s/it]
strict/inputs2 EXPORT:   7%|▋         | 18/256 [00:20&lt;03:34,  1.11it/s]
strict/inputs_empty_cache EXPORT:   7%|▋         | 18/256 [00:20&lt;03:34,  1.11it/s]
strict/inputs_empty_cache EXPORT:   7%|▋         | 19/256 [00:21&lt;03:46,  1.05it/s]
strict/inputs_batch1 EXPORT:   7%|▋         | 19/256 [00:21&lt;03:46,  1.05it/s]
strict/inputs_batch1 EXPORT:   8%|▊         | 20/256 [00:22&lt;03:07,  1.26it/s]
strict-rt/inputs EXPORT:   8%|▊         | 20/256 [00:22&lt;03:07,  1.26it/s]
strict-rt/inputs EXPORT:   8%|▊         | 21/256 [00:22&lt;02:39,  1.47it/s]
strict-rt/inputs2 EXPORT:   8%|▊         | 21/256 [00:22&lt;02:39,  1.47it/s]
strict-rt/inputs2 EXPORT:   9%|▊         | 22/256 [00:22&lt;02:23,  1.63it/s]
strict-rt/inputs_empty_cache EXPORT:   9%|▊         | 22/256 [00:22&lt;02:23,  1.63it/s]
strict-rt/inputs_empty_cache EXPORT:   9%|▉         | 23/256 [00:23&lt;02:08,  1.81it/s]
strict-rt/inputs_batch1 EXPORT:   9%|▉         | 23/256 [00:23&lt;02:08,  1.81it/s]
strict-rt/inputs_batch1 EXPORT:   9%|▉         | 24/256 [00:23&lt;02:00,  1.93it/s]
strict-oblivious/inputs EXPORT:   9%|▉         | 24/256 [00:23&lt;02:00,  1.93it/s]
strict-oblivious/inputs EXPORT:  10%|▉         | 25/256 [00:24&lt;01:58,  1.95it/s]
strict-oblivious/inputs2 EXPORT:  10%|▉         | 25/256 [00:24&lt;01:58,  1.95it/s]
strict-oblivious/inputs2 EXPORT:  10%|█         | 26/256 [00:24&lt;01:54,  2.00it/s]
strict-oblivious/inputs_empty_cache EXPORT:  10%|█         | 26/256 [00:24&lt;01:54,  2.00it/s]
strict-oblivious/inputs_empty_cache EXPORT:  11%|█         | 27/256 [00:25&lt;01:53,  2.01it/s]
strict-oblivious/inputs_batch1 EXPORT:  11%|█         | 27/256 [00:25&lt;01:53,  2.01it/s]
strict-oblivious/inputs_batch1 EXPORT:  11%|█         | 28/256 [00:25&lt;01:47,  2.12it/s]
strict-oblivious-rt/inputs EXPORT:  11%|█         | 28/256 [00:25&lt;01:47,  2.12it/s]
strict-oblivious-rt/inputs EXPORT:  11%|█▏        | 29/256 [00:26&lt;01:46,  2.12it/s]
strict-oblivious-rt/inputs2 EXPORT:  11%|█▏        | 29/256 [00:26&lt;01:46,  2.12it/s]
strict-oblivious-rt/inputs2 EXPORT:  12%|█▏        | 30/256 [00:26&lt;01:45,  2.15it/s]
strict-oblivious-rt/inputs_empty_cache EXPORT:  12%|█▏        | 30/256 [00:26&lt;01:45,  2.15it/s]
strict-oblivious-rt/inputs_empty_cache EXPORT:  12%|█▏        | 31/256 [00:26&lt;01:40,  2.24it/s]
strict-oblivious-rt/inputs_batch1 EXPORT:  12%|█▏        | 31/256 [00:26&lt;01:40,  2.24it/s]
strict-oblivious-rt/inputs_batch1 EXPORT:  12%|█▎        | 32/256 [00:27&lt;01:43,  2.17it/s]
cache/inputs EXPORT:  12%|█▎        | 32/256 [00:27&lt;01:43,  2.17it/s]
cache/inputs VALIDATE:  12%|█▎        | 32/256 [00:28&lt;01:43,  2.17it/s]
cache/inputs VALIDATE:  50%|█████     | 129/256 [00:28&lt;00:02, 46.33it/s]
cache/inputs2 EXPORT:  50%|█████     | 129/256 [00:28&lt;00:02, 46.33it/s]
cache/inputs2 VALIDATE:  50%|█████     | 129/256 [00:29&lt;00:02, 46.33it/s]
cache/inputs_empty_cache EXPORT:  50%|█████     | 129/256 [00:29&lt;00:02, 46.33it/s]
cache/inputs_batch1 EXPORT:  50%|█████     | 129/256 [00:30&lt;00:02, 46.33it/s]
cache/inputs_batch1 EXPORT:  52%|█████▏    | 132/256 [00:29&lt;00:03, 33.11it/s]
cache-rt/inputs EXPORT:  52%|█████▏    | 132/256 [00:29&lt;00:03, 33.11it/s]
cache-rt/inputs VALIDATE:  52%|█████▏    | 132/256 [00:30&lt;00:03, 33.11it/s]
cache-rt/inputs2 EXPORT:  52%|█████▏    | 132/256 [00:30&lt;00:03, 33.11it/s]
cache-rt/inputs2 VALIDATE:  52%|█████▏    | 132/256 [00:31&lt;00:03, 33.11it/s]
cache-rt/inputs_empty_cache EXPORT:  52%|█████▏    | 132/256 [00:31&lt;00:03, 33.11it/s]
cache-rt/inputs_empty_cache EXPORT:  53%|█████▎    | 135/256 [00:32&lt;00:11, 10.47it/s]
cache-rt/inputs_batch1 EXPORT:  53%|█████▎    | 135/256 [00:32&lt;00:11, 10.47it/s]
cache-oblivious/inputs EXPORT:  53%|█████▎    | 135/256 [00:33&lt;00:11, 10.47it/s]
cache-oblivious/inputs VALIDATE:  53%|█████▎    | 135/256 [00:34&lt;00:11, 10.47it/s]
cache-oblivious/inputs VALIDATE:  54%|█████▎    | 137/256 [00:34&lt;00:16,  7.37it/s]
cache-oblivious/inputs2 EXPORT:  54%|█████▎    | 137/256 [00:34&lt;00:16,  7.37it/s]
cache-oblivious/inputs2 VALIDATE:  54%|█████▎    | 137/256 [00:35&lt;00:16,  7.37it/s]
cache-oblivious/inputs_empty_cache EXPORT:  54%|█████▎    | 137/256 [00:35&lt;00:16,  7.37it/s]
cache-oblivious/inputs_empty_cache VALIDATE:  54%|█████▎    | 137/256 [00:36&lt;00:16,  7.37it/s]
cache-oblivious/inputs_empty_cache VALIDATE:  54%|█████▍    | 139/256 [00:36&lt;00:22,  5.13it/s]
cache-oblivious/inputs_batch1 EXPORT:  54%|█████▍    | 139/256 [00:36&lt;00:22,  5.13it/s]
cache-oblivious/inputs_batch1 VALIDATE:  54%|█████▍    | 139/256 [00:37&lt;00:22,  5.13it/s]
cache-oblivious/inputs_batch1 VALIDATE:  55%|█████▍    | 140/256 [00:37&lt;00:25,  4.46it/s]
cache-oblivious-rt/inputs EXPORT:  55%|█████▍    | 140/256 [00:37&lt;00:25,  4.46it/s]
cache-oblivious-rt/inputs VALIDATE:  55%|█████▍    | 140/256 [00:38&lt;00:25,  4.46it/s]
cache-oblivious-rt/inputs VALIDATE:  55%|█████▌    | 141/256 [00:38&lt;00:32,  3.54it/s]
cache-oblivious-rt/inputs2 EXPORT:  55%|█████▌    | 141/256 [00:38&lt;00:32,  3.54it/s]
cache-oblivious-rt/inputs2 VALIDATE:  55%|█████▌    | 141/256 [00:39&lt;00:32,  3.54it/s]
cache-oblivious-rt/inputs2 VALIDATE:  55%|█████▌    | 142/256 [00:39&lt;00:38,  2.95it/s]
cache-oblivious-rt/inputs_empty_cache EXPORT:  55%|█████▌    | 142/256 [00:39&lt;00:38,  2.95it/s]
cache-oblivious-rt/inputs_empty_cache VALIDATE:  55%|█████▌    | 142/256 [00:40&lt;00:38,  2.95it/s]
cache-oblivious-rt/inputs_empty_cache VALIDATE:  56%|█████▌    | 143/256 [00:40&lt;00:45,  2.50it/s]
cache-oblivious-rt/inputs_batch1 EXPORT:  56%|█████▌    | 143/256 [00:40&lt;00:45,  2.50it/s]
cache-oblivious-rt/inputs_batch1 VALIDATE:  56%|█████▌    | 143/256 [00:42&lt;00:45,  2.50it/s]
cache-oblivious-rt/inputs_batch1 VALIDATE:  56%|█████▋    | 144/256 [00:42&lt;01:01,  1.83it/s]
cache-strict/inputs EXPORT:  56%|█████▋    | 144/256 [00:42&lt;01:01,  1.83it/s]
cache-strict/inputs EXPORT:  57%|█████▋    | 145/256 [00:42&lt;00:58,  1.91it/s]
cache-strict/inputs2 EXPORT:  57%|█████▋    | 145/256 [00:42&lt;00:58,  1.91it/s]
cache-strict/inputs2 EXPORT:  57%|█████▋    | 146/256 [00:42&lt;00:55,  1.99it/s]
cache-strict/inputs_empty_cache EXPORT:  57%|█████▋    | 146/256 [00:42&lt;00:55,  1.99it/s]
cache-strict/inputs_empty_cache EXPORT:  57%|█████▋    | 147/256 [00:43&lt;00:52,  2.06it/s]
cache-strict/inputs_batch1 EXPORT:  57%|█████▋    | 147/256 [00:43&lt;00:52,  2.06it/s]
cache-strict/inputs_batch1 EXPORT:  58%|█████▊    | 148/256 [00:43&lt;00:50,  2.16it/s]
cache-strict-rt/inputs EXPORT:  58%|█████▊    | 148/256 [00:43&lt;00:50,  2.16it/s]
cache-strict-rt/inputs EXPORT:  58%|█████▊    | 149/256 [00:44&lt;00:48,  2.21it/s]
cache-strict-rt/inputs2 EXPORT:  58%|█████▊    | 149/256 [00:44&lt;00:48,  2.21it/s]
cache-strict-rt/inputs2 EXPORT:  59%|█████▊    | 150/256 [00:44&lt;00:46,  2.28it/s]
cache-strict-rt/inputs_empty_cache EXPORT:  59%|█████▊    | 150/256 [00:44&lt;00:46,  2.28it/s]
cache-strict-rt/inputs_empty_cache EXPORT:  59%|█████▉    | 151/256 [00:44&lt;00:46,  2.28it/s]
cache-strict-rt/inputs_batch1 EXPORT:  59%|█████▉    | 151/256 [00:44&lt;00:46,  2.28it/s]
cache-strict-rt/inputs_batch1 EXPORT:  59%|█████▉    | 152/256 [00:45&lt;00:44,  2.34it/s]
cache-strict-oblivious/inputs EXPORT:  59%|█████▉    | 152/256 [00:45&lt;00:44,  2.34it/s]
cache-strict-oblivious/inputs EXPORT:  60%|█████▉    | 153/256 [00:45&lt;00:44,  2.30it/s]
cache-strict-oblivious/inputs2 EXPORT:  60%|█████▉    | 153/256 [00:45&lt;00:44,  2.30it/s]
cache-strict-oblivious/inputs2 EXPORT:  60%|██████    | 154/256 [00:46&lt;00:43,  2.34it/s]
cache-strict-oblivious/inputs_empty_cache EXPORT:  60%|██████    | 154/256 [00:46&lt;00:43,  2.34it/s]
cache-strict-oblivious/inputs_empty_cache EXPORT:  61%|██████    | 155/256 [00:46&lt;00:43,  2.33it/s]
cache-strict-oblivious/inputs_batch1 EXPORT:  61%|██████    | 155/256 [00:46&lt;00:43,  2.33it/s]
cache-strict-oblivious/inputs_batch1 EXPORT:  61%|██████    | 156/256 [00:47&lt;00:42,  2.34it/s]
cache-strict-oblivious-rt/inputs EXPORT:  61%|██████    | 156/256 [00:47&lt;00:42,  2.34it/s]
cache-strict-oblivious-rt/inputs EXPORT:  61%|██████▏   | 157/256 [00:47&lt;00:41,  2.36it/s]
cache-strict-oblivious-rt/inputs2 EXPORT:  61%|██████▏   | 157/256 [00:47&lt;00:41,  2.36it/s]
cache-strict-oblivious-rt/inputs2 EXPORT:  62%|██████▏   | 158/256 [00:47&lt;00:41,  2.33it/s]
cache-strict-oblivious-rt/inputs_empty_cache EXPORT:  62%|██████▏   | 158/256 [00:47&lt;00:41,  2.33it/s]
cache-strict-oblivious-rt/inputs_empty_cache EXPORT:  62%|██████▏   | 159/256 [00:48&lt;00:43,  2.21it/s]
cache-strict-oblivious-rt/inputs_batch1 EXPORT:  62%|██████▏   | 159/256 [00:48&lt;00:43,  2.21it/s]
cache-strict-oblivious-rt/inputs_batch1 EXPORT:  62%|██████▎   | 160/256 [00:48&lt;00:45,  2.11it/s]
cache-cache_patch:all/inputs EXPORT:  62%|██████▎   | 160/256 [00:48&lt;00:45,  2.11it/s]
cache-cache_patch:all/inputs VALIDATE:  62%|██████▎   | 160/256 [00:49&lt;00:45,  2.11it/s]
cache-cache_patch:all/inputs VALIDATE:  63%|██████▎   | 161/256 [00:49&lt;00:59,  1.59it/s]
cache-cache_patch:all/inputs2 EXPORT:  63%|██████▎   | 161/256 [00:49&lt;00:59,  1.59it/s]
cache-cache_patch:all/inputs2 VALIDATE:  63%|██████▎   | 161/256 [00:50&lt;00:59,  1.59it/s]
cache-cache_patch:all/inputs2 VALIDATE:  63%|██████▎   | 162/256 [00:50&lt;01:07,  1.40it/s]
cache-cache_patch:all/inputs_empty_cache EXPORT:  63%|██████▎   | 162/256 [00:50&lt;01:07,  1.40it/s]
cache-cache_patch:all/inputs_empty_cache VALIDATE:  63%|██████▎   | 162/256 [00:51&lt;01:07,  1.40it/s]
cache-cache_patch:all/inputs_empty_cache VALIDATE:  64%|██████▎   | 163/256 [00:51&lt;01:10,  1.32it/s]
cache-cache_patch:all/inputs_batch1 EXPORT:  64%|██████▎   | 163/256 [00:51&lt;01:10,  1.32it/s]
cache-cache_patch:all/inputs_batch1 VALIDATE:  64%|██████▎   | 163/256 [00:52&lt;01:10,  1.32it/s]
cache-cache_patch:all/inputs_batch1 VALIDATE:  64%|██████▍   | 164/256 [00:52&lt;01:07,  1.36it/s]
cache-cache_patch:all-rt/inputs EXPORT:  64%|██████▍   | 164/256 [00:52&lt;01:07,  1.36it/s]
cache-cache_patch:all-rt/inputs VALIDATE:  64%|██████▍   | 164/256 [00:54&lt;01:07,  1.36it/s]
cache-cache_patch:all-rt/inputs VALIDATE:  64%|██████▍   | 165/256 [00:54&lt;01:33,  1.03s/it]
cache-cache_patch:all-rt/inputs2 EXPORT:  64%|██████▍   | 165/256 [00:54&lt;01:33,  1.03s/it]
cache-cache_patch:all-rt/inputs2 VALIDATE:  64%|██████▍   | 165/256 [00:54&lt;01:33,  1.03s/it]
cache-cache_patch:all-rt/inputs2 VALIDATE:  65%|██████▍   | 166/256 [00:55&lt;01:29,  1.01it/s]
cache-cache_patch:all-rt/inputs_empty_cache EXPORT:  65%|██████▍   | 166/256 [00:55&lt;01:29,  1.01it/s]
cache-cache_patch:all-rt/inputs_empty_cache VALIDATE:  65%|██████▍   | 166/256 [00:55&lt;01:29,  1.01it/s]
cache-cache_patch:all-rt/inputs_empty_cache VALIDATE:  65%|██████▌   | 167/256 [00:55&lt;01:23,  1.07it/s]
cache-cache_patch:all-rt/inputs_batch1 EXPORT:  65%|██████▌   | 167/256 [00:55&lt;01:23,  1.07it/s]
cache-cache_patch:all-rt/inputs_batch1 VALIDATE:  65%|██████▌   | 167/256 [00:56&lt;01:23,  1.07it/s]
cache-cache_patch:all-rt/inputs_batch1 VALIDATE:  66%|██████▌   | 168/256 [00:56&lt;01:15,  1.17it/s]
cache-cache_patch:all-oblivious/inputs EXPORT:  66%|██████▌   | 168/256 [00:56&lt;01:15,  1.17it/s]
cache-cache_patch:all-oblivious/inputs VALIDATE:  66%|██████▌   | 168/256 [00:57&lt;01:15,  1.17it/s]
cache-cache_patch:all-oblivious/inputs VALIDATE:  66%|██████▌   | 169/256 [00:57&lt;01:26,  1.01it/s]
cache-cache_patch:all-oblivious/inputs2 EXPORT:  66%|██████▌   | 169/256 [00:57&lt;01:26,  1.01it/s]
cache-cache_patch:all-oblivious/inputs2 VALIDATE:  66%|██████▌   | 169/256 [00:59&lt;01:26,  1.01it/s]
cache-cache_patch:all-oblivious/inputs2 VALIDATE:  66%|██████▋   | 170/256 [00:59&lt;01:31,  1.06s/it]
cache-cache_patch:all-oblivious/inputs_empty_cache EXPORT:  66%|██████▋   | 170/256 [00:59&lt;01:31,  1.06s/it]
cache-cache_patch:all-oblivious/inputs_empty_cache VALIDATE:  66%|██████▋   | 170/256 [01:00&lt;01:31,  1.06s/it]
cache-cache_patch:all-oblivious/inputs_empty_cache VALIDATE:  67%|██████▋   | 171/256 [01:00&lt;01:29,  1.06s/it]
cache-cache_patch:all-oblivious/inputs_batch1 EXPORT:  67%|██████▋   | 171/256 [01:00&lt;01:29,  1.06s/it]
cache-cache_patch:all-oblivious/inputs_batch1 VALIDATE:  67%|██████▋   | 171/256 [01:01&lt;01:29,  1.06s/it]
cache-cache_patch:all-oblivious/inputs_batch1 VALIDATE:  67%|██████▋   | 172/256 [01:01&lt;01:31,  1.09s/it]
cache-cache_patch:all-oblivious-rt/inputs EXPORT:  67%|██████▋   | 172/256 [01:01&lt;01:31,  1.09s/it]
cache-cache_patch:all-oblivious-rt/inputs VALIDATE:  67%|██████▋   | 172/256 [01:00&lt;01:31,  1.09s/it]
cache-cache_patch:all-oblivious-rt/inputs2 EXPORT:  67%|██████▋   | 172/256 [01:00&lt;01:31,  1.09s/it]
cache-cache_patch:all-oblivious-rt/inputs2 VALIDATE:  67%|██████▋   | 172/256 [01:01&lt;01:31,  1.09s/it]
cache-cache_patch:all-oblivious-rt/inputs2 VALIDATE:  68%|██████▊   | 174/256 [01:01&lt;00:57,  1.42it/s]
cache-cache_patch:all-oblivious-rt/inputs_empty_cache EXPORT:  68%|██████▊   | 174/256 [01:01&lt;00:57,  1.42it/s]
cache-cache_patch:all-oblivious-rt/inputs_empty_cache VALIDATE:  68%|██████▊   | 174/256 [01:03&lt;00:57,  1.42it/s]
cache-cache_patch:all-oblivious-rt/inputs_empty_cache VALIDATE:  68%|██████▊   | 175/256 [01:03&lt;01:08,  1.18it/s]
cache-cache_patch:all-oblivious-rt/inputs_batch1 EXPORT:  68%|██████▊   | 175/256 [01:03&lt;01:08,  1.18it/s]
cache-cache_patch:all-oblivious-rt/inputs_batch1 VALIDATE:  68%|██████▊   | 175/256 [01:04&lt;01:08,  1.18it/s]
cache-cache_patch:all-oblivious-rt/inputs_batch1 VALIDATE:  69%|██████▉   | 176/256 [01:04&lt;01:10,  1.13it/s]
cache-cache_patch:all-strict/inputs EXPORT:  69%|██████▉   | 176/256 [01:04&lt;01:10,  1.13it/s]
cache-cache_patch:all-strict/inputs EXPORT:  69%|██████▉   | 177/256 [01:06&lt;01:39,  1.25s/it]
cache-cache_patch:all-strict/inputs2 EXPORT:  69%|██████▉   | 177/256 [01:06&lt;01:39,  1.25s/it]
cache-cache_patch:all-strict/inputs2 EXPORT:  70%|██████▉   | 178/256 [01:07&lt;01:34,  1.21s/it]
cache-cache_patch:all-strict/inputs_empty_cache EXPORT:  70%|██████▉   | 178/256 [01:07&lt;01:34,  1.21s/it]
cache-cache_patch:all-strict/inputs_empty_cache EXPORT:  70%|██████▉   | 179/256 [01:08&lt;01:25,  1.10s/it]
cache-cache_patch:all-strict/inputs_batch1 EXPORT:  70%|██████▉   | 179/256 [01:08&lt;01:25,  1.10s/it]
cache-cache_patch:all-strict/inputs_batch1 EXPORT:  70%|███████   | 180/256 [01:09&lt;01:18,  1.03s/it]
cache-cache_patch:all-strict-rt/inputs EXPORT:  70%|███████   | 180/256 [01:09&lt;01:18,  1.03s/it]
cache-cache_patch:all-strict-rt/inputs EXPORT:  71%|███████   | 181/256 [01:09&lt;01:13,  1.02it/s]
cache-cache_patch:all-strict-rt/inputs2 EXPORT:  71%|███████   | 181/256 [01:09&lt;01:13,  1.02it/s]
cache-cache_patch:all-strict-rt/inputs2 EXPORT:  71%|███████   | 182/256 [01:11&lt;01:18,  1.06s/it]
cache-cache_patch:all-strict-rt/inputs_empty_cache EXPORT:  71%|███████   | 182/256 [01:11&lt;01:18,  1.06s/it]
cache-cache_patch:all-strict-rt/inputs_empty_cache EXPORT:  71%|███████▏  | 183/256 [01:12&lt;01:20,  1.10s/it]
cache-cache_patch:all-strict-rt/inputs_batch1 EXPORT:  71%|███████▏  | 183/256 [01:12&lt;01:20,  1.10s/it]
cache-cache_patch:all-strict-rt/inputs_batch1 EXPORT:  72%|███████▏  | 184/256 [01:13&lt;01:12,  1.01s/it]
cache-cache_patch:all-strict-oblivious/inputs EXPORT:  72%|███████▏  | 184/256 [01:13&lt;01:12,  1.01s/it]
cache-cache_patch:all-strict-oblivious/inputs EXPORT:  72%|███████▏  | 185/256 [01:14&lt;01:12,  1.02s/it]
cache-cache_patch:all-strict-oblivious/inputs2 EXPORT:  72%|███████▏  | 185/256 [01:14&lt;01:12,  1.02s/it]
cache-cache_patch:all-strict-oblivious/inputs2 EXPORT:  73%|███████▎  | 186/256 [01:15&lt;01:10,  1.01s/it]
cache-cache_patch:all-strict-oblivious/inputs_empty_cache EXPORT:  73%|███████▎  | 186/256 [01:15&lt;01:10,  1.01s/it]
cache-cache_patch:all-strict-oblivious/inputs_empty_cache EXPORT:  73%|███████▎  | 187/256 [01:16&lt;01:08,  1.00it/s]
cache-cache_patch:all-strict-oblivious/inputs_batch1 EXPORT:  73%|███████▎  | 187/256 [01:16&lt;01:08,  1.00it/s]
cache-cache_patch:all-strict-oblivious/inputs_batch1 EXPORT:  73%|███████▎  | 188/256 [01:17&lt;01:09,  1.02s/it]
cache-cache_patch:all-strict-oblivious-rt/inputs EXPORT:  73%|███████▎  | 188/256 [01:17&lt;01:09,  1.02s/it]
cache-cache_patch:all-strict-oblivious-rt/inputs EXPORT:  74%|███████▍  | 189/256 [01:18&lt;01:06,  1.00it/s]
cache-cache_patch:all-strict-oblivious-rt/inputs2 EXPORT:  74%|███████▍  | 189/256 [01:18&lt;01:06,  1.00it/s]
cache-cache_patch:all-strict-oblivious-rt/inputs2 EXPORT:  74%|███████▍  | 190/256 [01:19&lt;01:21,  1.23s/it]
cache-cache_patch:all-strict-oblivious-rt/inputs_empty_cache EXPORT:  74%|███████▍  | 190/256 [01:19&lt;01:21,  1.23s/it]
cache-cache_patch:all-strict-oblivious-rt/inputs_empty_cache EXPORT:  75%|███████▍  | 191/256 [01:20&lt;01:13,  1.13s/it]
cache-cache_patch:all-strict-oblivious-rt/inputs_batch1 EXPORT:  75%|███████▍  | 191/256 [01:20&lt;01:13,  1.13s/it]
cache-cache_patch:all-strict-oblivious-rt/inputs_batch1 EXPORT:  75%|███████▌  | 192/256 [01:21&lt;01:07,  1.06s/it]
cache-cache_patch:torch/inputs EXPORT:  75%|███████▌  | 192/256 [01:21&lt;01:07,  1.06s/it]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch/inputs EXPORT:  75%|███████▌  | 193/256 [01:22&lt;00:51,  1.22it/s]
cache-cache_patch:torch/inputs2 EXPORT:  75%|███████▌  | 193/256 [01:22&lt;00:51,  1.22it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch/inputs2 EXPORT:  76%|███████▌  | 194/256 [01:22&lt;00:39,  1.57it/s]
cache-cache_patch:torch/inputs_empty_cache EXPORT:  76%|███████▌  | 194/256 [01:22&lt;00:39,  1.57it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, 0, 96]&quot;, arg17_1: &quot;f32[s61, 1, 0, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s70)&quot; = 0 + sym_size_int;  sym_size_int = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(0, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s70)&quot; = 0 + sym_size_int_1
    add_2: &quot;Sym(s70)&quot; = add_1 + 0
    sym_size_int_2: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(-s53 + s70)&quot; = add_2 - sym_size_int_2;  add_2 = None
    gt: &quot;Sym(-s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s70)&quot; = sym_size_int_2 &gt; add_1;  sym_size_int_2 = gt_1 = None
    arange_1: &quot;i64[s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_3: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_3, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_3 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_5: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, 0, 96]&quot;, arg17_1: &quot;f32[s61, 1, 0, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s70)&quot; = 0 + sym_size_int;  sym_size_int = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(0, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s70)&quot; = 0 + sym_size_int_1
    add_2: &quot;Sym(s70)&quot; = add_1 + 0
    sym_size_int_2: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(-s53 + s70)&quot; = add_2 - sym_size_int_2;  add_2 = None
    gt: &quot;Sym(-s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s70)&quot; = sym_size_int_2 &gt; add_1;  sym_size_int_2 = gt_1 = None
    arange_1: &quot;i64[s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_3: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_3, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_3 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_5: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None


cache-cache_patch:torch/inputs_empty_cache EXPORT:  76%|███████▌  | 195/256 [01:22&lt;00:30,  1.99it/s]
cache-cache_patch:torch/inputs_batch1 EXPORT:  76%|███████▌  | 195/256 [01:22&lt;00:30,  1.99it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[1, s70]&quot;, arg14_1: &quot;i64[1, s53]&quot;, arg15_1: &quot;i64[1, s9]&quot;, arg16_1: &quot;f32[1, 1, s43, 96]&quot;, arg17_1: &quot;f32[1, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[1, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s43)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s43)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
    add: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[1, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s43 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s43 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s43 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s43 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s43 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s43 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
    select: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
    movedim_1: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
    select_1: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
    movedim_2: &quot;i64[s70]&quot; = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
    unsqueeze: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select, 0);  select = None
    expand: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
    expand_1: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s43 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_4: &quot;Sym(s43 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s70, s43 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[1, s70]&quot;, arg14_1: &quot;i64[1, s53]&quot;, arg15_1: &quot;i64[1, s9]&quot;, arg16_1: &quot;f32[1, 1, s43, 96]&quot;, arg17_1: &quot;f32[1, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[1, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s43)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s43)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
    add: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[1, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s43 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s43 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s43 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s43 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s43 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s43 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
    select: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
    movedim_1: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
    select_1: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
    movedim_2: &quot;i64[s70]&quot; = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
    unsqueeze: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select, 0);  select = None
    expand: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
    expand_1: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s43 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_4: &quot;Sym(s43 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s70, s43 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None


cache-cache_patch:torch/inputs_batch1 EXPORT:  77%|███████▋  | 196/256 [01:22&lt;00:23,  2.50it/s]
cache-cache_patch:torch-rt/inputs EXPORT:  77%|███████▋  | 196/256 [01:22&lt;00:23,  2.50it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-rt/inputs EXPORT:  77%|███████▋  | 197/256 [01:22&lt;00:20,  2.90it/s]
cache-cache_patch:torch-rt/inputs2 EXPORT:  77%|███████▋  | 197/256 [01:22&lt;00:20,  2.90it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-rt/inputs2 EXPORT:  77%|███████▋  | 198/256 [01:23&lt;00:17,  3.28it/s]
cache-cache_patch:torch-rt/inputs_empty_cache EXPORT:  77%|███████▋  | 198/256 [01:23&lt;00:17,  3.28it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, 0, 96]&quot;, arg17_1: &quot;f32[s61, 1, 0, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s70)&quot; = 0 + sym_size_int;  sym_size_int = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(0, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s70)&quot; = 0 + sym_size_int_1
    add_2: &quot;Sym(s70)&quot; = add_1 + 0
    sym_size_int_2: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(-s53 + s70)&quot; = add_2 - sym_size_int_2;  add_2 = None
    gt: &quot;Sym(-s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s70)&quot; = sym_size_int_2 &gt; add_1;  sym_size_int_2 = gt_1 = None
    arange_1: &quot;i64[s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_3: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_3, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_3 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_5: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, 0, 96]&quot;, arg17_1: &quot;f32[s61, 1, 0, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s70)&quot; = 0 + sym_size_int;  sym_size_int = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(0, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s70)&quot; = 0 + sym_size_int_1
    add_2: &quot;Sym(s70)&quot; = add_1 + 0
    sym_size_int_2: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(-s53 + s70)&quot; = add_2 - sym_size_int_2;  add_2 = None
    gt: &quot;Sym(-s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s70)&quot; = sym_size_int_2 &gt; add_1;  sym_size_int_2 = gt_1 = None
    arange_1: &quot;i64[s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_3: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_3, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_3 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_4, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_4, sym_size_int_1]);  unsqueeze_1 = sym_size_int_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_5: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_4, sym_size_int_5]);  unsqueeze_2 = sym_size_int_4 = sym_size_int_5 = expand_2 = None


cache-cache_patch:torch-rt/inputs_empty_cache EXPORT:  78%|███████▊  | 199/256 [01:23&lt;00:15,  3.68it/s]
cache-cache_patch:torch-rt/inputs_batch1 EXPORT:  78%|███████▊  | 199/256 [01:23&lt;00:15,  3.68it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[1, s70]&quot;, arg14_1: &quot;i64[1, s53]&quot;, arg15_1: &quot;i64[1, s9]&quot;, arg16_1: &quot;f32[1, 1, s43, 96]&quot;, arg17_1: &quot;f32[1, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[1, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s43)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s43)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
    add: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[1, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s43 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s43 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s43 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s43 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s43 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s43 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
    select: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
    movedim_1: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
    select_1: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
    movedim_2: &quot;i64[s70]&quot; = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
    unsqueeze: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select, 0);  select = None
    expand: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
    expand_1: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s43 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_4: &quot;Sym(s43 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s70, s43 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[1, s70]&quot;, arg14_1: &quot;i64[1, s53]&quot;, arg15_1: &quot;i64[1, s9]&quot;, arg16_1: &quot;f32[1, 1, s43, 96]&quot;, arg17_1: &quot;f32[1, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[1, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s43)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s43)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1);  arg13_1 = None
    add: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[1, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s43 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s43 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s43 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s43 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s43 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s43 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s43 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  arange_2 = None
    select: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim, 0, 0);  movedim = None
    movedim_1: &quot;i64[1]&quot; = torch.ops.aten.movedim.int(arange_3, 0, 0);  arange_3 = None
    select_1: &quot;i64[]&quot; = torch.ops.aten.select.int(movedim_1, 0, 0);  movedim_1 = None
    movedim_2: &quot;i64[s70]&quot; = torch.ops.aten.movedim.int(arange, 0, 0);  arange = movedim_2 = None
    unsqueeze: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select, 0);  select = None
    expand: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_2]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1]&quot; = torch.ops.aten.unsqueeze.default(select_1, 0);  select_1 = None
    expand_1: &quot;i64[s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_2]);  unsqueeze_1 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s43 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_4: &quot;Sym(s43 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s70, s43 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_2, sym_size_int_4]);  unsqueeze_2 = sym_size_int_2 = sym_size_int_4 = expand_2 = None


cache-cache_patch:torch-rt/inputs_batch1 EXPORT:  78%|███████▊  | 200/256 [01:23&lt;00:13,  4.07it/s]
cache-cache_patch:torch-oblivious/inputs EXPORT:  78%|███████▊  | 200/256 [01:23&lt;00:13,  4.07it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious/inputs EXPORT:  79%|███████▊  | 201/256 [01:23&lt;00:13,  3.96it/s]
cache-cache_patch:torch-oblivious/inputs2 EXPORT:  79%|███████▊  | 201/256 [01:23&lt;00:13,  3.96it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious/inputs2 EXPORT:  79%|███████▉  | 202/256 [01:23&lt;00:13,  4.09it/s]
cache-cache_patch:torch-oblivious/inputs_empty_cache EXPORT:  79%|███████▉  | 202/256 [01:23&lt;00:13,  4.09it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious/inputs_empty_cache EXPORT:  79%|███████▉  | 203/256 [01:24&lt;00:12,  4.21it/s]
cache-cache_patch:torch-oblivious/inputs_batch1 EXPORT:  79%|███████▉  | 203/256 [01:24&lt;00:12,  4.21it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious/inputs_batch1 EXPORT:  80%|███████▉  | 204/256 [01:24&lt;00:12,  4.27it/s]
cache-cache_patch:torch-oblivious-rt/inputs EXPORT:  80%|███████▉  | 204/256 [01:24&lt;00:12,  4.27it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious-rt/inputs EXPORT:  80%|████████  | 205/256 [01:24&lt;00:11,  4.41it/s]
cache-cache_patch:torch-oblivious-rt/inputs2 EXPORT:  80%|████████  | 205/256 [01:24&lt;00:11,  4.41it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious-rt/inputs2 EXPORT:  80%|████████  | 206/256 [01:24&lt;00:10,  4.57it/s]
cache-cache_patch:torch-oblivious-rt/inputs_empty_cache EXPORT:  80%|████████  | 206/256 [01:24&lt;00:10,  4.57it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious-rt/inputs_empty_cache EXPORT:  81%|████████  | 207/256 [01:24&lt;00:10,  4.53it/s]
cache-cache_patch:torch-oblivious-rt/inputs_batch1 EXPORT:  81%|████████  | 207/256 [01:24&lt;00:10,  4.53it/s]


def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None




def forward(self, arg0_1: &quot;f32[32000, 192]&quot;, arg1_1: &quot;f32[192, 192]&quot;, arg2_1: &quot;f32[96, 192]&quot;, arg3_1: &quot;f32[96, 192]&quot;, arg4_1: &quot;f32[192, 192]&quot;, arg5_1: &quot;f32[1024, 192]&quot;, arg6_1: &quot;f32[1024, 192]&quot;, arg7_1: &quot;f32[192, 1024]&quot;, arg8_1: &quot;f32[192]&quot;, arg9_1: &quot;f32[192]&quot;, arg10_1: &quot;f32[192]&quot;, arg11_1: &quot;f32[32000, 192]&quot;, arg12_1: &quot;f32[48]&quot;, arg13_1: &quot;i64[s72, s70]&quot;, arg14_1: &quot;i64[s43, s53]&quot;, arg15_1: &quot;i64[s44, s9]&quot;, arg16_1: &quot;f32[s67, 1, s45, 96]&quot;, arg17_1: &quot;f32[s61, 1, s21, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: &quot;f32[s72, s70, 192]&quot; = torch.ops.aten.embedding.default(arg0_1, arg13_1);  arg0_1 = embedding = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:401 in forward, code: past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    sym_numel_default: &quot;Sym(96*s45*s67)&quot; = torch.ops.aten.sym_numel.default(arg16_1)
    eq: &quot;Sym(False)&quot; = sym_numel_default == 0;  eq = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(arg16_1, 2);  arg16_1 = None
    sym_size_int_1: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arg13_1, 1)
    add: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_1;  sym_size_int_1 = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
    arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
    to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(arg14_1, device(type=&#39;cpu&#39;), torch.bool);  to = None
    eq_1: &quot;Sym(False)&quot; = sym_numel_default == 0;  sym_numel_default = eq_1 = None
    sym_size_int_2: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(arange, 0)
    add_1: &quot;Sym(s45 + s70)&quot; = sym_size_int + sym_size_int_2;  sym_size_int = None
    add_2: &quot;Sym(s45 + s70)&quot; = add_1 + 0
    sym_size_int_3: &quot;Sym(s53)&quot; = torch.ops.aten.sym_size.int(arg14_1, 1);  arg14_1 = None
    sub: &quot;Sym(s45 - s53 + s70)&quot; = add_2 - sym_size_int_3;  add_2 = None
    gt: &quot;Sym(s45 - s53 + s70 &gt; 0)&quot; = sub &gt; 0;  sub = gt = None
    gt_1: &quot;Sym(s53 &gt; s45 + s70)&quot; = sym_size_int_3 &gt; add_1;  sym_size_int_3 = gt_1 = None
    arange_1: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add_1, device = device(type=&#39;cpu&#39;), pin_memory = False);  add_1 = None
    add_: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add_.Tensor(arange_1, 0)
    sym_size_int_4: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arg13_1, 0);  arg13_1 = None
    arange_2: &quot;i64[s72]&quot; = torch.ops.aten.arange.default(sym_size_int_4, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_4 = None
    arange_3: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
    movedim: &quot;i64[s72]&quot; = torch.ops.aten.movedim.int(arange_2, 0, 0);  movedim = None
    unsqueeze: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None
    sym_size_int_5: &quot;Sym(s72)&quot; = torch.ops.aten.sym_size.int(arange_2, 0);  arange_2 = None
    expand: &quot;i64[s72, 1]&quot; = torch.ops.aten.expand.default(unsqueeze, [sym_size_int_5, 1]);  unsqueeze = expand = None
    unsqueeze_1: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
    expand_1: &quot;i64[s72, s70]&quot; = torch.ops.aten.expand.default(unsqueeze_1, [sym_size_int_5, sym_size_int_2]);  unsqueeze_1 = sym_size_int_2 = expand_1 = None
    unsqueeze_2: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_, 0);  add_ = None
    sym_size_int_6: &quot;Sym(s45 + s70)&quot; = torch.ops.aten.sym_size.int(arange_1, 0);  arange_1 = None
    expand_2: &quot;i64[s72, s45 + s70]&quot; = torch.ops.aten.expand.default(unsqueeze_2, [sym_size_int_5, sym_size_int_6]);  unsqueeze_2 = sym_size_int_5 = sym_size_int_6 = expand_2 = None


cache-cache_patch:torch-oblivious-rt/inputs_batch1 EXPORT:  81%|████████▏ | 208/256 [01:25&lt;00:10,  4.54it/s]
cache-cache_patch:torch-strict/inputs EXPORT:  81%|████████▏ | 208/256 [01:25&lt;00:10,  4.54it/s]
cache-cache_patch:torch-strict/inputs EXPORT:  82%|████████▏ | 209/256 [01:25&lt;00:10,  4.29it/s]
cache-cache_patch:torch-strict/inputs2 EXPORT:  82%|████████▏ | 209/256 [01:25&lt;00:10,  4.29it/s]
cache-cache_patch:torch-strict/inputs2 EXPORT:  82%|████████▏ | 210/256 [01:25&lt;00:12,  3.73it/s]
cache-cache_patch:torch-strict/inputs_empty_cache EXPORT:  82%|████████▏ | 210/256 [01:25&lt;00:12,  3.73it/s]
cache-cache_patch:torch-strict/inputs_empty_cache EXPORT:  82%|████████▏ | 211/256 [01:26&lt;00:11,  3.91it/s]
cache-cache_patch:torch-strict/inputs_batch1 EXPORT:  82%|████████▏ | 211/256 [01:26&lt;00:11,  3.91it/s]
cache-cache_patch:torch-strict/inputs_batch1 EXPORT:  83%|████████▎ | 212/256 [01:26&lt;00:10,  4.09it/s]
cache-cache_patch:torch-strict-rt/inputs EXPORT:  83%|████████▎ | 212/256 [01:26&lt;00:10,  4.09it/s]
cache-cache_patch:torch-strict-rt/inputs EXPORT:  83%|████████▎ | 213/256 [01:26&lt;00:09,  4.51it/s]
cache-cache_patch:torch-strict-rt/inputs2 EXPORT:  83%|████████▎ | 213/256 [01:26&lt;00:09,  4.51it/s]
cache-cache_patch:torch-strict-rt/inputs2 EXPORT:  84%|████████▎ | 214/256 [01:26&lt;00:08,  4.90it/s]
cache-cache_patch:torch-strict-rt/inputs_empty_cache EXPORT:  84%|████████▎ | 214/256 [01:26&lt;00:08,  4.90it/s]
cache-cache_patch:torch-strict-rt/inputs_empty_cache EXPORT:  84%|████████▍ | 215/256 [01:26&lt;00:08,  4.87it/s]
cache-cache_patch:torch-strict-rt/inputs_batch1 EXPORT:  84%|████████▍ | 215/256 [01:26&lt;00:08,  4.87it/s]
cache-cache_patch:torch-strict-rt/inputs_batch1 EXPORT:  84%|████████▍ | 216/256 [01:26&lt;00:08,  4.79it/s]
cache-cache_patch:torch-strict-oblivious/inputs EXPORT:  84%|████████▍ | 216/256 [01:26&lt;00:08,  4.79it/s]
cache-cache_patch:torch-strict-oblivious/inputs EXPORT:  85%|████████▍ | 217/256 [01:27&lt;00:07,  5.24it/s]
cache-cache_patch:torch-strict-oblivious/inputs2 EXPORT:  85%|████████▍ | 217/256 [01:27&lt;00:07,  5.24it/s]
cache-cache_patch:torch-strict-oblivious/inputs2 EXPORT:  85%|████████▌ | 218/256 [01:27&lt;00:06,  5.46it/s]
cache-cache_patch:torch-strict-oblivious/inputs_empty_cache EXPORT:  85%|████████▌ | 218/256 [01:27&lt;00:06,  5.46it/s]
cache-cache_patch:torch-strict-oblivious/inputs_empty_cache EXPORT:  86%|████████▌ | 219/256 [01:27&lt;00:07,  4.96it/s]
cache-cache_patch:torch-strict-oblivious/inputs_batch1 EXPORT:  86%|████████▌ | 219/256 [01:27&lt;00:07,  4.96it/s]
cache-cache_patch:torch-strict-oblivious/inputs_batch1 EXPORT:  86%|████████▌ | 220/256 [01:27&lt;00:06,  5.17it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs EXPORT:  86%|████████▌ | 220/256 [01:27&lt;00:06,  5.17it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs EXPORT:  86%|████████▋ | 221/256 [01:27&lt;00:06,  5.33it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs2 EXPORT:  86%|████████▋ | 221/256 [01:27&lt;00:06,  5.33it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs2 EXPORT:  87%|████████▋ | 222/256 [01:28&lt;00:06,  4.97it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs_empty_cache EXPORT:  87%|████████▋ | 222/256 [01:28&lt;00:06,  4.97it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs_empty_cache EXPORT:  87%|████████▋ | 223/256 [01:28&lt;00:06,  5.25it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs_batch1 EXPORT:  87%|████████▋ | 223/256 [01:28&lt;00:06,  5.25it/s]
cache-cache_patch:torch-strict-oblivious-rt/inputs_batch1 EXPORT:  88%|████████▊ | 224/256 [01:28&lt;00:05,  5.55it/s]
cache-cache_patch:transformers/inputs EXPORT:  88%|████████▊ | 224/256 [01:28&lt;00:05,  5.55it/s]
cache-cache_patch:transformers/inputs VALIDATE:  88%|████████▊ | 224/256 [01:29&lt;00:05,  5.55it/s]
cache-cache_patch:transformers/inputs VALIDATE:  88%|████████▊ | 225/256 [01:29&lt;00:15,  1.97it/s]
cache-cache_patch:transformers/inputs2 EXPORT:  88%|████████▊ | 225/256 [01:29&lt;00:15,  1.97it/s]
cache-cache_patch:transformers/inputs2 VALIDATE:  88%|████████▊ | 225/256 [01:31&lt;00:15,  1.97it/s]
cache-cache_patch:transformers/inputs2 VALIDATE:  88%|████████▊ | 226/256 [01:31&lt;00:24,  1.22it/s]
cache-cache_patch:transformers/inputs_empty_cache EXPORT:  88%|████████▊ | 226/256 [01:31&lt;00:24,  1.22it/s]
cache-cache_patch:transformers/inputs_batch1 EXPORT:  88%|████████▊ | 226/256 [01:30&lt;00:24,  1.22it/s]
cache-cache_patch:transformers/inputs_batch1 EXPORT:  89%|████████▉ | 228/256 [01:33&lt;00:24,  1.15it/s]
cache-cache_patch:transformers-rt/inputs EXPORT:  89%|████████▉ | 228/256 [01:33&lt;00:24,  1.15it/s]
cache-cache_patch:transformers-rt/inputs VALIDATE:  89%|████████▉ | 228/256 [01:34&lt;00:24,  1.15it/s]
cache-cache_patch:transformers-rt/inputs VALIDATE:  89%|████████▉ | 229/256 [01:34&lt;00:24,  1.11it/s]
cache-cache_patch:transformers-rt/inputs2 EXPORT:  89%|████████▉ | 229/256 [01:34&lt;00:24,  1.11it/s]
cache-cache_patch:transformers-rt/inputs2 VALIDATE:  89%|████████▉ | 229/256 [01:35&lt;00:24,  1.11it/s]
cache-cache_patch:transformers-rt/inputs2 VALIDATE:  90%|████████▉ | 230/256 [01:35&lt;00:25,  1.01it/s]
cache-cache_patch:transformers-rt/inputs_empty_cache EXPORT:  90%|████████▉ | 230/256 [01:35&lt;00:25,  1.01it/s]
cache-cache_patch:transformers-rt/inputs_empty_cache EXPORT:  90%|█████████ | 231/256 [01:36&lt;00:28,  1.12s/it]
cache-cache_patch:transformers-rt/inputs_batch1 EXPORT:  90%|█████████ | 231/256 [01:36&lt;00:28,  1.12s/it]
cache-cache_patch:transformers-rt/inputs_batch1 EXPORT:  91%|█████████ | 232/256 [01:37&lt;00:25,  1.06s/it]
cache-cache_patch:transformers-oblivious/inputs EXPORT:  91%|█████████ | 232/256 [01:37&lt;00:25,  1.06s/it]
cache-cache_patch:transformers-oblivious/inputs VALIDATE:  91%|█████████ | 232/256 [01:38&lt;00:25,  1.06s/it]
cache-cache_patch:transformers-oblivious/inputs VALIDATE:  91%|█████████ | 233/256 [01:38&lt;00:24,  1.06s/it]
cache-cache_patch:transformers-oblivious/inputs2 EXPORT:  91%|█████████ | 233/256 [01:38&lt;00:24,  1.06s/it]
cache-cache_patch:transformers-oblivious/inputs2 VALIDATE:  91%|█████████ | 233/256 [01:39&lt;00:24,  1.06s/it]
cache-cache_patch:transformers-oblivious/inputs2 VALIDATE:  91%|█████████▏| 234/256 [01:39&lt;00:22,  1.03s/it]
cache-cache_patch:transformers-oblivious/inputs_empty_cache EXPORT:  91%|█████████▏| 234/256 [01:39&lt;00:22,  1.03s/it]
cache-cache_patch:transformers-oblivious/inputs_empty_cache VALIDATE:  91%|█████████▏| 234/256 [01:40&lt;00:22,  1.03s/it]
cache-cache_patch:transformers-oblivious/inputs_empty_cache VALIDATE:  92%|█████████▏| 235/256 [01:40&lt;00:22,  1.05s/it]
cache-cache_patch:transformers-oblivious/inputs_batch1 EXPORT:  92%|█████████▏| 235/256 [01:40&lt;00:22,  1.05s/it]
cache-cache_patch:transformers-oblivious/inputs_batch1 VALIDATE:  92%|█████████▏| 235/256 [01:41&lt;00:22,  1.05s/it]
cache-cache_patch:transformers-oblivious/inputs_batch1 VALIDATE:  92%|█████████▏| 236/256 [01:41&lt;00:20,  1.01s/it]
cache-cache_patch:transformers-oblivious-rt/inputs EXPORT:  92%|█████████▏| 236/256 [01:41&lt;00:20,  1.01s/it]
cache-cache_patch:transformers-oblivious-rt/inputs VALIDATE:  92%|█████████▏| 236/256 [01:42&lt;00:20,  1.01s/it]
cache-cache_patch:transformers-oblivious-rt/inputs VALIDATE:  93%|█████████▎| 237/256 [01:42&lt;00:18,  1.00it/s]
cache-cache_patch:transformers-oblivious-rt/inputs2 EXPORT:  93%|█████████▎| 237/256 [01:42&lt;00:18,  1.00it/s]
cache-cache_patch:transformers-oblivious-rt/inputs2 VALIDATE:  93%|█████████▎| 237/256 [01:43&lt;00:18,  1.00it/s]
cache-cache_patch:transformers-oblivious-rt/inputs2 VALIDATE:  93%|█████████▎| 238/256 [01:43&lt;00:18,  1.01s/it]
cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache EXPORT:  93%|█████████▎| 238/256 [01:43&lt;00:18,  1.01s/it]
cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache VALIDATE:  93%|█████████▎| 238/256 [01:44&lt;00:18,  1.01s/it]
cache-cache_patch:transformers-oblivious-rt/inputs_empty_cache VALIDATE:  93%|█████████▎| 239/256 [01:44&lt;00:16,  1.01it/s]
cache-cache_patch:transformers-oblivious-rt/inputs_batch1 EXPORT:  93%|█████████▎| 239/256 [01:44&lt;00:16,  1.01it/s]
cache-cache_patch:transformers-oblivious-rt/inputs_batch1 VALIDATE:  93%|█████████▎| 239/256 [01:45&lt;00:16,  1.01it/s]
cache-cache_patch:transformers-oblivious-rt/inputs_batch1 VALIDATE:  94%|█████████▍| 240/256 [01:45&lt;00:15,  1.03it/s]
cache-cache_patch:transformers-strict/inputs EXPORT:  94%|█████████▍| 240/256 [01:45&lt;00:15,  1.03it/s]
cache-cache_patch:transformers-strict/inputs EXPORT:  94%|█████████▍| 241/256 [01:46&lt;00:14,  1.03it/s]
cache-cache_patch:transformers-strict/inputs2 EXPORT:  94%|█████████▍| 241/256 [01:46&lt;00:14,  1.03it/s]
cache-cache_patch:transformers-strict/inputs2 EXPORT:  95%|█████████▍| 242/256 [01:47&lt;00:13,  1.05it/s]
cache-cache_patch:transformers-strict/inputs_empty_cache EXPORT:  95%|█████████▍| 242/256 [01:47&lt;00:13,  1.05it/s]
cache-cache_patch:transformers-strict/inputs_empty_cache EXPORT:  95%|█████████▍| 243/256 [01:48&lt;00:11,  1.09it/s]
cache-cache_patch:transformers-strict/inputs_batch1 EXPORT:  95%|█████████▍| 243/256 [01:48&lt;00:11,  1.09it/s]
cache-cache_patch:transformers-strict/inputs_batch1 EXPORT:  95%|█████████▌| 244/256 [01:49&lt;00:10,  1.12it/s]
cache-cache_patch:transformers-strict-rt/inputs EXPORT:  95%|█████████▌| 244/256 [01:49&lt;00:10,  1.12it/s]
cache-cache_patch:transformers-strict-rt/inputs EXPORT:  96%|█████████▌| 245/256 [01:50&lt;00:09,  1.14it/s]
cache-cache_patch:transformers-strict-rt/inputs2 EXPORT:  96%|█████████▌| 245/256 [01:50&lt;00:09,  1.14it/s]
cache-cache_patch:transformers-strict-rt/inputs2 EXPORT:  96%|█████████▌| 246/256 [01:51&lt;00:11,  1.13s/it]
cache-cache_patch:transformers-strict-rt/inputs_empty_cache EXPORT:  96%|█████████▌| 246/256 [01:51&lt;00:11,  1.13s/it]
cache-cache_patch:transformers-strict-rt/inputs_empty_cache EXPORT:  96%|█████████▋| 247/256 [01:52&lt;00:09,  1.04s/it]
cache-cache_patch:transformers-strict-rt/inputs_batch1 EXPORT:  96%|█████████▋| 247/256 [01:52&lt;00:09,  1.04s/it]
cache-cache_patch:transformers-strict-rt/inputs_batch1 EXPORT:  97%|█████████▋| 248/256 [01:53&lt;00:07,  1.05it/s]
cache-cache_patch:transformers-strict-oblivious/inputs EXPORT:  97%|█████████▋| 248/256 [01:53&lt;00:07,  1.05it/s]
cache-cache_patch:transformers-strict-oblivious/inputs EXPORT:  97%|█████████▋| 249/256 [01:54&lt;00:06,  1.11it/s]
cache-cache_patch:transformers-strict-oblivious/inputs2 EXPORT:  97%|█████████▋| 249/256 [01:54&lt;00:06,  1.11it/s]
cache-cache_patch:transformers-strict-oblivious/inputs2 EXPORT:  98%|█████████▊| 250/256 [01:55&lt;00:05,  1.10it/s]
cache-cache_patch:transformers-strict-oblivious/inputs_empty_cache EXPORT:  98%|█████████▊| 250/256 [01:55&lt;00:05,  1.10it/s]
cache-cache_patch:transformers-strict-oblivious/inputs_empty_cache EXPORT:  98%|█████████▊| 251/256 [01:55&lt;00:04,  1.14it/s]
cache-cache_patch:transformers-strict-oblivious/inputs_batch1 EXPORT:  98%|█████████▊| 251/256 [01:55&lt;00:04,  1.14it/s]
cache-cache_patch:transformers-strict-oblivious/inputs_batch1 EXPORT:  98%|█████████▊| 252/256 [01:56&lt;00:03,  1.18it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs EXPORT:  98%|█████████▊| 252/256 [01:56&lt;00:03,  1.18it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs EXPORT:  99%|█████████▉| 253/256 [01:57&lt;00:02,  1.16it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs2 EXPORT:  99%|█████████▉| 253/256 [01:57&lt;00:02,  1.16it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs2 EXPORT:  99%|█████████▉| 254/256 [01:58&lt;00:01,  1.21it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs_empty_cache EXPORT:  99%|█████████▉| 254/256 [01:58&lt;00:01,  1.21it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs_empty_cache EXPORT: 100%|█████████▉| 255/256 [01:59&lt;00:00,  1.24it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs_batch1 EXPORT: 100%|█████████▉| 255/256 [01:59&lt;00:00,  1.24it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs_batch1 EXPORT: 100%|██████████| 256/256 [01:59&lt;00:00,  1.27it/s]
cache-cache_patch:transformers-strict-oblivious-rt/inputs_batch1 EXPORT: 100%|██████████| 256/256 [01:59&lt;00:00,  2.14it/s]
</pre></div>
</div>
<p>Let’s save the results.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">results</span></a></a><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">df</span><span class="o">.</span><span class="n">to_excel</span></a></a><span class="p">(</span><span class="s2">&quot;plot_export_tiny_llm_dim01.xlsx&quot;</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a></a>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cache</th>
      <th>cache_patch</th>
      <th>strict</th>
      <th>oblivious</th>
      <th>rt</th>
      <th>export_with</th>
      <th>EXPORT</th>
      <th>run_with</th>
      <th>WORKS</th>
      <th>ERR-RUN</th>
      <th>ERR-EXPORT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>inputs</td>
      <td>1</td>
      <td>inputs</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>inputs</td>
      <td>1</td>
      <td>inputs2</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>inputs</td>
      <td>1</td>
      <td>inputs_empty_cache</td>
      <td>0.0</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>inputs</td>
      <td>1</td>
      <td>inputs_batch1</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>inputs2</td>
      <td>1</td>
      <td>inputs</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>311</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>inputs_batch1</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>312</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>313</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs2</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>314</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs_empty_cache</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>315</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs_batch1</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
  </tbody>
</table>
<p>316 rows × 11 columns</p>
</div>
</div>
<br />
<br /><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">no_export</span></a></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a></a><span class="p">[</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span><span class="o">.</span><span class="n">EXPORT</span></a></a> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">no_export</span><span class="o">.</span><span class="n">to_excel</span></a></a><span class="p">(</span><span class="s2">&quot;plot_export_tiny_llm_dim01.no_export.xlsx&quot;</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">no_export</span></a></a>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cache</th>
      <th>cache_patch</th>
      <th>strict</th>
      <th>oblivious</th>
      <th>rt</th>
      <th>export_with</th>
      <th>EXPORT</th>
      <th>run_with</th>
      <th>WORKS</th>
      <th>ERR-RUN</th>
      <th>ERR-EXPORT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>inputs_empty_cache</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Found the following conflicts between user-spe...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>inputs_batch1</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Found the following conflicts between user-spe...</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>inputs_empty_cache</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Found the following conflicts between user-spe...</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>inputs_batch1</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Found the following conflicts between user-spe...</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>inputs</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Dynamo failed to run FX node with fake tensors...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>311</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>inputs_batch1</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>312</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>313</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs2</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>314</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs_empty_cache</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
    <tr>
      <th>315</th>
      <td>1</td>
      <td>transformers</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>inputs_batch1</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Failed to convert args/kwargs to proxy</td>
    </tr>
  </tbody>
</table>
<p>108 rows × 11 columns</p>
</div>
</div>
<br />
<br /><p>The validation failures.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">invalid</span></a></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span><span class="o">.</span><span class="n">EXPORT</span></a></a> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span><span class="o">.</span><span class="n">WORKS</span></a></a> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cache_patch&quot;</span><span class="p">,</span> <span class="s2">&quot;strict&quot;</span><span class="p">,</span> <span class="s2">&quot;oblivious&quot;</span><span class="p">,</span> <span class="s2">&quot;rt&quot;</span><span class="p">,</span> <span class="s2">&quot;export_with&quot;</span><span class="p">],</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;run_with&quot;</span><span class="p">],</span>
    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;WORKS&quot;</span><span class="p">,</span> <span class="s2">&quot;ERR-RUN&quot;</span><span class="p">],</span>
<span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">invalid</span><span class="o">.</span><span class="n">to_excel</span></a></a><span class="p">(</span><span class="s2">&quot;plot_export_tiny_llm_dim01.invalid.xlsx&quot;</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">invalid</span></a></a>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th colspan="4" halign="left">WORKS</th>
      <th colspan="4" halign="left">ERR-RUN</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th>run_with</th>
      <th>inputs</th>
      <th>inputs2</th>
      <th>inputs_batch1</th>
      <th>inputs_empty_cache</th>
      <th>inputs</th>
      <th>inputs2</th>
      <th>inputs_batch1</th>
      <th>inputs_empty_cache</th>
    </tr>
    <tr>
      <th>cache</th>
      <th>cache_patch</th>
      <th>strict</th>
      <th>oblivious</th>
      <th>rt</th>
      <th>export_with</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th rowspan="2" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="40" valign="top">1</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th rowspan="2" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="16" valign="top">all</th>
      <th rowspan="16" valign="top">0</th>
      <th rowspan="8" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Guard failed: input_ids.size()[0] == 1</td>
      <td>Guard failed: input_ids.size()[0] == 1</td>
      <td>NaN</td>
      <td>Guard failed: input_ids.size()[0] == 1</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>Guard failed: past_key_values['key_0'].size()[...</td>
      <td>Guard failed: past_key_values['key_0'].size()[...</td>
      <td>Guard failed: past_key_values['key_0'].size()[...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Guard failed: input_ids.size()[0] == 1</td>
      <td>Guard failed: input_ids.size()[0] == 1</td>
      <td>NaN</td>
      <td>Guard failed: input_ids.size()[0] == 1</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>Guard failed: past_key_values['key_0'].size()[...</td>
      <td>Guard failed: past_key_values['key_0'].size()[...</td>
      <td>Guard failed: past_key_values['key_0'].size()[...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="12" valign="top">transformers</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th rowspan="2" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: attention_mask.size()[1] &gt;= 4</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] == 1</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Guard failed: position_ids.size()[0] != 1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
      <td>Guard failed: input_ids.size()[0] &lt;= 2</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Eq(s44...</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Runtime assertion failed for expression Ne(s44...</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">success</span></a></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a></a><span class="p">[(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span><span class="o">.</span><span class="n">EXPORT</span></a></a> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series" title="pandas.core.series.Series" class="sphx-glr-backref-module-pandas-core-series sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span><span class="o">.</span><span class="n">WORKS</span></a></a> <span class="o">==</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cache_patch&quot;</span><span class="p">,</span> <span class="s2">&quot;strict&quot;</span><span class="p">,</span> <span class="s2">&quot;oblivious&quot;</span><span class="p">,</span> <span class="s2">&quot;rt&quot;</span><span class="p">,</span> <span class="s2">&quot;export_with&quot;</span><span class="p">],</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;run_with&quot;</span><span class="p">],</span>
    <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;WORKS&quot;</span><span class="p">],</span>
<span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">success</span><span class="o">.</span><span class="n">to_excel</span></a></a><span class="p">(</span><span class="s2">&quot;plot_export_tiny_llm_dim01.success.xlsx&quot;</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">success</span></a></a>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th colspan="4" halign="left">WORKS</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th>run_with</th>
      <th>inputs</th>
      <th>inputs2</th>
      <th>inputs_batch1</th>
      <th>inputs_empty_cache</th>
    </tr>
    <tr>
      <th>cache</th>
      <th>cache_patch</th>
      <th>strict</th>
      <th>oblivious</th>
      <th>rt</th>
      <th>export_with</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th rowspan="2" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="40" valign="top">1</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th rowspan="2" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="16" valign="top">all</th>
      <th rowspan="16" valign="top">0</th>
      <th rowspan="8" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="12" valign="top">transformers</th>
      <th rowspan="12" valign="top">0</th>
      <th rowspan="4" valign="top">0</th>
      <th rowspan="2" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">1</th>
      <th rowspan="4" valign="top">0</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">1</th>
      <th>inputs</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs2</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>inputs_batch1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>inputs_empty_cache</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<br />
<br /><p>If you have any error, then look at example
<a class="reference internal" href="plot_export_tiny_llm_patched.html#l-plot-tiny-llm-export-patched"><span class="std std-ref">Export Tiny-LLM with patches</span></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span><span class="o">.</span><span class="n">plot_legend</span><span class="p">(</span><span class="s2">&quot;Tiny-LLM</span><span class="se">\n</span><span class="s2">export with</span><span class="se">\n</span><span class="s2">dimension in {0,1}&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.export.export&quot;</span><span class="p">,</span> <span class="s2">&quot;tomato&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_export_tiny_llm_dim01_001.png" srcset="../_images/sphx_glr_plot_export_tiny_llm_dim01_001.png" alt="plot export tiny llm dim01" class = "sphx-glr-single-img"/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (2 minutes 0.157 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-export-tiny-llm-dim01-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4b540f642c1045ceab4c98185e29f97a/plot_export_tiny_llm_dim01.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_export_tiny_llm_dim01.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/c0e5d35675e22021b975f77d6efbc905/plot_export_tiny_llm_dim01.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_export_tiny_llm_dim01.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/60ba1bf4f76523d4eabd2a8fcc106c6e/plot_export_tiny_llm_dim01.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_export_tiny_llm_dim01.zip</span></code></a></p>
</div>
</div>
<p class="rubric">Related examples</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This duplicates the example l-plot-tiny-llm-export-dim01 but for torch.onnx.export. It checks what inputs can be used to export and with which inputs it can work."><img alt="" src="../_images/sphx_glr_plot_export_tiny_llm_dim01_onnx_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx.html#sphx-glr-auto-examples-plot-export-tiny-llm-dim01-onnx-py"><span class="std std-ref">Export with dynamic dimensions in {0,1} into ONNX</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export with dynamic dimensions in {0,1} into ONNX</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This duplicates the example l-plot-tiny-llm-export-dim01 but for experimental_experiment.torch_interpreter.to_onnx. It checks what inputs can be used to export and with which inputs it can work."><img alt="" src="../_images/sphx_glr_plot_export_tiny_llm_dim01_onnx_custom_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx_custom.html#sphx-glr-auto-examples-plot-export-tiny-llm-dim01-onnx-custom-py"><span class="std std-ref">Export with dynamic dimensions in {0,1} into ONNX (custom)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export with dynamic dimensions in {0,1} into ONNX (custom)</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This function exports an smaller untrained model with the same architecture. It is faster than the pretrained model. When this works, the untrained model can be replaced by the trained one."><img alt="" src="../_images/sphx_glr_plot_export_tiny_phi2_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_phi2.html#sphx-glr-auto-examples-plot-export-tiny-phi2-py"><span class="std std-ref">Export microsoft/phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export microsoft/phi-2</div>
</div></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="plot_export_tiny_llm_dim01_onnx.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_export_with_dynamic_cache.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Export with DynamicCache and guessed dynamic shapes</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code></a><ul>
<li><a class="reference internal" href="#available-input-sets">Available input sets</a></li>
<li><a class="reference internal" href="#export-with-options">Export with options</a></li>
<li><a class="reference internal" href="#the-main-loop">The main loop</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=6d507e9e"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>