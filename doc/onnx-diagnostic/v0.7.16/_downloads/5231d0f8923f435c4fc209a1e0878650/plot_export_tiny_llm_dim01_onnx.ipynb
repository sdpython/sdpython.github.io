{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export with dynamic dimensions in ``{0,1}`` into ONNX\n\nThis duplicates the example `l-plot-tiny-llm-export-dim01` but for\n:func:`torch.onnx.export`. It checks what inputs can be used to export\nand with which inputs it can work.\n\n## Available input sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas\nimport torch\nimport onnxruntime\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import max_diff, string_type, flatten_object\nfrom onnx_diagnostic.helpers.torch_helper import torch_deepcopy\nfrom onnx_diagnostic.helpers.rt_helper import make_feeds\nfrom onnx_diagnostic.torch_models.hghub.model_inputs import get_untrained_model_with_inputs\nfrom onnx_diagnostic.torch_export_patches.patch_inputs import use_dyn_not_str\nfrom onnx_diagnostic.torch_export_patches import (\n    torch_export_patches,\n    register_additional_serialization_functions,\n)\n\n\ndata = get_untrained_model_with_inputs(\"arnir0/Tiny-LLM\", add_second_input=True)\nmodel, dynamic_shapes = data[\"model\"], data[\"dynamic_shapes\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trained model can be obtained with:\n\n```python\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_sets = {k: v for k, v in data.items() if k.startswith(\"inputs\")}\n\nfor k, v in input_sets.items():\n    print(f\"{k:20}: {string_type(v, with_shape=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dynamic shapes are:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"dynamic_shapes: {string_type(dynamic_shapes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check they all work and compute the expected values.\nWe use deepcopy because caches are usually modified inplace.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expected = {}\nfor k, v in input_sets.items():\n    expected[k] = model(**torch_deepcopy(v))\n    print(f\"{k:20}: {string_type(expected[k], with_shape=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export with options\n\nWe try to export with the following options:\n\n- cache registration: register cache serialization with\n  :func:`onnx_diagnostic.torch_export_patches.register_additional_serialization_functions`\n\n- oblivious: an option to remove some the exception raises by the exporter\n\n- rt: see ``prefer_deferred_runtime_asserts_over_guards`` in :func:`torch.export.export`\n\n- cache_patch: patches the model before exporting with\n  :func:`onnx_diagnostic.torch_export_patches.torch_export_patches`\n\nSome function first.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def export_model(\n    model, dynamic_shapes, inputs, cache=False, oblivious=False, rt=False, cache_patch=False\n):\n    if cache and not cache_patch:\n        with register_additional_serialization_functions(patch_transformers=True):\n            return export_model(model, dynamic_shapes, inputs, oblivious=oblivious, rt=rt)\n    if cache_patch:\n        with torch_export_patches(\n            patch_torch=cache_patch in (\"all\", \"torch\", True, 1),\n            patch_transformers=cache_patch in (\"all\", \"transformers\", True, 1),\n        ):\n            return export_model(model, dynamic_shapes, inputs, oblivious=oblivious, rt=rt)\n    if oblivious:\n        with torch.fx.experimental._config.patch(backed_size_oblivious=True):\n            return export_model(model, dynamic_shapes, inputs, rt=rt)\n    ep = torch.export.export(\n        model,\n        (),\n        inputs,\n        dynamic_shapes=use_dyn_not_str(dynamic_shapes),\n        prefer_deferred_runtime_asserts_over_guards=rt,\n    )\n    return torch.onnx.export(ep, args=(), kwargs=inputs, dynamic_shapes=dynamic_shapes)\n\n\ndef try_export_model(\n    model, dynamic_shapes, inputs, cache=False, oblivious=False, rt=False, cache_patch=False\n):\n    try:\n        return export_model(\n            model,\n            dynamic_shapes,\n            inputs,\n            cache=cache,\n            oblivious=oblivious,\n            rt=rt,\n            cache_patch=cache_patch,\n        )\n    except Exception as e:\n        return e\n\n\ndef validation(ep, input_sets, expected, catch_exception=True):\n    sess = onnxruntime.InferenceSession(\n        ep.model_proto.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n    )\n    for k, v in input_sets.items():\n        feeds = make_feeds(sess, torch_deepcopy(v), use_numpy=True)\n        try:\n            got = sess.run(None, feeds)\n        except Exception as e:\n            if not catch_exception:\n                raise\n            yield k, e\n            continue\n        yield k, max_diff(flatten_object(expected[k], drop_keys=True), got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verification an example known to be working is.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = export_model(\n    model,\n    dynamic_shapes,\n    torch_deepcopy(input_sets[\"inputs\"]),\n    cache_patch=True,\n)\nres = list(validation(ep, dict(inputs=input_sets[\"inputs\"]), expected, catch_exception=False))\nassert res[0][1][\"abs\"] < 1e-5, f\"Unexpected issue with res={res}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The main loop\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = []\n\npossibilities = [*[[0, 1] for _ in range(4)], list(input_sets)]\npossibilities[1] = [0, \"all\", \"torch\", \"transformers\"]\nwith tqdm(list(itertools.product(*possibilities))) as pbar:\n    for cache, cache_patch, oblivious, rt, inputs in pbar:\n        if cache_patch and not cache:\n            # patches include caches.\n            continue\n        kwargs = dict(cache=cache, cache_patch=cache_patch, oblivious=oblivious, rt=rt)\n        legend = \"-\".join(\n            (k if isinstance(v, int) else f\"{k}:{v}\") for k, v in kwargs.items() if v\n        )\n        legend = f\"{legend}/{inputs}\"\n        pbar.set_description(f\"{legend} EXPORT\")\n\n        # export\n        ep = try_export_model(\n            model, dynamic_shapes, torch_deepcopy(input_sets[inputs]), **kwargs\n        )\n        if isinstance(ep, Exception):\n            obs = {\n                **kwargs,\n                \"export_with\": inputs,\n                \"EXPORT\": 0,\n                \"ERR-EXPORT\": str(ep).split(\"\\n\")[0],\n            }\n            results.append(obs)\n            continue\n\n        pbar.set_description(f\"{legend} VALIDATE\")\n        common = {**kwargs, \"export_with\": inputs, \"EXPORT\": 1}\n        for inp, res in validation(ep, input_sets, expected):\n            if isinstance(res, Exception):\n                obs = {\n                    **common,\n                    \"run_with\": inp,\n                    \"ERR-RUN\": str(res).split(\"\\n\")[0],\n                    \"WORKS\": 0,\n                }\n            else:\n                obs = {\n                    **common,\n                    \"run_with\": inp,\n                    \"WORKS\": int(~np.isnan(res[\"abs\"]) and res[\"abs\"] < 1e-3),\n                }\n            results.append(obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pandas.DataFrame(results)\ndf.to_excel(\"plot_export_tiny_llm_dim01_onnx.xlsx\")\ndf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "no_export = df[df.EXPORT == 0]\nno_export.to_excel(\"plot_export_tiny_llm_dim01_onnx.no_export.xlsx\")\nno_export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The validation failures.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "invalid = df[(df.EXPORT == 1) & (df.WORKS == 0)].pivot(\n    index=[\"cache\", \"cache_patch\", \"oblivious\", \"rt\", \"export_with\"],\n    columns=[\"run_with\"],\n    values=[\"WORKS\", \"ERR-RUN\"],\n)\ninvalid.to_excel(\"plot_export_tiny_llm_dim01_onnx.invalid.xlsx\")\ninvalid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "success = df[(df.EXPORT == 1) & (df.WORKS == 1)].pivot(\n    index=[\"cache\", \"cache_patch\", \"oblivious\", \"rt\", \"export_with\"],\n    columns=[\"run_with\"],\n    values=[\"WORKS\"],\n)\nsuccess.to_excel(\"plot_export_tiny_llm_dim01_onnx.success.xlsx\")\nsuccess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have any error, then look at example\n`l-plot-tiny-llm-export-patched`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"Tiny-LLM\\nexport with\\ndimension in {0,1}\", \"torch.onnx.export\", \"tomato\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}