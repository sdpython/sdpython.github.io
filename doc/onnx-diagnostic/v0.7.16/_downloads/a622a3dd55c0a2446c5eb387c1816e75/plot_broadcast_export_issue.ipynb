{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Dynamic Shapes and Broadcasting\n\n:func:`torch.export.export` makes strict assumption on dynamic shapes\nto the generic case. Let's consider two tensors with only one dimension.\n``x * y`` allows four configurations:\n\n* ``shape(x) = (1,)`` and ``shape(y) = (1,)``\n* ``shape(x) = (1,)`` and ``shape(y) = (p,)``\n* ``shape(x) = (q,)`` and ``shape(y) = (1,)``\n* ``shape(x) = (p,)`` and ``shape(y) = (p,)``\n\nThe expected shape for ``shape(x * y)`` is ``(max(p,q),)``.\n\n## Simple Case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.fx.experimental.symbolic_shapes import ShapeEnv\nfrom torch._subclasses.fake_tensor import FakeTensorMode\nfrom torch.fx.passes.fake_tensor_prop import FakeTensorProp\nfrom onnx_diagnostic.torch_export_patches import torch_export_patches\nfrom torch.fx import Tracer\n\n\nclass Model(torch.nn.Module):\n    def forward(self, x, y):\n        return x * y\n\n\nDim = torch.export.Dim\n\nep = torch.export.export(\n    Model(),\n    (torch.tensor([2, 3], dtype=torch.float32), torch.tensor([2, 3], dtype=torch.float32)),\n    dynamic_shapes=({0: Dim.DYNAMIC}, {0: Dim.DYNAMIC}),\n)\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see clearly that the export assumed that ``x`` ad ``y`` had the same shape.\nNo other configuration seemed to work at export time,\nincluding ``with torch.fx.experimental._config.patch(backed_size_oblivious=True):``\nthe shape of one tensor equal to ``(1,)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = [n for n in ep.graph.nodes if n.op == \"output\"][0]\nprint(\"output is \", output.name, \" arg is\", output.args[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final shape is:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shape = output.args[0][0].meta[\"val\"].shape\nprint(\"output shape is \", shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tracing\n\nLet's compare with what a simple tracing would do. Let's use :class:`torch.fx.Tracer`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = Tracer().trace(Model())\nprint(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = [n for n in graph.nodes if n.op == \"output\"][0]\nprint(\"output is \", output.name, \" arg is\", output.args[0])\nprint(\"The tracer leaves no trace:\", output.args[0].__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shape propagation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gm = torch.fx.GraphModule(Model(), graph)\n\nshape_env = ShapeEnv()\nfake_mode = FakeTensorMode(shape_env=shape_env)\n# d1 = shape_env.create_unbacked_symint()\n# d2 = shape_env.create_unbacked_symint()\nfake_inputs = fake_mode.from_tensor(\n    torch.zeros((3,), dtype=torch.float32), static_shapes=False\n), fake_mode.from_tensor(torch.zeros((3,), dtype=torch.float32), static_shapes=False)\n\nprint(\"fake_inputs are \", fake_inputs)\nres = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)\nprint(\"output is\", res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle Different Shapes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fake_inputs = fake_mode.from_tensor(\n    torch.zeros((2,), dtype=torch.float32), static_shapes=False\n), fake_mode.from_tensor(torch.zeros((1,), dtype=torch.float32), static_shapes=False)\n\nprint(\"fake_inputs are \", fake_inputs)\nres = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)\nprint(\"output is\", res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nWe need to give distinct dimensions to get distinct names.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fake_inputs = fake_mode.from_tensor(\n    torch.zeros((2,), dtype=torch.float32), static_shapes=False\n), fake_mode.from_tensor(torch.zeros((3,), dtype=torch.float32), static_shapes=False)\nprint(\"fake_inputs are \", fake_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    res = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)\nexcept Exception as e:\n    print(\"error\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By applying the patches:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch_export_patches():\n    res = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)\n    print(\"output is\", res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what we want. Let's go back to :func:`torch.export.export`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch_export_patches():\n    ep = torch.export.export(\n        Model(),\n        (\n            torch.tensor([2, 3], dtype=torch.float32),\n            torch.tensor([2, 3, 4], dtype=torch.float32),\n        ),\n        dynamic_shapes=({0: Dim.DYNAMIC}, {0: Dim.DYNAMIC}),\n    )\n    print(ep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = [n for n in ep.graph.nodes if n.op == \"output\"][0]\nprint(\"output is \", output.name, \" arg is\", output.args[0])\nshape = output.args[0][0].meta[\"val\"].shape\nprint(\"output shape is \", shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}