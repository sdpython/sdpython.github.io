{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reproducible Parallelized Reduction is difficult\n\nA reduction is a frequent operation in neural network. It appears in layer normalization,\nsoftmax. Because of the float precision, the result of the computation\nchanges based on the order of the elements. The following examples show the variation\nbased on different hypothesis on the vector distribution.\nWe consider a vector $X = (x_1, ..., x_n)$.\nIt computes the average:\n\n\\begin{align}mean(X) = \\frac{\\sum_{i=1}^n x_i}{n}\\end{align}\n\nOr the normalization of the vector:\n\n\\begin{align}norm(X)_i = \\frac{ X_i  - \\mathbb{E}X}{ \\sqrt{ \\mathbb{V}X}}\\end{align}\n\nWe draw 128 random permutation of X. The average or mean should not change.\nAnd the normalized vector should have the same value. In the first case, we compute\nthe difference between the highest and the lowest values obtained for the average.\nIn the second case, we look for the maximum difference between the original normalized\nvector and the permuted one (both sorted).\n\n## The computation code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas\n\nDATA = []\n\n\ndef str_dtype(dtype):\n    \"\"\"Displays numpy dtype in a nicer way.\"\"\"\n    if dtype == np.float64:\n        return \"fp64\"\n    if dtype == np.float32:\n        return \"fp32\"\n    if dtype == np.float16:\n        return \"fp16\"\n    raise ValueError(f\"Unexpected value {dtype}\")\n\n\ndef layer_norm(a, eps=1e-6):\n    \"\"\"\n    Normalized the vector a.\n    The computation is done in float32 or float64.\n    \"\"\"\n    ctype = np.float32 if a.dtype == np.float16 else a.dtype\n    a32 = a.astype(ctype)\n    m = a32.mean(axis=-1, keepdims=True)\n    c = a32 - m\n    va = np.sqrt((c * c).mean(axis=-1, keepdims=True))\n    va += eps\n    return (c / va).astype(a.dtype)\n\n\ndef compute(values, fct):\n    \"\"\"\n    Compare the results of function ``fct`` on a sample.\n    Loops over multiple sizes, dtypes. Tries 128 times.\n    \"\"\"\n\n    def make_value(base, value):\n        if value.size > 1:\n            return np.abs(np.sort(base) - np.sort(value)).max()\n        return value\n\n    sizes = [2, 4, 8, 16, 512, 1024, 2048, 4096, 8192]\n    dtypes = [np.float64, np.float32, np.float16]\n    N = list(range(128))\n    exps = list(itertools.product(sizes, dtypes, N))\n    data = []\n    ech = None\n    for size, dtype, n in tqdm(exps):\n        if n == 0:\n            ech = values[:size].astype(dtype)\n            base = fct(ech)\n            assert base.dtype == ech.dtype\n            obs = dict(\n                n=n, size=size, dtype=str_dtype(ech.dtype), value=make_value(base, fct(ech))\n            )\n            data.append(obs)\n\n        if n == 1:\n            new_ech = np.sort(ech)\n        elif n == 2:\n            new_ech = np.sort(ech)[::-1]\n        else:\n            new_ech = np.random.permutation(ech)\n        assert new_ech.dtype == ech.dtype\n        assert new_ech.shape == ech.shape\n        obs = dict(\n            n=n + 1,\n            size=size,\n            dtype=str_dtype(new_ech.dtype),\n            value=make_value(base, fct(new_ech)),\n        )\n        data.append(obs)\n\n    df = pandas.DataFrame(data)\n    agg = df.drop(\"n\", axis=1).groupby([\"dtype\", \"size\"], as_index=False).agg([\"min\", \"max\"])\n    agg[\"value\", \"delta\"] = agg[\"value\", \"max\"] - agg[\"value\", \"min\"]\n    piv = agg.pivot(index=\"size\", columns=\"dtype\", values=(\"value\", \"delta\"))\n    return piv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normal Law\n\nLet's see what it returns an on random sample following a normal law.\nFirst the average.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values = np.random.randn(4096)\nmean = compute(values, lambda x: np.mean(x).astype(x.dtype))\nmean[\"name\"] = \"normal\"\nprint(mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the layer normalization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ln = compute(values, layer_norm)\nln[\"name\"] = \"normal\"\nDATA.append(ln.reset_index(drop=True).max(axis=0))\nprint(ln)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fixed values\n\nWe try a fixed vector with one very high value and all the others are small.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values[:] = -1e-4\nvalues[::128] = 100\nmean = compute(values, lambda x: np.mean(x).astype(x.dtype))\nmean[\"name\"] = \"fixed\"\nprint(mean)\n\n\nln = compute(values, layer_norm)\nln[\"name\"] = \"fixed\"\nDATA.append(ln.reset_index(drop=True).max(axis=0))\nprint(ln)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pareto Distribution\n\nA law with a long tail.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values = np.random.pareto(1, (4096,))\nprint(values)\n\nmean = compute(values, lambda x: np.mean(x).astype(x.dtype))\nmean[\"name\"] = \"normal\"\nprint(mean)\n\n\nln = compute(values, layer_norm)\nln[\"name\"] = \"pareto\"\nDATA.append(ln.reset_index(drop=True).max(axis=0))\nprint(ln)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nWe consider the maximum difference obtained for any sample size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(DATA)\ndf = pandas.DataFrame(DATA).set_index(\"name\")\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = df.plot.bar(logy=True)\nfig = ax.get_figure()\nfig.savefig(\"plot_parallelized_reduction.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In a deep neural network\n\nSome of the vector have 500 values, 16x32x1024x1024. A layer normalization\ndoes 16x32x1024 ~ 2M reductions, over 20 layers.\nWhen a deep neural network is computed with a difference code,\ndoing a different parallelization (GPU/CPU for example),\nthe order of the reduction may change and therefore,\nsome errors will appear and propagate.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}