{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export attention from arnir0/Tiny-LLM with InputObserver\n\nThis shows how to only export attention from model\n[arnir0/Tiny-LLM](https://huggingface.co/arnir0/Tiny-LLM).\nIt uses what was shown in example\n`l-plot-tiny-llm-export-input-observer`.\n\n## Let's create a random model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.export.api import to_onnx\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.torch_export_patches import (\n    register_additional_serialization_functions,\n    torch_export_patches,\n)\nfrom onnx_diagnostic.investigate.input_observer import InputObserver\n\ndevice = \"cuda\"\nmodel_id = \"arnir0/Tiny-LLM\"\nprint(f\"get tokenizer {model_id!r}\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nprint(f\"get config {model_id!r}\")\nconfig = AutoConfig.from_pretrained(model_id)\nprint(f\"create model from config for {model_id!r}\")\nmodel = AutoModelForCausalLM.from_config(config)\nprint(f\"the model is created with {len(list(model.named_modules()))} subdmodules.\")\nmodel = model.to(device).to(torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We need to only export class LlamaAttention\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "export_module = None\nfor _name, sub in model.named_modules():\n    if sub.__class__.__name__ == \"LlamaAttention\":\n        export_module = sub\n\nassert export_module is not None, (\n    f\"Unable to find a submodule from class LlamaAttention in \"\n    f\"{set(sub.__class__.__name__ for _, sub in model.named_modules())}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the model and capture the inputs and outputs of the attention part.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_text(\n    prompt,\n    model,\n    tokenizer,\n    max_length=50,\n    temperature=0.01,\n    top_k=50,\n    top_p=0.95,\n    do_sample=True,\n):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        do_sample=do_sample,\n    )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n\nprompt = \"Continue: it rains, what should I do?\"\nobserver = InputObserver()\nwith (\n    register_additional_serialization_functions(patch_transformers=True),\n    observer(export_module),\n):\n    generate_text(prompt, model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export\n\nFirst, what was inferred.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kwargs = observer.infer_arguments()\ndynamic_shapes = observer.infer_dynamic_shapes()\nprint(\"attention type:\", type(export_module))\nprint(f\"kwargs={string_type(kwargs, with_shape=True, with_device=True)}\")\nprint(f\"dynamic_shapes={dynamic_shapes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, the export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"plot_export_tiny_llm_attention_input_observer.onnx\"\nwith torch_export_patches(patch_torch=True, patch_transformers=True):\n    to_onnx(\n        export_module,\n        args=(),\n        kwargs=kwargs,\n        filename=filename,\n        dynamic_shapes=dynamic_shapes,\n        exporter=\"custom\",\n        verbose=1,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's measure the discrepancies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = observer.check_discrepancies(\n    filename, progress_bar=True, atol=1e-2, include_io=True, skip_none=True\n)\ndf = pandas.DataFrame(data)\ndf.to_excel(\"plot_export_tiny_llm_attention_input_observer.xlsx\")\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's show the errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for row in data:\n    if not row[\"SUCCESS\"] and \"error\" in row:\n        print(row[\"error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.save_fig(doc.plot_dot(filename), f\"{filename}.png\", dpi=400)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}