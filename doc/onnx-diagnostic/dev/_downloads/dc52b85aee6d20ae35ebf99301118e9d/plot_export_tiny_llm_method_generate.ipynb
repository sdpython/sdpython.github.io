{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export a model through method generate (with Tiny-LLM)\n\nThe main issue when exporting a LLM is the example on HuggingFace is\nbased on method generate but we only need to export the forward method.\nExample `l-plot-tiny-llm-export` gives details on how to guess\ndummy inputs and dynamic shapes to do so.\nLet's see how to simplify that.\n\n## Dummy Example\n\nLet's use the example provided on\n[arnir0/Tiny-LLM](https://huggingface.co/arnir0/Tiny-LLM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.export.api import method_to_onnx\n\n\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n\ndef generate_text(\n    prompt, model, tokenizer, max_length=50, temperature=1, top_k=50, top_p=0.95\n):\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        do_sample=True,\n    )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n    # Define your prompt\n\n\nprompt = \"Continue: it rains...\"\ngenerated_text = generate_text(prompt, model, tokenizer)\nprint(\"-----------------\")\nprint(generated_text)\nprint(\"-----------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace forward method\n\nWe now modify the model to export the model by replacing the forward method.\nWe still call method ``generate`` but this one will call a different function\ncreated by :func:`onnx_diagnostic.export.api.method_to_onnx`.\nThis one captured the inputs of the forward method, 2 calls are needed or\nat least, 3 are recommended for LLMs as the first call does not contain any cache.\nIf the default settings do not work, ``skip_kwargs_names`` and ``dynamic_shapes``\ncan be changed to remove some undesired inputs or add more dynamic dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"plot_export_tiny_llm_method_generate.onnx\"\nforward_replacement = method_to_onnx(\n    model,\n    method_name=\"forward\",  # default value\n    exporter=\"custom\",  # onnx-dynamo to use the official exporter\n    filename=filename,  # onnx file to create\n    patch_kwargs=dict(patch_transformers=True),  # patches before eporting\n    # to see the progress, it is recommended on the first try to see\n    # how to set ``skip_kwargs_names`` and ``dynamic_shapes`` if it is needed\n    verbose=1,\n    # triggers the ONNX conversion after 3 calls to forward method,\n    # the onnx version is triggered with the last one,\n    # the others are used to infer the dynamic shape if they are not\n    # specified below\n    convert_after_n_calls=3,\n    # skips the following inputs even though they are captured,\n    # these ones are filled with default values we don't want in\n    # the onnx model\n    skip_kwargs_names={\"kwargs\", \"use_cache\", \"return_dict\", \"inputs_embeds\"},\n    # dynamic shape can be inferred from at least two calls to the forward method,\n    # 3 is better for LLMs, you can see the inference results with ``verbose=1``,\n    # this parameter is used to overwrite the inferred values,\n    # this is usually needed because the inferred dynamic shapes contains\n    # less dynamic dimension than requested.\n    dynamic_shapes={\n        \"cache_position\": {0: \"total_sequence_length\"},\n        \"past_key_values\": [\n            {0: \"batch_size\", 2: \"past_sequence_length\"},\n            {0: \"batch_size\", 2: \"past_sequence_length\"},\n        ],\n        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lambda function cannot be skipped as\nforward_replacement is a module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"type(forward_replacement)={type(forward_replacement)}\")\nmodel.forward = lambda *args, **kwargs: forward_replacement(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's call generate again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "generated_text = generate_text(prompt, model, tokenizer)\nprint(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"Tiny-LLM\\nforward inputs\\through generate\", \"onnx export\", \"tomato\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}