
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_with_dynamic_shapes_auto.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_with_dynamic_shapes_auto.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_with_dynamic_shapes_auto.py:


.. _l-plot-sxport-with-dynamio-shapes-auto:

Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints
====================================================================

Setting the dynamic shapes is not always easy.
Here are a few tricks to make it work.

dx + dy not allowed?
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 13-32

.. code-block:: Python


    import torch
    from onnx_diagnostic import doc


    class Model(torch.nn.Module):
        def forward(self, x, y, z):
            return torch.cat((x, y), axis=1) + z[:, ::2]


    model = Model()
    x = torch.randn(2, 3)
    y = torch.randn(2, 5)
    z = torch.randn(2, 16)
    model(x, y, z)


    print(torch.export.export(model, (x, y, z)).graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    graph():
        %x : [num_users=1] = placeholder[target=x]
        %y : [num_users=1] = placeholder[target=y]
        %z : [num_users=1] = placeholder[target=z]
        %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%x, %y], 1), kwargs = {})
        %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%z, 0, 0, 9223372036854775807), kwargs = {})
        %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_1, 1, 0, 9223372036854775807, 2), kwargs = {})
        %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%cat, %slice_2), kwargs = {})
        return (add,)




.. GENERATED FROM PYTHON SOURCE LINES 33-35

Everything is fine so far. With dynamic shapes now.
dx + dy is not allowed...

.. GENERATED FROM PYTHON SOURCE LINES 35-46

.. code-block:: Python


    batch = torch.export.Dim("batch")
    dx = torch.export.Dim("dx")
    dy = torch.export.Dim("dy")

    try:
        dz = dx + dy
        raise AssertionError("able to add dynamic dimensions, please update the tutorial")
    except NotImplementedError as e:
        print(f"unable to add dynamic dimensions because {type(e)}, {e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    unable to add dynamic dimensions because <class 'NotImplementedError'>, Attempted to add <class '__main__.dy'> to dx, where an integer was expected. (Only increasing linear operations with integer coefficients are supported.)




.. GENERATED FROM PYTHON SOURCE LINES 47-48

Then we could make it a different one.

.. GENERATED FROM PYTHON SOURCE LINES 48-65

.. code-block:: Python


    dz = torch.export.Dim("dz") * 2
    try:
        ep = torch.export.export(
            model,
            (x, y, z),
            dynamic_shapes={
                "x": {0: batch, 1: dx},
                "y": {0: batch, 1: dy},
                "z": {0: batch, 1: dz},
            },
        )
        print(ep)
        raise AssertionError("able to export this model, please update the tutorial")
    except torch._dynamo.exc.UserError as e:
        print(f"unable to use Dim('dz') because {type(e)}, {e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    unable to use Dim('dz') because <class 'torch._dynamo.exc.UserError'>, Constraints violated (batch)! For more information, run with TORCH_LOGS="+dynamic".
      - Not all values of batch = L['args'][0][0].size()[0] in the specified range satisfy the generated guard L['args'][0][0].size()[0] != 9223372036854775807.




.. GENERATED FROM PYTHON SOURCE LINES 66-69

That works. We could also use
``torch.export.Dim.DYNAMIC`` or ``torch.export.Dim.AUTO``
for the dimension we cannot set.

.. GENERATED FROM PYTHON SOURCE LINES 69-83

.. code-block:: Python


    DYNAMIC = torch.export.Dim.DYNAMIC
    ep = torch.export.export(
        model,
        (x, y, z),
        dynamic_shapes={
            "x": {0: DYNAMIC, 1: dx},
            "y": {0: DYNAMIC, 1: dy},
            "z": {0: DYNAMIC, 1: DYNAMIC},
        },
    )

    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s0, s1]", y: "f32[s0, s3]", z: "f32[s0, s5]"):
                 # 
                sym_size_int_3: "Sym(s1)" = torch.ops.aten.sym_size.int(x, 1)
                sym_size_int_4: "Sym(s3)" = torch.ops.aten.sym_size.int(y, 1)
                sym_size_int_5: "Sym(s5)" = torch.ops.aten.sym_size.int(z, 1)
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_shapes_auto.py:20 in forward, code: return torch.cat((x, y), axis=1) + z[:, ::2]
                cat: "f32[s0, s1 + s3]" = torch.ops.aten.cat.default([x, y], 1);  x = y = None
            
                 # 
                add_1: "Sym(s1 + s3)" = sym_size_int_3 + sym_size_int_4;  sym_size_int_3 = sym_size_int_4 = None
                add_2: "Sym(s5 + 1)" = 1 + sym_size_int_5;  sym_size_int_5 = None
                floordiv: "Sym(((s5 + 1)//2))" = add_2 // 2;  add_2 = None
                eq_2: "Sym(Eq(s1 + s3, ((s5 + 1)//2)))" = add_1 == floordiv;  add_1 = floordiv = None
                _assert_scalar_default = torch.ops.aten._assert_scalar.default(eq_2, "Runtime assertion failed for expression Eq(s1 + s3, ((s5 + 1)//2)) on node 'eq_2'");  eq_2 = _assert_scalar_default = None
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_shapes_auto.py:20 in forward, code: return torch.cat((x, y), axis=1) + z[:, ::2]
                slice_1: "f32[s0, s5]" = torch.ops.aten.slice.Tensor(z, 0, 0, 9223372036854775807);  z = None
                slice_2: "f32[s0, ((s5 + 1)//2)]" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807, 2);  slice_1 = None
                add: "f32[s0, s1 + s3]" = torch.ops.aten.add.Tensor(cat, slice_2);  cat = slice_2 = None
                return (add,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
        z: USER_INPUT
    
        # outputs
        add: USER_OUTPUT
    
    Range constraints: {s0: VR[2, int_oo], s1: VR[0, int_oo], s3: VR[0, int_oo], s5: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 84-85

The same result can be obtained with ``torch.export.Dim.AUTO``.

.. GENERATED FROM PYTHON SOURCE LINES 85-95

.. code-block:: Python


    AUTO = torch.export.Dim.AUTO
    print(
        torch.export.export(
            model,
            (x, y, z),
            dynamic_shapes=({0: AUTO, 1: AUTO}, {0: AUTO, 1: AUTO}, {0: AUTO, 1: AUTO}),
        )
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/backends/mkldnn/__init__.py:78: UserWarning: TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:148.)
      torch._C._set_onednn_allow_tf32(_allow_tf32)
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s0, s1]", y: "f32[s0, s3]", z: "f32[s0, s5]"):
                 # 
                sym_size_int_3: "Sym(s1)" = torch.ops.aten.sym_size.int(x, 1)
                sym_size_int_4: "Sym(s3)" = torch.ops.aten.sym_size.int(y, 1)
                sym_size_int_5: "Sym(s5)" = torch.ops.aten.sym_size.int(z, 1)
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_shapes_auto.py:20 in forward, code: return torch.cat((x, y), axis=1) + z[:, ::2]
                cat: "f32[s0, s1 + s3]" = torch.ops.aten.cat.default([x, y], 1);  x = y = None
            
                 # 
                add_1: "Sym(s1 + s3)" = sym_size_int_3 + sym_size_int_4;  sym_size_int_3 = sym_size_int_4 = None
                add_2: "Sym(s5 + 1)" = 1 + sym_size_int_5;  sym_size_int_5 = None
                floordiv: "Sym(((s5 + 1)//2))" = add_2 // 2;  add_2 = None
                eq_2: "Sym(Eq(s1 + s3, ((s5 + 1)//2)))" = add_1 == floordiv;  add_1 = floordiv = None
                _assert_scalar_default = torch.ops.aten._assert_scalar.default(eq_2, "Runtime assertion failed for expression Eq(s1 + s3, ((s5 + 1)//2)) on node 'eq_2'");  eq_2 = _assert_scalar_default = None
            
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/examples/plot_export_with_dynamic_shapes_auto.py:20 in forward, code: return torch.cat((x, y), axis=1) + z[:, ::2]
                slice_1: "f32[s0, s5]" = torch.ops.aten.slice.Tensor(z, 0, 0, 9223372036854775807);  z = None
                slice_2: "f32[s0, ((s5 + 1)//2)]" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807, 2);  slice_1 = None
                add: "f32[s0, s1 + s3]" = torch.ops.aten.add.Tensor(cat, slice_2);  cat = slice_2 = None
                return (add,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
        z: USER_INPUT
    
        # outputs
        add: USER_OUTPUT
    
    Range constraints: {s0: VR[2, int_oo], s1: VR[2, int_oo], s3: VR[2, int_oo], s5: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 96-98

.. code-block:: Python


    doc.plot_legend("dynamic shapes\ninferred", "torch.export.export", "tomato")



.. image-sg:: /auto_examples/images/sphx_glr_plot_export_with_dynamic_shapes_auto_001.png
   :alt: plot export with dynamic shapes auto
   :srcset: /auto_examples/images/sphx_glr_plot_export_with_dynamic_shapes_auto_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.465 seconds)


.. _sphx_glr_download_auto_examples_plot_export_with_dynamic_shapes_auto.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_with_dynamic_shapes_auto.ipynb <plot_export_with_dynamic_shapes_auto.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_with_dynamic_shapes_auto.py <plot_export_with_dynamic_shapes_auto.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_with_dynamic_shapes_auto.zip <plot_export_with_dynamic_shapes_auto.zip>`


.. include:: plot_export_with_dynamic_shapes_auto.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
