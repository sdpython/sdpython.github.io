
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_tiny_llm_method_generate.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_tiny_llm_method_generate.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_tiny_llm_method_generate.py:


.. _l-plot-tiny-llm-export-method-generate:

Export a model through method generate (with Tiny-LLM)
======================================================

The main issue when exporting a LLM is the example on HuggingFace is
based on method generate but we only need to export the forward method.
Example :ref:`l-plot-tiny-llm-export` gives details on how to guess
dummy inputs and dynamic shapes to do so.
Let's see how to simplify that.

Dummy Example
+++++++++++++

Let's use the example provided on
`arnir0/Tiny-LLM <https://huggingface.co/arnir0/Tiny-LLM>`_.

.. GENERATED FROM PYTHON SOURCE LINES 19-56

.. code-block:: Python


    from transformers import AutoModelForCausalLM, AutoTokenizer
    from onnx_diagnostic import doc
    from onnx_diagnostic.export.api import method_to_onnx


    MODEL_NAME = "arnir0/Tiny-LLM"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)


    def generate_text(
        prompt, model, tokenizer, max_length=50, temperature=1, top_k=50, top_p=0.95
    ):
        inputs = tokenizer.encode(prompt, return_tensors="pt")

        outputs = model.generate(
            inputs,
            max_length=max_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            do_sample=True,
        )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated_text

        # Define your prompt


    prompt = "Continue: it rains..."
    generated_text = generate_text(prompt, model, tokenizer)
    print("-----------------")
    print(generated_text)
    print("-----------------")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading weights:   0%|          | 0/12 [00:00<?, ?it/s]    Loading weights:   8%|▊         | 1/12 [00:00<00:00, 15947.92it/s, Materializing param=lm_head.weight]    Loading weights:   8%|▊         | 1/12 [00:00<00:00, 5793.24it/s, Materializing param=lm_head.weight]     Loading weights:  17%|█▋        | 2/12 [00:00<00:00, 100.16it/s, Materializing param=model.embed_tokens.weight]    Loading weights:  17%|█▋        | 2/12 [00:00<00:00, 98.79it/s, Materializing param=model.embed_tokens.weight]     Loading weights:  25%|██▌       | 3/12 [00:00<00:00, 146.44it/s, Materializing param=model.layers.0.input_layernorm.weight]    Loading weights:  25%|██▌       | 3/12 [00:00<00:00, 145.43it/s, Materializing param=model.layers.0.input_layernorm.weight]    Loading weights:  33%|███▎      | 4/12 [00:00<00:00, 192.31it/s, Materializing param=model.layers.0.mlp.down_proj.weight]      Loading weights:  33%|███▎      | 4/12 [00:00<00:00, 191.37it/s, Materializing param=model.layers.0.mlp.down_proj.weight]    Loading weights:  42%|████▏     | 5/12 [00:00<00:00, 237.54it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]    Loading weights:  42%|████▏     | 5/12 [00:00<00:00, 236.50it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]    Loading weights:  50%|█████     | 6/12 [00:00<00:00, 281.44it/s, Materializing param=model.layers.0.mlp.up_proj.weight]      Loading weights:  50%|█████     | 6/12 [00:00<00:00, 280.02it/s, Materializing param=model.layers.0.mlp.up_proj.weight]    Loading weights:  58%|█████▊    | 7/12 [00:00<00:00, 324.44it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]    Loading weights:  58%|█████▊    | 7/12 [00:00<00:00, 323.03it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]    Loading weights:  67%|██████▋   | 8/12 [00:00<00:00, 366.85it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]            Loading weights:  67%|██████▋   | 8/12 [00:00<00:00, 365.31it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]    Loading weights:  75%|███████▌  | 9/12 [00:00<00:00, 405.88it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]    Loading weights:  75%|███████▌  | 9/12 [00:00<00:00, 404.01it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]    Loading weights:  83%|████████▎ | 10/12 [00:00<00:00, 446.20it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]    Loading weights:  83%|████████▎ | 10/12 [00:00<00:00, 444.40it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]    Loading weights:  92%|█████████▏| 11/12 [00:00<00:00, 484.26it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]    Loading weights:  92%|█████████▏| 11/12 [00:00<00:00, 482.78it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]    Loading weights: 100%|██████████| 12/12 [00:00<00:00, 524.62it/s, Materializing param=model.norm.weight]                         Loading weights: 100%|██████████| 12/12 [00:00<00:00, 523.29it/s, Materializing param=model.norm.weight]    Loading weights: 100%|██████████| 12/12 [00:00<00:00, 520.54it/s, Materializing param=model.norm.weight]
    The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
    Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
    The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
    -----------------
    Continue: it rains... Read more
    by Brooke Ridge
    -----------------




.. GENERATED FROM PYTHON SOURCE LINES 57-61

Replace forward method
++++++++++++++++++++++

We now modify the model to export the model by replacing the forward method.

.. GENERATED FROM PYTHON SOURCE LINES 61-81

.. code-block:: Python

    filename = "plot_export_tiny_llm_method_generate.onnx"
    forward_replacement = method_to_onnx(
        model,
        method_name="forward",
        exporter="custom",
        filename=filename,
        patch_kwargs=dict(patch_transformers=True),
        verbose=1,
        convert_after_n_calls=3,
        skip_kwargs_names={"kwargs", "use_cache", "return_dict", "inputs_embeds"},
        dynamic_shapes={
            "cache_position": {0: "total_sequence_length"},
            "past_key_values": [
                {0: "batch_size", 2: "past_sequence_length"},
                {0: "batch_size", 2: "past_sequence_length"},
            ],
            "input_ids": {0: "batch_size", 1: "sequence_length"},
        },
    )








.. GENERATED FROM PYTHON SOURCE LINES 82-84

The lambda function cannot be skipped as
forward_replacement is a module.

.. GENERATED FROM PYTHON SOURCE LINES 84-89

.. code-block:: Python


    print(f"type(forward_replacement)={type(forward_replacement)}")
    model.forward = lambda *args, **kwargs: forward_replacement(*args, **kwargs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    type(forward_replacement)=<class 'onnx_diagnostic.export.api._WrapperToExportMethodToOnnx'>




.. GENERATED FROM PYTHON SOURCE LINES 90-91

Let's call generate again.

.. GENERATED FROM PYTHON SOURCE LINES 91-95

.. code-block:: Python

    generated_text = generate_text(prompt, model, tokenizer)
    print(generated_text)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
    Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
    [method_to_onnx] input[0]: ((),dict(cache_position:T7s8,input_ids:T7s1x8))
    [method_to_onnx] input[1]: ((),dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x8x96], value_cache=#1[T1s1x1x8x96]),input_ids:T7s1x1))
    [method_to_onnx] input[2]: ((),dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x9x96], value_cache=#1[T1s1x1x9x96]),input_ids:T7s1x1))
    [method_to_onnx] export args=()
    [method_to_onnx] export kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x9x96], value_cache=#1[T1s1x1x9x96]),input_ids:T7s1x1)
    [method_to_onnx] dynamic_shapes=#1[dict(cache_position:{0:DYN(total_sequence_length)},past_key_values:#2[{0:DYN(batch_size),2:DYN(past_sequence_length)},{0:DYN(batch_size),2:DYN(past_sequence_length)}],input_ids:{0:DYN(batch_size),1:DYN(sequence_length)})]
    [to_onnx] build the graph module from <class 'onnx_diagnostic.export.api._WrapperToExportMethodToOnnx._convert_method_to_onnx.<locals>.WrapWithExactSignature'>, type(args)=<class 'tuple'>
    [to_onnx] dynamic_shapes={'cache_position': {0: 'total_sequence_length'}, 'past_key_values': [{0: 'batch_size', 2: 'past_sequence_length'}, {0: 'batch_size', 2: 'past_sequence_length'}], 'input_ids': {0: 'batch_size', 1: 'sequence_length'}}
    [_make_builder_interpreter] export_options=ExportOptions(aten_as_function=('aten.index_copy.default', 'aten.index_put.default', 'aten.setitem', <built-in function setitem>))
    [_make_builder_interpreter] input args=()
    [_make_builder_interpreter] input kwargs=dict(cache_position:T7r1,past_key_values:DynamicCache(key_cache=#1[T1r4], value_cache=#1[T1r4]),input_ids:T7r2)
    [_make_builder_interpreter] dynamic_shapes={'cache_position': {0: 'total_sequence_length'}, 'past_key_values': [{0: 'batch_size', 2: 'past_sequence_length'}, {0: 'batch_size', 2: 'past_sequence_length'}], 'input_ids': {0: 'batch_size', 1: 'sequence_length'}}
    [_make_builder_interpreter] same_signature=True, tracing_mode=symbolic
    [ExportOptions.export] ExportOptions(aten_as_function=('aten.index_copy.default', 'aten.index_put.default', 'aten.setitem', <built-in function setitem>)) - torch._dynamo.export 'WrapWithExactSignature'
    [ExportOptions.export] aten_as_function=('aten.index_copy.default', 'aten.index_put.default', 'aten.setitem', <built-in function setitem>)
    [ExportOptions.export] torch_export strict=False, verbose=1
    [ExportOptions.export] dynamic_shapes={'cache_position': {0: 'total_sequence_length'}, 'past_key_values': [{0: 'batch_size', 2: 'past_sequence_length'}, {0: 'batch_size', 2: 'past_sequence_length'}], 'input_ids': {0: 'batch_size', 1: 'sequence_length'}}
    [ExportOptions.export] args=()
    [ExportOptions.export] kwargs=dict(cache_position:T7r1,past_key_values:DynamicCache(key_cache=#1[T1r4], value_cache=#1[T1r4]),input_ids:T7r2)
    [ExportOptions.export] export start with strict=False...
    [ExportOptions.export] export with backed_size_oblivious=auto
    [torch_export] backed_size_oblivious='auto'
    [torch_export] inferred backed_size_oblivious={'cache_position': {0: 'd=[1]'}, 'past_key_values': [{0: 'd=[1]'}, {0: 'd=[1]'}], 'input_ids': {0: 'd=[1]', 1: 'd=[1]'}}
    [torch_export] export starts with backed_size_oblivious={'cache_position': {0: 'd=[1]'}, 'past_key_values': [{0: 'd=[1]'}, {0: 'd=[1]'}], 'input_ids': {0: 'd=[1]', 1: 'd=[1]'}}
    [ExportOptions.export] export done in 2.9053409510088386
    [ExportOptions.export] post_process_exported_program with decomposition_table=None
    [ExportOptions.export] remove inplace nodes
    [ExportOptions.export] slices: 9 slices nodes were removed
    [CustomTracer.remove_inplace] starts with 131 nodes (n_inplace_submobules=0)
    [CustomTracer.remove_inplace] S1: 2 inplace nodes
    [CustomTracer.remove_inplace] S2: 2 inplace nodes and 100 iterations
    [CustomTracer.remove_inplace] end with 100 iterations and 123 nodes (n_inplace=2)
    [ExportOptions.export] inplaces: 2 inplaced nodes were removed
    [ExportOptions.export] done remove inplace in 0.00458542701380793, modified=2
    [ExportOptions.export] done with no decomposition in 0.004734682006528601
    [to_onnx] graph module done in 2.951561498994124 s
    [to_onnx] start creating the onnx nodes
    [to_onnx] interpreter.function_options=FunctionOptions(export_as_function=True, name='*', domain='*', external_threshold=256, move_initializer_to_constant=True, return_initializer=True, merge_allowed=True, rename_allowed=True)
    `loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
      0%|          | 0/123 [00:00<?, ?it/s]     69%|██████▉   | 85/123 [00:00<00:00, 845.69it/s]    100%|██████████| 123/123 [00:00<00:00, 870.87it/s]
    [to_onnx] 165 onnx nodes done in 0.2244724760093959 s
    [to_onnx] start conversion to onnx (before optimization) mask_outputs=[True, True, True]
    [GraphBuilder-UGE.inline_functions] begin inlining graph
    [GraphBuilder-UGE.inline_functions] skip_functions=set()
    [GraphBuilder-UGE._inline_functions_iterations] inline function 'submod_3' domain 'local_functions' [n_replacements=1]
    [GraphBuilder-UGE._inline_functions_iterations] done with 9 new nodes for 'submod_3', 'local_functions'
    [GraphBuilder-UGE.inline_functions] done inlining graph 127856935095712 in 0.004820289002964273
    [GraphBuilder-UGE._add_shape_information] dynamic shapes replacements={'batch_size': 'batch_size', 'sequence_length': 'sequence_length', 'past_sequence_length': 'past_sequence_length', 'total_sequence_length': 'total_sequence_length', 's58': 'total_sequence_length', 's61': 'batch_size', 's72': 'batch_size', 'batch_size^s61^batch_size^s61': 'batch_size', 's67': 'batch_size', 's70': 'sequence_length', 's21': 'past_sequence_length', 's43': 'past_sequence_length'}
    [GraphBuilder-UGE.optimize] start with 173 nodes
    [GraphBuilder-UGE.optimize] #patterns=109
    [GraphBuilder-UGE.optimize] start with subgraphs
    [GraphBuilder-UGE.optimize] done with subgraphs
    [GraphBuilderPatternOptimization-UGE.optimize] start with 143 nodes, 31 initializers, 109 patterns, priorities=[0, 1, 2, 3], max_iter=572
    [GraphBuilderPatternOptimization-UGE.optimize] same children={'SameChildrenPattern', 'SameChildrenFromInputPattern'}
    [GraphBuilderPatternOptimization-UGE.optimize] iteration 0: 143 nodes, priority=0
    [GraphBuilderPatternOptimization-UGE.optimize] applies 26 matches, 8*CastPattern, 1*IdentityPattern, 3*ShapeBasedReshapeIsSqueezePattern, 1*ShapeBasedStaticExpandPattern, 2*ShapeBasedEditDistanceReshapePattern, 4*SameChildrenPattern, 1*SameChildrenFromInputPattern, 2*SqueezeAddPattern, 1*SqueezeUnsqueezePattern, 2*UnsqueezeUnsqueezePattern, 1*FunctionAttentionPattern - time=0.012 | max_time=SoftmaxCrossEntropyLossCastPattern:0.002
    [GraphBuilderPatternOptimization-UGE.optimize] reapply {'SameChildrenPattern', 'SameChildrenFromInputPattern'}
    [GraphBuilderPatternOptimization-UGE.optimize] n_added=6, n_removed=8, n_applied=28 applied patterns, 105 nodes left with 2 iterations
    [GraphBuilderPatternOptimization-UGE.optimize] increase priority to 1
    [GraphBuilderPatternOptimization-UGE.optimize] iteration 1: 105 nodes, priority=1
    [GraphBuilderPatternOptimization-UGE.optimize] applies 19 matches, 2*ConcatTwiceUnaryPattern, 1*ConstantToInitializerPattern, 1*ShapeBasedConcatExpandPattern, 2*SlicesSplitPattern, 1*SqueezeBinaryUnsqueezePattern, 4*SqueezeUnsqueezePattern, 2*SwapUnsqueezeTransposePattern, 2*UnsqueezeUnsqueezePattern, 1*QuickGeluPattern, 3*SimplifiedLayerNormalizationPattern - time=0.018 | max_time=SoftmaxCrossEntropyLossCastPattern:0.002
    [GraphBuilderPatternOptimization-UGE.optimize] iteration 2: 79 nodes, priority=1
    [GraphBuilderPatternOptimization-UGE.optimize] applies 8 matches, 2*IdentityPattern, 1*UnsqueezeUnsqueezePattern, 2*FunctionHalfRotaryEmbeddingPattern, 3*SimplifiedLayerNormalizationMulPattern - time=0.016 | max_time=SoftmaxCrossEntropyLossCastPattern:0.002
    [GraphBuilderPatternOptimization-UGE.optimize] iteration 3: 63 nodes, priority=1
    [GraphBuilderPatternOptimization-UGE.optimize] applies 2 matches, 2*SkipSimplifiedLayerNormalizationPattern - time=0.006 | max_time=ShapeBasedEditDistanceReshapePattern:0.000
    [GraphBuilderPatternOptimization-UGE.optimize] iteration 4: 61 nodes, priority=1
    [GraphBuilderPatternOptimization-UGE.optimize] increase priority to 2
    [GraphBuilderPatternOptimization-UGE.optimize] iteration 5: 61 nodes, priority=2
    [GraphBuilderPatternOptimization-UGE.optimize] increase priority to 3
    [GraphBuilderPatternOptimization-UGE.optimize] iteration 6: 61 nodes, priority=3
    [GraphBuilderPatternOptimization-UGE.optimize] stops current_priority_index=4, priorities=[0, 1, 2, 3]
    [GraphBuilderPatternOptimization-UGE.optimize] done after 7 iterations with 61 nodes in 0.133
    [OrderOptimization.optimize] ALGO-2
    [OrderOptimization.random_order] -- starts with 61 nodes, 29 initializers
    [OrderOptimization.shape_order] done after in 0.00034910300746560097s with changed=4 scale=29
    [GraphBuilder-UGE.optimize] done with 61 nodes in 0.151
    [GraphBuilder-UGE.to_onnx] make_model 33 inits 12 params
    [GraphBuilder-UGE.time_evaluation_constants_] 0.0005457910010591149
    [GraphBuilder-UGE._build_initializers] start with 33 initializers, large_model=True, external_threshold=1024
    [GraphBuilder-UGE._build_initializers] switch low/high order
    [GraphBuilder-UGE._build_initializers] done in 2.1849991753697395e-06s with 29 initializers, 9 large initializers
    [GraphBuilder-UGE._add_shape_information] dynamic shapes replacements={'batch_size': 'batch_size', 'sequence_length': 'sequence_length', 'past_sequence_length': 'past_sequence_length', 'total_sequence_length': 'total_sequence_length', 's58': 'total_sequence_length', 's61': 'batch_size', 's72': 'batch_size', 'batch_size^s61^batch_size^s61': 'batch_size', 's67': 'batch_size', 's70': 'sequence_length', 's21': 'past_sequence_length', 's43': 'past_sequence_length'}
    [to_onnx] to_onnx done in 0.17088012798922136s and 61 nodes, 29 initializers, 4 inputs, 3 outputs
    Continue: it rains...
    If you are a real fan of the genre on fire, you should check out and take it to see what you are doing to get this stuff done.
    Because the kids do that is not




.. GENERATED FROM PYTHON SOURCE LINES 96-98

.. code-block:: Python


    doc.plot_legend("Tiny-LLM\nforward inputs\through generate", "torch.export.export", "tomato")



.. image-sg:: /auto_examples/images/sphx_glr_plot_export_tiny_llm_method_generate_001.png
   :alt: plot export tiny llm method generate
   :srcset: /auto_examples/images/sphx_glr_plot_export_tiny_llm_method_generate_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 14.393 seconds)


.. _sphx_glr_download_auto_examples_plot_export_tiny_llm_method_generate.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_tiny_llm_method_generate.ipynb <plot_export_tiny_llm_method_generate.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_tiny_llm_method_generate.py <plot_export_tiny_llm_method_generate.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_tiny_llm_method_generate.zip <plot_export_tiny_llm_method_generate.zip>`


.. include:: plot_export_tiny_llm_method_generate.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
