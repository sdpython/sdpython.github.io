
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_technical/plot_broadcast_export_issue.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_technical_plot_broadcast_export_issue.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_technical_plot_broadcast_export_issue.py:


Dynamic Shapes and Broadcasting
===============================

:func:`torch.export.export` makes strict assumption on dynamic shapes
to the generic case. Let's consider two tensors with only one dimension.
``x * y`` allows four configurations:

* ``shape(x) = (1,)`` and ``shape(y) = (1,)``
* ``shape(x) = (1,)`` and ``shape(y) = (p,)``
* ``shape(x) = (q,)`` and ``shape(y) = (1,)``
* ``shape(x) = (p,)`` and ``shape(y) = (p,)``

The expected shape for ``shape(x * y)`` is ``(max(p,q),)``.

Simple Case
+++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 20-43

.. code-block:: Python


    import torch
    from torch.fx.experimental.symbolic_shapes import ShapeEnv
    from torch._subclasses.fake_tensor import FakeTensorMode
    from torch.fx.passes.fake_tensor_prop import FakeTensorProp
    from onnx_diagnostic.torch_export_patches import torch_export_patches
    from torch.fx import Tracer


    class Model(torch.nn.Module):
        def forward(self, x, y):
            return x * y


    Dim = torch.export.Dim

    ep = torch.export.export(
        Model(),
        (torch.tensor([2, 3], dtype=torch.float32), torch.tensor([2, 3], dtype=torch.float32)),
        dynamic_shapes=({0: Dim.DYNAMIC}, {0: Dim.DYNAMIC}),
    )
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s17]", y: "f32[s17]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/technical/plot_broadcast_export_issue.py:31 in forward, code: return x * y
                mul: "f32[s17]" = torch.ops.aten.mul.Tensor(x, y);  x = y = None
                return (mul,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
    
        # outputs
        mul: USER_OUTPUT
    
    Range constraints: {s17: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 44-48

We see clearly that the export assumed that ``x`` ad ``y`` had the same shape.
No other configuration seemed to work at export time,
including ``with torch.fx.experimental._config.patch(backed_size_oblivious=True):``
the shape of one tensor equal to ``(1,)``.

.. GENERATED FROM PYTHON SOURCE LINES 48-52

.. code-block:: Python


    output = [n for n in ep.graph.nodes if n.op == "output"][0]
    print("output is ", output.name, " arg is", output.args[0])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    output is  output  arg is (mul,)




.. GENERATED FROM PYTHON SOURCE LINES 53-54

The final shape is:

.. GENERATED FROM PYTHON SOURCE LINES 54-58

.. code-block:: Python


    shape = output.args[0][0].meta["val"].shape
    print("output shape is ", shape)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    output shape is  torch.Size([s17])




.. GENERATED FROM PYTHON SOURCE LINES 59-63

Tracing
+++++++

Let's compare with what a simple tracing would do. Let's use :class:`torch.fx.Tracer`.

.. GENERATED FROM PYTHON SOURCE LINES 63-67

.. code-block:: Python


    graph = Tracer().trace(Model())
    print(graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %x : [num_users=1] = placeholder[target=x]
        %y : [num_users=1] = placeholder[target=y]
        %mul : [num_users=1] = call_function[target=operator.mul](args = (%x, %y), kwargs = {})
        return mul




.. GENERATED FROM PYTHON SOURCE LINES 68-72

.. code-block:: Python

    output = [n for n in graph.nodes if n.op == "output"][0]
    print("output is ", output.name, " arg is", output.args[0])
    print("The tracer leaves no trace:", output.args[0].__dict__)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    output is  output  arg is mul
    The tracer leaves no trace: {}




.. GENERATED FROM PYTHON SOURCE LINES 73-75

Shape propagation
+++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 75-90

.. code-block:: Python


    gm = torch.fx.GraphModule(Model(), graph)

    shape_env = ShapeEnv()
    fake_mode = FakeTensorMode(shape_env=shape_env)
    # d1 = shape_env.create_unbacked_symint()
    # d2 = shape_env.create_unbacked_symint()
    fake_inputs = fake_mode.from_tensor(
        torch.zeros((3,), dtype=torch.float32), static_shapes=False
    ), fake_mode.from_tensor(torch.zeros((3,), dtype=torch.float32), static_shapes=False)

    print("fake_inputs are ", fake_inputs)
    res = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)
    print("output is", res)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    fake_inputs are  (FakeTensor(..., size=(s26,)), FakeTensor(..., size=(s26,)))
    output is FakeTensor(..., size=(s26,))




.. GENERATED FROM PYTHON SOURCE LINES 91-93

Handle Different Shapes
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 93-102

.. code-block:: Python


    fake_inputs = fake_mode.from_tensor(
        torch.zeros((2,), dtype=torch.float32), static_shapes=False
    ), fake_mode.from_tensor(torch.zeros((1,), dtype=torch.float32), static_shapes=False)

    print("fake_inputs are ", fake_inputs)
    res = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)
    print("output is", res)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    fake_inputs are  (FakeTensor(..., size=(s53,)), FakeTensor(..., size=(1,)))
    output is FakeTensor(..., size=(s53,))




.. GENERATED FROM PYTHON SOURCE LINES 103-107

Conclusion
++++++++++

We need to give distinct dimensions to get distinct names.

.. GENERATED FROM PYTHON SOURCE LINES 107-114

.. code-block:: Python


    fake_inputs = fake_mode.from_tensor(
        torch.zeros((2,), dtype=torch.float32), static_shapes=False
    ), fake_mode.from_tensor(torch.zeros((3,), dtype=torch.float32), static_shapes=False)
    print("fake_inputs are ", fake_inputs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    fake_inputs are  (FakeTensor(..., size=(s53,)), FakeTensor(..., size=(s26,)))




.. GENERATED FROM PYTHON SOURCE LINES 115-120

.. code-block:: Python

    try:
        res = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)
    except Exception as e:
        print("error", e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    error The size of tensor a (s53) must match the size of tensor b (s26) at non-singleton dimension 0)

    While executing %mul : [num_users=1] = call_function[target=operator.mul](args = (%x, %y), kwargs = {})
    Original traceback:
    None
    Use tlparse to see full graph. (https://github.com/pytorch/tlparse?tab=readme-ov-file#tlparse-parse-structured-pt2-logs)




.. GENERATED FROM PYTHON SOURCE LINES 121-122

By applying the patches:

.. GENERATED FROM PYTHON SOURCE LINES 122-127

.. code-block:: Python


    with torch_export_patches():
        res = FakeTensorProp(gm, fake_mode).propagate(*fake_inputs)
        print("output is", res)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    output is FakeTensor(..., size=(Max(s26, s53),))




.. GENERATED FROM PYTHON SOURCE LINES 128-129

This is what we want. Let's go back to :func:`torch.export.export`

.. GENERATED FROM PYTHON SOURCE LINES 129-141

.. code-block:: Python


    with torch_export_patches():
        ep = torch.export.export(
            Model(),
            (
                torch.tensor([2, 3], dtype=torch.float32),
                torch.tensor([2, 3, 4], dtype=torch.float32),
            ),
            dynamic_shapes=({0: Dim.DYNAMIC}, {0: Dim.DYNAMIC}),
        )
        print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s77]", y: "f32[s17]"):
                 # File: /home/xadupre/github/onnx-diagnostic/_doc/technical/plot_broadcast_export_issue.py:31 in forward, code: return x * y
                mul: "f32[Max(s17, s77)]" = torch.ops.aten.mul.Tensor(x, y);  x = y = None
                return (mul,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
        y: USER_INPUT
    
        # outputs
        mul: USER_OUTPUT
    
    Range constraints: {s77: VR[2, int_oo], s17: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 142-146

.. code-block:: Python

    output = [n for n in ep.graph.nodes if n.op == "output"][0]
    print("output is ", output.name, " arg is", output.args[0])
    shape = output.args[0][0].meta["val"].shape
    print("output shape is ", shape)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    output is  output  arg is (mul,)
    output shape is  torch.Size([Max(s17, s77)])





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.482 seconds)


.. _sphx_glr_download_auto_technical_plot_broadcast_export_issue.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_broadcast_export_issue.ipynb <plot_broadcast_export_issue.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_broadcast_export_issue.py <plot_broadcast_export_issue.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_broadcast_export_issue.zip <plot_broadcast_export_issue.zip>`


.. include:: plot_broadcast_export_issue.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
