{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Dumps intermediate results of a torch model\n\nLooking for discrepancies is quickly annoying. Discrepancies\ncome from two results obtained with the same models\nimplemented in two different ways, :epkg:`pytorch` and :epkg:`onnx`.\nModels are big so where do they come from? That's the\nunavoidable question. Unless there is an obvious reason,\nthe only way is to compare intermediate outputs alon the computation.\nThe first step into that direction is to dump the intermediate results\ncoming from :epkg:`pytorch`.\nWe use :func:`onnx_diagnostic.helpers.torch_helper.steal_forward` for that.\n\n## A simple LLM Model\n\nSee :func:`onnx_diagnostic.helpers.torch_helper.dummy_llm`\nfor its definition. It is mostly used for unit test or example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nimport torch\nfrom onnx_array_api.plotting.graphviz_helper import plot_dot\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.helpers.torch_helper import dummy_llm\nfrom onnx_diagnostic.helpers.mini_onnx_builder import create_input_tensors_from_onnx_model\nfrom onnx_diagnostic.helpers.torch_helper import steal_forward\n\n\nmodel, inputs, ds = dummy_llm(dynamic_shapes=True)\n\nprint(f\"type(model)={type(model)}\")\nprint(f\"inputs={string_type(inputs, with_shape=True)}\")\nprint(f\"ds={string_type(ds, with_shape=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It contains the following submodules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for name, mod in model.named_modules():\n    print(f\"- {name}: {type(mod)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Steal and dump the output of submodules\n\nThe following context spies on the intermediate results\nfor the following module and submodules. It stores\nin one onnx file all the input/output for those.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with steal_forward(\n    [\n        (\"model\", model),\n        (\"model.decoder\", model.decoder),\n        (\"model.decoder.attention\", model.decoder.attention),\n        (\"model.decoder.feed_forward\", model.decoder.feed_forward),\n        (\"model.decoder.norm_1\", model.decoder.norm_1),\n        (\"model.decoder.norm_2\", model.decoder.norm_2),\n    ],\n    dump_file=\"plot_dump_intermediate_results.inputs.onnx\",\n    verbose=1,\n    storage_limit=2**28,\n):\n    model(*inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Restores saved inputs/outputs\n\nAll the intermediate tensors were saved in one unique onnx model,\nevery tensor is stored in a constant node.\nThe model can be run with any runtime to restore the inputs\nand function :func:`create_input_tensors_from_onnx_model\n<onnx_diagnostic.helpers.mini_onnx_builder.create_input_tensors_from_onnx_model>`\ncan restore their names.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "saved_tensors = create_input_tensors_from_onnx_model(\n    \"plot_dump_intermediate_results.inputs.onnx\"\n)\nfor k, v in saved_tensors.items():\n    print(f\"{k} -- {string_type(v, with_shape=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's explained the naming convention.\n\n::\n\n   ('model.decoder.norm_2', 0, 'I') -- ((T1s2x30x16,),{})\n               |            |   |\n               |            |   +--> input, the format is args, kwargs\n               |            |\n               |            +--> iteration, 0 means the first time the execution\n               |                 went through that module\n               |                 it is possible to call multiple times,\n               |                 the model to store more\n               |\n               +--> the name given to function steal_forward\n\nThe same goes for output except ``'I'`` is replaced by ``'O'``.\n\n::\n\n   ('model.decoder.norm_2', 0, 'O') -- T1s2x30x16\n\nThis trick can be used to compare intermediate results coming\nfrom pytorch to any other implementation of the same model\nas long as it is possible to map the stored inputs/outputs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversion to ONNX\n\nThe difficult point is to be able to map the saved intermediate\nresults to intermediate results in ONNX.\nLet's create the ONNX model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epo = torch.onnx.export(model, inputs, dynamic_shapes=ds, dynamo=True)\nepo.optimize()\nepo.save(\"plot_dump_intermediate_results.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks like the following.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = onnx.load(\"plot_dump_intermediate_results.onnx\")\nplot_dot(onx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.plot_legend(\"steal and dump\\nintermediate\\nresults\", \"steal_forward\", \"blue\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}