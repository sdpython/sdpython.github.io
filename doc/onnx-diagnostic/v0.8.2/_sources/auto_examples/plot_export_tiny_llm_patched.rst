
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_export_tiny_llm_patched.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_export_tiny_llm_patched.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_export_tiny_llm_patched.py:


.. _l-plot-tiny-llm-export-patched:

Export Tiny-LLM with patches
============================

Many models from :epkg:`transformers` cannot be converted because
the implementation uses cache classes. Let's see how to get around that.
We focus on the model :epkg:`arnir0/Tiny-LLM`.
To avoid downloading any weights, we write a function creating a
random model based on the same architecture.
This continues example :ref:`l-plot-tiny-llm-export`.

Errors
++++++

They depend on transformers version.

``transformers>=4.40,<4.50`` cannot serialize DynamicCache and cannot
map dynamic shapes to instances of DynamicCache. The following errors
would appear:

::

  torch._dynamo.exc.UserError: Cannot associate shape
      [[{0: <class '....batch'>, 2: <class '....cache_length'>}],
       [{0: <class '....batch'>, 2: <class '....cache_length'>}]]
      specified at `dynamic_shapes['past_key_values']`
      to non-tensor type <class 'transformers.cache_utils.DynamicCache'>
      at `inputs['past_key_values']` (expected None)
  For more information about this error,
  see: https://docs.pytorch.org/docs/stable/generated/exportdb/index.html#dynamic-shapes-validation

With ``transformers==4.50``, it shows the following:

::

  torch._dynamo.exc.UserError: Constraints violated (batch)!
  For more information, run with TORCH_LOGS="+dynamic".
      - Not all values of batch = L['args'][1]['input_ids'].size()[0]
          in the specified range batch <= 1024 are valid
          because batch was inferred to be a constant (2).
      - Not all values of batch = L['args'][1]['attention_mask'].size()[0]
          in the specified range batch <= 1024 are valid
          because batch was inferred to be a constant (2).
      - Not all values of batch = L['args'][1]['past_key_values']['key_cache'][0].size()[0]
          in the specified range batch <= 1024 are valid
          because batch was inferred to be a constant (2).
      - Not all values of batch = L['args'][1]['past_key_values']['value_cache'][0].size()[0]
          in the specified range batch <= 1024 are valid
          because batch was inferred to be a constant (2).
   Suggested fixes:
       batch = 2

However, this package implements a patch mechanism
with replaces the part causing these issues.

.. note:: restart after an export failure

    If the export fails, it is better to start executing again,
    or restart the kernel if you are in the notebook.
    The export may leave :epkg:`torch` in one unstable state.

.. GENERATED FROM PYTHON SOURCE LINES 64-86

.. code-block:: Python


    import copy
    import pprint
    import torch
    import transformers
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers.cache_helper import is_cache_dynamic_registered
    from onnx_diagnostic.helpers import string_type
    from onnx_diagnostic.torch_export_patches import torch_export_patches
    from onnx_diagnostic.torch_export_patches.patch_inputs import use_dyn_not_str
    from onnx_diagnostic.torch_models.llms import get_tiny_llm


    experiment = get_tiny_llm()
    untrained_model, inputs, dynamic_shapes = (
        experiment["model"],
        experiment["inputs"],
        experiment["dynamic_shapes"],
    )

    cloned_inputs = copy.deepcopy(inputs)








.. GENERATED FROM PYTHON SOURCE LINES 87-89

Let's show this inputs, this was inferred in
example :ref:`l-plot-tiny-llm-export`.

.. GENERATED FROM PYTHON SOURCE LINES 89-92

.. code-block:: Python


    print(string_type(inputs, with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x30x96], value_cache=#1[T1s2x1x30x96]))




.. GENERATED FROM PYTHON SOURCE LINES 93-94

And the dynamic shapes

.. GENERATED FROM PYTHON SOURCE LINES 94-96

.. code-block:: Python

    pprint.pprint(dynamic_shapes)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'attention_mask': {0: 'batch', 1: 'cache+seq'},
     'input_ids': {0: 'batch', 1: 'seq_length'},
     'past_key_values': [{0: 'batch', 2: 'cache_length'},
                         {0: 'batch', 2: 'cache_length'}],
     'position_ids': {0: 'batch', 1: 'seq_length'}}




.. GENERATED FROM PYTHON SOURCE LINES 97-100

Before exporting, we check :class:`transformers.cache_utils.DynamicCache`
can serialized and deserialized otherwise :func:`torch.export.export`
fails.

.. GENERATED FROM PYTHON SOURCE LINES 100-103

.. code-block:: Python


    print("-- DynamicCache registered: ", is_cache_dynamic_registered())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- DynamicCache registered:  True




.. GENERATED FROM PYTHON SOURCE LINES 104-107

If they are not registered, function
:func:`onnx_diagnostic.torch_export_patches.torch_export_patches`
should take care of it. Then we export.

.. GENERATED FROM PYTHON SOURCE LINES 107-120

.. code-block:: Python


    with torch_export_patches(patch_transformers=True, verbose=10) as modificator:
        assert is_cache_dynamic_registered()  # it must be true here
        ep = torch.export.export(
            untrained_model,
            (),
            kwargs=modificator(cloned_inputs),
            dynamic_shapes=use_dyn_not_str(dynamic_shapes),
            strict=False,  # mandatory for torch==2.6
        )
        print("It worked:")
        print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch_export_patches] patch_sympy=True
                         . patch_torch=True
                         . patch_transformers=True
                         . patch_diffusers=False
                         . catch_constraints=True
                         . stop_if_static=0
                         . patch=True
                         . custom_patches=None
    [torch_export_patches] dump_rewriting=None
    [torch_export_patches] replace torch.jit.isinstance, torch._dynamo.mark_static_address
    [_fix_registration] BaseModelOutput is unregistered and registered first
    [unregister_cache_serialization] unregistered BaseModelOutput
    [register_class_serialization] ---------- register BaseModelOutput
    [_fix_registration] BaseModelOutput done.
    [register_class_serialization] ---------- register DynamicCache
    [register_class_serialization] ---------- register HybridCache
    [register_class_serialization] ---------- register EncoderDecoderCache
    [register_class_serialization] ---------- register SlidingWindowCache
    [register_class_serialization] ---------- register StaticCache
    [register_class_serialization] ---------- register MambaCache
    [register_class_serialization] already registered BaseModelOutput
    [torch_export_patches] sympy.__version__='1.14.0'
    [torch_export_patches] patch sympy
    [torch_export_patches] torch.__version__='2.10.0.dev20251106+cu130'
    [torch_export_patches] stop_if_static=0
    [torch_export_patches] patch pytorch
    [torch_export_patches] modifies shape constraints
    [torch_export_patches] transformers.__version__='5.0.0.dev0'
    [torch_export_patches] patches transformers.masking_utils.eager_mask
    [torch_export_patches] patches transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
    [torch_export_patches] patches transformers.integrations.sdpa_attention.sdpa_attention_forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter: 
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
    [patch_module_or_classes] function: transformers.models.bart.modeling_bart.eager_attention_forward
    [patch_module_or_classes] function: transformers.models.marian.modeling_marian.eager_attention_forward
    [torch_export_patches] done patching
    It worked:
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, p_model_embed_tokens_weight: "f32[32000, 192]", p_model_layers_0_self_attn_q_proj_weight: "f32[192, 192]", p_model_layers_0_self_attn_k_proj_weight: "f32[96, 192]", p_model_layers_0_self_attn_v_proj_weight: "f32[96, 192]", p_model_layers_0_self_attn_o_proj_weight: "f32[192, 192]", p_model_layers_0_mlp_gate_proj_weight: "f32[1024, 192]", p_model_layers_0_mlp_up_proj_weight: "f32[1024, 192]", p_model_layers_0_mlp_down_proj_weight: "f32[192, 1024]", p_model_layers_0_input_layernorm_weight: "f32[192]", p_model_layers_0_post_attention_layernorm_weight: "f32[192]", p_model_norm_weight: "f32[192]", p_lm_head_weight: "f32[32000, 192]", b_model_rotary_emb_inv_freq: "f32[48]", input_ids: "i64[s44, s70]", attention_mask: "i64[s43, s53]", position_ids: "i64[s44, s70]", past_key_values_key_0: "f32[s44, 1, s45, 96]", past_key_values_value_0: "f32[s44, 1, s21, 96]"):
                # No stacktrace found for following nodes
                sym_size_int_16: "Sym(s70)" = torch.ops.aten.sym_size.int(input_ids, 1)
                sym_size_int_19: "Sym(s44)" = torch.ops.aten.sym_size.int(position_ids, 0)
                sym_size_int_22: "Sym(s45)" = torch.ops.aten.sym_size.int(past_key_values_key_0, 2)
                sym_size_int_24: "Sym(s21)" = torch.ops.aten.sym_size.int(past_key_values_value_0, 2)
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
                embedding: "f32[s44, s70, 192]" = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
                add: "Sym(s45 + s70)" = sym_size_int_22 + sym_size_int_16
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
                arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int_22, add, device = device(type='cpu'), pin_memory = False);  sym_size_int_22 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
                _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(attention_mask, dtype = torch.int64, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default = None
                to: "b8[s43, s53]" = torch.ops.aten.to.device(attention_mask, device(type='cpu'), torch.bool);  attention_mask = None
                arange_1: "i64[s44]" = torch.ops.aten.arange.default(sym_size_int_19, device = device(type='cpu'), pin_memory = False)
                arange_2: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
                arange_3: "i64[s45 + s70]" = torch.ops.aten.arange.default(add, device = device(type='cpu'), pin_memory = False)
                add_3: "i64[s45 + s70]" = torch.ops.aten.add.Tensor(arange_3, 0);  arange_3 = None
                slice_1: "i64[s44]" = torch.ops.aten.slice.Tensor(arange_1, 0, 0, 9223372036854775807);  arange_1 = None
                unsqueeze: "i64[s44, 1]" = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
                unsqueeze_1: "i64[s44, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None
                unsqueeze_2: "i64[s44, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None
                unsqueeze_3: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_2, 0);  arange_2 = None
                unsqueeze_4: "i64[1, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
                unsqueeze_5: "i64[1, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = unsqueeze_5 = None
                unsqueeze_6: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
                unsqueeze_7: "i64[1, 1, s70]" = torch.ops.aten.unsqueeze.default(unsqueeze_6, 1);  unsqueeze_6 = None
                slice_2: "i64[1, 1, s70]" = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
                unsqueeze_8: "i64[1, 1, s70, 1]" = torch.ops.aten.unsqueeze.default(slice_2, 3);  slice_2 = None
                unsqueeze_9: "i64[1, s45 + s70]" = torch.ops.aten.unsqueeze.default(add_3, 0);  add_3 = None
                unsqueeze_10: "i64[1, 1, s45 + s70]" = torch.ops.aten.unsqueeze.default(unsqueeze_9, 1);  unsqueeze_9 = None
                unsqueeze_11: "i64[1, 1, 1, s45 + s70]" = torch.ops.aten.unsqueeze.default(unsqueeze_10, 2);  unsqueeze_10 = None
                slice_3: "i64[1, 1, 1, s45 + s70]" = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
                new_ones: "b8[]" = torch.ops.aten.new_ones.default(unsqueeze_8, [], dtype = torch.bool, pin_memory = False)
                le_3: "b8[1, 1, s70, s45 + s70]" = torch.ops.aten.le.Tensor(slice_3, unsqueeze_8);  unsqueeze_8 = None
                _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(le_3, dtype = torch.bool, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
                to_1: "b8[1, 1, s70, s45 + s70]" = torch.ops.aten.to.dtype_layout(le_3, dtype = torch.bool, layout = torch.strided, device = device(type='cpu'));  le_3 = None
                and_1: "b8[1, 1, s70, s45 + s70]" = torch.ops.aten.__and__.Tensor(new_ones, to_1);  new_ones = to_1 = None
                index: "b8[s44, 1, 1, s45 + s70]" = torch.ops.aten.index.Tensor(to, [unsqueeze_2, slice_3]);  to = unsqueeze_2 = slice_3 = None
                _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(index, dtype = torch.bool, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
                to_2: "b8[s44, 1, 1, s45 + s70]" = torch.ops.aten.to.dtype_layout(index, dtype = torch.bool, layout = torch.strided, device = device(type='cpu'));  index = None
                and_2: "b8[s44, 1, s70, s45 + s70]" = torch.ops.aten.__and__.Tensor(and_1, to_2);  and_1 = to_2 = None
                expand: "b8[s44, 1, s70, s45 + s70]" = torch.ops.aten.expand.default(and_2, [sym_size_int_19, -1, sym_size_int_16, add]);  and_2 = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1549 in forward, code: inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
                unsqueeze_12: "f32[1, 48]" = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
                unsqueeze_13: "f32[1, 48, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_12, 2);  unsqueeze_12 = None
                _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_13, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
                to_3: "f32[1, 48, 1]" = torch.ops.aten.to.dtype(unsqueeze_13, torch.float32);  unsqueeze_13 = None
                expand_1: "f32[s44, 48, 1]" = torch.ops.aten.expand.default(to_3, [sym_size_int_19, -1, 1]);  to_3 = None
                _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
                to_4: "f32[s44, 48, 1]" = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  expand_1 = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1551 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
                slice_4: "i64[s44, s70]" = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
                unsqueeze_14: "i64[s44, 1, s70]" = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
                slice_5: "i64[s44, 1, s70]" = torch.ops.aten.slice.Tensor(unsqueeze_14, 2, 0, 9223372036854775807);  unsqueeze_14 = None
                _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(slice_5, dtype = torch.int64, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
                to_5: "f32[s44, 1, s70]" = torch.ops.aten.to.dtype(slice_5, torch.float32);  slice_5 = None
            
                # No stacktrace found for following nodes
                submod_3 = self.submod_1
                wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast('cpu', torch.bfloat16, False, False, submod_3, to_4, to_5);  submod_3 = to_4 = to_5 = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
                mul: "f32[s44, s70, 96]" = wrap_with_autocast[0]
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
                mul_1: "f32[s44, s70, 96]" = wrap_with_autocast[1];  wrap_with_autocast = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1564 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
                _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
                to_8: "f32[s44, s70, 96]" = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
                _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
                to_9: "f32[s44, s70, 96]" = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
                _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
                to_10: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
                pow_1: "f32[s44, s70, 192]" = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
                mean: "f32[s44, s70, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
                add_4: "f32[s44, s70, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
                rsqrt: "f32[s44, s70, 1]" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
                mul_2: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(to_10, rsqrt);  rsqrt = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
                _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
                to_11: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
                mul_3: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_11);  p_model_layers_0_input_layernorm_weight = to_11 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear: "f32[s44, s70, 192]" = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                view: "f32[s44, s70, 2, 96]" = torch.ops.aten.view.default(linear, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear = None
                transpose_1: "f32[s44, 2, s70, 96]" = torch.ops.aten.transpose.int(view, 1, 2);  view = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_1: "f32[s44, s70, 96]" = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:265 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                view_1: "f32[s44, s70, 1, 96]" = torch.ops.aten.view.default(linear_1, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_1 = None
                transpose_2: "f32[s44, 1, s70, 96]" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_2: "f32[s44, s70, 96]" = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:266 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                view_2: "f32[s44, s70, 1, 96]" = torch.ops.aten.view.default(linear_2, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_2 = None
                transpose_3: "f32[s44, 1, s70, 96]" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:269 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
                unsqueeze_15: "f32[s44, 1, s70, 96]" = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
                unsqueeze_16: "f32[s44, 1, s70, 96]" = torch.ops.aten.unsqueeze.default(to_9, 1);  to_9 = None
                mul_4: "f32[s44, 2, s70, 96]" = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_15)
                slice_6: "f32[s44, 2, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 48)
                slice_7: "f32[s44, 2, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_1, 3, 48, 9223372036854775807);  transpose_1 = None
                neg: "f32[s44, 2, s70, 48]" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
                cat_1: "f32[s44, 2, s70, 96]" = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
                mul_5: "f32[s44, 2, s70, 96]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_16);  cat_1 = None
                add_5: "f32[s44, 2, s70, 96]" = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
                mul_6: "f32[s44, 1, s70, 96]" = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_15);  unsqueeze_15 = None
                slice_8: "f32[s44, 1, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 48)
                slice_9: "f32[s44, 1, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_2, 3, 48, 9223372036854775807);  transpose_2 = None
                neg_1: "f32[s44, 1, s70, 48]" = torch.ops.aten.neg.default(slice_9);  slice_9 = None
                cat_2: "f32[s44, 1, s70, 96]" = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
                mul_7: "f32[s44, 1, s70, 96]" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_16);  cat_2 = unsqueeze_16 = None
                add_6: "f32[s44, 1, s70, 96]" = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:274 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
                cat_3: "f32[s44, 1, s45 + s70, 96]" = torch.ops.aten.cat.default([past_key_values_key_0, add_6], -2);  past_key_values_key_0 = add_6 = None
                cat_4: "f32[s44, 1, s21 + s70, 96]" = torch.ops.aten.cat.default([past_key_values_value_0, transpose_3], -2);  past_key_values_value_0 = transpose_3 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:280 in forward, code: attn_output, attn_weights = attention_interface(
                slice_10: "f32[s44, 1, s45 + s70, 96]" = torch.ops.aten.slice.Tensor(cat_3, 0, 0, 9223372036854775807)
                unsqueeze_17: "f32[s44, 1, 1, s45 + s70, 96]" = torch.ops.aten.unsqueeze.default(slice_10, 2);  slice_10 = None
                slice_11: "f32[s44, 1, 1, s45 + s70, 96]" = torch.ops.aten.slice.Tensor(unsqueeze_17, 3, 0, 9223372036854775807);  unsqueeze_17 = None
                expand_2: "f32[s44, 1, 2, s45 + s70, 96]" = torch.ops.aten.expand.default(slice_11, [sym_size_int_19, 1, 2, add, 96]);  slice_11 = None
                reshape: "f32[s44, 2, s45 + s70, 96]" = torch.ops.aten.reshape.default(expand_2, [sym_size_int_19, 2, add, 96]);  expand_2 = None
                slice_12: "f32[s44, 1, s21 + s70, 96]" = torch.ops.aten.slice.Tensor(cat_4, 0, 0, 9223372036854775807)
                unsqueeze_18: "f32[s44, 1, 1, s21 + s70, 96]" = torch.ops.aten.unsqueeze.default(slice_12, 2);  slice_12 = None
                add_11: "Sym(s21 + s70)" = sym_size_int_24 + sym_size_int_16;  sym_size_int_24 = None
                slice_13: "f32[s44, 1, 1, s21 + s70, 96]" = torch.ops.aten.slice.Tensor(unsqueeze_18, 3, 0, 9223372036854775807);  unsqueeze_18 = None
                expand_3: "f32[s44, 1, 2, s21 + s70, 96]" = torch.ops.aten.expand.default(slice_13, [sym_size_int_19, 1, 2, add_11, 96]);  slice_13 = None
                reshape_1: "f32[s44, 2, s21 + s70, 96]" = torch.ops.aten.reshape.default(expand_3, [sym_size_int_19, 2, add_11, 96]);  expand_3 = add_11 = None
                slice_14: "b8[s44, 1, s70, s45 + s70]" = torch.ops.aten.slice.Tensor(expand, 3, None, add);  expand = add = None
                scaled_dot_product_attention: "f32[s44, 2, s70, 96]" = torch.ops.aten.scaled_dot_product_attention.default(add_5, reshape, reshape_1, slice_14, scale = 0.10206207261596575);  add_5 = reshape = reshape_1 = slice_14 = None
                transpose_4: "f32[s44, s70, 2, 96]" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:291 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
                reshape_2: "f32[s44, s70, 192]" = torch.ops.aten.reshape.default(transpose_4, [sym_size_int_19, sym_size_int_16, -1]);  transpose_4 = sym_size_int_19 = sym_size_int_16 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_3: "f32[s44, s70, 192]" = torch.ops.aten.linear.default(reshape_2, p_model_layers_0_self_attn_o_proj_weight);  reshape_2 = p_model_layers_0_self_attn_o_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
                add_7: "f32[s44, s70, 192]" = torch.ops.aten.add.Tensor(to_10, linear_3);  to_10 = linear_3 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
                _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
                to_12: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
                pow_2: "f32[s44, s70, 192]" = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
                mean_1: "f32[s44, s70, 1]" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
                add_8: "f32[s44, s70, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
                rsqrt_1: "f32[s44, s70, 1]" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
                mul_16: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(to_12, rsqrt_1);  rsqrt_1 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
                _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_16, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
                to_13: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(mul_16, torch.float32);  mul_16 = None
                mul_17: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_13);  p_model_layers_0_post_attention_layernorm_weight = to_13 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_4: "f32[s44, s70, 1024]" = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/activations.py:103 in forward, code: return nn.functional.silu(input)
                silu: "f32[s44, s70, 1024]" = torch.ops.aten.silu.default(linear_4);  linear_4 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_5: "f32[s44, s70, 1024]" = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_up_proj_weight);  mul_17 = p_model_layers_0_mlp_up_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:184 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                mul_18: "f32[s44, s70, 1024]" = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_6: "f32[s44, s70, 192]" = torch.ops.aten.linear.default(mul_18, p_model_layers_0_mlp_down_proj_weight);  mul_18 = p_model_layers_0_mlp_down_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
                add_9: "f32[s44, s70, 192]" = torch.ops.aten.add.Tensor(to_12, linear_6);  to_12 = linear_6 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
                _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_9, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
                to_14: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(add_9, torch.float32);  add_9 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
                pow_3: "f32[s44, s70, 192]" = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
                mean_2: "f32[s44, s70, 1]" = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
                add_10: "f32[s44, s70, 1]" = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
                rsqrt_2: "f32[s44, s70, 1]" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
                mul_19: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(to_14, rsqrt_2);  to_14 = rsqrt_2 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
                _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_19, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
                to_15: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(mul_19, torch.float32);  mul_19 = None
                mul_20: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_15);  p_model_norm_weight = to_15 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:500 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
                slice_15: "f32[s44, s70, 192]" = torch.ops.aten.slice.Tensor(mul_20, 0, 0, 9223372036854775807);  mul_20 = None
                slice_16: "f32[s44, s70, 192]" = torch.ops.aten.slice.Tensor(slice_15, 1, 0, 9223372036854775807);  slice_15 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_7: "f32[s44, s70, 32000]" = torch.ops.aten.linear.default(slice_16, p_lm_head_weight);  slice_16 = p_lm_head_weight = None
                return (linear_7, cat_3, cat_4)
            
            class submod_1(torch.nn.Module):
                def forward(self, to_4: "f32[s44, 48, 1]", to_5: "f32[s44, 1, s70]"):
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1559 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                    _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                    to_6: "f32[s44, 48, 1]" = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                    _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(to_5, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                    to_7: "f32[s44, 1, s70]" = torch.ops.aten.to.dtype(to_5, torch.float32);  to_5 = None
                    matmul: "f32[s44, 48, s70]" = torch.ops.aten.matmul.default(to_6, to_7);  to_6 = to_7 = None
                    transpose: "f32[s44, s70, 48]" = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None
                
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1560 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                    cat: "f32[s44, s70, 96]" = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None
                
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
                    cos: "f32[s44, s70, 96]" = torch.ops.aten.cos.default(cat)
                    mul: "f32[s44, s70, 96]" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
                
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
                    sin: "f32[s44, s70, 96]" = torch.ops.aten.sin.default(cat);  cat = None
                    mul_1: "f32[s44, s70, 96]" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                    return (mul, mul_1)
                
    Graph signature: 
        # inputs
        p_model_embed_tokens_weight: PARAMETER target='model.embed_tokens.weight'
        p_model_layers_0_self_attn_q_proj_weight: PARAMETER target='model.layers.0.self_attn.q_proj.weight'
        p_model_layers_0_self_attn_k_proj_weight: PARAMETER target='model.layers.0.self_attn.k_proj.weight'
        p_model_layers_0_self_attn_v_proj_weight: PARAMETER target='model.layers.0.self_attn.v_proj.weight'
        p_model_layers_0_self_attn_o_proj_weight: PARAMETER target='model.layers.0.self_attn.o_proj.weight'
        p_model_layers_0_mlp_gate_proj_weight: PARAMETER target='model.layers.0.mlp.gate_proj.weight'
        p_model_layers_0_mlp_up_proj_weight: PARAMETER target='model.layers.0.mlp.up_proj.weight'
        p_model_layers_0_mlp_down_proj_weight: PARAMETER target='model.layers.0.mlp.down_proj.weight'
        p_model_layers_0_input_layernorm_weight: PARAMETER target='model.layers.0.input_layernorm.weight'
        p_model_layers_0_post_attention_layernorm_weight: PARAMETER target='model.layers.0.post_attention_layernorm.weight'
        p_model_norm_weight: PARAMETER target='model.norm.weight'
        p_lm_head_weight: PARAMETER target='lm_head.weight'
        b_model_rotary_emb_inv_freq: BUFFER target='model.rotary_emb.inv_freq' persistent=False
        input_ids: USER_INPUT
        attention_mask: USER_INPUT
        position_ids: USER_INPUT
        past_key_values_key_0: USER_INPUT
        past_key_values_value_0: USER_INPUT
    
        # outputs
        linear_7: USER_OUTPUT
        cat_3: USER_OUTPUT
        cat_4: USER_OUTPUT
    
    Range constraints: {s44: VR[2, int_oo], s70: VR[2, int_oo], s43: VR[2, int_oo], s53: VR[4, int_oo], s45: VR[2, int_oo], s21: VR[2, int_oo]}

    [torch_export_patches] remove patches
    [torch_export_patches] restored sympy functions
    [torch_export_patches] restored pytorch functions
    [torch_export_patches] restored shape constraints
    [torch_export_patches] unpatches transformers
    [torch_export_patches] restored transformers.masking_utils.eager_mask
    [torch_export_patches] restored transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
    [torch_export_patches] restored transformers.integrations.sdpa_attention.sdpa_attention_forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter: 
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
    [unpatch_module_or_classes] function transformers.models.bart.modeling_bart.eager_attention_forward
    [unpatch_module_or_classes] function transformers.models.marian.modeling_marian.eager_attention_forward




.. GENERATED FROM PYTHON SOURCE LINES 121-123

With the original model
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 123-141

.. code-block:: Python


    MODEL_NAME = "arnir0/Tiny-LLM"
    tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)
    model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)

    cloned_inputs = copy.deepcopy(inputs)

    with torch_export_patches(patch_transformers=True, verbose=10) as modificator:
        ep = torch.export.export(
            model,
            (),
            kwargs=modificator(cloned_inputs),
            dynamic_shapes=use_dyn_not_str(dynamic_shapes),
            strict=False,  # mandatory for torch==2.6
        )
        print("It worked:")
        print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch_export_patches] patch_sympy=True
                         . patch_torch=True
                         . patch_transformers=True
                         . patch_diffusers=False
                         . catch_constraints=True
                         . stop_if_static=0
                         . patch=True
                         . custom_patches=None
    [torch_export_patches] dump_rewriting=None
    [torch_export_patches] replace torch.jit.isinstance, torch._dynamo.mark_static_address
    [_fix_registration] DynamicCache is unregistered and registered first
    [unregister_cache_serialization] unregistered DynamicCache
    [register_class_serialization] ---------- register DynamicCache
    [_fix_registration] DynamicCache done.
    [register_class_serialization] already registered DynamicCache
    [register_class_serialization] already registered HybridCache
    [register_class_serialization] already registered EncoderDecoderCache
    [register_class_serialization] already registered SlidingWindowCache
    [register_class_serialization] already registered StaticCache
    [register_class_serialization] already registered MambaCache
    [register_class_serialization] already registered BaseModelOutput
    [torch_export_patches] sympy.__version__='1.14.0'
    [torch_export_patches] patch sympy
    [torch_export_patches] torch.__version__='2.10.0.dev20251106+cu130'
    [torch_export_patches] stop_if_static=0
    [torch_export_patches] patch pytorch
    [torch_export_patches] modifies shape constraints
    [torch_export_patches] transformers.__version__='5.0.0.dev0'
    [torch_export_patches] patches transformers.masking_utils.eager_mask
    [torch_export_patches] patches transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
    [torch_export_patches] patches transformers.integrations.sdpa_attention.sdpa_attention_forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter: 
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
    [patch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
    [patch_module_or_classes] function: transformers.models.bart.modeling_bart.eager_attention_forward
    [patch_module_or_classes] function: transformers.models.marian.modeling_marian.eager_attention_forward
    [torch_export_patches] done patching
    It worked:
    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, p_model_embed_tokens_weight: "f32[32000, 192]", p_model_layers_0_self_attn_q_proj_weight: "f32[192, 192]", p_model_layers_0_self_attn_k_proj_weight: "f32[96, 192]", p_model_layers_0_self_attn_v_proj_weight: "f32[96, 192]", p_model_layers_0_self_attn_o_proj_weight: "f32[192, 192]", p_model_layers_0_mlp_gate_proj_weight: "f32[1024, 192]", p_model_layers_0_mlp_up_proj_weight: "f32[1024, 192]", p_model_layers_0_mlp_down_proj_weight: "f32[192, 1024]", p_model_layers_0_input_layernorm_weight: "f32[192]", p_model_layers_0_post_attention_layernorm_weight: "f32[192]", p_model_norm_weight: "f32[192]", p_lm_head_weight: "f32[32000, 192]", b_model_rotary_emb_inv_freq: "f32[48]", input_ids: "i64[s44, s70]", attention_mask: "i64[s43, s53]", position_ids: "i64[s44, s70]", past_key_values_key_0: "f32[s44, 1, s45, 96]", past_key_values_value_0: "f32[s44, 1, s21, 96]"):
                # No stacktrace found for following nodes
                sym_size_int_16: "Sym(s70)" = torch.ops.aten.sym_size.int(input_ids, 1)
                sym_size_int_19: "Sym(s44)" = torch.ops.aten.sym_size.int(position_ids, 0)
                sym_size_int_22: "Sym(s45)" = torch.ops.aten.sym_size.int(past_key_values_key_0, 2)
                sym_size_int_24: "Sym(s21)" = torch.ops.aten.sym_size.int(past_key_values_value_0, 2)
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
                embedding: "f32[s44, s70, 192]" = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
                add: "Sym(s45 + s70)" = sym_size_int_22 + sym_size_int_16
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
                arange: "i64[s70]" = torch.ops.aten.arange.start(sym_size_int_22, add, device = device(type='cpu'), pin_memory = False);  sym_size_int_22 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
                _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(attention_mask, dtype = torch.int64, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default = None
                to: "b8[s43, s53]" = torch.ops.aten.to.device(attention_mask, device(type='cpu'), torch.bool);  attention_mask = None
                arange_1: "i64[s44]" = torch.ops.aten.arange.default(sym_size_int_19, device = device(type='cpu'), pin_memory = False)
                arange_2: "i64[1]" = torch.ops.aten.arange.default(1, device = device(type='cpu'), pin_memory = False)
                arange_3: "i64[s45 + s70]" = torch.ops.aten.arange.default(add, device = device(type='cpu'), pin_memory = False)
                add_3: "i64[s45 + s70]" = torch.ops.aten.add.Tensor(arange_3, 0);  arange_3 = None
                slice_1: "i64[s44]" = torch.ops.aten.slice.Tensor(arange_1, 0, 0, 9223372036854775807);  arange_1 = None
                unsqueeze: "i64[s44, 1]" = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
                unsqueeze_1: "i64[s44, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None
                unsqueeze_2: "i64[s44, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None
                unsqueeze_3: "i64[1, 1]" = torch.ops.aten.unsqueeze.default(arange_2, 0);  arange_2 = None
                unsqueeze_4: "i64[1, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
                unsqueeze_5: "i64[1, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = unsqueeze_5 = None
                unsqueeze_6: "i64[1, s70]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
                unsqueeze_7: "i64[1, 1, s70]" = torch.ops.aten.unsqueeze.default(unsqueeze_6, 1);  unsqueeze_6 = None
                slice_2: "i64[1, 1, s70]" = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
                unsqueeze_8: "i64[1, 1, s70, 1]" = torch.ops.aten.unsqueeze.default(slice_2, 3);  slice_2 = None
                unsqueeze_9: "i64[1, s45 + s70]" = torch.ops.aten.unsqueeze.default(add_3, 0);  add_3 = None
                unsqueeze_10: "i64[1, 1, s45 + s70]" = torch.ops.aten.unsqueeze.default(unsqueeze_9, 1);  unsqueeze_9 = None
                unsqueeze_11: "i64[1, 1, 1, s45 + s70]" = torch.ops.aten.unsqueeze.default(unsqueeze_10, 2);  unsqueeze_10 = None
                slice_3: "i64[1, 1, 1, s45 + s70]" = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
                new_ones: "b8[]" = torch.ops.aten.new_ones.default(unsqueeze_8, [], dtype = torch.bool, pin_memory = False)
                le_3: "b8[1, 1, s70, s45 + s70]" = torch.ops.aten.le.Tensor(slice_3, unsqueeze_8);  unsqueeze_8 = None
                _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(le_3, dtype = torch.bool, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
                to_1: "b8[1, 1, s70, s45 + s70]" = torch.ops.aten.to.dtype_layout(le_3, dtype = torch.bool, layout = torch.strided, device = device(type='cpu'));  le_3 = None
                and_1: "b8[1, 1, s70, s45 + s70]" = torch.ops.aten.__and__.Tensor(new_ones, to_1);  new_ones = to_1 = None
                index: "b8[s44, 1, 1, s45 + s70]" = torch.ops.aten.index.Tensor(to, [unsqueeze_2, slice_3]);  to = unsqueeze_2 = slice_3 = None
                _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(index, dtype = torch.bool, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
                to_2: "b8[s44, 1, 1, s45 + s70]" = torch.ops.aten.to.dtype_layout(index, dtype = torch.bool, layout = torch.strided, device = device(type='cpu'));  index = None
                and_2: "b8[s44, 1, s70, s45 + s70]" = torch.ops.aten.__and__.Tensor(and_1, to_2);  and_1 = to_2 = None
                expand: "b8[s44, 1, s70, s45 + s70]" = torch.ops.aten.expand.default(and_2, [sym_size_int_19, -1, sym_size_int_16, add]);  and_2 = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1549 in forward, code: inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
                unsqueeze_12: "f32[1, 48]" = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
                unsqueeze_13: "f32[1, 48, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_12, 2);  unsqueeze_12 = None
                _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_13, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
                to_3: "f32[1, 48, 1]" = torch.ops.aten.to.dtype(unsqueeze_13, torch.float32);  unsqueeze_13 = None
                expand_1: "f32[s44, 48, 1]" = torch.ops.aten.expand.default(to_3, [sym_size_int_19, -1, 1]);  to_3 = None
                _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
                to_4: "f32[s44, 48, 1]" = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  expand_1 = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1551 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
                slice_4: "i64[s44, s70]" = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
                unsqueeze_14: "i64[s44, 1, s70]" = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
                slice_5: "i64[s44, 1, s70]" = torch.ops.aten.slice.Tensor(unsqueeze_14, 2, 0, 9223372036854775807);  unsqueeze_14 = None
                _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(slice_5, dtype = torch.int64, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
                to_5: "f32[s44, 1, s70]" = torch.ops.aten.to.dtype(slice_5, torch.float32);  slice_5 = None
            
                # No stacktrace found for following nodes
                submod_3 = self.submod_1
                wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast('cpu', torch.bfloat16, False, False, submod_3, to_4, to_5);  submod_3 = to_4 = to_5 = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
                mul: "f32[s44, s70, 96]" = wrap_with_autocast[0]
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
                mul_1: "f32[s44, s70, 96]" = wrap_with_autocast[1];  wrap_with_autocast = None
            
                # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1564 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
                _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
                to_8: "f32[s44, s70, 96]" = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
                _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
                to_9: "f32[s44, s70, 96]" = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
                _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
                to_10: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
                pow_1: "f32[s44, s70, 192]" = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
                mean: "f32[s44, s70, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
                add_4: "f32[s44, s70, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
                rsqrt: "f32[s44, s70, 1]" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
                mul_2: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(to_10, rsqrt);  rsqrt = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
                _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
                to_11: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
                mul_3: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_11);  p_model_layers_0_input_layernorm_weight = to_11 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear: "f32[s44, s70, 192]" = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                view: "f32[s44, s70, 2, 96]" = torch.ops.aten.view.default(linear, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear = None
                transpose_1: "f32[s44, 2, s70, 96]" = torch.ops.aten.transpose.int(view, 1, 2);  view = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_1: "f32[s44, s70, 96]" = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:265 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                view_1: "f32[s44, s70, 1, 96]" = torch.ops.aten.view.default(linear_1, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_1 = None
                transpose_2: "f32[s44, 1, s70, 96]" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_2: "f32[s44, s70, 96]" = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:266 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
                view_2: "f32[s44, s70, 1, 96]" = torch.ops.aten.view.default(linear_2, [sym_size_int_19, sym_size_int_16, -1, 96]);  linear_2 = None
                transpose_3: "f32[s44, 1, s70, 96]" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:269 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
                unsqueeze_15: "f32[s44, 1, s70, 96]" = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
                unsqueeze_16: "f32[s44, 1, s70, 96]" = torch.ops.aten.unsqueeze.default(to_9, 1);  to_9 = None
                mul_4: "f32[s44, 2, s70, 96]" = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_15)
                slice_6: "f32[s44, 2, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 48)
                slice_7: "f32[s44, 2, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_1, 3, 48, 9223372036854775807);  transpose_1 = None
                neg: "f32[s44, 2, s70, 48]" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
                cat_1: "f32[s44, 2, s70, 96]" = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
                mul_5: "f32[s44, 2, s70, 96]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_16);  cat_1 = None
                add_5: "f32[s44, 2, s70, 96]" = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
                mul_6: "f32[s44, 1, s70, 96]" = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_15);  unsqueeze_15 = None
                slice_8: "f32[s44, 1, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 48)
                slice_9: "f32[s44, 1, s70, 48]" = torch.ops.aten.slice.Tensor(transpose_2, 3, 48, 9223372036854775807);  transpose_2 = None
                neg_1: "f32[s44, 1, s70, 48]" = torch.ops.aten.neg.default(slice_9);  slice_9 = None
                cat_2: "f32[s44, 1, s70, 96]" = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
                mul_7: "f32[s44, 1, s70, 96]" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_16);  cat_2 = unsqueeze_16 = None
                add_6: "f32[s44, 1, s70, 96]" = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:274 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
                cat_3: "f32[s44, 1, s45 + s70, 96]" = torch.ops.aten.cat.default([past_key_values_key_0, add_6], -2);  past_key_values_key_0 = add_6 = None
                cat_4: "f32[s44, 1, s21 + s70, 96]" = torch.ops.aten.cat.default([past_key_values_value_0, transpose_3], -2);  past_key_values_value_0 = transpose_3 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:280 in forward, code: attn_output, attn_weights = attention_interface(
                slice_10: "f32[s44, 1, s45 + s70, 96]" = torch.ops.aten.slice.Tensor(cat_3, 0, 0, 9223372036854775807)
                unsqueeze_17: "f32[s44, 1, 1, s45 + s70, 96]" = torch.ops.aten.unsqueeze.default(slice_10, 2);  slice_10 = None
                slice_11: "f32[s44, 1, 1, s45 + s70, 96]" = torch.ops.aten.slice.Tensor(unsqueeze_17, 3, 0, 9223372036854775807);  unsqueeze_17 = None
                expand_2: "f32[s44, 1, 2, s45 + s70, 96]" = torch.ops.aten.expand.default(slice_11, [sym_size_int_19, 1, 2, add, 96]);  slice_11 = None
                reshape: "f32[s44, 2, s45 + s70, 96]" = torch.ops.aten.reshape.default(expand_2, [sym_size_int_19, 2, add, 96]);  expand_2 = None
                slice_12: "f32[s44, 1, s21 + s70, 96]" = torch.ops.aten.slice.Tensor(cat_4, 0, 0, 9223372036854775807)
                unsqueeze_18: "f32[s44, 1, 1, s21 + s70, 96]" = torch.ops.aten.unsqueeze.default(slice_12, 2);  slice_12 = None
                add_11: "Sym(s21 + s70)" = sym_size_int_24 + sym_size_int_16;  sym_size_int_24 = None
                slice_13: "f32[s44, 1, 1, s21 + s70, 96]" = torch.ops.aten.slice.Tensor(unsqueeze_18, 3, 0, 9223372036854775807);  unsqueeze_18 = None
                expand_3: "f32[s44, 1, 2, s21 + s70, 96]" = torch.ops.aten.expand.default(slice_13, [sym_size_int_19, 1, 2, add_11, 96]);  slice_13 = None
                reshape_1: "f32[s44, 2, s21 + s70, 96]" = torch.ops.aten.reshape.default(expand_3, [sym_size_int_19, 2, add_11, 96]);  expand_3 = add_11 = None
                slice_14: "b8[s44, 1, s70, s45 + s70]" = torch.ops.aten.slice.Tensor(expand, 3, None, add);  expand = add = None
                scaled_dot_product_attention: "f32[s44, 2, s70, 96]" = torch.ops.aten.scaled_dot_product_attention.default(add_5, reshape, reshape_1, slice_14, scale = 0.10206207261596575);  add_5 = reshape = reshape_1 = slice_14 = None
                transpose_4: "f32[s44, s70, 2, 96]" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:291 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
                reshape_2: "f32[s44, s70, 192]" = torch.ops.aten.reshape.default(transpose_4, [sym_size_int_19, sym_size_int_16, -1]);  transpose_4 = sym_size_int_19 = sym_size_int_16 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_3: "f32[s44, s70, 192]" = torch.ops.aten.linear.default(reshape_2, p_model_layers_0_self_attn_o_proj_weight);  reshape_2 = p_model_layers_0_self_attn_o_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
                add_7: "f32[s44, s70, 192]" = torch.ops.aten.add.Tensor(to_10, linear_3);  to_10 = linear_3 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
                _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
                to_12: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
                pow_2: "f32[s44, s70, 192]" = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
                mean_1: "f32[s44, s70, 1]" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
                add_8: "f32[s44, s70, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
                rsqrt_1: "f32[s44, s70, 1]" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
                mul_16: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(to_12, rsqrt_1);  rsqrt_1 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
                _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_16, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
                to_13: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(mul_16, torch.float32);  mul_16 = None
                mul_17: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_13);  p_model_layers_0_post_attention_layernorm_weight = to_13 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_4: "f32[s44, s70, 1024]" = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/activations.py:103 in forward, code: return nn.functional.silu(input)
                silu: "f32[s44, s70, 1024]" = torch.ops.aten.silu.default(linear_4);  linear_4 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_5: "f32[s44, s70, 1024]" = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_up_proj_weight);  mul_17 = p_model_layers_0_mlp_up_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:184 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                mul_18: "f32[s44, s70, 1024]" = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_6: "f32[s44, s70, 192]" = torch.ops.aten.linear.default(mul_18, p_model_layers_0_mlp_down_proj_weight);  mul_18 = p_model_layers_0_mlp_down_proj_weight = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
                add_9: "f32[s44, s70, 192]" = torch.ops.aten.add.Tensor(to_12, linear_6);  to_12 = linear_6 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
                _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_9, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
                to_14: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(add_9, torch.float32);  add_9 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
                pow_3: "f32[s44, s70, 192]" = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
                mean_2: "f32[s44, s70, 1]" = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
                add_10: "f32[s44, s70, 1]" = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
                rsqrt_2: "f32[s44, s70, 1]" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
                mul_19: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(to_14, rsqrt_2);  to_14 = rsqrt_2 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
                _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_19, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
                to_15: "f32[s44, s70, 192]" = torch.ops.aten.to.dtype(mul_19, torch.float32);  mul_19 = None
                mul_20: "f32[s44, s70, 192]" = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_15);  p_model_norm_weight = to_15 = None
            
                # File: /home/xadupre/github/transformers/src/transformers/models/llama/modeling_llama.py:500 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
                slice_15: "f32[s44, s70, 192]" = torch.ops.aten.slice.Tensor(mul_20, 0, 0, 9223372036854775807);  mul_20 = None
                slice_16: "f32[s44, s70, 192]" = torch.ops.aten.slice.Tensor(slice_15, 1, 0, 9223372036854775807);  slice_15 = None
            
                # File: /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
                linear_7: "f32[s44, s70, 32000]" = torch.ops.aten.linear.default(slice_16, p_lm_head_weight);  slice_16 = p_lm_head_weight = None
                return (linear_7, cat_3, cat_4)
            
            class submod_1(torch.nn.Module):
                def forward(self, to_4: "f32[s44, 48, 1]", to_5: "f32[s44, 1, s70]"):
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1559 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                    _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                    to_6: "f32[s44, 48, 1]" = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                    _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(to_5, dtype = torch.float32, device = device(type='cpu'), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                    to_7: "f32[s44, 1, s70]" = torch.ops.aten.to.dtype(to_5, torch.float32);  to_5 = None
                    matmul: "f32[s44, 48, s70]" = torch.ops.aten.matmul.default(to_6, to_7);  to_6 = to_7 = None
                    transpose: "f32[s44, s70, 48]" = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None
                
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1560 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                    cat: "f32[s44, s70, 96]" = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None
                
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1561 in forward, code: cos = emb.cos() * attention_scaling
                    cos: "f32[s44, s70, 96]" = torch.ops.aten.cos.default(cat)
                    mul: "f32[s44, s70, 96]" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
                
                    # File: /home/xadupre/github/onnx-diagnostic/onnx_diagnostic/torch_export_patches/patches/patch_transformers.py:1562 in forward, code: sin = emb.sin() * attention_scaling
                    sin: "f32[s44, s70, 96]" = torch.ops.aten.sin.default(cat);  cat = None
                    mul_1: "f32[s44, s70, 96]" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                    return (mul, mul_1)
                
    Graph signature: 
        # inputs
        p_model_embed_tokens_weight: PARAMETER target='model.embed_tokens.weight'
        p_model_layers_0_self_attn_q_proj_weight: PARAMETER target='model.layers.0.self_attn.q_proj.weight'
        p_model_layers_0_self_attn_k_proj_weight: PARAMETER target='model.layers.0.self_attn.k_proj.weight'
        p_model_layers_0_self_attn_v_proj_weight: PARAMETER target='model.layers.0.self_attn.v_proj.weight'
        p_model_layers_0_self_attn_o_proj_weight: PARAMETER target='model.layers.0.self_attn.o_proj.weight'
        p_model_layers_0_mlp_gate_proj_weight: PARAMETER target='model.layers.0.mlp.gate_proj.weight'
        p_model_layers_0_mlp_up_proj_weight: PARAMETER target='model.layers.0.mlp.up_proj.weight'
        p_model_layers_0_mlp_down_proj_weight: PARAMETER target='model.layers.0.mlp.down_proj.weight'
        p_model_layers_0_input_layernorm_weight: PARAMETER target='model.layers.0.input_layernorm.weight'
        p_model_layers_0_post_attention_layernorm_weight: PARAMETER target='model.layers.0.post_attention_layernorm.weight'
        p_model_norm_weight: PARAMETER target='model.norm.weight'
        p_lm_head_weight: PARAMETER target='lm_head.weight'
        b_model_rotary_emb_inv_freq: BUFFER target='model.rotary_emb.inv_freq' persistent=False
        input_ids: USER_INPUT
        attention_mask: USER_INPUT
        position_ids: USER_INPUT
        past_key_values_key_0: USER_INPUT
        past_key_values_value_0: USER_INPUT
    
        # outputs
        linear_7: USER_OUTPUT
        cat_3: USER_OUTPUT
        cat_4: USER_OUTPUT
    
    Range constraints: {s44: VR[2, int_oo], s70: VR[2, int_oo], s43: VR[2, int_oo], s53: VR[4, int_oo], s45: VR[2, int_oo], s21: VR[2, int_oo]}

    [torch_export_patches] remove patches
    [torch_export_patches] restored sympy functions
    [torch_export_patches] restored pytorch functions
    [torch_export_patches] restored shape constraints
    [torch_export_patches] unpatches transformers
    [torch_export_patches] restored transformers.masking_utils.eager_mask
    [torch_export_patches] restored transformers.masking_utils.eager_mask in ALL_MASK_ATTENTION_FUNCTIONS
    [torch_export_patches] restored transformers.integrations.sdpa_attention.sdpa_attention_forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_AttentionMaskConverter: 
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_DynamicLayer: lazy_initialization
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma2RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3Model: get_placeholder_mask
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Gemma3RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GemmaRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_GenerationMixin: _cache_dependant_input_preparation, _cache_dependant_input_preparation_exporting
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsAttention: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_IdeficsEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_LlamaRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MistralRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_MixtralRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi3RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Phi4MultimodalRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_PhiRotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLForConditionalGeneration: prepare_inputs_for_generation
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VLVisionAttention: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen2_5_VisionTransformerPretrainedModel: get_window_index, forward, rot_pos_emb
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_Qwen3MoeSparseMoeBlock: forward, _forward_expert_loop
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SamMaskDecoder: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_SmolLM3RotaryEmbedding: forward
    [unpatch_module_or_classes] onnx_diagnostic.torch_export_patches.patches.patch_transformers.patched_VisionAttention: forward
    [unpatch_module_or_classes] function transformers.models.bart.modeling_bart.eager_attention_forward
    [unpatch_module_or_classes] function transformers.models.marian.modeling_marian.eager_attention_forward




.. GENERATED FROM PYTHON SOURCE LINES 142-143

.. code-block:: Python

    doc.plot_legend("Tiny-LLM patched", "torch.export.export", "green")



.. image-sg:: /auto_examples/images/sphx_glr_plot_export_tiny_llm_patched_001.png
   :alt: plot export tiny llm patched
   :srcset: /auto_examples/images/sphx_glr_plot_export_tiny_llm_patched_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 5.888 seconds)


.. _sphx_glr_download_auto_examples_plot_export_tiny_llm_patched.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_export_tiny_llm_patched.ipynb <plot_export_tiny_llm_patched.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_export_tiny_llm_patched.py <plot_export_tiny_llm_patched.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_export_tiny_llm_patched.zip <plot_export_tiny_llm_patched.zip>`


.. include:: plot_export_tiny_llm_patched.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
