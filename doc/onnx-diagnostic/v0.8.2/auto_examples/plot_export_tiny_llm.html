<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="Test the export on untrained models" href="plot_export_hub_codellama.html"><link rel="prev" title="Intermediate results with onnxruntime" href="plot_failing_onnxruntime_evaluator.html">
        <link rel="prefetch" href="../_static/logo.png" as="image">

    <!-- Generated with Sphinx 8.1.3 and Furo 2025.09.25 -->
        <title>Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM) - onnx-diagnostic 0.8.2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a7360d90" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">onnx-diagnostic 0.8.2 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">onnx-diagnostic 0.8.2 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../patches.html">Patches Explained</a><input aria-label="Toggle navigation of Patches Explained" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../status/index.html">Exporter Status</a><input aria-label="Toggle navigation of Exporter Status" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../status/exported_program_dynamic.html">Exported Programs with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/exporter_dynamic.html">Exported ONNX with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/patches_coverage.html">Coverage of the Patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../status/patches_diff.html">Patches Diff</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API of onnx_diagnostic</a><input aria-label="Toggle navigation of API of onnx_diagnostic" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/export/index.html">onnx_diagnostic.export</a><input aria-label="Toggle navigation of onnx_diagnostic.export" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/export/api.html">onnx_diagnostic.export.api</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/control_flow.html">onnx_diagnostic.export.control_flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/dynamic_shapes.html">onnx_diagnostic.export.dynamic_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/shape_helper.html">onnx_diagnostic.export.shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/validate.html">onnx_diagnostic.export.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/helpers/index.html">onnx_diagnostic.helpers</a><input aria-label="Toggle navigation of onnx_diagnostic.helpers" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/args_helper.html">onnx_diagnostic.helpers.args_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/bench_run.html">onnx_diagnostic.helpers.bench_run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/cache_helper.html">onnx_diagnostic.helpers.cache_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/config_helper.html">onnx_diagnostic.helpers.config_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/doc_helper.html">onnx_diagnostic.helpers.doc_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/fake_tensor_helper.html">onnx_diagnostic.helpers.fake_tensor_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/graph_helper.html">onnx_diagnostic.helpers.graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/helper.html">onnx_diagnostic.helpers.helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/_log_helper.html">onnx_diagnostic.helpers._log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/log_helper.html">onnx_diagnostic.helpers.log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/memory_peak.html">onnx_diagnostic.helpers.memory_peak</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/mini_onnx_builder.html">onnx_diagnostic.helpers.mini_onnx_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/model_builder_helper.html">onnx_diagnostic.helpers.model_builder_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/onnx_helper.html">onnx_diagnostic.helpers.onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/ort_session.html">onnx_diagnostic.helpers.ort_session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/rt_helper.html">onnx_diagnostic.helpers.rt_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_helper.html">onnx_diagnostic.helpers.torch_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">onnx_diagnostic.reference</a><input aria-label="Toggle navigation of onnx_diagnostic.reference" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">onnx_diagnostic.reference.ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.ops" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">onnx_diagnostic.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">onnx_diagnostic.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">onnx_diagnostic.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">onnx_diagnostic.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">onnx_diagnostic.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">onnx_diagnostic.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">onnx_diagnostic.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">onnx_diagnostic.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">onnx_diagnostic.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">onnx_diagnostic.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">onnx_diagnostic.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">onnx_diagnostic.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">onnx_diagnostic.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">onnx_diagnostic.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">onnx_diagnostic.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">onnx_diagnostic.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">onnx_diagnostic.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">onnx_diagnostic.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">onnx_diagnostic.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">onnx_diagnostic.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">onnx_diagnostic.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">onnx_diagnostic.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">onnx_diagnostic.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/torch_ops/index.html">onnx_diagnostic.reference.torch_ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.torch_ops" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/access_ops.html">onnx_diagnostic.reference.torch_ops.access_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/binary_ops.html">onnx_diagnostic.reference.torch_ops.binary_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/controlflow_ops.html">onnx_diagnostic.reference.torch_ops.controlflow_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/generator_ops.html">onnx_diagnostic.reference.torch_ops.generator_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/nn_ops.html">onnx_diagnostic.reference.torch_ops.nn_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/other_ops.html">onnx_diagnostic.reference.torch_ops.other_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/reduce_ops.html">onnx_diagnostic.reference.torch_ops.reduce_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/sequence_ops.html">onnx_diagnostic.reference.torch_ops.sequence_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/shape_ops.html">onnx_diagnostic.reference.torch_ops.shape_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/unary_ops.html">onnx_diagnostic.reference.torch_ops.unary_ops</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">onnx_diagnostic.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">onnx_diagnostic.reference.quantized_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">onnx_diagnostic.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/report_results_comparison.html">onnx_diagnostic.reference.report_results_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/torch_evaluator.html">onnx_diagnostic.reference.torch_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/tasks/index.html">onnx_diagnostic.tasks</a><input aria-label="Toggle navigation of onnx_diagnostic.tasks" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/automatic_speech_recognition.html">onnx_diagnostic.tasks.automatic_speech_recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/fill_mask.html">onnx_diagnostic.tasks.fill_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/feature_extraction.html">onnx_diagnostic.tasks.feature_extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_classification.html">onnx_diagnostic.tasks.image_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_text_to_text.html">onnx_diagnostic.export.image_text_to_text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/mixture_of_expert.html">onnx_diagnostic.tasks.mixture_of_expert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/object_detection.html">onnx_diagnostic.tasks.object_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/sentence_similarity.html">onnx_diagnostic.tasks.sentence_similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/summarization.html">onnx_diagnostic.tasks.summarization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_classification.html">onnx_diagnostic.tasks.text_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_generation.html">onnx_diagnostic.tasks.text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_to_image.html">onnx_diagnostic.tasks.text_to_image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text2text_generation.html">onnx_diagnostic.tasks.text2text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/zero_shot_image_classification.html">onnx_diagnostic.tasks.zero_shot_image_classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_export_patches/index.html">onnx_diagnostic.torch_export_patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/eval/index.html">onnx_diagnostic.torch_export_patches.eval</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.eval" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/eval/model_cases.html">onnx_diagnostic.torch_export_patches.eval.model_cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_errors.html">onnx_diagnostic.torch_export_patches.onnx_export_errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_serialization.html">onnx_diagnostic.torch_export_patches.onnx_export_serialization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/patches/index.html">onnx_diagnostic.torch_export_patches.patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.patches" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_torch.html">onnx_diagnostic.torch_export_patches.patches.patch_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_transformers.html">onnx_diagnostic.torch_export_patches.patches.patch_transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_details.html">onnx_diagnostic.torch_export_patches.patch_details</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_expressions.html">onnx_diagnostic.torch_export_patches.patch_expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_inputs.html">onnx_diagnostic.torch_export_patches.patch_inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module.html">onnx_diagnostic.torch_export_patches.patch_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module_helper.html">onnx_diagnostic.torch_export_patches.patch_module_helper</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/serialization/index.html">onnx_diagnostic.torch_export_patches.serialization</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.serialization" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/diffusers_impl.html">onnx_diagnostic.torch_export_patches.serialization.diffusers_impl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/transformers_impl.html">onnx_diagnostic.torch_export_patches.serialization.transformers_impl</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">onnx_diagnostic.torch_models</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/code_sample.html">onnx_diagnostic.torch_models.code_sample</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_models/hghub/index.html">onnx_diagnostic.torch_models.hghub</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models.hghub" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_api.html">onnx_diagnostic.torch_models.hghub.hub_api</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_data.html">onnx_diagnostic.torch_models.hghub.hub_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/model_inputs.html">onnx_diagnostic.torch_models.hghub.model_inputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llms.html">onnx_diagnostic.torch_models.llms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/validate.html">onnx_diagnostic.torch_models.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_onnx/index.html">onnx_diagnostic.torch_onnx</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_onnx" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/runtime_info.html">onnx_diagnostic.torch_onnx.runtime_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs.html">onnx_diagnostic.torch_onnx.sbs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/api.html">onnx_diagnostic.api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">onnx_diagnostic.ext_test_case</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cmds/index.html">Command Lines</a><input aria-label="Toggle navigation of Command Lines" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cmds/config.html">-m onnx_diagnostic config … prints the config for a model id</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/validate.html">-m onnx_diagnostic validate … validate a model id</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Examples Gallery</a><input aria-label="Toggle navigation of Examples Gallery" checked="" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_dump_intermediate_results.html">Dumps intermediate results of a torch model</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_args_kwargs.html">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_patched.html">Export Tiny-LLM with patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_phi2.html">Export microsoft/phi-2</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_with_dynamic_cache.html">Export with DynamicCache and guessed dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_tiny_llm_dim01_onnx_custom.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX (custom)</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_locate_issue.html">Find and fix an export issue due to dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_model_extract.html">Find where a model is failing by running submodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_reference_evaluator.html">Intermediate results with (ONNX) ReferenceEvaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_failing_onnxruntime_evaluator.html">Intermediate results with onnxruntime</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_export_hub_codellama.html">Test the export on untrained models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_recipes/index.html">Common Export Issues</a><input aria-label="Toggle navigation of Common Export Issues" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_dim1.html">0, 1, 2 for a Dynamic Dimension in the dummy example to export a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_what.html">Builds dynamic shapes from any input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_max.html">Cannot export <code class="docutils literal notranslate"><span class="pre">torch.sym_max(x.shape[0],</span> <span class="pre">y.shape[0])</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_python_int.html">Do not use python int with dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_cond.html">Export a model with a control flow (If)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_nonzero.html">Half certain nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_json.html">JSON returns list when the original dynamic shapes are list or tuple</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_with_dynamic.html">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_technical/index.html">Technical Details</a><input aria-label="Toggle navigation of Technical Details" class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_broadcast_export_issue.html">Dynamic Shapes and Broadcasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_generate.html">From a LLM to processing a prompt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_layer_norm_discrepancies.html">LayerNormalization implementation cannot be exchanged</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_parallelized_reduction.html">Reproducible Parallelized Reduction is difficult</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_examples/plot_export_tiny_llm.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-export-tiny-llm-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="steel-method-forward-to-guess-inputs-and-dynamic-shapes-with-tiny-llm">
<span id="l-plot-tiny-llm-export"></span><span id="sphx-glr-auto-examples-plot-export-tiny-llm-py"></span><h1>Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)<a class="headerlink" href="#steel-method-forward-to-guess-inputs-and-dynamic-shapes-with-tiny-llm" title="Link to this heading">¶</a></h1>
<p>Inputs are always dynamic with LLMs that is why dynamic shapes
needs to be specified when a LLM is exported with <a class="reference external" href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a>.
Most of the examples on <a class="reference external" href="https://huggingface.co/docs/hub/en/index">HuggingFace</a> use method
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.GenerationMixin.generate()</span></code> but we only want to
export the model and its method <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
<p>That example shows to guess the inputs of this method even though the model
is executed through meth <code class="docutils literal notranslate"><span class="pre">generate</span></code>.</p>
<p>We focus on the model <a class="reference external" href="https://huggingface.co/arnir0/Tiny-LLM">arnir0/Tiny-LLM</a>.
To avoid downloading any weights, we write a function creating a
random model based on the same architecture.</p>
<section id="steel-the-forward-method">
<h2>Steel the forward method<a class="headerlink" href="#steel-the-forward-method" title="Link to this heading">¶</a></h2>
<p>The first step is to guess the dummy inputs.
Let’s use the true model for that.
We use the dummy example from the model page.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pprint</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic</span><span class="w"> </span><span class="kn">import</span> <span class="n">doc</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.helpers</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.helpers.torch_helper</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><span class="n">steal_forward</span></a></a></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_models.llms</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><span class="n">get_tiny_llm</span></a></a></a></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches.patch_inputs</span><span class="w"> </span><span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a></a>


<a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">MODEL_NAME</span></a></a></a></a> <span class="o">=</span> <span class="s2">&quot;arnir0/Tiny-LLM&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">MODEL_NAME</span></a></a></a></a><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">MODEL_NAME</span></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<p>We rewrite the forward method to print the cache dimension.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_forward_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">_f</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">_f</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><span class="n">torch</span><span class="o">.</span><span class="n">compiler</span></a></a></a></a><span class="p">,</span> <span class="s2">&quot;is_exporting&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_exporting</span></a></a></a></a><span class="p">():</span>
        <span class="c1"># torch.compiler.is_exporting requires torch&gt;=2.7</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt;-&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_min_max</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><a href="https://docs.pytorch.org/docs/stable/torch.compiler_api.html#module-torch.compiler" title="torch.compiler" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-module"><span class="n">torch</span><span class="o">.</span><span class="n">compiler</span></a></a></a></a><span class="p">,</span> <span class="s2">&quot;is_exporting&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting" title="torch.compiler.is_exporting" class="sphx-glr-backref-module-torch-compiler sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">is_exporting</span></a></a></a></a><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_min_max</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="n">keep_model_forward</span> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">forward</span></a></a></a></a>
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">forward</span></a></a></a></a> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">_f</span><span class="o">=</span><span class="n">keep_model_forward</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">_forward_</span><span class="p">(</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">_f</span><span class="o">=</span><span class="n">_f</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let’s run the model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">prompt</span></a></a></a></a> <span class="o">=</span> <span class="s2">&quot;Continue: it rains...&quot;</span>
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">prompt</span></a></a></a></a><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outputs</span></a></a></a></a> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">generated_text</span></a></a></a></a> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outputs</span></a></a></a></a><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-- prompt&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">prompt</span></a></a></a></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-- answer&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">generated_text</span></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;- ((),dict(cache_position:T7s8[0,7:A3.5],input_ids:T7s1x8[1,29901:A6305.375],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x8x32000[-15.516718864440918,15.75765609741211:A-3.381915190983544],past_key_values:DynamicCache(key_cache=#1[T1s1x1x8x96[-5.490959167480469,6.226877689361572:A-0.11321351693110653]], value_cache=#1[T1s1x1x8x96[-0.6787744760513306,0.49568021297454834:A0.007227749521139988]]))
&lt;- ((),dict(cache_position:T7s1[8,8:A8.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x8x96[-5.490959167480469,6.226877689361572:A-0.11321351693110653]], value_cache=#1[T1s1x1x8x96[-0.6787744760513306,0.49568021297454834:A0.007227749521139988]]),input_ids:T7s1x1[2866,2866:A2866.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-12.076018333435059,16.944217681884766:A-2.592398094831733],past_key_values:DynamicCache(key_cache=#1[T1s1x1x9x96[-5.490959167480469,6.226877689361572:A-0.1302779765439347]], value_cache=#1[T1s1x1x9x96[-0.6787744760513306,0.49568021297454834:A0.007744434695858352]]))
&lt;- ((),dict(cache_position:T7s1[9,9:A9.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x9x96[-5.490959167480469,6.226877689361572:A-0.1302779765439347]], value_cache=#1[T1s1x1x9x96[-0.6787744760513306,0.49568021297454834:A0.007744434695858352]]),input_ids:T7s1x1[14150,14150:A14150.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-18.20236587524414,6.324185371398926:A-8.229752516841982],past_key_values:DynamicCache(key_cache=#1[T1s1x1x10x96[-5.490959167480469,6.226877689361572:A-0.1353976684111937]], value_cache=#1[T1s1x1x10x96[-0.6787744760513306,0.49568021297454834:A0.008736979494627425]]))
&lt;- ((),dict(cache_position:T7s1[10,10:A10.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x10x96[-5.490959167480469,6.226877689361572:A-0.1353976684111937]], value_cache=#1[T1s1x1x10x96[-0.6787744760513306,0.49568021297454834:A0.008736979494627425]]),input_ids:T7s1x1[278,278:A278.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-17.76398468017578,3.467536449432373:A-9.690286429880652],past_key_values:DynamicCache(key_cache=#1[T1s1x1x11x96[-5.490959167480469,6.226877689361572:A-0.15163987638218465]], value_cache=#1[T1s1x1x11x96[-0.6787744760513306,0.49568021297454834:A0.009184762458792675]]))
&lt;- ((),dict(cache_position:T7s1[11,11:A11.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x11x96[-5.490959167480469,6.226877689361572:A-0.15163987638218465]], value_cache=#1[T1s1x1x11x96[-0.6787744760513306,0.49568021297454834:A0.009184762458792675]]),input_ids:T7s1x1[2446,2446:A2446.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-17.323726654052734,5.4670844078063965:A-7.865214796903078],past_key_values:DynamicCache(key_cache=#1[T1s1x1x12x96[-5.490959167480469,6.226877689361572:A-0.14727237609086943]], value_cache=#1[T1s1x1x12x96[-0.6787744760513306,0.49568021297454834:A0.009050533552087674]]))
&lt;- ((),dict(cache_position:T7s1[12,12:A12.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x12x96[-5.490959167480469,6.226877689361572:A-0.14727237609086943]], value_cache=#1[T1s1x1x12x96[-0.6787744760513306,0.49568021297454834:A0.009050533552087674]]),input_ids:T7s1x1[29991,29991:A29991.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-15.184151649475098,7.759481430053711:A-9.368468943121377],past_key_values:DynamicCache(key_cache=#1[T1s1x1x13x96[-5.490959167480469,6.226877689361572:A-0.14272359549319774]], value_cache=#1[T1s1x1x13x96[-0.6787744760513306,0.49568021297454834:A0.009175964193731581]]))
&lt;- ((),dict(cache_position:T7s1[13,13:A13.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x13x96[-5.490959167480469,6.226877689361572:A-0.14272359549319774]], value_cache=#1[T1s1x1x13x96[-0.6787744760513306,0.49568021297454834:A0.009175964193731581]]),input_ids:T7s1x1[13,13:A13.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-8.932031631469727,9.351970672607422:A-3.3105489786481486],past_key_values:DynamicCache(key_cache=#1[T1s1x1x14x96[-5.511512279510498,6.282632827758789:A-0.1452894162609612]], value_cache=#1[T1s1x1x14x96[-0.6787744760513306,0.7704185843467712:A0.010539780633421071]]))
&lt;- ((),dict(cache_position:T7s1[14,14:A14.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x14x96[-5.511512279510498,6.282632827758789:A-0.1452894162609612]], value_cache=#1[T1s1x1x14x96[-0.6787744760513306,0.7704185843467712:A0.010539780633421071]]),input_ids:T7s1x1[29940,29940:A29940.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-9.800661087036133,11.163089752197266:A-1.0843636879529803],past_key_values:DynamicCache(key_cache=#1[T1s1x1x15x96[-5.511512279510498,6.282632827758789:A-0.14685925084462118]], value_cache=#1[T1s1x1x15x96[-0.6787744760513306,0.7704185843467712:A0.009878809622579057]]))
&lt;- ((),dict(cache_position:T7s1[15,15:A15.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x15x96[-5.511512279510498,6.282632827758789:A-0.14685925084462118]], value_cache=#1[T1s1x1x15x96[-0.6787744760513306,0.7704185843467712:A0.009878809622579057]]),input_ids:T7s1x1[711,711:A711.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-13.302396774291992,12.005083084106445:A-2.538844227918424],past_key_values:DynamicCache(key_cache=#1[T1s1x1x16x96[-5.511512279510498,6.282632827758789:A-0.14132002103807886]], value_cache=#1[T1s1x1x16x96[-0.6787744760513306,0.7704185843467712:A0.007490537232939687]]))
&lt;- ((),dict(cache_position:T7s1[16,16:A16.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x16x96[-5.511512279510498,6.282632827758789:A-0.14132002103807886]], value_cache=#1[T1s1x1x16x96[-0.6787744760513306,0.7704185843467712:A0.007490537232939687]]),input_ids:T7s1x1[1486,1486:A1486.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-13.352119445800781,4.841743469238281:A-7.302925526224077],past_key_values:DynamicCache(key_cache=#1[T1s1x1x17x96[-5.511512279510498,6.282632827758789:A-0.12528488661786574]], value_cache=#1[T1s1x1x17x96[-0.6787744760513306,0.7704185843467712:A0.006690134637219493]]))
&lt;- ((),dict(cache_position:T7s1[17,17:A17.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x17x96[-5.511512279510498,6.282632827758789:A-0.12528488661786574]], value_cache=#1[T1s1x1x17x96[-0.6787744760513306,0.7704185843467712:A0.006690134637219493]]),input_ids:T7s1x1[29901,29901:A29901.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-16.473407745361328,3.7950451374053955:A-9.055886989319232],past_key_values:DynamicCache(key_cache=#1[T1s1x1x18x96[-5.511512279510498,6.282632827758789:A-0.12704408231224484]], value_cache=#1[T1s1x1x18x96[-0.6787744760513306,0.7704185843467712:A0.007187216785656279]]))
&lt;- ((),dict(cache_position:T7s1[18,18:A18.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x18x96[-5.511512279510498,6.282632827758789:A-0.12704408231224484]], value_cache=#1[T1s1x1x18x96[-0.6787744760513306,0.7704185843467712:A0.007187216785656279]]),input_ids:T7s1x1[6439,6439:A6439.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-15.637372970581055,8.48501205444336:A-6.666262734174262],past_key_values:DynamicCache(key_cache=#1[T1s1x1x19x96[-5.511512279510498,6.282632827758789:A-0.12709062617687505]], value_cache=#1[T1s1x1x19x96[-0.6787744760513306,0.7704185843467712:A0.007587694405126393]]))
&lt;- ((),dict(cache_position:T7s1[19,19:A19.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x19x96[-5.511512279510498,6.282632827758789:A-0.12709062617687505]], value_cache=#1[T1s1x1x19x96[-0.6787744760513306,0.7704185843467712:A0.007587694405126393]]),input_ids:T7s1x1[29892,29892:A29892.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-14.878786087036133,6.441829204559326:A-7.103221681401133],past_key_values:DynamicCache(key_cache=#1[T1s1x1x20x96[-5.511512279510498,6.282632827758789:A-0.12636854723211097]], value_cache=#1[T1s1x1x20x96[-0.6787744760513306,0.7704185843467712:A0.007900931346064984]]))
&lt;- ((),dict(cache_position:T7s1[20,20:A20.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x20x96[-5.511512279510498,6.282632827758789:A-0.12636854723211097]], value_cache=#1[T1s1x1x20x96[-0.6787744760513306,0.7704185843467712:A0.007900931346064984]]),input_ids:T7s1x1[590,590:A590.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-19.573440551757812,4.970290184020996:A-8.706373463791795],past_key_values:DynamicCache(key_cache=#1[T1s1x1x21x96[-6.10756778717041,6.282632827758789:A-0.12324468264491842]], value_cache=#1[T1s1x1x21x96[-0.6787744760513306,0.7704185843467712:A0.007973992656980766]]))
&lt;- ((),dict(cache_position:T7s1[21,21:A21.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x21x96[-6.10756778717041,6.282632827758789:A-0.12324468264491842]], value_cache=#1[T1s1x1x21x96[-0.6787744760513306,0.7704185843467712:A0.007973992656980766]]),input_ids:T7s1x1[7339,7339:A7339.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-14.548786163330078,8.159546852111816:A-6.970149930046405],past_key_values:DynamicCache(key_cache=#1[T1s1x1x22x96[-6.10756778717041,6.282632827758789:A-0.11999419265422065]], value_cache=#1[T1s1x1x22x96[-0.6787744760513306,0.7704185843467712:A0.008538347612880953]]))
&lt;- ((),dict(cache_position:T7s1[22,22:A22.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x22x96[-6.10756778717041,6.282632827758789:A-0.11999419265422065]], value_cache=#1[T1s1x1x22x96[-0.6787744760513306,0.7704185843467712:A0.008538347612880953]]),input_ids:T7s1x1[16846,16846:A16846.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-11.448102951049805,13.524051666259766:A-3.142901279407088],past_key_values:DynamicCache(key_cache=#1[T1s1x1x23x96[-6.10756778717041,6.282632827758789:A-0.12003202550297737]], value_cache=#1[T1s1x1x23x96[-0.6787744760513306,0.7704185843467712:A0.008957836799689412]]))
&lt;- ((),dict(cache_position:T7s1[23,23:A23.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x23x96[-6.10756778717041,6.282632827758789:A-0.12003202550297737]], value_cache=#1[T1s1x1x23x96[-0.6787744760513306,0.7704185843467712:A0.008957836799689412]]),input_ids:T7s1x1[29876,29876:A29876.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-15.23011302947998,5.398580551147461:A-7.507193362160586],past_key_values:DynamicCache(key_cache=#1[T1s1x1x24x96[-6.10756778717041,6.282632827758789:A-0.11118035720407231]], value_cache=#1[T1s1x1x24x96[-0.6787744760513306,0.7704185843467712:A0.00852774141735482]]))
&lt;- ((),dict(cache_position:T7s1[24,24:A24.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x24x96[-6.10756778717041,6.282632827758789:A-0.11118035720407231]], value_cache=#1[T1s1x1x24x96[-0.6787744760513306,0.7704185843467712:A0.00852774141735482]]),input_ids:T7s1x1[29991,29991:A29991.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-14.851299285888672,5.789419651031494:A-9.332220061377622],past_key_values:DynamicCache(key_cache=#1[T1s1x1x25x96[-6.10756778717041,6.282632827758789:A-0.11118051617323847]], value_cache=#1[T1s1x1x25x96[-0.6787744760513306,0.7704185843467712:A0.008613877036398966]]))
&lt;- ((),dict(cache_position:T7s1[25,25:A25.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x25x96[-6.10756778717041,6.282632827758789:A-0.11118051617323847]], value_cache=#1[T1s1x1x25x96[-0.6787744760513306,0.7704185843467712:A0.008613877036398966]]),input_ids:T7s1x1[29871,29871:A29871.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-11.931562423706055,11.070415496826172:A-3.9143916586777197],past_key_values:DynamicCache(key_cache=#1[T1s1x1x26x96[-6.10756778717041,6.282632827758789:A-0.10740538675240831]], value_cache=#1[T1s1x1x26x96[-0.6787744760513306,0.7704185843467712:A0.00708381281466385]]))
&lt;- ((),dict(cache_position:T7s1[26,26:A26.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x26x96[-6.10756778717041,6.282632827758789:A-0.10740538675240831]], value_cache=#1[T1s1x1x26x96[-0.6787744760513306,0.7704185843467712:A0.00708381281466385]]),input_ids:T7s1x1[29906,29906:A29906.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-18.610342025756836,5.410274505615234:A-9.61340464850422],past_key_values:DynamicCache(key_cache=#1[T1s1x1x27x96[-6.10756778717041,6.282632827758789:A-0.10291547219646789]], value_cache=#1[T1s1x1x27x96[-0.6787744760513306,0.7704185843467712:A0.006612986321496139]]))
&lt;- ((),dict(cache_position:T7s1[27,27:A27.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x27x96[-6.10756778717041,6.282632827758789:A-0.10291547219646789]], value_cache=#1[T1s1x1x27x96[-0.6787744760513306,0.7704185843467712:A0.006612986321496139]]),input_ids:T7s1x1[29900,29900:A29900.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-17.976022720336914,7.121726036071777:A-9.940053578583523],past_key_values:DynamicCache(key_cache=#1[T1s1x1x28x96[-7.410092830657959,6.282632827758789:A-0.10118713294819567]], value_cache=#1[T1s1x1x28x96[-0.6787744760513306,0.7704185843467712:A0.005286526548491652]]))
&lt;- ((),dict(cache_position:T7s1[28,28:A28.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x28x96[-7.410092830657959,6.282632827758789:A-0.10118713294819567]], value_cache=#1[T1s1x1x28x96[-0.6787744760513306,0.7704185843467712:A0.005286526548491652]]),input_ids:T7s1x1[29906,29906:A29906.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-15.976371765136719,11.98576545715332:A-8.17162115960475],past_key_values:DynamicCache(key_cache=#1[T1s1x1x29x96[-7.410092830657959,6.282632827758789:A-0.10303693728038242]], value_cache=#1[T1s1x1x29x96[-0.6787744760513306,0.7704185843467712:A0.004910146236444893]]))
&lt;- ((),dict(cache_position:T7s1[29,29:A29.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x29x96[-7.410092830657959,6.282632827758789:A-0.10303693728038242]], value_cache=#1[T1s1x1x29x96[-0.6787744760513306,0.7704185843467712:A0.004910146236444893]]),input_ids:T7s1x1[29900,29900:A29900.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-19.45410919189453,4.403109550476074:A-10.726719597123564],past_key_values:DynamicCache(key_cache=#1[T1s1x1x30x96[-7.410092830657959,6.282632827758789:A-0.10299661416336474]], value_cache=#1[T1s1x1x30x96[-0.6787744760513306,0.7704185843467712:A0.003728878451142413]]))
&lt;- ((),dict(cache_position:T7s1[30,30:A30.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x30x96[-7.410092830657959,6.282632827758789:A-0.10299661416336474]], value_cache=#1[T1s1x1x30x96[-0.6787744760513306,0.7704185843467712:A0.003728878451142413]]),input_ids:T7s1x1[13,13:A13.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-7.6409077644348145,10.016256332397461:A-2.7200095351033378],past_key_values:DynamicCache(key_cache=#1[T1s1x1x31x96[-7.410092830657959,6.282632827758789:A-0.09867025076805817]], value_cache=#1[T1s1x1x31x96[-0.6787744760513306,0.7704185843467712:A0.004520507996246995]]))
&lt;- ((),dict(cache_position:T7s1[31,31:A31.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x31x96[-7.410092830657959,6.282632827758789:A-0.09867025076805817]], value_cache=#1[T1s1x1x31x96[-0.6787744760513306,0.7704185843467712:A0.004520507996246995]]),input_ids:T7s1x1[29924,29924:A29924.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-8.641586303710938,10.785520553588867:A-1.2355172414700502],past_key_values:DynamicCache(key_cache=#1[T1s1x1x32x96[-7.410092830657959,6.282632827758789:A-0.09192560731022088]], value_cache=#1[T1s1x1x32x96[-0.6787744760513306,0.7704185843467712:A0.004210412411296716]]))
&lt;- ((),dict(cache_position:T7s1[32,32:A32.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x32x96[-7.410092830657959,6.282632827758789:A-0.09192560731022088]], value_cache=#1[T1s1x1x32x96[-0.6787744760513306,0.7704185843467712:A0.004210412411296716]]),input_ids:T7s1x1[858,858:A858.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-10.559972763061523,11.023469924926758:A-3.6022008529237937],past_key_values:DynamicCache(key_cache=#1[T1s1x1x33x96[-7.410092830657959,6.282632827758789:A-0.0856626743322219]], value_cache=#1[T1s1x1x33x96[-0.6787744760513306,0.7704185843467712:A0.0039097890680750425]]))
&lt;- ((),dict(cache_position:T7s1[33,33:A33.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x33x96[-7.410092830657959,6.282632827758789:A-0.0856626743322219]], value_cache=#1[T1s1x1x33x96[-0.6787744760513306,0.7704185843467712:A0.0039097890680750425]]),input_ids:T7s1x1[279,279:A279.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-13.961400985717773,5.305764198303223:A-7.576864839469082],past_key_values:DynamicCache(key_cache=#1[T1s1x1x34x96[-7.410092830657959,6.282632827758789:A-0.07616603146611763]], value_cache=#1[T1s1x1x34x96[-0.6787744760513306,0.7704185843467712:A0.00391728923921387]]))
&lt;- ((),dict(cache_position:T7s1[34,34:A34.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x34x96[-7.410092830657959,6.282632827758789:A-0.07616603146611763]], value_cache=#1[T1s1x1x34x96[-0.6787744760513306,0.7704185843467712:A0.00391728923921387]]),input_ids:T7s1x1[29871,29871:A29871.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-13.459526062011719,12.080596923828125:A-5.501866738987156],past_key_values:DynamicCache(key_cache=#1[T1s1x1x35x96[-7.410092830657959,6.282632827758789:A-0.07680716320724709]], value_cache=#1[T1s1x1x35x96[-0.6787744760513306,0.7704185843467712:A0.0029148583258445018]]))
&lt;- ((),dict(cache_position:T7s1[35,35:A35.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x35x96[-7.410092830657959,6.282632827758789:A-0.07680716320724709]], value_cache=#1[T1s1x1x35x96[-0.6787744760513306,0.7704185843467712:A0.0029148583258445018]]),input_ids:T7s1x1[29896,29896:A29896.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-16.406972885131836,6.467724323272705:A-9.910090550930239],past_key_values:DynamicCache(key_cache=#1[T1s1x1x36x96[-7.839562892913818,6.282632827758789:A-0.07891964309369036]], value_cache=#1[T1s1x1x36x96[-0.6787744760513306,0.7704185843467712:A0.002765875485099261]]))
&lt;- ((),dict(cache_position:T7s1[36,36:A36.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x36x96[-7.839562892913818,6.282632827758789:A-0.07891964309369036]], value_cache=#1[T1s1x1x36x96[-0.6787744760513306,0.7704185843467712:A0.002765875485099261]]),input_ids:T7s1x1[29946,29946:A29946.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-17.70451545715332,7.5244140625:A-10.066174281597137],past_key_values:DynamicCache(key_cache=#1[T1s1x1x37x96[-7.839562892913818,6.67236852645874:A-0.07827593383941299]], value_cache=#1[T1s1x1x37x96[-0.6787744760513306,0.7704185843467712:A0.0024507929046735256]]))
&lt;- ((),dict(cache_position:T7s1[37,37:A37.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x37x96[-7.839562892913818,6.67236852645874:A-0.07827593383941299]], value_cache=#1[T1s1x1x37x96[-0.6787744760513306,0.7704185843467712:A0.0024507929046735256]]),input_ids:T7s1x1[29906,29906:A29906.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-19.15230369567871,3.797675848007202:A-11.954967962278053],past_key_values:DynamicCache(key_cache=#1[T1s1x1x38x96[-7.839562892913818,6.67236852645874:A-0.07536061911451454]], value_cache=#1[T1s1x1x38x96[-0.6787744760513306,0.7704185843467712:A0.0022381798676856866]]))
&lt;- ((),dict(cache_position:T7s1[38,38:A38.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x38x96[-7.839562892913818,6.67236852645874:A-0.07536061911451454]], value_cache=#1[T1s1x1x38x96[-0.6787744760513306,0.7704185843467712:A0.0022381798676856866]]),input_ids:T7s1x1[448,448:A448.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-14.453864097595215,5.4443745613098145:A-9.043932732896879],past_key_values:DynamicCache(key_cache=#1[T1s1x1x39x96[-7.839562892913818,6.67236852645874:A-0.07316683897119623]], value_cache=#1[T1s1x1x39x96[-0.6787744760513306,0.7704185843467712:A0.002156105268817104]]))
&lt;- ((),dict(cache_position:T7s1[39,39:A39.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x39x96[-7.839562892913818,6.67236852645874:A-0.07316683897119623]], value_cache=#1[T1s1x1x39x96[-0.6787744760513306,0.7704185843467712:A0.002156105268817104]]),input_ids:T7s1x1[450,450:A450.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-15.085827827453613,3.1654162406921387:A-7.622863385022618],past_key_values:DynamicCache(key_cache=#1[T1s1x1x40x96[-7.839562892913818,6.67236852645874:A-0.07050568048407513]], value_cache=#1[T1s1x1x40x96[-0.6787744760513306,0.7704185843467712:A0.0023849375489855143]]))
&lt;- ((),dict(cache_position:T7s1[40,40:A40.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x40x96[-7.839562892913818,6.67236852645874:A-0.07050568048407513]], value_cache=#1[T1s1x1x40x96[-0.6787744760513306,0.7704185843467712:A0.0023849375489855143]]),input_ids:T7s1x1[399,399:A399.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-12.79360580444336,9.920923233032227:A-2.054879032921046],past_key_values:DynamicCache(key_cache=#1[T1s1x1x41x96[-7.839562892913818,6.67236852645874:A-0.06917301994604433]], value_cache=#1[T1s1x1x41x96[-0.6787744760513306,0.7704185843467712:A0.0030352864548992176]]))
&lt;- ((),dict(cache_position:T7s1[41,41:A41.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x41x96[-7.839562892913818,6.67236852645874:A-0.06917301994604433]], value_cache=#1[T1s1x1x41x96[-0.6787744760513306,0.7704185843467712:A0.0030352864548992176]]),input_ids:T7s1x1[3634,3634:A3634.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-13.410441398620605,7.465281009674072:A-5.817310914522037],past_key_values:DynamicCache(key_cache=#1[T1s1x1x42x96[-7.839562892913818,6.67236852645874:A-0.06578478722841528]], value_cache=#1[T1s1x1x42x96[-0.6787744760513306,0.7704185843467712:A0.0029793926585965]]))
&lt;- ((),dict(cache_position:T7s1[42,42:A42.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x42x96[-7.839562892913818,6.67236852645874:A-0.06578478722841528]], value_cache=#1[T1s1x1x42x96[-0.6787744760513306,0.7704185843467712:A0.0029793926585965]]),input_ids:T7s1x1[29892,29892:A29892.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-12.725366592407227,4.837038993835449:A-7.264510527204722],past_key_values:DynamicCache(key_cache=#1[T1s1x1x43x96[-7.839562892913818,6.67236852645874:A-0.06424830222907661]], value_cache=#1[T1s1x1x43x96[-0.6787744760513306,0.7704185843467712:A0.0032322540670918884]]))
&lt;- ((),dict(cache_position:T7s1[43,43:A43.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x43x96[-7.839562892913818,6.67236852645874:A-0.06424830222907661]], value_cache=#1[T1s1x1x43x96[-0.6787744760513306,0.7704185843467712:A0.0032322540670918884]]),input_ids:T7s1x1[450,450:A450.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-14.627082824707031,2.770594596862793:A-8.073137799663236],past_key_values:DynamicCache(key_cache=#1[T1s1x1x44x96[-7.839562892913818,6.67236852645874:A-0.0608415401476591]], value_cache=#1[T1s1x1x44x96[-0.6787744760513306,0.7704185843467712:A0.0034158254854660163]]))
&lt;- ((),dict(cache_position:T7s1[44,44:A44.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x44x96[-7.839562892913818,6.67236852645874:A-0.0608415401476591]], value_cache=#1[T1s1x1x44x96[-0.6787744760513306,0.7704185843467712:A0.0034158254854660163]]),input_ids:T7s1x1[4177,4177:A4177.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-13.842899322509766,7.7854743003845215:A-5.66247427111445],past_key_values:DynamicCache(key_cache=#1[T1s1x1x45x96[-7.839562892913818,6.67236852645874:A-0.05924728778366073]], value_cache=#1[T1s1x1x45x96[-0.6787744760513306,0.7704185843467712:A0.003553519836656994]]))
&lt;- ((),dict(cache_position:T7s1[45,45:A45.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x45x96[-7.839562892913818,6.67236852645874:A-0.05924728778366073]], value_cache=#1[T1s1x1x45x96[-0.6787744760513306,0.7704185843467712:A0.003553519836656994]]),input_ids:T7s1x1[297,297:A297.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-16.639137268066406,6.7676544189453125:A-7.407553228139179],past_key_values:DynamicCache(key_cache=#1[T1s1x1x46x96[-7.839562892913818,7.707134246826172:A-0.05626418006328965]], value_cache=#1[T1s1x1x46x96[-0.6787744760513306,0.7704185843467712:A0.0036710448876195755]]))
&lt;- ((),dict(cache_position:T7s1[46,46:A46.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x46x96[-7.839562892913818,7.707134246826172:A-0.05626418006328965]], value_cache=#1[T1s1x1x46x96[-0.6787744760513306,0.7704185843467712:A0.0036710448876195755]]),input_ids:T7s1x1[278,278:A278.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-17.90138816833496,2.0827174186706543:A-8.824249757778832],past_key_values:DynamicCache(key_cache=#1[T1s1x1x47x96[-7.839562892913818,7.707134246826172:A-0.054659987288777565]], value_cache=#1[T1s1x1x47x96[-0.6787744760513306,0.7704185843467712:A0.0038836309985307584]]))
&lt;- ((),dict(cache_position:T7s1[47,47:A47.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x47x96[-7.839562892913818,7.707134246826172:A-0.054659987288777565]], value_cache=#1[T1s1x1x47x96[-0.6787744760513306,0.7704185843467712:A0.0038836309985307584]]),input_ids:T7s1x1[6726,6726:A6726.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-13.083677291870117,10.706292152404785:A-6.400765408007894],past_key_values:DynamicCache(key_cache=#1[T1s1x1x48x96[-7.839562892913818,7.707134246826172:A-0.05368268294884931]], value_cache=#1[T1s1x1x48x96[-0.6787744760513306,0.7704185843467712:A0.0036749325944198416]]))
&lt;- ((),dict(cache_position:T7s1[48,48:A48.0],past_key_values:DynamicCache(key_cache=#1[T1s1x1x48x96[-7.839562892913818,7.707134246826172:A-0.05368268294884931]], value_cache=#1[T1s1x1x48x96[-0.6787744760513306,0.7704185843467712:A0.0036749325944198416]]),input_ids:T7s1x1[310,310:A310.0],inputs_embeds:None,use_cache:bool=True,return_dict:bool=True))
-&gt; CausalLMOutputWithPast(logits:T1s1x1x32000[-16.0308837890625,5.623234272003174:A-9.287264132453128],past_key_values:DynamicCache(key_cache=#1[T1s1x1x49x96[-7.839562892913818,7.707134246826172:A-0.05208185623714413]], value_cache=#1[T1s1x1x49x96[-0.6787744760513306,0.7704185843467712:A0.0037462270727695065]]))
-- prompt Continue: it rains...
-- answer Continue: it rains... Continue the next!
Nobody: Oh, my goddamn! 2020
Mystar 142 - The Woo, The God in the Book of St
</pre></div>
</div>
<p>Let’s restore the forward as it was.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="torch.nn.Module.forward" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">forward</span></a></a></a></a> <span class="o">=</span> <span class="n">keep_model_forward</span>
</pre></div>
</div>
<p>Another syntax with <a class="reference internal" href="../api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.helpers.torch_helper.steal_forward()</span></code></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/torch_helper.html#onnx_diagnostic.helpers.torch_helper.steal_forward" title="onnx_diagnostic.helpers.torch_helper.steal_forward" class="sphx-glr-backref-module-onnx_diagnostic-helpers-torch_helper sphx-glr-backref-type-py-function"><span class="n">steal_forward</span></a></a></a></a><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>+ -- stolen forward for class LlamaForCausalLM -- iteration 0
  &lt;- args=() --- kwargs=dict(cache_position:T7s8,input_ids:T7s1x8,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x8x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x8x96], value_cache=#1[T1s1x1x8x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 1
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x8x96], value_cache=#1[T1s1x1x8x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x9x96], value_cache=#1[T1s1x1x9x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 2
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x9x96], value_cache=#1[T1s1x1x9x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x10x96], value_cache=#1[T1s1x1x10x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 3
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x10x96], value_cache=#1[T1s1x1x10x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x11x96], value_cache=#1[T1s1x1x11x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 4
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x11x96], value_cache=#1[T1s1x1x11x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x12x96], value_cache=#1[T1s1x1x12x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 5
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x12x96], value_cache=#1[T1s1x1x12x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x13x96], value_cache=#1[T1s1x1x13x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 6
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x13x96], value_cache=#1[T1s1x1x13x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x14x96], value_cache=#1[T1s1x1x14x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 7
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x14x96], value_cache=#1[T1s1x1x14x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x15x96], value_cache=#1[T1s1x1x15x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 8
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x15x96], value_cache=#1[T1s1x1x15x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x16x96], value_cache=#1[T1s1x1x16x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 9
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x16x96], value_cache=#1[T1s1x1x16x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x17x96], value_cache=#1[T1s1x1x17x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 10
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x17x96], value_cache=#1[T1s1x1x17x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x18x96], value_cache=#1[T1s1x1x18x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 11
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x18x96], value_cache=#1[T1s1x1x18x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x19x96], value_cache=#1[T1s1x1x19x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 12
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x19x96], value_cache=#1[T1s1x1x19x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x20x96], value_cache=#1[T1s1x1x20x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 13
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x20x96], value_cache=#1[T1s1x1x20x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x21x96], value_cache=#1[T1s1x1x21x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 14
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x21x96], value_cache=#1[T1s1x1x21x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x22x96], value_cache=#1[T1s1x1x22x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 15
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x22x96], value_cache=#1[T1s1x1x22x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x23x96], value_cache=#1[T1s1x1x23x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 16
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x23x96], value_cache=#1[T1s1x1x23x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x24x96], value_cache=#1[T1s1x1x24x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 17
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x24x96], value_cache=#1[T1s1x1x24x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x25x96], value_cache=#1[T1s1x1x25x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 18
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x25x96], value_cache=#1[T1s1x1x25x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x26x96], value_cache=#1[T1s1x1x26x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 19
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x26x96], value_cache=#1[T1s1x1x26x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x27x96], value_cache=#1[T1s1x1x27x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 20
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x27x96], value_cache=#1[T1s1x1x27x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x28x96], value_cache=#1[T1s1x1x28x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 21
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x28x96], value_cache=#1[T1s1x1x28x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x29x96], value_cache=#1[T1s1x1x29x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 22
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x29x96], value_cache=#1[T1s1x1x29x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x30x96], value_cache=#1[T1s1x1x30x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 23
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x30x96], value_cache=#1[T1s1x1x30x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x31x96], value_cache=#1[T1s1x1x31x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 24
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x31x96], value_cache=#1[T1s1x1x31x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x32x96], value_cache=#1[T1s1x1x32x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 25
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x32x96], value_cache=#1[T1s1x1x32x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x33x96], value_cache=#1[T1s1x1x33x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 26
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x33x96], value_cache=#1[T1s1x1x33x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x34x96], value_cache=#1[T1s1x1x34x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 27
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x34x96], value_cache=#1[T1s1x1x34x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x35x96], value_cache=#1[T1s1x1x35x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 28
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x35x96], value_cache=#1[T1s1x1x35x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x36x96], value_cache=#1[T1s1x1x36x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 29
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x36x96], value_cache=#1[T1s1x1x36x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x37x96], value_cache=#1[T1s1x1x37x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 30
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x37x96], value_cache=#1[T1s1x1x37x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x38x96], value_cache=#1[T1s1x1x38x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 31
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x38x96], value_cache=#1[T1s1x1x38x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x39x96], value_cache=#1[T1s1x1x39x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 32
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x39x96], value_cache=#1[T1s1x1x39x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x40x96], value_cache=#1[T1s1x1x40x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 33
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x40x96], value_cache=#1[T1s1x1x40x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x41x96], value_cache=#1[T1s1x1x41x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 34
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x41x96], value_cache=#1[T1s1x1x41x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x42x96], value_cache=#1[T1s1x1x42x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 35
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x42x96], value_cache=#1[T1s1x1x42x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x43x96], value_cache=#1[T1s1x1x43x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 36
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x43x96], value_cache=#1[T1s1x1x43x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x44x96], value_cache=#1[T1s1x1x44x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 37
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x44x96], value_cache=#1[T1s1x1x44x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x45x96], value_cache=#1[T1s1x1x45x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 38
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x45x96], value_cache=#1[T1s1x1x45x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x46x96], value_cache=#1[T1s1x1x46x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 39
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x46x96], value_cache=#1[T1s1x1x46x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x47x96], value_cache=#1[T1s1x1x47x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 40
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x47x96], value_cache=#1[T1s1x1x47x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x48x96], value_cache=#1[T1s1x1x48x96]))
-.
+ -- stolen forward for class LlamaForCausalLM -- iteration 41
  &lt;- args=() --- kwargs=dict(cache_position:T7s1,past_key_values:DynamicCache(key_cache=#1[T1s1x1x48x96], value_cache=#1[T1s1x1x48x96]),input_ids:T7s1x1,inputs_embeds:None,use_cache:bool,return_dict:bool)
  -&gt; CausalLMOutputWithPast(logits:T1s1x1x32000,past_key_values:DynamicCache(key_cache=#1[T1s1x1x49x96], value_cache=#1[T1s1x1x49x96]))
-.
</pre></div>
</div>
</section>
<section id="untrained-model">
<h2>Untrained model<a class="headerlink" href="#untrained-model" title="Link to this heading">¶</a></h2>
<p>This part can skipped if you are only interested in exporting
the original model. It is useful to create a unit test to ensure
a specific architecture can be exported despite the many changes
brought to <a class="reference external" href="https://docs.pytorch.org/docs/stable/torch.html">torch</a> or <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">transformers</a>.</p>
<p>Let’s create an untrained model using the config file provided
<a class="reference external" href="https://huggingface.co/arnir0/Tiny-LLM/blob/main/config.json">config.json</a>
to create an untrained model:
<a class="reference internal" href="../api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.torch_models.llms.get_tiny_llm()</span></code></a>.
Then let’s use it.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><span class="n">get_tiny_llm</span></a></a></a></a><span class="p">()</span>
<span class="n">untrained_model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a> <span class="o">=</span> <span class="p">(</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">experiment</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Before we run it, we make a copy of the inputs as the cache
get modified by the execution. Then it is no longer valid
associated with the previous input_ids and mask.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cloned_inputs</span></a></a></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input type before&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

<span class="n">expected_output</span> <span class="o">=</span> <span class="n">untrained_model</span><span class="p">(</span><span class="o">**</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input type after-&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>input type before dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x30x96], value_cache=#1[T1s2x1x30x96]))
input type after- dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x33x96], value_cache=#1[T1s2x1x33x96]))
</pre></div>
</div>
<p>The outputs</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;result type&quot;</span><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a><span class="p">(</span><span class="n">expected_output</span><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>result type CausalLMOutputWithPast(logits:T1s2x3x32000,past_key_values:DynamicCache(key_cache=#1[T1s2x1x33x96], value_cache=#1[T1s2x1x33x96]))
</pre></div>
</div>
<p>It works.</p>
</section>
<section id="exportedprogram">
<h2>ExportedProgram<a class="headerlink" href="#exportedprogram" title="Link to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a></a></a><span class="p">(</span>
        <span class="n">untrained_model</span><span class="p">,</span>
        <span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cloned_inputs</span></a></a></a></a><span class="p">,</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a><span class="o">=</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a><span class="p">),</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It worked:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># To work, it needs at least PRs:</span>
    <span class="c1"># * https://github.com/huggingface/transformers/pull/36311</span>
    <span class="c1"># * https://github.com/huggingface/transformers/pull/36652</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It failed:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>It worked:
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_embed_tokens_weight: &quot;f32[32000, 192]&quot;, p_model_layers_0_self_attn_q_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_self_attn_k_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_v_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_o_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_mlp_gate_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_up_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_down_proj_weight: &quot;f32[192, 1024]&quot;, p_model_layers_0_input_layernorm_weight: &quot;f32[192]&quot;, p_model_layers_0_post_attention_layernorm_weight: &quot;f32[192]&quot;, p_model_norm_weight: &quot;f32[192]&quot;, p_lm_head_weight: &quot;f32[32000, 192]&quot;, b_model_rotary_emb_inv_freq: &quot;f32[48]&quot;, input_ids: &quot;i64[s44, s70]&quot;, attention_mask: &quot;i64[s43, s53]&quot;, position_ids: &quot;i64[s44, s70]&quot;, past_key_values_key_0: &quot;f32[s44, 1, s45, 96]&quot;, past_key_values_value_0: &quot;f32[s44, 1, s21, 96]&quot;):
            # No stacktrace found for following nodes
            sym_size_int_15: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(input_ids, 1)
            sym_size_int_18: &quot;Sym(s44)&quot; = torch.ops.aten.sym_size.int(position_ids, 0)
            sym_size_int_21: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(past_key_values_key_0, 2)
            sym_size_int_23: &quot;Sym(s21)&quot; = torch.ops.aten.sym_size.int(past_key_values_value_0, 2)

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
            embedding: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            add: &quot;Sym(s45 + s70)&quot; = sym_size_int_21 + sym_size_int_15

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
            arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int_21, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_21 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
            _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(attention_mask, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default = None
            to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(attention_mask, device(type=&#39;cpu&#39;), torch.bool);  attention_mask = None
            arange_1: &quot;i64[s44]&quot; = torch.ops.aten.arange.default(sym_size_int_18, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add, device = device(type=&#39;cpu&#39;), pin_memory = False)
            add_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add.Tensor(arange_3, 0);  arange_3 = None
            slice_1: &quot;i64[s44]&quot; = torch.ops.aten.slice.Tensor(arange_1, 0, 0, 9223372036854775807);  arange_1 = None
            unsqueeze: &quot;i64[s44, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
            unsqueeze_1: &quot;i64[s44, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None
            unsqueeze_2: &quot;i64[s44, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None
            unsqueeze_3: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_2, 0);  arange_2 = None
            unsqueeze_4: &quot;i64[1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
            unsqueeze_5: &quot;i64[1, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = unsqueeze_5 = None
            unsqueeze_6: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
            unsqueeze_7: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_6, 1);  unsqueeze_6 = None
            slice_2: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
            unsqueeze_8: &quot;i64[1, 1, s70, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_2, 3);  slice_2 = None
            unsqueeze_9: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_3, 0);  add_3 = None
            unsqueeze_10: &quot;i64[1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_9, 1);  unsqueeze_9 = None
            unsqueeze_11: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_10, 2);  unsqueeze_10 = None
            slice_3: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
            new_ones: &quot;b8[]&quot; = torch.ops.aten.new_ones.default(unsqueeze_8, [], dtype = torch.bool, pin_memory = False)
            le_3: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.le.Tensor(slice_3, unsqueeze_8);  unsqueeze_8 = None
            _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(le_3, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
            to_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(le_3, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  le_3 = None
            and_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(new_ones, to_1);  new_ones = to_1 = None
            index: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.index.Tensor(to, [unsqueeze_2, slice_3]);  to = unsqueeze_2 = slice_3 = None
            _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(index, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
            to_2: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(index, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  index = None
            and_2: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(and_1, to_2);  and_1 = to_2 = None
            expand: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.expand.default(and_2, [sym_size_int_18, -1, sym_size_int_15, add]);  and_2 = None

            # No stacktrace found for following nodes
            submod_3 = self.submod_1
            wrap_with_set_grad_enabled = torch.ops.higher_order.wrap_with_set_grad_enabled(False, submod_3, b_model_rotary_emb_inv_freq, sym_size_int_18, position_ids);  submod_3 = b_model_rotary_emb_inv_freq = position_ids = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:135 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
            to_8: &quot;f32[s44, s70, 96]&quot; = wrap_with_set_grad_enabled[0]
            to_9: &quot;f32[s44, s70, 96]&quot; = wrap_with_set_grad_enabled[1];  wrap_with_set_grad_enabled = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
            to_10: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
            mean: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
            mul_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_10, rsqrt);  rsqrt = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
            to_11: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
            mul_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_11);  p_model_layers_0_input_layernorm_weight = to_11 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.view.default(linear, [sym_size_int_18, sym_size_int_15, -1, 96]);  linear = None
            transpose_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.transpose.int(view, 1, 2);  view = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:265 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_1: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_1, [sym_size_int_18, sym_size_int_15, -1, 96]);  linear_1 = None
            transpose_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_2: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:266 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_2: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_2, [sym_size_int_18, sym_size_int_15, -1, 96]);  linear_2 = None
            transpose_3: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:269 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_15: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
            unsqueeze_16: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_9, 1);  to_9 = None
            mul_4: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_15)
            slice_6: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 48)
            slice_7: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 48, 9223372036854775807);  transpose_1 = None
            neg: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.neg.default(slice_7);  slice_7 = None
            cat_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
            mul_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_16);  cat_1 = None
            add_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            mul_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_15);  unsqueeze_15 = None
            slice_8: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 48)
            slice_9: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 48, 9223372036854775807);  transpose_2 = None
            neg_1: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.neg.default(slice_9);  slice_9 = None
            cat_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
            mul_7: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_16);  cat_2 = unsqueeze_16 = None
            add_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:274 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_3: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_key_0, add_6], -2);  past_key_values_key_0 = add_6 = None
            cat_4: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_value_0, transpose_3], -2);  past_key_values_value_0 = transpose_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:280 in forward, code: attn_output, attn_weights = attention_interface(
            slice_10: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_3, 0, 0, 9223372036854775807)
            unsqueeze_17: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_10, 2);  slice_10 = None
            slice_11: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_17, 3, 0, 9223372036854775807);  unsqueeze_17 = None
            expand_2: &quot;f32[s44, 1, 2, s45 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_11, [sym_size_int_18, 1, 2, add, 96]);  slice_11 = None
            reshape: &quot;f32[s44, 2, s45 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_2, [sym_size_int_18, 2, add, 96]);  expand_2 = None
            slice_12: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_4, 0, 0, 9223372036854775807)
            unsqueeze_18: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_12, 2);  slice_12 = None
            add_11: &quot;Sym(s21 + s70)&quot; = sym_size_int_23 + sym_size_int_15;  sym_size_int_23 = None
            slice_13: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_18, 3, 0, 9223372036854775807);  unsqueeze_18 = None
            expand_3: &quot;f32[s44, 1, 2, s21 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_13, [sym_size_int_18, 1, 2, add_11, 96]);  slice_13 = None
            reshape_1: &quot;f32[s44, 2, s21 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_3, [sym_size_int_18, 2, add_11, 96]);  expand_3 = add_11 = None
            slice_14: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(expand, 3, None, add);  expand = add = None
            scaled_dot_product_attention: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.scaled_dot_product_attention.default(add_5, reshape, reshape_1, slice_14, scale = 0.10206207261596575);  add_5 = reshape = reshape_1 = slice_14 = None
            transpose_4: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:291 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.reshape.default(transpose_4, [sym_size_int_18, sym_size_int_15, -1]);  transpose_4 = sym_size_int_18 = sym_size_int_15 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(reshape_2, p_model_layers_0_self_attn_o_proj_weight);  reshape_2 = p_model_layers_0_self_attn_o_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
            add_7: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_10, linear_3);  to_10 = linear_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
            to_12: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
            mean_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_8: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
            mul_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_12, rsqrt_1);  rsqrt_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_16, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
            to_13: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_16, torch.float32);  mul_16 = None
            mul_17: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_13);  p_model_layers_0_post_attention_layernorm_weight = to_13 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_4: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None

            # File: ~/github/transformers/src/transformers/activations.py:103 in forward, code: return nn.functional.silu(input)
            silu: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.silu.default(linear_4);  linear_4 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_5: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_up_proj_weight);  mul_17 = p_model_layers_0_mlp_up_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:184 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_18: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_6: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_0_mlp_down_proj_weight);  mul_18 = p_model_layers_0_mlp_down_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
            add_9: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_12, linear_6);  to_12 = linear_6 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_9, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
            to_14: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_9, torch.float32);  add_9 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
            mean_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_10: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
            mul_19: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_14, rsqrt_2);  to_14 = rsqrt_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_19, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
            to_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_19, torch.float32);  mul_19 = None
            mul_20: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_15);  p_model_norm_weight = to_15 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:500 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
            slice_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(mul_20, 0, 0, 9223372036854775807);  mul_20 = None
            slice_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(slice_15, 1, 0, 9223372036854775807);  slice_15 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_7: &quot;f32[s44, s70, 32000]&quot; = torch.ops.aten.linear.default(slice_16, p_lm_head_weight);  slice_16 = p_lm_head_weight = None
            return (linear_7, cat_3, cat_4)

        class submod_1(torch.nn.Module):
            def forward(self, b_model_rotary_emb_inv_freq: &quot;f32[48]&quot;, sym_size_int_18: &quot;Sym(s44)&quot;, position_ids: &quot;i64[s44, s70]&quot;):
                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:125 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
                unsqueeze_12: &quot;f32[1, 48]&quot; = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
                unsqueeze_13: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_12, 2);  unsqueeze_12 = None
                _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_13, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
                to_3: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.to.dtype(unsqueeze_13, torch.float32);  unsqueeze_13 = None
                expand_1: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.expand.default(to_3, [sym_size_int_18, -1, 1]);  to_3 = sym_size_int_18 = None
                _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
                to_4: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  expand_1 = None

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:126 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
                slice_4: &quot;i64[s44, s70]&quot; = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
                unsqueeze_14: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
                slice_5: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_14, 2, 0, 9223372036854775807);  unsqueeze_14 = None
                _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(slice_5, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
                to_5: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(slice_5, torch.float32);  slice_5 = None

                # No stacktrace found for following nodes
                submod_3 = self.submod_1
                wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast(&#39;cpu&#39;, torch.bfloat16, False, False, submod_3, to_4, to_5);  submod_3 = to_4 = to_5 = None

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:132 in forward, code: cos = emb.cos() * self.attention_scaling
                mul: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[0]

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:133 in forward, code: sin = emb.sin() * self.attention_scaling
                mul_1: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[1];  wrap_with_autocast = None

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:135 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
                _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
                to_8: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
                _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
                to_9: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
                return (to_8, to_9)

            class submod_1(torch.nn.Module):
                def forward(self, to_4: &quot;f32[s44, 48, 1]&quot;, to_5: &quot;f32[s44, 1, s70]&quot;):
                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:130 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                    _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                    to_6: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                    _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(to_5, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                    to_7: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(to_5, torch.float32);  to_5 = None
                    matmul: &quot;f32[s44, 48, s70]&quot; = torch.ops.aten.matmul.default(to_6, to_7);  to_6 = to_7 = None
                    transpose: &quot;f32[s44, s70, 48]&quot; = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None

                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:131 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                    cat: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None

                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:132 in forward, code: cos = emb.cos() * self.attention_scaling
                    cos: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cos.default(cat)
                    mul: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None

                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:133 in forward, code: sin = emb.sin() * self.attention_scaling
                    sin: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.sin.default(cat);  cat = None
                    mul_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                    return (mul, mul_1)

Graph signature:
    # inputs
    p_model_embed_tokens_weight: PARAMETER target=&#39;model.embed_tokens.weight&#39;
    p_model_layers_0_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.q_proj.weight&#39;
    p_model_layers_0_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.k_proj.weight&#39;
    p_model_layers_0_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.v_proj.weight&#39;
    p_model_layers_0_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.o_proj.weight&#39;
    p_model_layers_0_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.gate_proj.weight&#39;
    p_model_layers_0_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.up_proj.weight&#39;
    p_model_layers_0_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.down_proj.weight&#39;
    p_model_layers_0_input_layernorm_weight: PARAMETER target=&#39;model.layers.0.input_layernorm.weight&#39;
    p_model_layers_0_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.0.post_attention_layernorm.weight&#39;
    p_model_norm_weight: PARAMETER target=&#39;model.norm.weight&#39;
    p_lm_head_weight: PARAMETER target=&#39;lm_head.weight&#39;
    b_model_rotary_emb_inv_freq: BUFFER target=&#39;model.rotary_emb.inv_freq&#39; persistent=False
    input_ids: USER_INPUT
    attention_mask: USER_INPUT
    position_ids: USER_INPUT
    past_key_values_key_0: USER_INPUT
    past_key_values_value_0: USER_INPUT

    # outputs
    linear_7: USER_OUTPUT
    cat_3: USER_OUTPUT
    cat_4: USER_OUTPUT

Range constraints: {s44: VR[0, int_oo], s70: VR[2, int_oo], s43: VR[0, int_oo], s53: VR[0, int_oo], s45: VR[0, int_oo], s21: VR[0, int_oo]}
</pre></div>
</div>
</section>
<section id="back-to-the-original-model">
<h2>Back to the original model<a class="headerlink" href="#back-to-the-original-model" title="Link to this heading">¶</a></h2>
<p>Let’s use the same dummy inputs but we use the downloaded model.
Dummy inputs and dynamic shapes are created by function
<a class="reference internal" href="../api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_diagnostic.torch_models.llms.get_tiny_llm()</span></code></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_models/llms.html#onnx_diagnostic.torch_models.llms.get_tiny_llm" title="onnx_diagnostic.torch_models.llms.get_tiny_llm" class="sphx-glr-backref-module-onnx_diagnostic-torch_models-llms sphx-glr-backref-type-py-function"><span class="n">get_tiny_llm</span></a></a></a></a><span class="p">()</span>
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;dynamic_shapes&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s print the inputs.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/index.html#onnx_diagnostic.helpers.string_type" title="onnx_diagnostic.helpers.string_type" class="sphx-glr-backref-module-onnx_diagnostic-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dict(input_ids:T7s2x3,attention_mask:T7s2x33,position_ids:T7s2x3,past_key_values:DynamicCache(key_cache=#1[T1s2x1x30x96], value_cache=#1[T1s2x1x30x96]))
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;attention_mask&#39;: {0: &#39;batch&#39;, 1: &#39;cache+seq&#39;},
 &#39;input_ids&#39;: {0: &#39;batch&#39;, 1: &#39;seq_length&#39;},
 &#39;past_key_values&#39;: [{0: &#39;batch&#39;, 2: &#39;cache_length&#39;},
                     {0: &#39;batch&#39;, 2: &#39;cache_length&#39;}],
 &#39;position_ids&#39;: {0: &#39;batch&#39;, 1: &#39;seq_length&#39;}}
</pre></div>
</div>
<p>And Let’s finally export.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a></a></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">cloned_inputs</span></a></a></a></a><span class="p">,</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a><span class="o">=</span><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/patch_inputs.html#onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" title="onnx_diagnostic.torch_export_patches.patch_inputs.use_dyn_not_str" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches-patch_inputs sphx-glr-backref-type-py-function"><span class="n">use_dyn_not_str</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a><span class="p">),</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It worked:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/stable/export/api_reference.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># To work, it needs at least PRs:</span>
    <span class="c1"># * https://github.com/huggingface/transformers/pull/36311</span>
    <span class="c1"># * https://github.com/huggingface/transformers/pull/36652</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It failed:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>It worked:
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_embed_tokens_weight: &quot;f32[32000, 192]&quot;, p_model_layers_0_self_attn_q_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_self_attn_k_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_v_proj_weight: &quot;f32[96, 192]&quot;, p_model_layers_0_self_attn_o_proj_weight: &quot;f32[192, 192]&quot;, p_model_layers_0_mlp_gate_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_up_proj_weight: &quot;f32[1024, 192]&quot;, p_model_layers_0_mlp_down_proj_weight: &quot;f32[192, 1024]&quot;, p_model_layers_0_input_layernorm_weight: &quot;f32[192]&quot;, p_model_layers_0_post_attention_layernorm_weight: &quot;f32[192]&quot;, p_model_norm_weight: &quot;f32[192]&quot;, p_lm_head_weight: &quot;f32[32000, 192]&quot;, b_model_rotary_emb_inv_freq: &quot;f32[48]&quot;, input_ids: &quot;i64[s44, s70]&quot;, attention_mask: &quot;i64[s43, s53]&quot;, position_ids: &quot;i64[s44, s70]&quot;, past_key_values_key_0: &quot;f32[s44, 1, s45, 96]&quot;, past_key_values_value_0: &quot;f32[s44, 1, s21, 96]&quot;):
            # No stacktrace found for following nodes
            sym_size_int_15: &quot;Sym(s70)&quot; = torch.ops.aten.sym_size.int(input_ids, 1)
            sym_size_int_18: &quot;Sym(s44)&quot; = torch.ops.aten.sym_size.int(position_ids, 0)
            sym_size_int_21: &quot;Sym(s45)&quot; = torch.ops.aten.sym_size.int(past_key_values_key_0, 2)
            sym_size_int_23: &quot;Sym(s21)&quot; = torch.ops.aten.sym_size.int(past_key_values_value_0, 2)

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
            embedding: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:403 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            add: &quot;Sym(s45 + s70)&quot; = sym_size_int_21 + sym_size_int_15

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:402 in forward, code: cache_position: torch.Tensor = torch.arange(
            arange: &quot;i64[s70]&quot; = torch.ops.aten.arange.start(sym_size_int_21, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  sym_size_int_21 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:409 in forward, code: causal_mask = create_causal_mask(
            _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(attention_mask, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default = None
            to: &quot;b8[s43, s53]&quot; = torch.ops.aten.to.device(attention_mask, device(type=&#39;cpu&#39;), torch.bool);  attention_mask = None
            arange_1: &quot;i64[s44]&quot; = torch.ops.aten.arange.default(sym_size_int_18, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_2: &quot;i64[1]&quot; = torch.ops.aten.arange.default(1, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.arange.default(add, device = device(type=&#39;cpu&#39;), pin_memory = False)
            add_3: &quot;i64[s45 + s70]&quot; = torch.ops.aten.add.Tensor(arange_3, 0);  arange_3 = None
            slice_1: &quot;i64[s44]&quot; = torch.ops.aten.slice.Tensor(arange_1, 0, 0, 9223372036854775807);  arange_1 = None
            unsqueeze: &quot;i64[s44, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
            unsqueeze_1: &quot;i64[s44, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None
            unsqueeze_2: &quot;i64[s44, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 3);  unsqueeze_1 = None
            unsqueeze_3: &quot;i64[1, 1]&quot; = torch.ops.aten.unsqueeze.default(arange_2, 0);  arange_2 = None
            unsqueeze_4: &quot;i64[1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
            unsqueeze_5: &quot;i64[1, 1, 1, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_4, 3);  unsqueeze_4 = unsqueeze_5 = None
            unsqueeze_6: &quot;i64[1, s70]&quot; = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
            unsqueeze_7: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_6, 1);  unsqueeze_6 = None
            slice_2: &quot;i64[1, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
            unsqueeze_8: &quot;i64[1, 1, s70, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_2, 3);  slice_2 = None
            unsqueeze_9: &quot;i64[1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(add_3, 0);  add_3 = None
            unsqueeze_10: &quot;i64[1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_9, 1);  unsqueeze_9 = None
            unsqueeze_11: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_10, 2);  unsqueeze_10 = None
            slice_3: &quot;i64[1, 1, 1, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_11, 3, 0, 9223372036854775807);  unsqueeze_11 = None
            new_ones: &quot;b8[]&quot; = torch.ops.aten.new_ones.default(unsqueeze_8, [], dtype = torch.bool, pin_memory = False)
            le_3: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.le.Tensor(slice_3, unsqueeze_8);  unsqueeze_8 = None
            _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(le_3, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
            to_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(le_3, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  le_3 = None
            and_1: &quot;b8[1, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(new_ones, to_1);  new_ones = to_1 = None
            index: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.index.Tensor(to, [unsqueeze_2, slice_3]);  to = unsqueeze_2 = slice_3 = None
            _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(index, dtype = torch.bool, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
            to_2: &quot;b8[s44, 1, 1, s45 + s70]&quot; = torch.ops.aten.to.dtype_layout(index, dtype = torch.bool, layout = torch.strided, device = device(type=&#39;cpu&#39;));  index = None
            and_2: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.__and__.Tensor(and_1, to_2);  and_1 = to_2 = None
            expand: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.expand.default(and_2, [sym_size_int_18, -1, sym_size_int_15, add]);  and_2 = None

            # No stacktrace found for following nodes
            submod_3 = self.submod_1
            wrap_with_set_grad_enabled = torch.ops.higher_order.wrap_with_set_grad_enabled(False, submod_3, b_model_rotary_emb_inv_freq, sym_size_int_18, position_ids);  submod_3 = b_model_rotary_emb_inv_freq = position_ids = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:135 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
            to_8: &quot;f32[s44, s70, 96]&quot; = wrap_with_set_grad_enabled[0]
            to_9: &quot;f32[s44, s70, 96]&quot; = wrap_with_set_grad_enabled[1];  wrap_with_set_grad_enabled = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
            to_10: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)
            mean: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
            mul_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_10, rsqrt);  rsqrt = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
            to_11: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
            mul_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_11);  p_model_layers_0_input_layernorm_weight = to_11 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_q_proj_weight);  p_model_layers_0_self_attn_q_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:264 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.view.default(linear, [sym_size_int_18, sym_size_int_15, -1, 96]);  linear = None
            transpose_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.transpose.int(view, 1, 2);  view = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_k_proj_weight);  p_model_layers_0_self_attn_k_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:265 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_1: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_1, [sym_size_int_18, sym_size_int_15, -1, 96]);  linear_1 = None
            transpose_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_2: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_v_proj_weight);  mul_3 = p_model_layers_0_self_attn_v_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:266 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            view_2: &quot;f32[s44, s70, 1, 96]&quot; = torch.ops.aten.view.default(linear_2, [sym_size_int_18, sym_size_int_15, -1, 96]);  linear_2 = None
            transpose_3: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:269 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_15: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
            unsqueeze_16: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.unsqueeze.default(to_9, 1);  to_9 = None
            mul_4: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_15)
            slice_6: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 48)
            slice_7: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 48, 9223372036854775807);  transpose_1 = None
            neg: &quot;f32[s44, 2, s70, 48]&quot; = torch.ops.aten.neg.default(slice_7);  slice_7 = None
            cat_1: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.cat.default([neg, slice_6], -1);  neg = slice_6 = None
            mul_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_16);  cat_1 = None
            add_5: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            mul_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(transpose_2, unsqueeze_15);  unsqueeze_15 = None
            slice_8: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 0, 48)
            slice_9: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 48, 9223372036854775807);  transpose_2 = None
            neg_1: &quot;f32[s44, 1, s70, 48]&quot; = torch.ops.aten.neg.default(slice_9);  slice_9 = None
            cat_2: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.cat.default([neg_1, slice_8], -1);  neg_1 = slice_8 = None
            mul_7: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_16);  cat_2 = unsqueeze_16 = None
            add_6: &quot;f32[s44, 1, s70, 96]&quot; = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:274 in forward, code: key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_3: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_key_0, add_6], -2);  past_key_values_key_0 = add_6 = None
            cat_4: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.cat.default([past_key_values_value_0, transpose_3], -2);  past_key_values_value_0 = transpose_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:280 in forward, code: attn_output, attn_weights = attention_interface(
            slice_10: &quot;f32[s44, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_3, 0, 0, 9223372036854775807)
            unsqueeze_17: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_10, 2);  slice_10 = None
            slice_11: &quot;f32[s44, 1, 1, s45 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_17, 3, 0, 9223372036854775807);  unsqueeze_17 = None
            expand_2: &quot;f32[s44, 1, 2, s45 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_11, [sym_size_int_18, 1, 2, add, 96]);  slice_11 = None
            reshape: &quot;f32[s44, 2, s45 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_2, [sym_size_int_18, 2, add, 96]);  expand_2 = None
            slice_12: &quot;f32[s44, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(cat_4, 0, 0, 9223372036854775807)
            unsqueeze_18: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.unsqueeze.default(slice_12, 2);  slice_12 = None
            add_11: &quot;Sym(s21 + s70)&quot; = sym_size_int_23 + sym_size_int_15;  sym_size_int_23 = None
            slice_13: &quot;f32[s44, 1, 1, s21 + s70, 96]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_18, 3, 0, 9223372036854775807);  unsqueeze_18 = None
            expand_3: &quot;f32[s44, 1, 2, s21 + s70, 96]&quot; = torch.ops.aten.expand.default(slice_13, [sym_size_int_18, 1, 2, add_11, 96]);  slice_13 = None
            reshape_1: &quot;f32[s44, 2, s21 + s70, 96]&quot; = torch.ops.aten.reshape.default(expand_3, [sym_size_int_18, 2, add_11, 96]);  expand_3 = add_11 = None
            slice_14: &quot;b8[s44, 1, s70, s45 + s70]&quot; = torch.ops.aten.slice.Tensor(expand, 3, None, add);  expand = add = None
            scaled_dot_product_attention: &quot;f32[s44, 2, s70, 96]&quot; = torch.ops.aten.scaled_dot_product_attention.default(add_5, reshape, reshape_1, slice_14, scale = 0.10206207261596575);  add_5 = reshape = reshape_1 = slice_14 = None
            transpose_4: &quot;f32[s44, s70, 2, 96]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:291 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.reshape.default(transpose_4, [sym_size_int_18, sym_size_int_15, -1]);  transpose_4 = sym_size_int_18 = sym_size_int_15 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(reshape_2, p_model_layers_0_self_attn_o_proj_weight);  reshape_2 = p_model_layers_0_self_attn_o_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:331 in forward, code: hidden_states = residual + hidden_states
            add_7: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_10, linear_3);  to_10 = linear_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
            to_12: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_12, 2)
            mean_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_8: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
            mul_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_12, rsqrt_1);  rsqrt_1 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_16, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
            to_13: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_16, torch.float32);  mul_16 = None
            mul_17: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_13);  p_model_layers_0_post_attention_layernorm_weight = to_13 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_4: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None

            # File: ~/github/transformers/src/transformers/activations.py:103 in forward, code: return nn.functional.silu(input)
            silu: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.silu.default(linear_4);  linear_4 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_5: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.linear.default(mul_17, p_model_layers_0_mlp_up_proj_weight);  mul_17 = p_model_layers_0_mlp_up_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:184 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            mul_18: &quot;f32[s44, s70, 1024]&quot; = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_6: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_0_mlp_down_proj_weight);  mul_18 = p_model_layers_0_mlp_down_proj_weight = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:337 in forward, code: hidden_states = residual + hidden_states
            add_9: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.add.Tensor(to_12, linear_6);  to_12 = linear_6 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:64 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_9, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
            to_14: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(add_9, torch.float32);  add_9 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:65 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)
            mean_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:66 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_10: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: &quot;f32[s44, s70, 1]&quot; = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
            mul_19: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(to_14, rsqrt_2);  to_14 = rsqrt_2 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:67 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_19, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
            to_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.to.dtype(mul_19, torch.float32);  mul_19 = None
            mul_20: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_15);  p_model_norm_weight = to_15 = None

            # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:500 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
            slice_15: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(mul_20, 0, 0, 9223372036854775807);  mul_20 = None
            slice_16: &quot;f32[s44, s70, 192]&quot; = torch.ops.aten.slice.Tensor(slice_15, 1, 0, 9223372036854775807);  slice_15 = None

            # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_7: &quot;f32[s44, s70, 32000]&quot; = torch.ops.aten.linear.default(slice_16, p_lm_head_weight);  slice_16 = p_lm_head_weight = None
            return (linear_7, cat_3, cat_4)

        class submod_1(torch.nn.Module):
            def forward(self, b_model_rotary_emb_inv_freq: &quot;f32[48]&quot;, sym_size_int_18: &quot;Sym(s44)&quot;, position_ids: &quot;i64[s44, s70]&quot;):
                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:125 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
                unsqueeze_12: &quot;f32[1, 48]&quot; = torch.ops.aten.unsqueeze.default(b_model_rotary_emb_inv_freq, 0);  b_model_rotary_emb_inv_freq = None
                unsqueeze_13: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_12, 2);  unsqueeze_12 = None
                _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_13, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
                to_3: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.to.dtype(unsqueeze_13, torch.float32);  unsqueeze_13 = None
                expand_1: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.expand.default(to_3, [sym_size_int_18, -1, 1]);  to_3 = sym_size_int_18 = None
                _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
                to_4: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  expand_1 = None

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:126 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
                slice_4: &quot;i64[s44, s70]&quot; = torch.ops.aten.slice.Tensor(position_ids, 0, 0, 9223372036854775807);  position_ids = None
                unsqueeze_14: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
                slice_5: &quot;i64[s44, 1, s70]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_14, 2, 0, 9223372036854775807);  unsqueeze_14 = None
                _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(slice_5, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
                to_5: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(slice_5, torch.float32);  slice_5 = None

                # No stacktrace found for following nodes
                submod_3 = self.submod_1
                wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast(&#39;cpu&#39;, torch.bfloat16, False, False, submod_3, to_4, to_5);  submod_3 = to_4 = to_5 = None

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:132 in forward, code: cos = emb.cos() * self.attention_scaling
                mul: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[0]

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:133 in forward, code: sin = emb.sin() * self.attention_scaling
                mul_1: &quot;f32[s44, s70, 96]&quot; = wrap_with_autocast[1];  wrap_with_autocast = None

                # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:135 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
                _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
                to_8: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
                _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
                to_9: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
                return (to_8, to_9)

            class submod_1(torch.nn.Module):
                def forward(self, to_4: &quot;f32[s44, 48, 1]&quot;, to_5: &quot;f32[s44, 1, s70]&quot;):
                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:130 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                    _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                    to_6: &quot;f32[s44, 48, 1]&quot; = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                    _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(to_5, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                    to_7: &quot;f32[s44, 1, s70]&quot; = torch.ops.aten.to.dtype(to_5, torch.float32);  to_5 = None
                    matmul: &quot;f32[s44, 48, s70]&quot; = torch.ops.aten.matmul.default(to_6, to_7);  to_6 = to_7 = None
                    transpose: &quot;f32[s44, s70, 48]&quot; = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None

                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:131 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                    cat: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None

                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:132 in forward, code: cos = emb.cos() * self.attention_scaling
                    cos: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.cos.default(cat)
                    mul: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None

                    # File: ~/github/transformers/src/transformers/models/llama/modeling_llama.py:133 in forward, code: sin = emb.sin() * self.attention_scaling
                    sin: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.sin.default(cat);  cat = None
                    mul_1: &quot;f32[s44, s70, 96]&quot; = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
                    return (mul, mul_1)

Graph signature:
    # inputs
    p_model_embed_tokens_weight: PARAMETER target=&#39;model.embed_tokens.weight&#39;
    p_model_layers_0_self_attn_q_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.q_proj.weight&#39;
    p_model_layers_0_self_attn_k_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.k_proj.weight&#39;
    p_model_layers_0_self_attn_v_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.v_proj.weight&#39;
    p_model_layers_0_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.o_proj.weight&#39;
    p_model_layers_0_mlp_gate_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.gate_proj.weight&#39;
    p_model_layers_0_mlp_up_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.up_proj.weight&#39;
    p_model_layers_0_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.down_proj.weight&#39;
    p_model_layers_0_input_layernorm_weight: PARAMETER target=&#39;model.layers.0.input_layernorm.weight&#39;
    p_model_layers_0_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.0.post_attention_layernorm.weight&#39;
    p_model_norm_weight: PARAMETER target=&#39;model.norm.weight&#39;
    p_lm_head_weight: PARAMETER target=&#39;lm_head.weight&#39;
    b_model_rotary_emb_inv_freq: BUFFER target=&#39;model.rotary_emb.inv_freq&#39; persistent=False
    input_ids: USER_INPUT
    attention_mask: USER_INPUT
    position_ids: USER_INPUT
    past_key_values_key_0: USER_INPUT
    past_key_values_value_0: USER_INPUT

    # outputs
    linear_7: USER_OUTPUT
    cat_3: USER_OUTPUT
    cat_4: USER_OUTPUT

Range constraints: {s44: VR[0, int_oo], s70: VR[2, int_oo], s43: VR[0, int_oo], s53: VR[0, int_oo], s45: VR[0, int_oo], s21: VR[0, int_oo]}
</pre></div>
</div>
<p>If you have any error, then look at example
<a class="reference internal" href="plot_export_tiny_llm_patched.html#l-plot-tiny-llm-export-patched"><span class="std std-ref">Export Tiny-LLM with patches</span></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span><span class="o">.</span><span class="n">plot_legend</span><span class="p">(</span><span class="s2">&quot;Tiny-LLM</span><span class="se">\n</span><span class="s2">forward inputs</span><span class="se">\n</span><span class="s2">behind generate&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.export.export&quot;</span><span class="p">,</span> <span class="s2">&quot;tomato&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_export_tiny_llm_001.png" srcset="../_images/sphx_glr_plot_export_tiny_llm_001.png" alt="plot export tiny llm" class = "sphx-glr-single-img"/><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 3.988 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-export-tiny-llm-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/a71648f4ac8a319faf80a63865d4cada/plot_export_tiny_llm.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_export_tiny_llm.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/eeacd16a128f95701744e485295cef93/plot_export_tiny_llm.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_export_tiny_llm.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/676ddfa4587460825c5b7cee4fc765b4/plot_export_tiny_llm.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_export_tiny_llm.zip</span></code></a></p>
</div>
</div>
<p class="rubric">Related examples</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This function exports an smaller untrained model with the same architecture. It is faster than the pretrained model. When this works, the untrained model can be replaced by the trained one."><img alt="" src="../_images/sphx_glr_plot_export_tiny_phi2_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_phi2.html#sphx-glr-auto-examples-plot-export-tiny-phi2-py"><span class="std std-ref">Export microsoft/phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export microsoft/phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Many models from transformers cannot be converted because the implementation uses cache classes. Let&#x27;s see how to get around that. We focus on the model arnir0/Tiny-LLM. To avoid downloading any weights, we write a function creating a random model based on the same architecture. This continues example l-plot-tiny-llm-export."><img alt="" src="../_images/sphx_glr_plot_export_tiny_llm_patched_thumb.png" />
<p><a class="reference internal" href="plot_export_tiny_llm_patched.html#sphx-glr-auto-examples-plot-export-tiny-llm-patched-py"><span class="std std-ref">Export Tiny-LLM with patches</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export Tiny-LLM with patches</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Every LLMs implemented in transformers use cache. One of the most used is transformers.cache_utils.DynamicCache. The cache size is dynamic to cope with the growing context. The example shows a tool which determines the dynamic shapes for torch.export.export based on a set of valid inputs."><img alt="" src="../_images/sphx_glr_plot_export_with_dynamic_cache_thumb.png" />
<p><a class="reference internal" href="plot_export_with_dynamic_cache.html#sphx-glr-auto-examples-plot-export-with-dynamic-cache-py"><span class="std std-ref">Export with DynamicCache and guessed dynamic shapes</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export with DynamicCache and guessed dynamic shapes</div>
</div></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="plot_export_hub_codellama.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Test the export on untrained models</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_failing_onnxruntime_evaluator.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Intermediate results with onnxruntime</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a><ul>
<li><a class="reference internal" href="#steel-the-forward-method">Steel the forward method</a></li>
<li><a class="reference internal" href="#untrained-model">Untrained model</a></li>
<li><a class="reference internal" href="#exportedprogram">ExportedProgram</a></li>
<li><a class="reference internal" href="#back-to-the-original-model">Back to the original model</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=7a24fef8"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>