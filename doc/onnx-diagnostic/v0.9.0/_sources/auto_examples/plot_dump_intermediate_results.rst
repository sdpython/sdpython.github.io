
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_dump_intermediate_results.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_dump_intermediate_results.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_dump_intermediate_results.py:


.. _l-plot-intermediate-results:

Dumps intermediate results of a torch model
===========================================

Looking for discrepancies is quickly annoying. Discrepancies
come from two results obtained with the same models
implemented in two different ways, :epkg:`pytorch` and :epkg:`onnx`.
Models are big so where do they come from? That's the
unavoidable question. Unless there is an obvious reason,
the only way is to compare intermediate outputs alon the computation.
The first step into that direction is to dump the intermediate results
coming from :epkg:`pytorch`.
We use :func:`onnx_diagnostic.helpers.torch_helper.steal_forward` for that.

A simple LLM Model
++++++++++++++++++

See :func:`onnx_diagnostic.helpers.torch_helper.dummy_llm`
for its definition. It is mostly used for unit test or example.

.. GENERATED FROM PYTHON SOURCE LINES 23-37

.. code-block:: Python


    import numpy as np
    import pandas
    import onnx
    import torch
    import onnxruntime
    from onnx_diagnostic import doc
    from onnx_diagnostic.helpers import max_diff, string_diff, string_type
    from onnx_diagnostic.helpers.torch_helper import dummy_llm, steal_forward
    from onnx_diagnostic.helpers.mini_onnx_builder import create_input_tensors_from_onnx_model
    from onnx_diagnostic.reference import OnnxruntimeEvaluator, ReportResultComparison

    model, inputs, ds = dummy_llm(dynamic_shapes=True)








.. GENERATED FROM PYTHON SOURCE LINES 38-39

We use float16.

.. GENERATED FROM PYTHON SOURCE LINES 39-41

.. code-block:: Python

    model = model.to(torch.float16)








.. GENERATED FROM PYTHON SOURCE LINES 42-43

Let's check.

.. GENERATED FROM PYTHON SOURCE LINES 43-48

.. code-block:: Python


    print(f"type(model)={type(model)}")
    print(f"inputs={string_type(inputs, with_shape=True)}")
    print(f"ds={string_type(ds, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    type(model)=<class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.LLM'>
    inputs=(T7s2x30,)
    ds=dict(input_ids:{0:Dim(batch),1:Dim(length)})




.. GENERATED FROM PYTHON SOURCE LINES 49-50

It contains the following submodules.

.. GENERATED FROM PYTHON SOURCE LINES 50-54

.. code-block:: Python


    for name, mod in model.named_modules():
        print(f"- {name}: {type(mod)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    - : <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.LLM'>
    - embedding: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.Embedding'>
    - embedding.embedding: <class 'torch.nn.modules.sparse.Embedding'>
    - embedding.pe: <class 'torch.nn.modules.sparse.Embedding'>
    - decoder: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.DecoderLayer'>
    - decoder.attention: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.MultiAttentionBlock'>
    - decoder.attention.attention: <class 'torch.nn.modules.container.ModuleList'>
    - decoder.attention.attention.0: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.AttentionBlock'>
    - decoder.attention.attention.0.query: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.0.key: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.0.value: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.AttentionBlock'>
    - decoder.attention.attention.1.query: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1.key: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.attention.1.value: <class 'torch.nn.modules.linear.Linear'>
    - decoder.attention.linear: <class 'torch.nn.modules.linear.Linear'>
    - decoder.feed_forward: <class 'onnx_diagnostic.helpers.torch_helper.dummy_llm.<locals>.FeedForward'>
    - decoder.feed_forward.linear_1: <class 'torch.nn.modules.linear.Linear'>
    - decoder.feed_forward.relu: <class 'torch.nn.modules.activation.ReLU'>
    - decoder.feed_forward.linear_2: <class 'torch.nn.modules.linear.Linear'>
    - decoder.norm_1: <class 'torch.nn.modules.normalization.LayerNorm'>
    - decoder.norm_2: <class 'torch.nn.modules.normalization.LayerNorm'>




.. GENERATED FROM PYTHON SOURCE LINES 55-61

Steal and dump the output of submodules
+++++++++++++++++++++++++++++++++++++++

The following context spies on the intermediate results
for the following module and submodules. It stores
in one onnx file all the input/output for those.

.. GENERATED FROM PYTHON SOURCE LINES 61-78

.. code-block:: Python


    with steal_forward(
        [
            ("model", model),
            ("model.decoder", model.decoder),
            ("model.decoder.attention", model.decoder.attention),
            ("model.decoder.feed_forward", model.decoder.feed_forward),
            ("model.decoder.norm_1", model.decoder.norm_1),
            ("model.decoder.norm_2", model.decoder.norm_2),
        ],
        dump_file="plot_dump_intermediate_results.inputs.onnx",
        verbose=1,
        storage_limit=2**28,
    ):
        expected = model(*inputs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    +model -- stolen forward for class LLM -- iteration 0
      <- args=(T7s2x30,) --- kwargs={}
    +model.decoder -- stolen forward for class DecoderLayer -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
    +model.decoder.norm_1 -- stolen forward for class LayerNorm -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.norm_1.
    -- stores key=('model.decoder.norm_1', 0), size 1Kb -- T10s2x30x16
    +model.decoder.attention -- stolen forward for class MultiAttentionBlock -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.attention.
    -- stores key=('model.decoder.attention', 0), size 1Kb -- T10s2x30x16
    +model.decoder.norm_2 -- stolen forward for class LayerNorm -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.norm_2.
    -- stores key=('model.decoder.norm_2', 0), size 1Kb -- T10s2x30x16
    +model.decoder.feed_forward -- stolen forward for class FeedForward -- iteration 0
      <- args=(T10s2x30x16,) --- kwargs={}
      -> T10s2x30x16
    -model.decoder.feed_forward.
    -- stores key=('model.decoder.feed_forward', 0), size 1Kb -- T10s2x30x16
      -> T10s2x30x16
    -model.decoder.
    -- stores key=('model.decoder', 0), size 1Kb -- T10s2x30x16
      -> T10s2x30x16
    -model.
    -- stores key=('model', 0), size 1Kb -- T10s2x30x16
    -- gather stored 12 objects, size=0 Mb
    -- dumps stored objects
    -- done dump stored objects




.. GENERATED FROM PYTHON SOURCE LINES 79-88

Restores saved inputs/outputs
+++++++++++++++++++++++++++++

All the intermediate tensors were saved in one unique onnx model,
every tensor is stored in a constant node.
The model can be run with any runtime to restore the inputs
and function :func:`create_input_tensors_from_onnx_model
<onnx_diagnostic.helpers.mini_onnx_builder.create_input_tensors_from_onnx_model>`
can restore their names.

.. GENERATED FROM PYTHON SOURCE LINES 88-95

.. code-block:: Python


    saved_tensors = create_input_tensors_from_onnx_model(
        "plot_dump_intermediate_results.inputs.onnx"
    )
    for k, v in saved_tensors.items():
        print(f"{k} -- {string_type(v, with_shape=True)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ('model', 0, 'I') -- ((T7s2x30,),{})
    ('model.decoder', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_1', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_1', 0, 'O') -- T10s2x30x16
    ('model.decoder.attention', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.attention', 0, 'O') -- T10s2x30x16
    ('model.decoder.norm_2', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.norm_2', 0, 'O') -- T10s2x30x16
    ('model.decoder.feed_forward', 0, 'I') -- ((T10s2x30x16,),{})
    ('model.decoder.feed_forward', 0, 'O') -- T10s2x30x16
    ('model.decoder', 0, 'O') -- T10s2x30x16
    ('model', 0, 'O') -- T10s2x30x16




.. GENERATED FROM PYTHON SOURCE LINES 96-120

Let's explained the naming convention.

::

   ('model.decoder.norm_2', 0, 'I') -- ((T1s2x30x16,),{})
               |            |   |
               |            |   +--> input, the format is args, kwargs
               |            |
               |            +--> iteration, 0 means the first time the execution
               |                 went through that module
               |                 it is possible to call multiple times,
               |                 the model to store more
               |
               +--> the name given to function steal_forward

The same goes for output except ``'I'`` is replaced by ``'O'``.

::

   ('model.decoder.norm_2', 0, 'O') -- T1s2x30x16

This trick can be used to compare intermediate results coming
from pytorch to any other implementation of the same model
as long as it is possible to map the stored inputs/outputs.

.. GENERATED FROM PYTHON SOURCE LINES 122-128

Conversion to ONNX
++++++++++++++++++

The difficult point is to be able to map the saved intermediate
results to intermediate results in ONNX.
Let's create the ONNX model.

.. GENERATED FROM PYTHON SOURCE LINES 128-134

.. code-block:: Python


    ep = torch.export.export(model, inputs, dynamic_shapes=ds)
    epo = torch.onnx.export(ep)
    epo.optimize()
    epo.save("plot_dump_intermediate_results.onnx")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch.onnx] Run decompositions...
    /usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.
      return cls.__new__(cls, *args)
    [torch.onnx] Run decompositions... ✅
    [torch.onnx] Translate the graph into ONNX...
    [torch.onnx] Translate the graph into ONNX... ✅
    [torch.onnx] Optimize the ONNX graph...
    Applied 4 of general pattern rewrite rules.
    [torch.onnx] Optimize the ONNX graph... ✅




.. GENERATED FROM PYTHON SOURCE LINES 135-142

Discrepancies
+++++++++++++

We have a torch model, intermediate results and an ONNX graph
equivalent to the torch model.
Let's see how we can check the discrepancies.
First the discrepancies of the whole model.

.. GENERATED FROM PYTHON SOURCE LINES 142-153

.. code-block:: Python


    sess = onnxruntime.InferenceSession(
        "plot_dump_intermediate_results.onnx", providers=["CPUExecutionProvider"]
    )
    feeds = dict(
        zip([i.name for i in sess.get_inputs()], [t.detach().cpu().numpy() for t in inputs])
    )
    got = sess.run(None, feeds)
    diff = max_diff(expected, got)
    print(f"discrepancies torch/ORT: {string_diff(diff)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    discrepancies torch/ORT: abs=0.001953125, rel=0.2968311309814453, n=960.0,amax=0,0,6, dev=0




.. GENERATED FROM PYTHON SOURCE LINES 154-157

What about intermediate results?
Let's use a runtime still based on :epkg:`onnxruntime`
running an eager evaluation.

.. GENERATED FROM PYTHON SOURCE LINES 157-168

.. code-block:: Python


    sess_eager = OnnxruntimeEvaluator(
        "plot_dump_intermediate_results.onnx",
        providers=["CPUExecutionProvider"],
        torch_or_numpy=True,
    )
    feeds_tensor = dict(zip([i.name for i in sess.get_inputs()], inputs))
    got = sess_eager.run(None, feeds_tensor)
    diff = max_diff(expected, got)
    print(f"discrepancies torch/eager ORT: {string_diff(diff)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    discrepancies torch/eager ORT: abs=0.001953125, rel=0.041583499667332, n=960.0,amax=0,0,14, dev=0




.. GENERATED FROM PYTHON SOURCE LINES 169-173

They are almost the same. That's good.
Let's now dig into the intermediate results.
They are compared to the outputs stored in saved_tensors
during the execution of the model.

.. GENERATED FROM PYTHON SOURCE LINES 173-183

.. code-block:: Python

    baseline = {}
    for k, v in saved_tensors.items():
        if k[-1] == "I":  # inputs are excluded
            continue
        if isinstance(v, torch.Tensor):
            baseline[f"{k[0]}.{k[1]}".replace("model.decoder", "decoder")] = v

    report_cmp = ReportResultComparison(baseline)
    sess_eager.run(None, feeds_tensor, report_cmp=report_cmp)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [tensor([[[-9.1406e-01, -4.3579e-01,  2.3853e-01,  9.7461e-01,  2.0288e-01,
               8.0078e-02, -2.2090e+00,  1.4385e+00,  1.8672e+00,  1.1279e+00,
              -6.1621e-01,  2.6855e-01, -3.2520e-01,  1.7793e+00,  1.0195e+00,
              -3.8867e-01],
             [ 7.0020e-01, -2.2656e+00,  3.8555e+00, -8.1836e-01, -1.7847e-01,
              -1.2217e+00,  1.4160e+00,  2.0234e+00, -5.2246e-02, -1.4199e+00,
               2.0332e+00,  7.2021e-01,  5.7031e-01, -1.5889e+00,  5.1025e-01,
              -9.7412e-01],
             [ 1.6230e+00, -7.3242e-01,  2.5513e-01, -6.4258e-01, -2.0508e+00,
               2.9053e-01,  4.6948e-01,  9.4043e-01, -1.3984e+00,  7.9932e-01,
              -6.3037e-01,  1.0039e+00, -5.9766e-01, -1.3994e+00,  1.9834e+00,
              -1.7207e+00],
             [ 1.4531e+00, -8.1787e-02, -5.9131e-01,  7.8125e-03, -2.2930e+00,
              -9.3506e-01, -2.3193e-02, -5.5078e-01,  9.5508e-01,  1.4199e+00,
               8.1592e-01, -3.1226e-01,  7.8906e-01, -7.0605e-01, -2.6660e-01,
               5.6104e-01],
             [-4.6558e-01, -1.3799e+00,  1.2549e+00, -4.3237e-01,  1.0117e+00,
              -3.7109e-01,  2.0391e+00,  3.7769e-01, -1.8018e+00,  3.5181e-01,
               3.0020e+00, -7.7930e-01,  1.1855e+00, -1.2090e+00, -2.5537e-01,
               3.4814e-01],
             [-3.0879e+00,  1.8945e+00,  2.0996e+00,  1.3662e+00, -1.1846e+00,
               3.2930e+00,  3.8794e-01,  1.1621e+00,  1.5898e+00, -1.0791e+00,
              -3.7451e-01,  2.0977e+00, -8.4082e-01, -2.4487e-01,  3.4668e-01,
               5.7959e-01],
             [-2.9590e-01,  1.6094e+00,  3.0396e-01,  3.1226e-01, -2.4316e+00,
               4.2603e-02, -1.7949e+00,  7.4463e-02,  2.2344e+00, -2.1367e+00,
              -6.3330e-01, -1.1660e+00,  2.6270e-01, -2.1758e+00,  5.7495e-02,
               8.8074e-02],
             [-2.3145e+00,  1.6289e+00,  8.0859e-01, -1.4766e+00, -1.7505e-01,
               1.3652e+00,  1.1973e+00,  2.3594e+00,  1.9424e+00,  2.4365e-01,
              -6.1328e-01, -3.0605e+00,  2.3071e-01,  6.6309e-01,  2.2910e+00,
               6.8555e-01],
             [ 9.7461e-01,  4.2920e-01,  1.1230e-02, -1.5859e+00, -1.3779e+00,
              -1.2488e-01,  5.2148e-01,  1.9375e+00,  1.3262e+00, -1.4082e+00,
               1.1035e+00,  4.7803e-01,  1.5029e+00, -1.2090e+00, -1.0078e+00,
               5.0195e-01],
             [ 2.2930e+00,  3.0312e+00,  2.6172e+00, -9.6680e-02, -6.0059e-01,
               8.0566e-02,  3.1152e+00,  6.2207e-01, -2.8359e+00, -9.0039e-01,
              -2.4062e+00, -3.6562e+00,  1.1885e+00, -1.2236e+00,  1.7520e+00,
               1.1455e+00],
             [ 4.6338e-01, -8.0273e-01,  1.5947e+00, -2.0293e+00,  6.4697e-01,
               7.5000e-01, -8.2617e-01,  1.6406e-01, -1.3340e+00,  2.4536e-01,
              -7.5781e-01, -6.6309e-01, -1.7578e-02,  1.3809e+00,  4.7021e-01,
               1.0137e+00],
             [ 1.2354e-01,  6.3721e-01,  6.3574e-01,  1.5684e+00, -2.0996e+00,
               1.3633e+00,  1.2207e+00,  1.0234e+00, -3.3789e-01,  1.7783e+00,
              -2.0586e+00,  2.2422e+00,  7.5146e-01, -9.8682e-01, -1.7810e-01,
              -4.7559e-01],
             [ 3.1543e-01, -1.5957e+00,  6.6162e-01, -1.8457e+00, -1.3164e+00,
               1.3730e+00,  4.3311e-01, -1.0029e+00, -2.2422e+00,  1.7100e+00,
               6.0107e-01, -1.9805e+00, -2.3379e+00, -1.2412e+00,  1.4805e+00,
              -1.5747e-01],
             [ 1.1152e+00,  6.0742e-01, -1.7598e+00,  9.1455e-01,  8.3789e-01,
               2.4941e+00, -2.8945e+00, -2.3691e+00,  1.3555e+00, -2.5488e+00,
              -2.2095e-01, -1.8877e+00, -8.4668e-01, -1.9512e+00, -1.2627e+00,
               2.3555e+00],
             [ 1.0508e+00, -1.7881e+00, -3.1777e+00, -8.4668e-01,  1.1855e+00,
               4.3555e-01, -1.1780e-02, -7.7539e-01,  1.5273e+00,  2.6641e+00,
              -4.6509e-01, -1.3306e-01, -8.9160e-01, -1.8516e+00, -2.9512e+00,
              -4.7217e-01],
             [-1.3320e+00,  1.9326e+00,  7.3145e-01,  1.7080e+00, -1.0986e+00,
               1.6230e+00,  8.1250e-01,  1.2979e+00, -6.7969e-01,  1.0195e+00,
              -6.5527e-01,  3.1250e+00,  2.0293e+00,  1.1133e-01, -1.5371e+00,
              -1.5996e+00],
             [-4.4434e-01,  1.4082e+00, -9.6680e-02,  2.2812e+00,  8.9941e-01,
               8.4375e-01,  3.5078e+00,  1.0625e+00,  2.4402e-01,  1.4023e+00,
              -1.7324e+00, -2.8340e+00, -7.8467e-01,  2.9785e-01, -2.9785e-01,
              -3.5400e-02],
             [ 7.9150e-01, -9.3896e-01, -1.3662e+00, -3.6328e-01,  1.4727e+00,
               2.2559e+00, -4.6338e-01, -1.0410e+00, -1.5723e+00, -1.0088e+00,
              -1.4551e+00,  1.4551e+00,  1.1123e+00, -7.1191e-01, -1.8291e+00,
               6.7578e-01],
             [-1.1191e+00, -2.3125e+00, -5.4590e-01, -1.7900e+00, -2.0000e+00,
              -6.5125e-02,  1.0469e+00,  2.1523e+00, -1.1797e+00,  6.4453e-01,
              -2.0625e+00,  1.4268e+00, -1.7666e+00,  5.8105e-02,  1.1230e-02,
              -6.2012e-01],
             [ 5.5664e-01, -7.9443e-01,  1.7246e+00, -2.1250e+00,  6.1230e-01,
               8.6963e-01, -6.7578e-01,  1.1176e-01, -1.3330e+00,  3.0859e-01,
              -7.8271e-01, -4.9536e-01, -5.2246e-02,  1.4199e+00,  4.0186e-01,
               1.0068e+00],
             [ 1.2412e+00,  2.6250e+00,  1.5459e+00, -3.8208e-02,  8.2031e-01,
               1.3242e+00, -4.4971e-01,  1.3320e+00,  5.5615e-01, -7.9150e-01,
              -2.8984e+00, -3.8940e-01, -1.6582e+00, -1.5273e+00, -6.8457e-01,
              -2.5781e+00],
             [-6.7432e-01,  1.3633e+00,  9.2285e-01, -2.2051e+00, -1.2227e+00,
              -1.9287e-01, -2.4570e+00, -1.8738e-01,  1.5293e+00, -1.4258e+00,
               2.6582e+00, -5.6885e-02, -1.0723e+00,  3.8818e-02, -2.3584e-01,
              -1.2100e+00],
             [ 7.5977e-01,  2.9414e+00,  2.2668e-01,  3.4277e-01, -2.3694e-01,
               5.8398e-01, -5.3125e-01,  2.5840e+00, -1.4734e-01,  5.5713e-01,
              -2.7283e-02, -3.1172e+00, -1.1487e-01, -5.7764e-01, -5.6836e-01,
              -2.4182e-01],
             [ 8.6963e-01, -3.1738e-03,  7.2266e-01,  8.6035e-01,  9.0576e-01,
               2.2617e+00, -6.7773e-01, -7.7979e-01,  1.9844e+00, -1.4355e-01,
              -1.6455e+00, -4.3896e-01,  3.3984e-01,  1.5352e+00, -1.6821e-01,
               1.5898e+00],
             [ 3.1309e+00,  2.3984e+00,  1.2275e+00, -4.1187e-01,  1.3757e-01,
              -2.6484e+00, -1.2480e+00,  3.9734e-02, -3.3521e-01,  1.1094e+00,
              -4.1064e-01, -1.3662e+00,  7.6270e-01,  7.5977e-01,  4.8096e-01,
              -8.7207e-01],
             [ 8.5596e-01,  1.0342e+00,  2.1289e+00, -1.4844e+00,  6.6797e-01,
               7.9834e-02, -7.1680e-01,  1.6826e+00, -6.3965e-02,  6.5771e-01,
              -8.8379e-01, -6.9043e-01, -1.3086e-01, -9.6777e-01,  1.6260e-01,
              -3.4062e+00],
             [-2.8149e-01, -1.1504e+00, -5.9814e-02, -7.3975e-01,  1.4941e+00,
               1.4697e+00, -3.5498e-01,  1.9971e+00, -1.1338e+00,  1.1152e+00,
               2.9175e-01, -1.1406e+00, -1.1316e-01, -5.5176e-01,  1.0430e+00,
               1.2236e+00],
             [-9.2090e-01, -1.0625e+00,  6.8750e-01,  9.4727e-01,  3.5938e-01,
              -7.9688e-01,  1.2793e+00, -1.2246e+00,  9.2139e-01,  1.8164e-01,
               2.2109e+00, -8.0469e-01,  2.0254e+00, -7.6514e-01, -1.2217e+00,
              -2.1973e+00],
             [ 1.0254e+00, -1.4336e+00,  4.8438e-01,  1.9062e+00, -2.0059e+00,
               8.2666e-01, -1.0996e+00,  4.0601e-01, -9.1016e-01,  6.1182e-01,
              -1.8027e+00,  8.3008e-01, -4.4019e-01,  2.2246e+00, -1.1963e+00,
              -4.4067e-01],
             [ 1.2031e+00,  4.8242e-01,  1.1416e+00, -9.1650e-01,  2.1992e+00,
               1.0195e+00,  1.6006e+00, -6.4551e-01, -2.8789e+00, -1.3760e+00,
               2.7402e+00, -1.5771e+00,  1.1816e+00,  2.7905e-01,  2.4453e+00,
               1.6553e-01]],

            [[-1.1113e+00, -1.5869e-01, -1.8574e+00, -2.8965e+00,  8.3057e-01,
               9.8047e-01,  2.5527e+00, -5.4199e-01,  5.8655e-02,  2.2949e-02,
              -1.4600e+00, -1.9414e+00,  8.1055e-01,  9.9219e-01,  6.9775e-01,
              -2.0938e+00],
             [-7.2021e-01,  1.7871e+00, -2.1875e-01,  1.1055e+00,  1.7764e+00,
              -1.3828e+00,  8.8916e-01,  1.7969e-01,  5.8594e-02, -7.3145e-01,
              -2.7598e+00,  1.6240e+00,  2.7461e+00,  1.0508e+00,  3.5693e-01,
               1.3252e+00],
             [ 4.3115e-01,  1.7773e-01,  1.9893e+00, -7.9053e-01, -1.6006e+00,
               1.3555e+00,  2.5059e+00,  1.4043e+00, -2.3389e-01,  1.8848e-01,
              -5.0098e-01,  2.1719e+00, -2.2441e+00,  6.6504e-01, -1.0557e+00,
               9.1162e-01],
             [ 2.8833e-01, -6.9629e-01,  1.7656e+00,  5.6592e-01, -1.4287e+00,
               2.1094e+00,  3.0898e+00,  8.7744e-01,  1.8838e+00,  1.0547e+00,
               4.2847e-01,  1.4746e+00, -1.3408e+00, -2.4194e-01,  5.6836e-01,
               6.4600e-01],
             [-5.4834e-01,  1.0459e+00,  6.4990e-01, -9.4922e-01,  7.3242e-01,
              -3.0518e-01,  3.6250e+00, -2.2125e-02, -1.8994e+00, -4.3359e-01,
               5.4590e-01, -1.1230e+00, -1.6602e-02,  1.5918e+00,  3.4521e-01,
              -2.2461e+00],
             [ 3.6387e+00, -5.9375e-01, -8.7305e-01, -3.0991e-02, -1.2817e-02,
              -1.7998e+00,  1.3340e+00,  5.6641e-01, -2.9414e+00, -1.9873e+00,
               1.1582e+00,  5.1855e-01,  1.1523e+00, -4.7754e-01, -6.8311e-01,
               3.5742e-01],
             [-2.1484e+00, -2.0039e+00, -1.1631e+00, -1.4492e+00, -8.3350e-01,
               3.2422e+00,  2.5371e+00, -3.1201e-01,  2.9707e+00,  1.5625e-02,
              -4.3701e-02, -2.6133e+00, -1.6479e-02,  1.9121e+00,  1.6260e+00,
              -1.4258e+00],
             [ 1.7412e+00, -9.1602e-01,  3.3276e-01, -1.8828e+00,  9.7461e-01,
               7.2168e-01,  2.2949e+00,  6.2842e-01,  1.2949e+00, -9.1260e-01,
               2.5781e+00,  1.5942e-01, -2.1533e-01, -3.3643e-01, -7.8003e-02,
               1.6455e-01],
             [ 2.1643e-01,  1.4658e+00, -7.2803e-01, -1.1807e+00, -1.9219e+00,
               1.1395e-01,  5.0049e-02, -1.8555e-02,  1.8184e+00, -5.9375e-01,
              -1.2324e+00, -2.8076e-01,  1.2432e+00,  2.2598e+00, -2.1621e+00,
               1.8066e+00],
             [ 3.7617e+00,  5.2148e-01, -8.1738e-01, -8.3105e-01,  6.9824e-01,
               2.4414e+00,  4.0112e-01, -4.8767e-02, -3.4316e+00, -2.0569e-01,
              -3.2852e+00,  2.3193e-01, -1.9043e-01,  4.3677e-01,  2.5195e+00,
              -2.1313e-01],
             [ 9.6680e-02,  1.5596e+00,  7.9980e-01, -1.1475e+00, -1.2910e+00,
              -1.1338e+00,  2.0312e+00,  1.6719e+00, -3.2500e+00,  2.5098e+00,
               3.9746e-01, -2.1484e-02,  5.6592e-01, -1.3115e+00, -8.4619e-01,
               1.1875e+00],
             [-2.9062e+00, -2.3555e+00,  1.4678e+00, -1.7930e+00, -5.8984e-01,
              -2.2324e+00,  2.6816e+00,  1.0586e+00,  2.6914e+00, -1.3701e+00,
              -2.5449e+00,  4.0405e-01,  6.3770e-01,  8.6230e-01,  8.4595e-02,
               1.1104e+00],
             [ 1.8828e+00,  8.8184e-01,  5.7812e-01, -1.5791e+00,  4.6313e-01,
              -1.9502e+00,  1.5264e+00,  3.3789e+00,  8.5986e-01,  1.7666e+00,
               2.3320e+00,  1.5686e-02, -1.0430e+00, -4.4971e-01, -1.0234e+00,
              -8.2910e-01],
             [ 3.1201e-01,  5.7080e-01, -2.5977e-01, -4.5337e-01, -1.4414e+00,
              -4.2651e-01,  5.1172e-01,  2.8516e-01,  2.2207e+00,  2.4939e-01,
               7.7295e-01, -7.0605e-01, -4.3086e+00, -1.4868e-01, -7.0459e-01,
              -9.3701e-01],
             [-1.2363e+00, -5.0659e-02,  1.0479e+00,  9.8694e-02, -7.1582e-01,
              -2.5586e+00,  1.7871e+00, -1.1963e+00, -1.3369e+00,  7.8979e-02,
              -2.5708e-01, -1.5107e+00, -1.3662e+00,  1.6562e+00, -1.1562e+00,
              -2.3364e-01],
             [-2.9199e-01,  2.2051e+00, -8.7549e-01,  1.7236e+00, -7.2327e-03,
               5.0488e-01,  4.4629e-01,  1.3369e+00,  2.8730e+00,  1.4189e+00,
              -6.9946e-02,  4.2236e-01, -1.4883e+00,  1.8281e+00,  1.6953e+00,
              -2.3010e-01],
             [ 1.1816e+00,  2.8340e+00, -2.7051e-01,  1.1953e+00, -3.5078e+00,
              -6.3818e-01,  7.2510e-02, -7.5049e-01,  2.5391e+00,  8.3447e-01,
               1.4951e+00, -1.1719e-02, -1.3730e+00,  7.0068e-02,  1.3682e+00,
               8.3398e-01],
             [ 1.7148e+00,  1.7598e+00,  9.1113e-01,  9.4336e-01, -8.4277e-01,
               4.2065e-01,  2.1973e+00, -1.8594e+00,  2.0020e+00, -2.2400e-01,
               1.3000e-01, -5.4102e-01, -2.5269e-02, -2.4414e-01,  2.5830e-01,
               1.9424e+00],
             [ 5.2148e-01, -1.2822e+00, -7.5342e-01, -2.1191e+00,  1.6484e+00,
               3.5229e-01,  2.4451e-01,  5.4395e-01,  2.2793e+00,  2.2461e+00,
               1.9453e+00,  2.6523e+00,  3.3809e+00, -1.3745e-01,  8.3545e-01,
               2.0654e-01],
             [ 9.6533e-01, -8.0615e-01,  1.1992e+00, -1.6602e+00, -1.9629e+00,
               6.9336e-01,  2.1719e+00,  6.0938e-01, -1.3535e+00,  2.1465e+00,
               7.1289e-01, -2.8027e+00,  1.2236e+00,  9.2188e-01,  1.6689e+00,
               1.3037e+00],
             [ 4.5959e-02,  1.5540e-01,  1.9502e+00,  9.2871e-01,  4.9854e-01,
               7.3145e-01, -1.4150e+00, -1.8018e-01,  7.3730e-01,  8.3691e-01,
              -2.1436e-01, -4.2871e-01, -7.1484e-01, -1.2520e+00, -1.2949e+00,
              -3.2949e+00],
             [ 2.2188e+00,  8.8281e-01,  1.0645e+00, -3.9180e+00,  8.3984e-01,
              -3.0786e-01, -7.6709e-01,  1.8340e+00, -2.1934e+00, -4.1162e-01,
              -2.2046e-01,  8.9600e-02,  1.4736e+00, -1.0020e+00,  7.4609e-01,
              -3.5986e-01],
             [-2.1387e+00,  2.9688e+00, -2.0156e+00, -1.0410e+00, -3.3228e-01,
              -1.9385e-01,  2.1406e+00,  2.0371e+00, -2.9370e-01,  2.7002e-01,
               3.2266e+00,  1.0352e+00,  2.2852e+00, -1.9404e+00, -3.2983e-01,
               9.0820e-01],
             [-2.7930e+00,  1.9263e-01, -1.3828e+00, -6.3184e-01, -9.0625e-01,
               1.5469e+00,  3.3545e-01,  5.3174e-01, -3.7109e-01,  1.2100e+00,
              -7.1289e-01,  1.5771e-01, -2.8984e+00, -1.4795e+00, -2.7637e+00,
               3.2520e-01],
             [-2.6719e+00,  1.5449e+00, -6.3867e-01, -1.3203e+00, -6.2378e-02,
              -1.7334e+00,  1.1514e+00, -2.1777e-01,  4.7852e-02, -1.3193e+00,
               1.2031e+00,  1.2559e+00,  1.9414e+00, -7.0801e-01, -6.6113e-01,
               1.2324e+00],
             [-3.1226e-01, -4.7070e-01, -8.4424e-01, -2.7124e-01, -3.0103e-01,
               4.6802e-01,  1.0986e+00, -3.6670e-01, -2.3184e+00,  1.4951e+00,
               2.2773e+00,  5.4810e-02, -1.2378e-01,  4.3018e-01,  5.0830e-01,
               1.2881e+00],
             [ 1.9111e+00,  5.5420e-01, -5.1074e-01, -1.1823e-01, -1.0771e+00,
               6.0352e-01,  9.3506e-01, -2.2305e+00,  1.3779e+00,  4.5459e-01,
               6.4062e-01,  1.8555e+00, -1.2041e+00, -3.2275e-01, -5.3076e-01,
               2.0625e+00],
             [ 3.6108e-01, -2.3887e+00,  1.4453e+00,  1.0225e+00, -1.0898e+00,
              -2.5664e+00,  0.0000e+00, -1.3535e+00,  4.6362e-01, -9.6387e-01,
               1.5254e+00, -2.2827e-01, -3.9404e-01, -6.5527e-01,  5.9863e-01,
              -9.6289e-01],
             [ 9.5410e-01,  1.2256e+00,  2.1699e+00, -1.4648e+00,  6.3525e-01,
               8.8501e-02, -7.3779e-01,  1.6621e+00,  2.2021e-01,  8.8672e-01,
              -1.0078e+00, -5.9619e-01, -4.0234e-01, -9.0381e-01,  2.1423e-01,
              -3.3594e+00],
             [ 2.0508e+00, -2.7661e-01,  2.7969e+00,  1.4902e+00,  4.8779e-01,
               6.0303e-01, -1.2119e+00,  1.5605e+00,  8.3496e-01,  9.0625e-01,
               1.3398e+00, -1.2402e+00,  2.4756e-01,  1.8584e+00,  8.3203e-01,
              -5.7129e-01]]], dtype=torch.float16)]



.. GENERATED FROM PYTHON SOURCE LINES 184-185

Let's see the results.

.. GENERATED FROM PYTHON SOURCE LINES 185-191

.. code-block:: Python


    data = report_cmp.data
    df = pandas.DataFrame(data)
    piv = df.pivot(index=("run_index", "run_name"), columns="ref_name", values="abs")
    print(piv)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ref_name                decoder.0  decoder.attention.0  decoder.feed_forward.0  decoder.norm_1.0  decoder.norm_2.0   model.0
    run_index run_name                                                                                                          
    1         embedding      3.345703             3.190369                3.465820          2.735352          2.765625  3.345703
    2         embedding_1    3.384766             3.206787                3.153809          3.176758          3.212891  3.384766
    3         add_8          1.001953             4.054932                4.116699          1.782227          1.835938  1.001953
    4         layer_norm     1.834961             3.049072                3.110840          0.000000          0.523926  1.834961
    5         linear         4.498047             2.390625                1.978516          3.373047          3.315430  4.498047
    6         linear_1       4.451172             1.872314                2.030884          3.132812          3.193359  4.451172
    7         linear_2       4.710938             1.530762                2.179688          3.556641          3.539062  4.710938
    17        matmul_1       4.721436             1.466797                1.974854          3.567139          3.549561  4.721436
    18        linear_3       4.803711             2.524414                2.449219          3.710938          3.666016  4.803711
    19        linear_4       4.684326             1.819794                1.836487          4.004883          3.946289  4.684326
    20        linear_5       4.797852             1.982422                2.142090          3.942383          3.961914  4.797852
    30        matmul_3       4.466797             1.799805                1.421753          3.061157          3.043579  4.466797
    32        val_48         4.104004             0.154053                1.033752          2.949707          2.932129  4.104004
    33        linear_6       4.203430             0.000488                1.004150          3.049133          3.031555  4.203430
    34        add_115        0.979980             4.160400                4.222168          1.769531          1.766602  0.979980
    35        layer_norm_1   1.720703             3.031494                3.093262          0.523926          0.001953  1.720703
    39        val_54         4.348938             0.916260                0.088135          3.194641          3.177063  4.348938
    40        linear_8       4.265076             1.004150                0.000488          3.110779          3.093201  4.265076
    41        add_136        0.001953             4.203369                4.265137          1.834961          1.720703  0.001953




.. GENERATED FROM PYTHON SOURCE LINES 192-193

Let's clean a little bit.

.. GENERATED FROM PYTHON SOURCE LINES 193-196

.. code-block:: Python

    piv[piv >= 1] = np.nan
    print(piv.dropna(axis=0, how="all"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ref_name                decoder.0  decoder.attention.0  decoder.feed_forward.0  decoder.norm_1.0  decoder.norm_2.0   model.0
    run_index run_name                                                                                                          
    4         layer_norm          NaN                  NaN                     NaN          0.000000          0.523926       NaN
    32        val_48              NaN             0.154053                     NaN               NaN               NaN       NaN
    33        linear_6            NaN             0.000488                     NaN               NaN               NaN       NaN
    34        add_115        0.979980                  NaN                     NaN               NaN               NaN  0.979980
    35        layer_norm_1        NaN                  NaN                     NaN          0.523926          0.001953       NaN
    39        val_54              NaN             0.916260                0.088135               NaN               NaN       NaN
    40        linear_8            NaN                  NaN                0.000488               NaN               NaN       NaN
    41        add_136        0.001953                  NaN                     NaN               NaN               NaN  0.001953




.. GENERATED FROM PYTHON SOURCE LINES 197-198

We can identity which results is mapped to which expected tensor.

.. GENERATED FROM PYTHON SOURCE LINES 200-202

Picture of the model
++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 202-206

.. code-block:: Python


    onx = onnx.load("plot_dump_intermediate_results.onnx")
    doc.plot_dot(onx)




.. image-sg:: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_001.png
   :alt: plot dump intermediate results
   :srcset: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 207-208

.. code-block:: Python

    doc.plot_legend("steal and dump\nintermediate\nresults", "steal_forward", "blue")



.. image-sg:: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_002.png
   :alt: plot dump intermediate results
   :srcset: /auto_examples/images/sphx_glr_plot_dump_intermediate_results_002.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 5.001 seconds)


.. _sphx_glr_download_auto_examples_plot_dump_intermediate_results.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_dump_intermediate_results.ipynb <plot_dump_intermediate_results.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_dump_intermediate_results.py <plot_dump_intermediate_results.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_dump_intermediate_results.zip <plot_dump_intermediate_results.zip>`


.. include:: plot_dump_intermediate_results.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
