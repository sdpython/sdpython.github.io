<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="API of onnx_diagnostic" href="../api/index.html"><link rel="prev" title="Coverage of the Patches" href="patches_coverage.html">
        <link rel="prefetch" href="../_static/logo.png" as="image">

    <!-- Generated with Sphinx 9.1.0 and Furo 2025.12.19 -->
        <title>Patches Diff - onnx-diagnostic 0.9.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a7360d90" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">onnx-diagnostic 0.9.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">onnx-diagnostic 0.9.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="../patches.html">Patches Explained</a><input aria-label="Toggle navigation of Patches Explained" checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">Exporter Status</a><input aria-label="Toggle navigation of Exporter Status" checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="exported_program_dynamic.html">Exported Programs with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="exporter_dynamic.html">Exported ONNX with Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="patches_coverage.html">Coverage of the Patches</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">Patches Diff</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API of onnx_diagnostic</a><input aria-label="Toggle navigation of API of onnx_diagnostic" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/ci_models/index.html">onnx_diagnostic.ci_models</a><input aria-label="Toggle navigation of onnx_diagnostic.ci_models" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/ci_models/ci_helpers.html">onnx_diagnostic.ci_models.ci_helpers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ci_models/export_qwen25_vl.html">onnx_diagnostic.ci_models.export_qwen25_vl</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/export/index.html">onnx_diagnostic.export</a><input aria-label="Toggle navigation of onnx_diagnostic.export" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/export/api.html">onnx_diagnostic.export.api</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/control_flow_onnx.html">onnx_diagnostic.export.control_flow_onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/dynamic_shapes.html">onnx_diagnostic.export.dynamic_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/onnx_plug.html">onnx_diagnostic.export.onnx_plug</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/shape_helper.html">onnx_diagnostic.export.shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/validate.html">onnx_diagnostic.export.validate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/export/cf_simple_loop_for.html">onnx_diagnostic.export.cf_simple_loop_for</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/helpers/index.html">onnx_diagnostic.helpers</a><input aria-label="Toggle navigation of onnx_diagnostic.helpers" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/args_helper.html">onnx_diagnostic.helpers.args_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/bench_run.html">onnx_diagnostic.helpers.bench_run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/cache_helper.html">onnx_diagnostic.helpers.cache_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/config_helper.html">onnx_diagnostic.helpers.config_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/doc_helper.html">onnx_diagnostic.helpers.doc_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/dot_helper.html">onnx_diagnostic.helpers.dot_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/fake_tensor_helper.html">onnx_diagnostic.helpers.fake_tensor_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/graph_helper.html">onnx_diagnostic.helpers.graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/helper.html">onnx_diagnostic.helpers.helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/_log_helper.html">onnx_diagnostic.helpers._log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/log_helper.html">onnx_diagnostic.helpers.log_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/memory_peak.html">onnx_diagnostic.helpers.memory_peak</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/mini_onnx_builder.html">onnx_diagnostic.helpers.mini_onnx_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/model_builder_helper.html">onnx_diagnostic.helpers.model_builder_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/onnx_helper.html">onnx_diagnostic.helpers.onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/optim_helper.html">onnx_diagnostic.helpers.optim_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/ort_session.html">onnx_diagnostic.helpers.ort_session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/rt_helper.html">onnx_diagnostic.helpers.rt_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_fx_graph_helper.html">onnx_diagnostic.helpers.torch_fx_graph_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/helpers/torch_helper.html">onnx_diagnostic.helpers.torch_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/investigate/index.html">onnx_diagnostic.investigate</a><input aria-label="Toggle navigation of onnx_diagnostic.investigate" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/investigate/input_observer.html">onnx_diagnostic.investigate.input_observer</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">onnx_diagnostic.reference</a><input aria-label="Toggle navigation of onnx_diagnostic.reference" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">onnx_diagnostic.reference.ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.ops" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">onnx_diagnostic.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">onnx_diagnostic.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">onnx_diagnostic.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">onnx_diagnostic.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">onnx_diagnostic.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">onnx_diagnostic.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">onnx_diagnostic.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">onnx_diagnostic.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">onnx_diagnostic.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">onnx_diagnostic.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">onnx_diagnostic.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">onnx_diagnostic.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">onnx_diagnostic.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">onnx_diagnostic.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">onnx_diagnostic.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">onnx_diagnostic.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">onnx_diagnostic.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">onnx_diagnostic.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">onnx_diagnostic.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">onnx_diagnostic.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">onnx_diagnostic.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">onnx_diagnostic.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">onnx_diagnostic.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/torch_ops/index.html">onnx_diagnostic.reference.torch_ops</a><input aria-label="Toggle navigation of onnx_diagnostic.reference.torch_ops" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/access_ops.html">onnx_diagnostic.reference.torch_ops.access_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/binary_ops.html">onnx_diagnostic.reference.torch_ops.binary_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/controlflow_ops.html">onnx_diagnostic.reference.torch_ops.controlflow_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/generator_ops.html">onnx_diagnostic.reference.torch_ops.generator_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/nn_ops.html">onnx_diagnostic.reference.torch_ops.nn_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/other_ops.html">onnx_diagnostic.reference.torch_ops.other_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/reduce_ops.html">onnx_diagnostic.reference.torch_ops.reduce_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/sequence_ops.html">onnx_diagnostic.reference.torch_ops.sequence_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/shape_ops.html">onnx_diagnostic.reference.torch_ops.shape_ops</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/torch_ops/unary_ops.html">onnx_diagnostic.reference.torch_ops.unary_ops</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">onnx_diagnostic.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">onnx_diagnostic.reference.quantized_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">onnx_diagnostic.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/report_results_comparison.html">onnx_diagnostic.reference.report_results_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/torch_evaluator.html">onnx_diagnostic.reference.torch_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/tasks/index.html">onnx_diagnostic.tasks</a><input aria-label="Toggle navigation of onnx_diagnostic.tasks" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/automatic_speech_recognition.html">onnx_diagnostic.tasks.automatic_speech_recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/fill_mask.html">onnx_diagnostic.tasks.fill_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/feature_extraction.html">onnx_diagnostic.tasks.feature_extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_classification.html">onnx_diagnostic.tasks.image_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/image_text_to_text.html">onnx_diagnostic.export.image_text_to_text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/mixture_of_expert.html">onnx_diagnostic.tasks.mixture_of_expert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/object_detection.html">onnx_diagnostic.tasks.object_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/sentence_similarity.html">onnx_diagnostic.tasks.sentence_similarity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/summarization.html">onnx_diagnostic.tasks.summarization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_classification.html">onnx_diagnostic.tasks.text_classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_generation.html">onnx_diagnostic.tasks.text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text_to_image.html">onnx_diagnostic.tasks.text_to_image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/text2text_generation.html">onnx_diagnostic.tasks.text2text_generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/tasks/zero_shot_image_classification.html">onnx_diagnostic.tasks.zero_shot_image_classification</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_export_patches/index.html">onnx_diagnostic.torch_export_patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/eval/index.html">onnx_diagnostic.torch_export_patches.eval</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.eval" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/eval/model_cases.html">onnx_diagnostic.torch_export_patches.eval.model_cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_errors.html">onnx_diagnostic.torch_export_patches.onnx_export_errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/onnx_export_serialization.html">onnx_diagnostic.torch_export_patches.onnx_export_serialization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/patches/index.html">onnx_diagnostic.torch_export_patches.patches</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.patches" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_torch.html">onnx_diagnostic.torch_export_patches.patches.patch_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/patches/patch_transformers.html">onnx_diagnostic.torch_export_patches.patches.patch_transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_details.html">onnx_diagnostic.torch_export_patches.patch_details</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_expressions.html">onnx_diagnostic.torch_export_patches.patch_expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_inputs.html">onnx_diagnostic.torch_export_patches.patch_inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module.html">onnx_diagnostic.torch_export_patches.patch_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_export_patches/patch_module_helper.html">onnx_diagnostic.torch_export_patches.patch_module_helper</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_export_patches/serialization/index.html">onnx_diagnostic.torch_export_patches.serialization</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_export_patches.serialization" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/diffusers_impl.html">onnx_diagnostic.torch_export_patches.serialization.diffusers_impl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_export_patches/serialization/transformers_impl.html">onnx_diagnostic.torch_export_patches.serialization.transformers_impl</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">onnx_diagnostic.torch_models</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/code_sample.html">onnx_diagnostic.torch_models.code_sample</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_models/hghub/index.html">onnx_diagnostic.torch_models.hghub</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_models.hghub" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_api.html">onnx_diagnostic.torch_models.hghub.hub_api</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/hub_data.html">onnx_diagnostic.torch_models.hghub.hub_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_models/hghub/model_inputs.html">onnx_diagnostic.torch_models.hghub.model_inputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llms.html">onnx_diagnostic.torch_models.llms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/validate.html">onnx_diagnostic.torch_models.validate</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_onnx/index.html">onnx_diagnostic.torch_onnx</a><input aria-label="Toggle navigation of onnx_diagnostic.torch_onnx" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/compare.html">onnx_diagnostic.torch_onnx.compare</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/runtime_info.html">onnx_diagnostic.torch_onnx.runtime_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs.html">onnx_diagnostic.torch_onnx.sbs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_onnx/sbs_dataclasses.html">onnx_diagnostic.torch_onnx.sbs_dataclasses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/typing.html">onnx_diagnostic.typing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">onnx_diagnostic.ext_test_case</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cmds/index.html">Command Lines</a><input aria-label="Toggle navigation of Command Lines" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cmds/compare.html">-m onnx_diagnostic compare … compares two models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/config.html">-m onnx_diagnostic config … prints the config for a model id</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/optimize.html">-m onnx_diagnostic optimize … optimizes an onnx model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/partition.html">-m onnx_diagnostic partition … move layer nodes in local functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/sbs.html">-m onnx_diagnostic sbs … runs a side-by-side torch/onnx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cmds/validate.html">-m onnx_diagnostic validate … validate a model id</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_examples/index.html">Examples Gallery</a><input aria-label="Toggle navigation of Examples Gallery" class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_dump_intermediate_results.html">Dumps intermediate results of a torch model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_with_args_kwargs.html">Dynamic Shapes for <code class="docutils literal notranslate"><span class="pre">*args</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_llm_patched.html">Export Tiny-LLM with patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_phi2.html">Export microsoft/phi-2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_with_dynamic_cache.html">Export with DynamicCache and guessed dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_llm_dim01.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_llm_dim01_onnx.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_llm_dim01_onnx_custom.html">Export with dynamic dimensions in <code class="docutils literal notranslate"><span class="pre">{0,1}</span></code> into ONNX (custom)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_locate_issue.html">Find and fix an export issue due to dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_failing_model_extract.html">Find where a model is failing by running submodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_failing_reference_evaluator.html">Intermediate results with (ONNX) ReferenceEvaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_failing_onnxruntime_evaluator.html">Intermediate results with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_tiny_llm.html">Steel method forward to guess inputs and dynamic shapes (with Tiny-LLM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_export_hub_codellama.html">Test the export on untrained models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_recipes/index.html">Common Export Issues</a><input aria-label="Toggle navigation of Common Export Issues" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_dim1.html">0, 1, 2 for a Dynamic Dimension in the dummy example to export a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_what.html">Builds dynamic shapes from any input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_max.html">Cannot export <code class="docutils literal notranslate"><span class="pre">torch.sym_max(x.shape[0],</span> <span class="pre">y.shape[0])</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_python_int.html">Do not use python int with dynamic shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_cond.html">Export a model with a control flow (If)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_nonzero.html">Half certain nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_dynamic_shapes_json.html">JSON returns list when the original dynamic shapes are list or tuple</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_recipes/plot_export_with_dynamic.html">Use DYNAMIC or AUTO when exporting if dynamic shapes has constraints</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_technical/index.html">Technical Details</a><input aria-label="Toggle navigation of Technical Details" class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_broadcast_export_issue.html">Dynamic Shapes and Broadcasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_simple_for_loop.html">Export with loops</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_generate.html">From a LLM to processing a prompt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_gemm_or_matmul_add.html">Gemm or Matmul + Add</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_layer_norm_discrepancies.html">LayerNormalization implementation cannot be exchanged</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_technical/plot_parallelized_reduction.html">Reproducible Parallelized Reduction is difficult</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/status/patches_diff.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="patches-diff">
<span id="l-patch-diff"></span><h1>Patches Diff<a class="headerlink" href="#patches-diff" title="Link to this heading">¶</a></h1>
<p>Patches are not always needed to export a LLM.
Most of the time, only serialization function are needed to export
a LLM with cache (<code class="docutils literal notranslate"><span class="pre">DynamicCache</span></code>, …).
Function <a class="reference internal" href="../api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_additional_serialization_functions</span></code></a>
is enough in many cases.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_additional_serialization_functions</span>

<span class="k">with</span> <span class="n">register_additional_serialization_functions</span><span class="p">(</span><span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>Function <a class="reference internal" href="../api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.torch_export_patches" title="onnx_diagnostic.torch_export_patches.torch_export_patches"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch_export_patches</span></code></a>
helps fixing some issues for many models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches</span><span class="w"> </span><span class="kn">import</span> <span class="n">torch_export_patches</span>

<span class="k">with</span> <span class="n">torch_export_patches</span><span class="p">(</span><span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>Class <a class="reference internal" href="../api/torch_export_patches/patch_details.html#onnx_diagnostic.torch_export_patches.patch_details.PatchDetails" title="onnx_diagnostic.torch_export_patches.patch_details.PatchDetails"><code class="xref py py-class docutils literal notranslate"><span class="pre">PatchDetails</span></code></a>
gives an example on how to retrieve the list of involded patches for a specific model.
Those patches belong to the following list which depends on <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">transformers</a> and
<a class="reference external" href="https://pytorch.org/">pytorch</a> versions.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="mf">2.11.0</span><span class="o">.</span><span class="n">dev20260204</span><span class="o">+</span><span class="n">cu130</span> <span class="mf">5.0.1</span><span class="o">.</span><span class="n">dev0</span>
</pre></div>
</div>
<p>Those two versions leads to the following list of patches.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches.patch_details</span><span class="w"> </span><span class="kn">import</span> <span class="n">PatchDetails</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnx_diagnostic.torch_export_patches</span><span class="w"> </span><span class="kn">import</span> <span class="n">torch_export_patches</span>

<span class="n">details</span> <span class="o">=</span> <span class="n">PatchDetails</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch_export_patches</span><span class="p">(</span>
    <span class="n">patch_transformers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">patch_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">patch_diffusers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">patch_details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">pass</span>
<span class="k">for</span> <span class="n">patch</span> <span class="ow">in</span> <span class="n">details</span><span class="o">.</span><span class="n">patched</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;* </span><span class="si">{</span><span class="n">patch</span><span class="o">.</span><span class="n">family</span><span class="si">}</span><span class="s2"> - </span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="n">patch</span><span class="o">.</span><span class="n">function_to_patch</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;__name__&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">patch</span><span class="o">.</span><span class="n">function_to_patch</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">()</span>
<span class="k">for</span> <span class="n">patch</span> <span class="ow">in</span> <span class="n">details</span><span class="o">.</span><span class="n">patched</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">patch</span><span class="o">.</span><span class="n">function_to_patch</span> <span class="o">==</span> <span class="n">patch</span><span class="o">.</span><span class="n">patch</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">rst</span> <span class="o">=</span> <span class="n">patch</span><span class="o">.</span><span class="n">format_diff</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">&quot;rst&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">rst</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<ul class="simple">
<li><p>sympy - sympy.core.numbers.IntegerConstant.name</p></li>
<li><p>torch - infer_size</p></li>
<li><p>torch - _broadcast_shapes</p></li>
<li><p>torch - _constrain_user_specified_dimhint_range</p></li>
<li><p>torch - _broadcast_in_dim_meta</p></li>
<li><p>torch - _maybe_broadcast</p></li>
<li><p>torch - _evaluate_expr</p></li>
<li><p>patch_transformers - dynamic_rope_update</p></li>
<li><p>transformers - sdpa_mask</p></li>
<li><p>transformers - eager_mask</p></li>
<li><p>transformers - sdpa_attention_forward</p></li>
<li><p>auto/patch_transformers - lazy_initialization</p></li>
<li><p>auto/patch_transformers - relative_pos</p></li>
<li><p>auto/patch_transformers - relative_positional_attention</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - get_placeholder_mask</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - _cache_dependant_input_preparation</p></li>
<li><p>auto/patch_transformers - _cache_dependant_input_preparation_exporting</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - prepare_inputs_for_generation</p></li>
<li><p>auto/patch_transformers - get_placeholder_mask</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - get_window_index</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - rot_pos_emb</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - Qwen3MoeSparseMoeBlock_forward_expert_loop</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - forward</p></li>
<li><p>auto/patch_transformers - eager_attention_forward</p></li>
<li><p>auto/patch_transformers - eager_attention_forward</p></li>
</ul>
<section id="sympy-sympy-core-numbers-integerconstant-name-patch-sympy-locals-lambda">
<h2>sympy: ‘sympy.core.numbers.IntegerConstant.name’ -&gt; _patch_sympy.&lt;locals&gt;.&lt;lambda&gt;<a class="headerlink" href="#sympy-sympy-core-numbers-integerconstant-name-patch-sympy-locals-lambda" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>sympy.core.numbers.IntegerConstant.name = lambda self: f&quot;IntCst{str(self)}&quot;
</pre></div>
</div>
</section>
<section id="torch-infer-size-patched-infer-size">
<h2>torch: infer_size -&gt; patched_infer_size<a class="headerlink" href="#torch-infer-size-patched-infer-size" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,10 +1,11 @@</span>
<span class="linenos"> 4</span><span class="gd">-def infer_size(a: Sequence[IntLikeType], b: Sequence[IntLikeType]) -&gt; tuple[IntLikeType, ...]:</span>
<span class="linenos"> 5</span><span class="gi">+def patched_infer_size(a, b):</span>
<span class="linenos"> 6</span><span class="gi">+    &quot;&quot;&quot;Patches ``torch._subclasses.fake_impls.infer_size``.&quot;&quot;&quot;</span>
<span class="linenos"> 7</span><span class="w"> </span>    from torch.fx.experimental.symbolic_shapes import guard_or_false
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="w"> </span>    dimsA = len(a)
<span class="linenos">10</span><span class="w"> </span>    dimsB = len(b)
<span class="linenos">11</span><span class="w"> </span>    ndim = max(dimsA, dimsB)
<span class="linenos">12</span><span class="gd">-    expandedSizes: list[IntLikeType] = [0] * ndim</span>
<span class="linenos">13</span><span class="gi">+    expandedSizes = [0] * ndim</span>
<span class="linenos">14</span><span class="w"> </span>    for i in range(ndim - 1, -1, -1):
<span class="linenos">15</span><span class="w"> </span>        offset = ndim - 1 - i
<span class="linenos">16</span><span class="w"> </span>        dimA = dimsA - 1 - offset
<span class="linenos">17</span><span class="gu">@@ -23,11 +24,21 @@</span>
<span class="linenos">18</span><span class="w"> </span>        # expression of an or statement as-is, without bool()&#39;ing it; if this
<span class="linenos">19</span><span class="w"> </span>        # were not the case, we&#39;d need to write this using torch.sym_or() or
<span class="linenos">20</span><span class="w"> </span>        # something like that).
<span class="linenos">21</span><span class="gd">-        torch._check(</span>
<span class="linenos">22</span><span class="gd">-            guard_or_false(sizeA == 1) or guard_or_false(sizeB == 1) or sizeA == sizeB,</span>
<span class="linenos">23</span><span class="gd">-            lambda: f&quot;The size of tensor a ({sizeA}) &quot;</span>
<span class="linenos">24</span><span class="gd">-            f&quot;must match the size of tensor b ({sizeB}) &quot;</span>
<span class="linenos">25</span><span class="gd">-            f&quot;at non-singleton dimension {i})&quot;,</span>
<span class="linenos">26</span><span class="gd">-        )</span>
<span class="linenos">27</span><span class="gd">-        expandedSizes[i] = sizeB if guard_or_false(sizeA == 1) else sizeA</span>
<span class="linenos">28</span><span class="gi">+        try:</span>
<span class="linenos">29</span><span class="gi">+            b1 = guard_or_false(sizeA == 1)</span>
<span class="linenos">30</span><span class="gi">+        except torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode:</span>
<span class="linenos">31</span><span class="gi">+            b1 = False</span>
<span class="linenos">32</span><span class="gi">+        try:</span>
<span class="linenos">33</span><span class="gi">+            b2 = guard_or_false(sizeB == 1)</span>
<span class="linenos">34</span><span class="gi">+        except torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode:</span>
<span class="linenos">35</span><span class="gi">+            b2 = False</span>
<span class="linenos">36</span><span class="gi">+        try:</span>
<span class="linenos">37</span><span class="gi">+            b3 = guard_or_false(sizeA == sizeB)</span>
<span class="linenos">38</span><span class="gi">+        except torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode:</span>
<span class="linenos">39</span><span class="gi">+            b3 = False</span>
<span class="linenos">40</span><span class="gi">+        if b1 or b2 or b3:</span>
<span class="linenos">41</span><span class="gi">+            expandedSizes[i] = sizeB if guard_or_false(sizeA == 1) else sizeA</span>
<span class="linenos">42</span><span class="gi">+        else:</span>
<span class="linenos">43</span><span class="gi">+            # PATCHED: generic case, the dimension is known, no need to assert</span>
<span class="linenos">44</span><span class="gi">+            expandedSizes[i] = torch.sym_max(sizeA, sizeB)</span>
<span class="linenos">45</span><span class="w"> </span>    return tuple(expandedSizes)
</pre></div>
</div>
</section>
<section id="torch-broadcast-shapes-patched-broadcast-shapes">
<h2>torch: _broadcast_shapes -&gt; patched__broadcast_shapes<a class="headerlink" href="#torch-broadcast-shapes-patched-broadcast-shapes" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,12 +1,11 @@</span>
<span class="linenos"> 4</span><span class="gd">-def _broadcast_shapes(*_shapes):</span>
<span class="linenos"> 5</span><span class="gi">+def patched__broadcast_shapes(*_shapes):</span>
<span class="linenos"> 6</span><span class="gi">+    &quot;&quot;&quot;Patches ``torch._refs._broadcast_shapes``.&quot;&quot;&quot;</span>
<span class="linenos"> 7</span><span class="gi">+    from functools import reduce</span>
<span class="linenos"> 8</span><span class="gi">+    from torch._prims_common import IntLike</span>
<span class="linenos"> 9</span><span class="w"> </span>    from torch.fx.experimental.symbolic_shapes import (
<span class="linenos">10</span><span class="w"> </span>        guard_or_false,
<span class="linenos">11</span><span class="gd">-        has_hint,</span>
<span class="linenos">12</span><span class="w"> </span>        is_nested_int,
<span class="linenos">13</span><span class="gd">-        size_hint,</span>
<span class="linenos">14</span><span class="w"> </span>    )
<span class="linenos">15</span><span class="gd">-</span>
<span class="linenos">16</span><span class="gd">-    backed_so = torch.fx.experimental._config.backed_size_oblivious</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="w"> </span>    shapes = tuple(
<span class="linenos">19</span><span class="w"> </span>        (x,) if isinstance(x, IntLike) else x for x in filter(lambda x: x is not None, _shapes)
<span class="linenos">20</span><span class="gu">@@ -19,17 +18,15 @@</span>
<span class="linenos">21</span><span class="w"> </span>    for shape in shapes:
<span class="linenos">22</span><span class="w"> </span>        if not isinstance(shape, Sequence):
<span class="linenos">23</span><span class="w"> </span>            raise RuntimeError(
<span class="linenos">24</span><span class="gd">-                &quot;Input shapes should be of type ints, a tuple of ints, or a list of ints, got &quot;,</span>
<span class="linenos">25</span><span class="gi">+                &quot;Input shapes should be of type ints, a tuple of ints, &quot;</span>
<span class="linenos">26</span><span class="gi">+                &quot;or a list of ints, got &quot;,</span>
<span class="linenos">27</span><span class="w"> </span>                shape,
<span class="linenos">28</span><span class="w"> </span>            )
<span class="linenos">29</span>
<span class="linenos">30</span><span class="w"> </span>    # Computes common shape
<span class="linenos">31</span><span class="gd">-    common_shape: list[Union[int, torch.SymInt]] = [</span>
<span class="linenos">32</span><span class="gd">-        1,</span>
<span class="linenos">33</span><span class="gd">-    ] * reduce(max, (len(shape) for shape in shapes))</span>
<span class="linenos">34</span><span class="gd">-    for arg_idx, shape in enumerate(shapes):</span>
<span class="linenos">35</span><span class="gi">+    common_shape = [1] * reduce(max, (len(shape) for shape in shapes))</span>
<span class="linenos">36</span><span class="gi">+    for _arg_idx, shape in enumerate(shapes):</span>
<span class="linenos">37</span><span class="w"> </span>        for idx in range(-1, -1 - len(shape), -1):
<span class="linenos">38</span><span class="gd">-            # NB: handle nested ints specially to avoid invalid guarding on Ne(j0, 1).</span>
<span class="linenos">39</span><span class="w"> </span>            if is_nested_int(shape[idx]):
<span class="linenos">40</span><span class="w"> </span>                # Broadcasting is allowed for (j0, 1) or (j0, j0);
<span class="linenos">41</span><span class="w"> </span>                # not (j0, j1), (j0, 5), etc.
<span class="linenos">42</span><span class="gu">@@ -38,40 +35,17 @@</span>
<span class="linenos">43</span><span class="w"> </span>                ):
<span class="linenos">44</span><span class="w"> </span>                    continue
<span class="linenos">45</span><span class="w"> </span>            else:
<span class="linenos">46</span><span class="gd">-                # When backed size oblivious is used, we specialize for broadcasting</span>
<span class="linenos">47</span><span class="gd">-                # if its the only way to compile the example input.</span>
<span class="linenos">48</span><span class="gd">-                # i.e: s0:1, s1:1 ==&gt;</span>
<span class="linenos">49</span><span class="gd">-                #           assert s0==s1, no specialization on ==1 or !=1.</span>
<span class="linenos">50</span><span class="gd">-                #            The non-broadcast path is picked</span>
<span class="linenos">51</span><span class="gd">-                #      s0:1, s1:4 ==&gt;</span>
<span class="linenos">52</span><span class="gd">-                #           specialize(s0) to be 1.</span>
<span class="linenos">53</span><span class="gd">-                #      s0:4, s1:1 ==&gt;</span>
<span class="linenos">54</span><span class="gd">-                #           specialize(s1) to be 1.</span>
<span class="linenos">55</span><span class="gd">-                if backed_so and has_hint(shape[idx]) and has_hint(common_shape[idx]):</span>
<span class="linenos">56</span><span class="gd">-                    a = size_hint(shape[idx])</span>
<span class="linenos">57</span><span class="gd">-                    b = size_hint(common_shape[idx])</span>
<span class="linenos">58</span><span class="gd">-                    if a == 1 and b != 1:</span>
<span class="linenos">59</span><span class="gd">-                        torch._check(shape[idx] == 1)</span>
<span class="linenos">60</span><span class="gd">-                    if b == 1 and a != 1:</span>
<span class="linenos">61</span><span class="gd">-                        torch._check(common_shape[idx] == 1)</span>
<span class="linenos">62</span><span class="w"> </span>                if guard_or_false(shape[idx] == common_shape[idx]):
<span class="linenos">63</span><span class="w"> </span>                    continue
<span class="linenos">64</span><span class="gd">-</span>
<span class="linenos">65</span><span class="gd">-            if guard_or_false(common_shape[idx] == 1):</span>
<span class="linenos">66</span><span class="gi">+            # PATCHED: two cases, if == for sure, no broadcast,</span>
<span class="linenos">67</span><span class="gi">+            # otherwise maybe broadcast with max(dimensions)</span>
<span class="linenos">68</span><span class="gi">+            if guard_or_false(common_shape[idx] != 1):</span>
<span class="linenos">69</span><span class="gi">+                pass</span>
<span class="linenos">70</span><span class="gi">+            elif guard_or_false(common_shape[idx] == 1) or guard_or_false(shape[idx] != 1):</span>
<span class="linenos">71</span><span class="w"> </span>                if shape[idx] &lt; 0:
<span class="linenos">72</span><span class="w"> </span>                    raise ValueError(&quot;Attempting to broadcast a dimension with negative length!&quot;)
<span class="linenos">73</span><span class="w"> </span>                common_shape[idx] = shape[idx]
<span class="linenos">74</span><span class="gd">-</span>
<span class="linenos">75</span><span class="gd">-            if not is_nested_int(shape[idx]) and guard_or_false(shape[idx] == 1):</span>
<span class="linenos">76</span><span class="gd">-                # broadcast case .</span>
<span class="linenos">77</span><span class="gd">-                continue</span>
<span class="linenos">78</span><span class="w"> </span>            else:
<span class="linenos">79</span><span class="gd">-                # If broadcasting is undecided we pick non-broadcast path and add runtime assertion.</span>
<span class="linenos">80</span><span class="gd">-                torch._check(</span>
<span class="linenos">81</span><span class="gd">-                    common_shape[idx] == shape[idx],</span>
<span class="linenos">82</span><span class="gd">-                    lambda: f&quot;Attempting to broadcast a dimension of length {shape[idx]} at {idx}! &quot;</span>
<span class="linenos">83</span><span class="gd">-                    f&quot;Mismatching argument at index {arg_idx} had {shape}; but expected shape &quot;</span>
<span class="linenos">84</span><span class="gd">-                    f&quot;should be broadcastable to {common_shape}&quot;,</span>
<span class="linenos">85</span><span class="gd">-                )</span>
<span class="linenos">86</span><span class="gi">+                common_shape[idx] = torch.sym_max(common_shape[idx], shape[idx])</span>
<span class="linenos">87</span>
<span class="linenos">88</span><span class="w"> </span>    return common_shape
</pre></div>
</div>
</section>
<section id="torch-constrain-user-specified-dimhint-range-patched-constrain-user-specified-dimhint-range">
<h2>torch: _constrain_user_specified_dimhint_range -&gt; patched__constrain_user_specified_dimhint_range<a class="headerlink" href="#torch-constrain-user-specified-dimhint-range-patched-constrain-user-specified-dimhint-range" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,28 +1,31 @@</span>
<span class="linenos"> 4</span><span class="gd">-def _constrain_user_specified_dimhint_range(</span>
<span class="linenos"> 5</span><span class="gi">+def patched__constrain_user_specified_dimhint_range(</span>
<span class="linenos"> 6</span><span class="w"> </span>    symint: torch.SymInt,
<span class="linenos"> 7</span><span class="w"> </span>    hint: int,
<span class="linenos"> 8</span><span class="gd">-    dim: _DimHint,</span>
<span class="linenos"> 9</span><span class="gi">+    dim: &quot;_DimHint&quot;,  # noqa: F821</span>
<span class="linenos">10</span><span class="w"> </span>    range_constraints,
<span class="linenos">11</span><span class="w"> </span>    shape_env,
<span class="linenos">12</span><span class="gd">-    keypath: KeyPath,</span>
<span class="linenos">13</span><span class="gi">+    keypath: &quot;KeyPath&quot;,  # noqa: F821</span>
<span class="linenos">14</span><span class="w"> </span>    i: Optional[int] = None,
<span class="linenos">15</span><span class="w"> </span>) -&gt; Optional[str]:
<span class="linenos">16</span><span class="gi">+    &quot;&quot;&quot;Patches ``torch._export.non_strict_utils._constrain_user_specified_dimhint_range``.&quot;&quot;&quot;</span>
<span class="linenos">17</span><span class="gi">+    from torch._export.non_strict_utils import is_int, int_oo, _DimHintType, ValueRanges</span>
<span class="linenos">18</span><span class="gi">+</span>
<span class="linenos">19</span><span class="w"> </span>    trace_vr = (
<span class="linenos">20</span><span class="w"> </span>        range_constraints[symint.node.expr]
<span class="linenos">21</span><span class="w"> </span>        if not is_int(symint)
<span class="linenos">22</span><span class="w"> </span>        else ValueRanges(int(symint), int(symint))
<span class="linenos">23</span><span class="w"> </span>    )
<span class="linenos">24</span><span class="gd">-</span>
<span class="linenos">25</span><span class="w"> </span>    # warn on 0/1 specialization for Dim.AUTO; not an actual error
<span class="linenos">26</span><span class="gd">-    if dim.type == _DimHintType.AUTO and trace_vr.is_singleton() and hint in (0, 1):</span>
<span class="linenos">27</span><span class="gd">-        pathstr = f&quot;inputs{pytree.keystr(keypath)}&quot;</span>
<span class="linenos">28</span><span class="gd">-        if i is not None:</span>
<span class="linenos">29</span><span class="gd">-            pathstr += f&quot;.shape[{i}]&quot;</span>
<span class="linenos">30</span><span class="gd">-        msg = (</span>
<span class="linenos">31</span><span class="gd">-            f&quot;dimension {pathstr} 0/1 specialized; Dim.AUTO was specified along &quot;</span>
<span class="linenos">32</span><span class="gd">-            + f&quot;with a sample input with hint = {hint}.&quot;</span>
<span class="linenos">33</span><span class="gd">-        )</span>
<span class="linenos">34</span><span class="gd">-        log.warning(msg)</span>
<span class="linenos">35</span><span class="gi">+    # PATCHED: remove logging</span>
<span class="linenos">36</span><span class="gi">+    # if dim.type == _DimHintType.AUTO and trace_vr.is_singleton() and hint in (0, 1):</span>
<span class="linenos">37</span><span class="gi">+    #    pathstr = f&quot;inputs{pytree.keystr(keypath)}&quot;</span>
<span class="linenos">38</span><span class="gi">+    #    if i is not None:</span>
<span class="linenos">39</span><span class="gi">+    #        pathstr += f&quot;.shape[{i}]&quot;</span>
<span class="linenos">40</span><span class="gi">+    #    msg = (</span>
<span class="linenos">41</span><span class="gi">+    #        f&quot;dimension {pathstr} 0/1 specialized; Dim.AUTO was specified along &quot;</span>
<span class="linenos">42</span><span class="gi">+    #        f&quot;with a sample input with hint = {hint}.&quot;</span>
<span class="linenos">43</span><span class="gi">+    #    )</span>
<span class="linenos">44</span><span class="gi">+    #    log.warning(msg)</span>
<span class="linenos">45</span>
<span class="linenos">46</span><span class="w"> </span>    try:
<span class="linenos">47</span><span class="w"> </span>        user_vr = ValueRanges(
<span class="linenos">48</span><span class="gu">@@ -38,32 +41,40 @@</span>
<span class="linenos">49</span>
<span class="linenos">50</span><span class="w"> </span>        # check for Dim.DYNAMIC specializations; special case error message on 0/1
<span class="linenos">51</span><span class="w"> </span>        if dim.type == _DimHintType.DYNAMIC and out_vr.is_singleton():
<span class="linenos">52</span><span class="gd">-            path = f&quot;inputs{pytree.keystr(keypath)}&quot;</span>
<span class="linenos">53</span><span class="gi">+            path = f&quot;inputs{torch.utils._pytree.keystr(keypath)}&quot;</span>
<span class="linenos">54</span><span class="w"> </span>            if i is not None:
<span class="linenos">55</span><span class="w"> </span>                path += f&quot;.shape[{i}]&quot;
<span class="linenos">56</span><span class="w"> </span>            if (
<span class="linenos">57</span><span class="w"> </span>                trace_vr.is_singleton()
<span class="linenos">58</span><span class="w"> </span>                and hint in (0, 1)
<span class="linenos">59</span><span class="gd">-                and not torch.fx.experimental._config.backed_size_oblivious</span>
<span class="linenos">60</span><span class="gi">+                # PATCHED: line removed</span>
<span class="linenos">61</span><span class="gi">+                # and not torch.fx.experimental._config.backed_size_oblivious</span>
<span class="linenos">62</span><span class="w"> </span>            ):
<span class="linenos">63</span><span class="gd">-                msg = (</span>
<span class="linenos">64</span><span class="gd">-                    f&quot;- Received user-specified dim hint Dim.DYNAMIC(min={dim.min}, max={dim.max}), &quot;</span>
<span class="linenos">65</span><span class="gd">-                    f&quot;but export 0/1 specialized due to hint of {hint} for dimension {path}.&quot;</span>
<span class="linenos">66</span><span class="gd">-                )</span>
<span class="linenos">67</span><span class="gi">+                return None</span>
<span class="linenos">68</span><span class="gi">+                # PATCHED: line removed</span>
<span class="linenos">69</span><span class="gi">+                # msg = (</span>
<span class="linenos">70</span><span class="gi">+                #     f&quot;- Received user-specified dim hint &quot;</span>
<span class="linenos">71</span><span class="gi">+                #     f&quot;Dim.DYNAMIC(min={dim.min}, max={dim.max}), &quot;</span>
<span class="linenos">72</span><span class="gi">+                #     f&quot;but export 0/1 specialized due to hint of &quot;</span>
<span class="linenos">73</span><span class="gi">+                #     f&quot;{hint} for dimension {path}.&quot;</span>
<span class="linenos">74</span><span class="gi">+                # )</span>
<span class="linenos">75</span><span class="w"> </span>            else:
<span class="linenos">76</span><span class="w"> </span>                msg = (
<span class="linenos">77</span><span class="gd">-                    f&quot;- Received user-specified dim hint Dim.DYNAMIC(min={dim.min}, max={dim.max}), &quot;</span>
<span class="linenos">78</span><span class="gd">-                    f&quot;but tracing inferred a static shape of {out_vr.lower} for dimension {path}.&quot;</span>
<span class="linenos">79</span><span class="gi">+                    f&quot;- Received user-specified dim hint &quot;</span>
<span class="linenos">80</span><span class="gi">+                    f&quot;Dim.DYNAMIC(min={dim.min}, max={dim.max}), &quot;</span>
<span class="linenos">81</span><span class="gi">+                    f&quot;but tracing inferred a static shape of &quot;</span>
<span class="linenos">82</span><span class="gi">+                    f&quot;{out_vr.lower} for dimension {path}.&quot;</span>
<span class="linenos">83</span><span class="w"> </span>                )
<span class="linenos">84</span><span class="w"> </span>            return msg
<span class="linenos">85</span>
<span class="linenos">86</span><span class="w"> </span>    except torch.utils._sympy.value_ranges.ValueRangeError:
<span class="linenos">87</span><span class="gd">-        path = f&quot;inputs{pytree.keystr(keypath)}&quot;</span>
<span class="linenos">88</span><span class="gi">+        path = f&quot;inputs{torch.utils._pytree.keystr(keypath)}&quot;</span>
<span class="linenos">89</span><span class="w"> </span>        if i is not None:
<span class="linenos">90</span><span class="w"> </span>            path += f&quot;.shape[{i}]&quot;
<span class="linenos">91</span><span class="w"> </span>        msg = (
<span class="linenos">92</span><span class="w"> </span>            f&quot;- Received user-specified min/max range of [{dim.min}, {dim.max}], &quot;
<span class="linenos">93</span><span class="gd">-            f&quot;conflicting with the inferred min/max range of [{trace_vr.lower}, {trace_vr.upper}], &quot;</span>
<span class="linenos">94</span><span class="gi">+            f&quot;conflicting with the inferred min/max range of &quot;</span>
<span class="linenos">95</span><span class="gi">+            f&quot;[{trace_vr.lower}, {trace_vr.upper}], &quot;</span>
<span class="linenos">96</span><span class="w"> </span>            f&quot;for {path}.&quot;
<span class="linenos">97</span><span class="w"> </span>        )
<span class="linenos">98</span><span class="w"> </span>        return msg
</pre></div>
</div>
</section>
<section id="torch-broadcast-in-dim-meta-patched-broadcast-in-dim-meta">
<h2>torch: _broadcast_in_dim_meta -&gt; patched__broadcast_in_dim_meta<a class="headerlink" href="#torch-broadcast-in-dim-meta-patched-broadcast-in-dim-meta" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,6 +1,9 @@</span>
<span class="linenos"> 4</span><span class="gd">-def _broadcast_in_dim_meta(</span>
<span class="linenos"> 5</span><span class="gd">-    a: TensorLikeType, shape: ShapeType, broadcast_dimensions: Sequence[int]</span>
<span class="linenos"> 6</span><span class="gi">+def patched__broadcast_in_dim_meta(</span>
<span class="linenos"> 7</span><span class="gi">+    a: torch._prims_common.TensorLikeType,</span>
<span class="linenos"> 8</span><span class="gi">+    shape: torch._prims_common.ShapeType,</span>
<span class="linenos"> 9</span><span class="gi">+    broadcast_dimensions: Sequence[int],</span>
<span class="linenos">10</span><span class="w"> </span>):
<span class="linenos">11</span><span class="gi">+    &quot;&quot;&quot;Patches ``torch._prims._broadcast_in_dim_meta``.&quot;&quot;&quot;</span>
<span class="linenos">12</span><span class="w"> </span>    from torch.fx.experimental.symbolic_shapes import (
<span class="linenos">13</span><span class="w"> </span>        guard_or_false,
<span class="linenos">14</span><span class="w"> </span>        guard_or_true,
<span class="linenos">15</span><span class="gu">@@ -8,37 +11,23 @@</span>
<span class="linenos">16</span><span class="w"> </span>    )
<span class="linenos">17</span>
<span class="linenos">18</span><span class="w"> </span>    # Type checks
<span class="linenos">19</span><span class="gd">-    if not isinstance(a, TensorLike):</span>
<span class="linenos">20</span><span class="gd">-        raise AssertionError(f&quot;a must be TensorLike, got {type(a)}&quot;)  # mypy</span>
<span class="linenos">21</span><span class="gd">-    if not isinstance(shape, Sequence):</span>
<span class="linenos">22</span><span class="gd">-        raise AssertionError(f&quot;shape must be a Sequence, got {type(shape)}&quot;)</span>
<span class="linenos">23</span><span class="gd">-    if not isinstance(broadcast_dimensions, Sequence):</span>
<span class="linenos">24</span><span class="gd">-        raise AssertionError(</span>
<span class="linenos">25</span><span class="gd">-            f&quot;broadcast_dimensions must be a Sequence, got {type(broadcast_dimensions)}&quot;</span>
<span class="linenos">26</span><span class="gd">-        )</span>
<span class="linenos">27</span><span class="gi">+    assert isinstance(a, torch._prims_common.TensorLike)</span>
<span class="linenos">28</span><span class="gi">+    assert isinstance(shape, Sequence)</span>
<span class="linenos">29</span><span class="gi">+    assert isinstance(broadcast_dimensions, Sequence)</span>
<span class="linenos">30</span>
<span class="linenos">31</span><span class="w"> </span>    # every dimension must be accounted for
<span class="linenos">32</span><span class="gd">-    if a.ndim != len(broadcast_dimensions):</span>
<span class="linenos">33</span><span class="gd">-        raise AssertionError(</span>
<span class="linenos">34</span><span class="gd">-            f&quot;a.ndim ({a.ndim}) != len(broadcast_dimensions) ({len(broadcast_dimensions)})&quot;</span>
<span class="linenos">35</span><span class="gd">-        )</span>
<span class="linenos">36</span><span class="gi">+    assert a.ndim == len(broadcast_dimensions)</span>
<span class="linenos">37</span>
<span class="linenos">38</span><span class="w"> </span>    # broadcast shape must have weakly more dimensions
<span class="linenos">39</span><span class="gd">-    if len(shape) &lt; a.ndim:</span>
<span class="linenos">40</span><span class="gd">-        raise AssertionError(f&quot;len(shape) ({len(shape)}) must be &gt;= a.ndim ({a.ndim})&quot;)</span>
<span class="linenos">41</span><span class="gi">+    assert len(shape) &gt;= a.ndim</span>
<span class="linenos">42</span>
<span class="linenos">43</span><span class="w"> </span>    # broadcast_dimensions must be an ascending sequence
<span class="linenos">44</span><span class="w"> </span>    # (no relative reordering of dims) of integers and
<span class="linenos">45</span><span class="w"> </span>    # each dimension must be within the new shape
<span class="linenos">46</span><span class="w"> </span>    def _greater_than_reduce(acc, x):
<span class="linenos">47</span><span class="gd">-        if not isinstance(x, Dim):</span>
<span class="linenos">48</span><span class="gd">-            raise AssertionError(f&quot;broadcast_dimensions element must be Dim, got {type(x)}&quot;)</span>
<span class="linenos">49</span><span class="gd">-        if x &lt;= acc:</span>
<span class="linenos">50</span><span class="gd">-            raise AssertionError(f&quot;broadcast_dimensions must be strictly ascending: {x} &lt;= {acc}&quot;)</span>
<span class="linenos">51</span><span class="gd">-        if x &gt;= len(shape):</span>
<span class="linenos">52</span><span class="gd">-            raise AssertionError(</span>
<span class="linenos">53</span><span class="gd">-                f&quot;broadcast_dimension {x} out of bounds for shape of length {len(shape)}&quot;</span>
<span class="linenos">54</span><span class="gd">-            )</span>
<span class="linenos">55</span><span class="gi">+        assert isinstance(x, (int, torch.export.Dim)), f&quot;unexpected type {type(x)} for x&quot;</span>
<span class="linenos">56</span><span class="gi">+        assert x &gt; acc</span>
<span class="linenos">57</span><span class="gi">+        assert x &lt; len(shape)</span>
<span class="linenos">58</span>
<span class="linenos">59</span><span class="w"> </span>        return x
<span class="linenos">60</span>
<span class="linenos">61</span><span class="gu">@@ -48,7 +37,9 @@</span>
<span class="linenos">62</span><span class="w"> </span>    for idx, new_idx in enumerate(broadcast_dimensions):
<span class="linenos">63</span><span class="w"> </span>        torch._check(
<span class="linenos">64</span><span class="w"> </span>            sym_or(a.shape[idx] == 1, shape[new_idx] == a.shape[idx]),
<span class="linenos">65</span><span class="gd">-            lambda: f&quot;{a.shape[idx]} must be broadcastable to {shape[new_idx]}&quot;,</span>
<span class="linenos">66</span><span class="gi">+            lambda idx=idx, new_idx=new_idx: (</span>
<span class="linenos">67</span><span class="gi">+                f&quot;{a.shape[idx]} must be broadcastable to {shape[new_idx]}&quot;</span>
<span class="linenos">68</span><span class="gi">+            ),</span>
<span class="linenos">69</span><span class="w"> </span>        )
<span class="linenos">70</span>
<span class="linenos">71</span><span class="w"> </span>    new_strides = []
<span class="linenos">72</span><span class="gu">@@ -62,10 +53,26 @@</span>
<span class="linenos">73</span><span class="w"> </span>                    new_strides.append(a.stride()[original_idx])
<span class="linenos">74</span><span class="w"> </span>                else:
<span class="linenos">75</span><span class="w"> </span>                    new_strides.append(0)
<span class="linenos">76</span><span class="gi">+            # PATCHED: disabled this check</span>
<span class="linenos">77</span><span class="gi">+            elif guard_or_false(a.shape[original_idx] != 1):</span>
<span class="linenos">78</span><span class="gi">+                new_strides.append(a.stride()[original_idx])</span>
<span class="linenos">79</span><span class="w"> </span>            else:
<span class="linenos">80</span><span class="gi">+                # This checks generates the following issue:</span>
<span class="linenos">81</span><span class="gi">+                # non-broadcasting semantics require s3 == Max(s10, s3), False,</span>
<span class="linenos">82</span><span class="gi">+                # guard_or_false(a.shape[idx]==1)=False, a.stride()=(1, 2),</span>
<span class="linenos">83</span><span class="gi">+                # idx=1, a.shape=torch.Size([2, s3]), shape=[2, Max(s10, s3)],</span>
<span class="linenos">84</span><span class="gi">+                # original_idx=1</span>
<span class="linenos">85</span><span class="w"> </span>                torch._check(
<span class="linenos">86</span><span class="w"> </span>                    a.shape[original_idx] == shape[idx],
<span class="linenos">87</span><span class="gd">-                    lambda: f&quot;non-broadcasting semantics require {a.shape[original_idx]} == {shape[idx]}&quot;,</span>
<span class="linenos">88</span><span class="gi">+                    lambda idx=idx, original_idx=original_idx: (</span>
<span class="linenos">89</span><span class="gi">+                        f&quot;non-broadcasting semantics require &quot;</span>
<span class="linenos">90</span><span class="gi">+                        f&quot;{a.shape[original_idx]} == {shape[idx]}, &quot;</span>
<span class="linenos">91</span><span class="gi">+                        f&quot;{guard_or_false(a.shape[idx] != 1)}, &quot;</span>
<span class="linenos">92</span><span class="gi">+                        f&quot;guard_or_false(a.shape[idx]==1)=&quot;</span>
<span class="linenos">93</span><span class="gi">+                        f&quot;{guard_or_false(a.shape[idx] == 1)}, &quot;</span>
<span class="linenos">94</span><span class="gi">+                        f&quot;a.stride()={a.stride()}, idx={idx}, a.shape={a.shape}, &quot;</span>
<span class="linenos">95</span><span class="gi">+                        f&quot;shape={shape}, original_idx={original_idx}&quot;</span>
<span class="linenos">96</span><span class="gi">+                    ),</span>
<span class="linenos">97</span><span class="w"> </span>                )
<span class="linenos">98</span><span class="w"> </span>                new_strides.append(a.stride()[original_idx])
<span class="linenos">99</span><span class="w"> </span>            original_idx = original_idx + 1
</pre></div>
</div>
</section>
<section id="torch-maybe-broadcast-patched-maybe-broadcast">
<h2>torch: _maybe_broadcast -&gt; patched__maybe_broadcast<a class="headerlink" href="#torch-maybe-broadcast-patched-maybe-broadcast" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,6 +1,9 @@</span>
<span class="linenos"> 4</span><span class="gd">-def _maybe_broadcast(*args, preserve_cpu_scalar_tensors=True):</span>
<span class="linenos"> 5</span><span class="gi">+def patched__maybe_broadcast(*args, preserve_cpu_scalar_tensors=True):</span>
<span class="linenos"> 6</span><span class="gi">+    &quot;&quot;&quot;Patches ``torch._refs._maybe_broadcast``.&quot;&quot;&quot;</span>
<span class="linenos"> 7</span><span class="gi">+    from torch._prims_common import ShapeType, TensorLike, Number</span>
<span class="linenos"> 8</span><span class="gi">+</span>
<span class="linenos"> 9</span><span class="w"> </span>    # Computes common shape
<span class="linenos">10</span><span class="gd">-    common_shape = _broadcast_shapes(</span>
<span class="linenos">11</span><span class="gi">+    common_shape = patched__broadcast_shapes(</span>
<span class="linenos">12</span><span class="w"> </span>        *(t.shape if isinstance(t, TensorLike) else None for t in args)
<span class="linenos">13</span><span class="w"> </span>    )
<span class="linenos">14</span>
<span class="linenos">15</span><span class="gu">@@ -29,10 +32,15 @@</span>
<span class="linenos">16</span><span class="w"> </span>                return True
<span class="linenos">17</span>
<span class="linenos">18</span><span class="w"> </span>            # u0==u1 assume the same, no broadcasting!
<span class="linenos">19</span><span class="gd">-            torch._check(</span>
<span class="linenos">20</span><span class="gd">-                x == y,</span>
<span class="linenos">21</span><span class="gd">-                lambda: &quot;sizes assumed to be the same due to unbacked broadcasting semantics&quot;,</span>
<span class="linenos">22</span><span class="gd">-            )</span>
<span class="linenos">23</span><span class="gi">+            # PATCHED: avoid errors</span>
<span class="linenos">24</span><span class="gi">+            return True  # guard_or_true(x != y)</span>
<span class="linenos">25</span><span class="gi">+            # torch._check(</span>
<span class="linenos">26</span><span class="gi">+            #    x == y,</span>
<span class="linenos">27</span><span class="gi">+            #    lambda x=x, y=y: (</span>
<span class="linenos">28</span><span class="gi">+            #        f&quot;sizes assumed to be the same due to unbacked &quot;</span>
<span class="linenos">29</span><span class="gi">+            #        f&quot;broadcasting semantics x={x!r}, y={y!r}&quot;</span>
<span class="linenos">30</span><span class="gi">+            #    ),</span>
<span class="linenos">31</span><span class="gi">+            # )</span>
<span class="linenos">32</span>
<span class="linenos">33</span><span class="w"> </span>        return False
<span class="linenos">34</span>
<span class="linenos">35</span><span class="gu">@@ -42,7 +50,7 @@</span>
<span class="linenos">36</span><span class="w"> </span>        elif isinstance(x, Number):
<span class="linenos">37</span><span class="w"> </span>            return x
<span class="linenos">38</span><span class="w"> </span>        elif isinstance(x, TensorLike):
<span class="linenos">39</span><span class="gd">-            if preserve_cpu_scalar_tensors and utils.is_cpu_scalar_tensor(x):</span>
<span class="linenos">40</span><span class="gi">+            if preserve_cpu_scalar_tensors and torch._prims_common.is_cpu_scalar_tensor(x):</span>
<span class="linenos">41</span><span class="w"> </span>                return x
<span class="linenos">42</span>
<span class="linenos">43</span><span class="w"> </span>            if should_expand(x.shape, common_shape):
<span class="linenos">44</span><span class="gu">@@ -50,6 +58,6 @@</span>
<span class="linenos">45</span>
<span class="linenos">46</span><span class="w"> </span>            return x
<span class="linenos">47</span><span class="w"> </span>        else:
<span class="linenos">48</span><span class="gd">-            raise RuntimeError(&quot;Unexpected type when broadcasting: &quot; + str(type(x)) + &quot;!&quot;)</span>
<span class="linenos">49</span><span class="gi">+            raise RuntimeError(f&quot;Unexpected type when broadcasting: {str(type(x))}!&quot;)</span>
<span class="linenos">50</span>
<span class="linenos">51</span><span class="w"> </span>    return tuple(__maybe_broadcast(x, common_shape) for x in args)
</pre></div>
</div>
</section>
<section id="torch-shapeenv-evaluate-expr-patched-shapeenv-evaluate-expr">
<h2>torch: ShapeEnv._evaluate_expr -&gt; patched_ShapeEnv._evaluate_expr<a class="headerlink" href="#torch-shapeenv-evaluate-expr-patched-shapeenv-evaluate-expr" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,14 +1,24 @@</span>
<span class="linenos">  4</span><span class="w"> </span>def _evaluate_expr(
<span class="linenos">  5</span><span class="w"> </span>    self,
<span class="linenos">  6</span><span class="gd">-    orig_expr: sympy.Basic,</span>
<span class="linenos">  7</span><span class="gi">+    orig_expr: &quot;sympy.Basic&quot;,  # noqa: F821</span>
<span class="linenos">  8</span><span class="w"> </span>    hint: Optional[Union[bool, int, float]] = None,
<span class="linenos">  9</span><span class="w"> </span>    fx_node: Optional[torch.fx.Node] = None,
<span class="linenos"> 10</span><span class="w"> </span>    size_oblivious: bool = False,
<span class="linenos"> 11</span><span class="w"> </span>    fallback_value: Optional[bool] = None,
<span class="linenos"> 12</span><span class="w"> </span>    *,
<span class="linenos"> 13</span><span class="w"> </span>    forcing_spec: bool = False,
<span class="linenos"> 14</span><span class="gd">-) -&gt; sympy.Basic:</span>
<span class="linenos"> 15</span><span class="gi">+) -&gt; &quot;sympy.Basic&quot;:  # noqa: F821</span>
<span class="linenos"> 16</span><span class="w"> </span>    # TODO: split conjunctions and evaluate them separately
<span class="linenos"> 17</span><span class="gi">+    import sympy</span>
<span class="linenos"> 18</span><span class="gi">+    from torch.fx.experimental import _config as config</span>
<span class="linenos"> 19</span><span class="gi">+    from torch.fx.experimental.symbolic_shapes import (</span>
<span class="linenos"> 20</span><span class="gi">+        SympyBoolean,</span>
<span class="linenos"> 21</span><span class="gi">+        log,</span>
<span class="linenos"> 22</span><span class="gi">+        SymT,</span>
<span class="linenos"> 23</span><span class="gi">+        symbol_is_type,</span>
<span class="linenos"> 24</span><span class="gi">+    )</span>
<span class="linenos"> 25</span><span class="gi">+    from torch._guards import ShapeGuard</span>
<span class="linenos"> 26</span><span class="gi">+</span>
<span class="linenos"> 27</span><span class="w"> </span>    if isinstance(
<span class="linenos"> 28</span><span class="w"> </span>        orig_expr,
<span class="linenos"> 29</span><span class="w"> </span>        (sympy.logic.boolalg.BooleanTrue, sympy.logic.boolalg.BooleanFalse),
<span class="linenos"> 30</span><span class="gu">@@ -23,8 +33,7 @@</span>
<span class="linenos"> 31</span><span class="w"> </span>            # This is only ever called for expressions WITHOUT unbacked
<span class="linenos"> 32</span><span class="w"> </span>            # symbols
<span class="linenos"> 33</span><span class="w"> </span>            r = self.size_hint(orig_expr)
<span class="linenos"> 34</span><span class="gd">-            if r is None:</span>
<span class="linenos"> 35</span><span class="gd">-                raise AssertionError(&quot;r must not be None&quot;)</span>
<span class="linenos"> 36</span><span class="gi">+            assert r is not None</span>
<span class="linenos"> 37</span><span class="w"> </span>            return r
<span class="linenos"> 38</span><span class="w"> </span>        else:
<span class="linenos"> 39</span><span class="w"> </span>            return sympy.sympify(hint)
<span class="linenos"> 40</span><span class="gu">@@ -61,8 +70,7 @@</span>
<span class="linenos"> 41</span><span class="w"> </span>            eql, _ = self._create_fx_call_function(operator.eq, (fx_node, concrete_val))
<span class="linenos"> 42</span><span class="w"> </span>            node, fresh = self._create_fx_call_function(torch._assert, (eql,))
<span class="linenos"> 43</span>
<span class="linenos"> 44</span><span class="gd">-        if node is None:</span>
<span class="linenos"> 45</span><span class="gd">-            raise AssertionError(&quot;node must not be None&quot;)</span>
<span class="linenos"> 46</span><span class="gi">+        assert node is not None</span>
<span class="linenos"> 47</span><span class="w"> </span>        # If this is a fresh node, we have to remember the event index that
<span class="linenos"> 48</span><span class="w"> </span>        # corresponds to this assertion node.
<span class="linenos"> 49</span><span class="w"> </span>        # Reason: so that, given an assertion node, we can replay the ShapeEnv
<span class="linenos"> 50</span><span class="gu">@@ -84,11 +92,9 @@</span>
<span class="linenos"> 51</span><span class="w"> </span>            self.log.debug(&quot;eval %s [trivial]&quot;, orig_expr)
<span class="linenos"> 52</span><span class="w"> </span>            if hint is not None:
<span class="linenos"> 53</span><span class="w"> </span>                if isinstance(hint, bool):
<span class="linenos"> 54</span><span class="gd">-                    if orig_expr != hint:</span>
<span class="linenos"> 55</span><span class="gd">-                        raise AssertionError(f&quot;{orig_expr} != {hint}&quot;)</span>
<span class="linenos"> 56</span><span class="gi">+                    assert orig_expr == hint, f&quot;{orig_expr} != {hint}&quot;</span>
<span class="linenos"> 57</span><span class="w"> </span>                else:
<span class="linenos"> 58</span><span class="gd">-                    if not sympy.Eq(orig_expr, hint):</span>
<span class="linenos"> 59</span><span class="gd">-                        raise AssertionError(f&quot;{orig_expr} != {hint}&quot;)</span>
<span class="linenos"> 60</span><span class="gi">+                    assert sympy.Eq(orig_expr, hint), f&quot;{orig_expr} != {hint}&quot;</span>
<span class="linenos"> 61</span><span class="w"> </span>            return orig_expr
<span class="linenos"> 62</span>
<span class="linenos"> 63</span><span class="w"> </span>        expr = orig_expr
<span class="linenos"> 64</span><span class="gu">@@ -103,20 +109,21 @@</span>
<span class="linenos"> 65</span><span class="w"> </span>            if not size_oblivious and config.backed_size_oblivious and hint is not None:
<span class="linenos"> 66</span><span class="w"> </span>                # TODO: maybe reconcile this with use of counterfactual hints
<span class="linenos"> 67</span><span class="w"> </span>                # in unbacked case
<span class="linenos"> 68</span><span class="gd">-                if static_expr != hint:</span>
<span class="linenos"> 69</span><span class="gd">-                    raise AssertionError(f&quot;{static_expr} != {hint}&quot;)</span>
<span class="linenos"> 70</span><span class="gi">+                assert static_expr == hint, f&quot;{static_expr} != {hint}&quot;</span>
<span class="linenos"> 71</span><span class="w"> </span>            return static_expr
<span class="linenos"> 72</span>
<span class="linenos"> 73</span><span class="w"> </span>        transmute_into_runtime_assert = False
<span class="linenos"> 74</span>
<span class="linenos"> 75</span><span class="gi">+        backed_var_to_val = (</span>
<span class="linenos"> 76</span><span class="gi">+            self.backed_var_to_val if hasattr(self, &quot;backed_var_to_val&quot;) else self.var_to_val</span>
<span class="linenos"> 77</span><span class="gi">+        )</span>
<span class="linenos"> 78</span><span class="w"> </span>        concrete_val = None
<span class="linenos"> 79</span><span class="gd">-        if not (expr.free_symbols &lt;= self.backed_var_to_val.keys()):</span>
<span class="linenos"> 80</span><span class="gi">+        if not (expr.free_symbols &lt;= backed_var_to_val.keys()):</span>
<span class="linenos"> 81</span><span class="w"> </span>            # TODO: dedupe this with _maybe_evaluate_static
<span class="linenos"> 82</span><span class="w"> </span>            # Attempt to eliminate the unbacked SymInt
<span class="linenos"> 83</span><span class="w"> </span>            new_expr = self._maybe_evaluate_static(expr, unbacked_only=True)
<span class="linenos"> 84</span><span class="gd">-            if new_expr is None:</span>
<span class="linenos"> 85</span><span class="gd">-                raise AssertionError(&quot;new_expr must not be None&quot;)</span>
<span class="linenos"> 86</span><span class="gd">-            if not (new_expr.free_symbols &lt;= self.backed_var_to_val.keys()):</span>
<span class="linenos"> 87</span><span class="gi">+            assert new_expr is not None</span>
<span class="linenos"> 88</span><span class="gi">+            if not (new_expr.free_symbols &lt;= backed_var_to_val.keys()):</span>
<span class="linenos"> 89</span><span class="w"> </span>                ok = False
<span class="linenos"> 90</span>
<span class="linenos"> 91</span><span class="w"> </span>                # fallback_value is set when guard_or_true or guard_or_false are used.
<span class="linenos"> 92</span><span class="gu">@@ -124,37 +131,68 @@</span>
<span class="linenos"> 93</span><span class="w"> </span>                    self._log_suppressed_dde(orig_expr, fallback_value)
<span class="linenos"> 94</span><span class="w"> </span>                    return fallback_value
<span class="linenos"> 95</span>
<span class="linenos"> 96</span><span class="gd">-                # real_tensor_prop_unbacked_vals is not None iff propagate_real_tensors is on.</span>
<span class="linenos"> 97</span><span class="gd">-                # if propagate_real_tensors is on, we check the example values to generate (unsound_result)</span>
<span class="linenos"> 98</span><span class="gi">+                # oblivious_var_to_val will be defined iff we have sizes</span>
<span class="linenos"> 99</span><span class="gi">+                # with DimDynamic.OBLIVIOUS_SIZE type.</span>
<span class="linenos">100</span><span class="gi">+                # See https://github.com/pytorch/pytorch/issues/137100#issuecomment-2495778113</span>
<span class="linenos">101</span><span class="gi">+                if (</span>
<span class="linenos">102</span><span class="gi">+                    backed_var_to_val</span>
<span class="linenos">103</span><span class="gi">+                    and getattr(self, &quot;real_tensor_prop_unbacked_vals&quot;, True)</span>
<span class="linenos">104</span><span class="gi">+                    and not (correct_hint := orig_expr.xreplace(backed_var_to_val)).free_symbols</span>
<span class="linenos">105</span><span class="gi">+                    and not (</span>
<span class="linenos">106</span><span class="gi">+                        counterfactual_hint := orig_expr.xreplace(</span>
<span class="linenos">107</span><span class="gi">+                            {k: max(2, v) for k, v in backed_var_to_val.items()}</span>
<span class="linenos">108</span><span class="gi">+                        )</span>
<span class="linenos">109</span><span class="gi">+                    ).free_symbols</span>
<span class="linenos">110</span><span class="gi">+                    and correct_hint == counterfactual_hint</span>
<span class="linenos">111</span><span class="gi">+                ):</span>
<span class="linenos">112</span><span class="gi">+                    # TODO: better logging</span>
<span class="linenos">113</span><span class="gi">+                    log.info(</span>
<span class="linenos">114</span><span class="gi">+                        &quot;oblivious_size %s -&gt; %s (passed counterfactual)&quot;,</span>
<span class="linenos">115</span><span class="gi">+                        orig_expr,</span>
<span class="linenos">116</span><span class="gi">+                        # pyrefly: ignore  # unbound-name</span>
<span class="linenos">117</span><span class="gi">+                        correct_hint,</span>
<span class="linenos">118</span><span class="gi">+                    )</span>
<span class="linenos">119</span><span class="gi">+                    # pyrefly: ignore  # unbound-name</span>
<span class="linenos">120</span><span class="gi">+                    concrete_val = correct_hint</span>
<span class="linenos">121</span><span class="gi">+                    # NB: do NOT transmute into runtime assert</span>
<span class="linenos">122</span><span class="gi">+                    ok = True</span>
<span class="linenos">123</span><span class="gi">+</span>
<span class="linenos">124</span><span class="gi">+                # unbacked_var_to_val is not None iff propagate_real_tensors is on.</span>
<span class="linenos">125</span><span class="gi">+                # if propagate_real_tensors is on, we check the example values</span>
<span class="linenos">126</span><span class="gi">+                # to generate (unsound_result)</span>
<span class="linenos">127</span><span class="w"> </span>                # and if they pass we add a runtime assertions and continue.
<span class="linenos">128</span><span class="w"> </span>                if (
<span class="linenos">129</span><span class="w"> </span>                    not ok
<span class="linenos">130</span><span class="gd">-                    and self.real_tensor_prop_unbacked_vals</span>
<span class="linenos">131</span><span class="gi">+                    and backed_var_to_val</span>
<span class="linenos">132</span><span class="w"> </span>                    and not (
<span class="linenos">133</span><span class="gd">-                        unsound_result := orig_expr.xreplace(</span>
<span class="linenos">134</span><span class="gd">-                            self.real_tensor_prop_unbacked_vals</span>
<span class="linenos">135</span><span class="gd">-                        ).xreplace(self.backed_var_to_val)</span>
<span class="linenos">136</span><span class="gi">+                        unsound_result := orig_expr.xreplace(backed_var_to_val).xreplace(</span>
<span class="linenos">137</span><span class="gi">+                            backed_var_to_val</span>
<span class="linenos">138</span><span class="gi">+                        )</span>
<span class="linenos">139</span><span class="w"> </span>                    ).free_symbols
<span class="linenos">140</span><span class="w"> </span>                ):
<span class="linenos">141</span><span class="gi">+                    # pyrefly: ignore  # unbound-name</span>
<span class="linenos">142</span><span class="w"> </span>                    self._log_real_tensor_propagation(orig_expr, unsound_result)
<span class="linenos">143</span><span class="w"> </span>                    transmute_into_runtime_assert = True
<span class="linenos">144</span><span class="gd">-</span>
<span class="linenos">145</span><span class="gi">+                    # pyrefly: ignore  # unbound-name</span>
<span class="linenos">146</span><span class="w"> </span>                    concrete_val = unsound_result
<span class="linenos">147</span><span class="w"> </span>                    ok = True
<span class="linenos">148</span>
<span class="linenos">149</span><span class="gd">-                # Check if this is coming from a python assert statement, if so, convert it to a runtime assertion</span>
<span class="linenos">150</span><span class="gi">+                # Check if this is coming from a python assert statement,</span>
<span class="linenos">151</span><span class="gi">+                # if so, convert it to a runtime assertion</span>
<span class="linenos">152</span><span class="w"> </span>                # instead of failing.
<span class="linenos">153</span><span class="w"> </span>                if not ok and self.trace_asserts and self._is_python_assert():
<span class="linenos">154</span><span class="w"> </span>                    concrete_val = sympy.true
<span class="linenos">155</span><span class="w"> </span>                    transmute_into_runtime_assert = True
<span class="linenos">156</span><span class="w"> </span>                    ok = True
<span class="linenos">157</span>
<span class="linenos">158</span><span class="gd">-                if not ok:</span>
<span class="linenos">159</span><span class="gd">-                    raise self._make_data_dependent_error(</span>
<span class="linenos">160</span><span class="gd">-                        expr.xreplace(self.backed_var_to_val),</span>
<span class="linenos">161</span><span class="gd">-                        expr,</span>
<span class="linenos">162</span><span class="gd">-                        expr_sym_node_id=self._expr_sym_node_id,</span>
<span class="linenos">163</span><span class="gd">-                    )</span>
<span class="linenos">164</span><span class="gi">+                # PATCHED: ok -&gt; True</span>
<span class="linenos">165</span><span class="gi">+                ok = True</span>
<span class="linenos">166</span><span class="gi">+                # if not ok:</span>
<span class="linenos">167</span><span class="gi">+                #    raise self._make_data_dependent_error(</span>
<span class="linenos">168</span><span class="gi">+                #        expr.xreplace(self.var_to_val),</span>
<span class="linenos">169</span><span class="gi">+                #        expr,</span>
<span class="linenos">170</span><span class="gi">+                #        expr_sym_node_id=self._expr_sym_node_id,</span>
<span class="linenos">171</span><span class="gi">+                #    )</span>
<span class="linenos">172</span><span class="w"> </span>            else:
<span class="linenos">173</span><span class="w"> </span>                expr = new_expr
</pre></div>
</div>
</section>
<section id="patch-transformers-dynamic-rope-update-patched-dynamic-rope-update">
<h2>patch_transformers: dynamic_rope_update -&gt; patched_dynamic_rope_update<a class="headerlink" href="#patch-transformers-dynamic-rope-update-patched-dynamic-rope-update" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos">  4</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos">  5</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">  6</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos">  7</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos">  8</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos">  9</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 10</span>
<span class="linenos"> 11</span><span class="gd">-    Args:</span>
<span class="linenos"> 12</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 13</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 14</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 15</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 16</span>
<span class="linenos"> 17</span><span class="gd">-    Returns:</span>
<span class="linenos"> 18</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 19</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 20</span><span class="gi">+</span>
<span class="linenos"> 21</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 22</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 23</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 24</span><span class="gi">+        else:</span>
<span class="linenos"> 25</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 26</span><span class="gi">+</span>
<span class="linenos"> 27</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 28</span><span class="gi">+</span>
<span class="linenos"> 29</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 30</span><span class="gi">+</span>
<span class="linenos"> 31</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 32</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 33</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 34</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 35</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 36</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 37</span><span class="gi">+                else:</span>
<span class="linenos"> 38</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 39</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 40</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 41</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 42</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 43</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 44</span><span class="gi">+                        )</span>
<span class="linenos"> 45</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 46</span><span class="gi">+                else:</span>
<span class="linenos"> 47</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 48</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 49</span><span class="gi">+</span>
<span class="linenos"> 50</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 51</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 52</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 53</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 54</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 55</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 56</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 59</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 60</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 61</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 62</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos"> 65</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos"> 66</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos"> 67</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos"> 68</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos"> 69</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos"> 70</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos"> 71</span><span class="gi">+</span>
<span class="linenos"> 72</span><span class="gi">+            return wrapper</span>
<span class="linenos"> 73</span><span class="gi">+</span>
<span class="linenos"> 74</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos"> 75</span>
<span class="linenos"> 76</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos"> 77</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos"> 78</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos"> 79</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos"> 80</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos"> 81</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos"> 82</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos"> 83</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos"> 84</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 85</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos"> 86</span><span class="gi">+        else:</span>
<span class="linenos"> 87</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos"> 88</span>
<span class="linenos"> 89</span><span class="w"> </span>        if layer_type is None:
<span class="linenos"> 90</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos"> 91</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos"> 92</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos"> 93</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos"> 94</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos"> 95</span><span class="gd">-            ]</span>
<span class="linenos"> 96</span><span class="gd">-        else:</span>
<span class="linenos"> 97</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos"> 98</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos"> 99</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">100</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">101</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">102</span><span class="gd">-            ]</span>
<span class="linenos">103</span><span class="gd">-</span>
<span class="linenos">104</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">105</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">106</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">107</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">108</span><span class="gd">-                    self.config,</span>
<span class="linenos">109</span><span class="gd">-                    device,</span>
<span class="linenos">110</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">111</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">112</span><span class="gd">-                )</span>
<span class="linenos">113</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">114</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">115</span><span class="gd">-        else:</span>
<span class="linenos">116</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">117</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">118</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">119</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">120</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">121</span><span class="gd">-</span>
<span class="linenos">122</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">123</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">124</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">125</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">126</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">127</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">128</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">129</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">130</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">131</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">132</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">133</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">134</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">135</span><span class="w"> </span>        else:
<span class="linenos">136</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">137</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">138</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">139</span><span class="gd">-            )</span>
<span class="linenos">140</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">141</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">142</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">143</span>
<span class="linenos">144</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">145</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">146</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">147</span><span class="gd">-                self.config,</span>
<span class="linenos">148</span><span class="gd">-                device,</span>
<span class="linenos">149</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">150</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">151</span><span class="gd">-            )</span>
<span class="linenos">152</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">153</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">154</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">155</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">156</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">157</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">158</span><span class="gi">+        )</span>
<span class="linenos">159</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">160</span>
<span class="linenos">161</span><span class="gd">-        if (</span>
<span class="linenos">162</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">163</span><span class="gd">-        ):  # reset</span>
<span class="linenos">164</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">165</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">166</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">167</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">168</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">169</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">170</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">171</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">172</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">173</span><span class="gi">+            cond,</span>
<span class="linenos">174</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">175</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">176</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">177</span><span class="gi">+        )</span>
<span class="linenos">178</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">179</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">180</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">181</span><span class="gi">+        # else:</span>
<span class="linenos">182</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">183</span><span class="gi">+</span>
<span class="linenos">184</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">185</span><span class="gi">+        # constructor:</span>
<span class="linenos">186</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">187</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">188</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">189</span><span class="gi">+</span>
<span class="linenos">190</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">191</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">192</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">193</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">194</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">195</span><span class="gi">+</span>
<span class="linenos">196</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">197</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">198</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">199</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">200</span><span class="gi">+        #</span>
<span class="linenos">201</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">202</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">203</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">204</span><span class="gi">+        #    )</span>
<span class="linenos">205</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">206</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">207</span><span class="gi">+        #</span>
<span class="linenos">208</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">209</span><span class="gi">+        #</span>
<span class="linenos">210</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">211</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">212</span><span class="gi">+        #    cond,</span>
<span class="linenos">213</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">214</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">215</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">216</span><span class="gi">+        # )</span>
<span class="linenos">217</span><span class="gi">+</span>
<span class="linenos">218</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">219</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">222</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">223</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">224</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">225</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">226</span><span class="gi">+        else:</span>
<span class="linenos">227</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">228</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">229</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">230</span><span class="gi">+            # )</span>
<span class="linenos">231</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">232</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">233</span><span class="gi">+</span>
<span class="linenos">234</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">235</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">236</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">237</span><span class="gi">+</span>
<span class="linenos">238</span><span class="gi">+        # if (</span>
<span class="linenos">239</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">240</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">241</span><span class="gi">+        # ):</span>
<span class="linenos">242</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">243</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">244</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">245</span><span class="gi">+</span>
<span class="linenos">246</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">247</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">248</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">249</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">250</span><span class="gi">+            cond,</span>
<span class="linenos">251</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">252</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">253</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">254</span><span class="gi">+        )</span>
<span class="linenos">255</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">256</span>
<span class="linenos">257</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">258</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">259</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">260</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">261</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">262</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">263</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">264</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">265</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">266</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">267</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">268</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">269</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">270</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">271</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">272</span><span class="gi">+</span>
<span class="linenos">273</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">274</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">275</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">276</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">277</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">278</span>
<span class="linenos">279</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="transformers-sdpa-mask-patched-sdpa-mask">
<h2>transformers: sdpa_mask -&gt; patched_sdpa_mask<a class="headerlink" href="#transformers-sdpa-mask-patched-sdpa-mask" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,4 +1,4 @@</span>
<span class="linenos">  4</span><span class="gd">-def sdpa_mask(</span>
<span class="linenos">  5</span><span class="gi">+def patched_sdpa_mask(</span>
<span class="linenos">  6</span><span class="w"> </span>    batch_size: int,
<span class="linenos">  7</span><span class="w"> </span>    cache_position: torch.Tensor,
<span class="linenos">  8</span><span class="w"> </span>    kv_length: int,
<span class="linenos">  9</span><span class="gu">@@ -12,105 +12,7 @@</span>
<span class="linenos"> 10</span><span class="w"> </span>    use_vmap: bool = False,
<span class="linenos"> 11</span><span class="w"> </span>    **kwargs,
<span class="linenos"> 12</span><span class="w"> </span>) -&gt; torch.Tensor | None:
<span class="linenos"> 13</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 14</span><span class="gd">-    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that</span>
<span class="linenos"> 15</span><span class="gd">-    the element should take part in the attention computation, and False that it should not.</span>
<span class="linenos"> 16</span><span class="gd">-    This function can only be used with torch&gt;=2.5, as the context manager is otherwise not available.</span>
<span class="linenos"> 17</span><span class="gd">-</span>
<span class="linenos"> 18</span><span class="gd">-    Args:</span>
<span class="linenos"> 19</span><span class="gd">-        batch_size (`int`):</span>
<span class="linenos"> 20</span><span class="gd">-            The batch size of the input sequence.</span>
<span class="linenos"> 21</span><span class="gd">-        cache_position (`torch.Tensor`):</span>
<span class="linenos"> 22</span><span class="gd">-            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.</span>
<span class="linenos"> 23</span><span class="gd">-        kv_length (`int`):</span>
<span class="linenos"> 24</span><span class="gd">-            The size that the key and value states will have during the attention computation.</span>
<span class="linenos"> 25</span><span class="gd">-        kv_offset (`int`, optional):</span>
<span class="linenos"> 26</span><span class="gd">-            An optional offset to indicate at which first position the key and values states will refer to.</span>
<span class="linenos"> 27</span><span class="gd">-        mask_function (`Callable`):</span>
<span class="linenos"> 28</span><span class="gd">-            The mask factory function describing the mask pattern.</span>
<span class="linenos"> 29</span><span class="gd">-        attention_mask (`torch.Tensor`, optional):</span>
<span class="linenos"> 30</span><span class="gd">-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)</span>
<span class="linenos"> 31</span><span class="gd">-        local_size (`int`, optional):</span>
<span class="linenos"> 32</span><span class="gd">-            The size of the local attention, if we do not use full attention. This is used only if `allow_is_causal_skip=True`</span>
<span class="linenos"> 33</span><span class="gd">-            to try to skip mask creation if possible.</span>
<span class="linenos"> 34</span><span class="gd">-        allow_is_causal_skip (`bool`, optional):</span>
<span class="linenos"> 35</span><span class="gd">-            Whether to allow to return `None` for the mask under conditions where we can use the `is_causal` argument in</span>
<span class="linenos"> 36</span><span class="gd">-            `torch.sdpa` instead. Default to `True`.</span>
<span class="linenos"> 37</span><span class="gd">-        allow_is_bidirectional_skip (`bool`, optional):</span>
<span class="linenos"> 38</span><span class="gd">-            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,</span>
<span class="linenos"> 39</span><span class="gd">-            i.e. full attention without any padding. Default to `False`.</span>
<span class="linenos"> 40</span><span class="gd">-        allow_torch_fix (`bool`, optional):</span>
<span class="linenos"> 41</span><span class="gd">-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch&#39;s older</span>
<span class="linenos"> 42</span><span class="gd">-            versions. We need an arg to skip it when using eager. By default `True`.</span>
<span class="linenos"> 43</span><span class="gd">-        use_vmap (`bool`, optional):</span>
<span class="linenos"> 44</span><span class="gd">-            Whether to use `vmap` during the mask construction or not. Allows powerful custom patterns that may not be</span>
<span class="linenos"> 45</span><span class="gd">-            index-based (for the cost of speed performance). By default `False`.</span>
<span class="linenos"> 46</span><span class="gd">-</span>
<span class="linenos"> 47</span><span class="gd">-</span>
<span class="linenos"> 48</span><span class="gd">-    ## Creating a simple causal mask:</span>
<span class="linenos"> 49</span><span class="gd">-</span>
<span class="linenos"> 50</span><span class="gd">-    To create the following causal mask:</span>
<span class="linenos"> 51</span><span class="gd">-</span>
<span class="linenos"> 52</span><span class="gd">-        0 ■ ⬚ ⬚ ⬚ ⬚</span>
<span class="linenos"> 53</span><span class="gd">-        1 ■ ■ ⬚ ⬚ ⬚</span>
<span class="linenos"> 54</span><span class="gd">-        2 ■ ■ ■ ⬚ ⬚</span>
<span class="linenos"> 55</span><span class="gd">-        3 ■ ■ ■ ■ ⬚</span>
<span class="linenos"> 56</span><span class="gd">-        4 ■ ■ ■ ■ ■</span>
<span class="linenos"> 57</span><span class="gd">-</span>
<span class="linenos"> 58</span><span class="gd">-    You can do</span>
<span class="linenos"> 59</span><span class="gd">-</span>
<span class="linenos"> 60</span><span class="gd">-    ```python</span>
<span class="linenos"> 61</span><span class="gd">-    &gt;&gt;&gt; sdpa_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5)</span>
<span class="linenos"> 62</span><span class="gd">-    &gt;&gt;&gt; tensor([[[[ True, False, False, False, False],</span>
<span class="linenos"> 63</span><span class="gd">-                  [ True,  True, False, False, False],</span>
<span class="linenos"> 64</span><span class="gd">-                  [ True,  True,  True, False, False],</span>
<span class="linenos"> 65</span><span class="gd">-                  [ True,  True,  True,  True, False],</span>
<span class="linenos"> 66</span><span class="gd">-                  [ True,  True,  True,  True,  True]]]])</span>
<span class="linenos"> 67</span><span class="gd">-    ```</span>
<span class="linenos"> 68</span><span class="gd">-</span>
<span class="linenos"> 69</span><span class="gd">-    ## Creating a sliding window mask:</span>
<span class="linenos"> 70</span><span class="gd">-</span>
<span class="linenos"> 71</span><span class="gd">-    To create the following sliding window mask (`sliding_window=3`):</span>
<span class="linenos"> 72</span><span class="gd">-</span>
<span class="linenos"> 73</span><span class="gd">-        0 ■ ⬚ ⬚ ⬚ ⬚</span>
<span class="linenos"> 74</span><span class="gd">-        1 ■ ■ ⬚ ⬚ ⬚</span>
<span class="linenos"> 75</span><span class="gd">-        2 ■ ■ ■ ⬚ ⬚</span>
<span class="linenos"> 76</span><span class="gd">-        3 ⬚ ■ ■ ■ ⬚</span>
<span class="linenos"> 77</span><span class="gd">-        4 ⬚ ⬚ ■ ■ ■</span>
<span class="linenos"> 78</span><span class="gd">-</span>
<span class="linenos"> 79</span><span class="gd">-    You can do</span>
<span class="linenos"> 80</span><span class="gd">-</span>
<span class="linenos"> 81</span><span class="gd">-    ```python</span>
<span class="linenos"> 82</span><span class="gd">-    &gt;&gt;&gt; sdpa_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=sliding_window_causal_mask_function(3))</span>
<span class="linenos"> 83</span><span class="gd">-    &gt;&gt;&gt; tensor([[[[ True, False, False, False, False],</span>
<span class="linenos"> 84</span><span class="gd">-                  [ True,  True, False, False, False],</span>
<span class="linenos"> 85</span><span class="gd">-                  [ True,  True,  True, False, False],</span>
<span class="linenos"> 86</span><span class="gd">-                  [False,  True,  True,  True, False],</span>
<span class="linenos"> 87</span><span class="gd">-                  [False, False,  True,  True,  True]]]])</span>
<span class="linenos"> 88</span><span class="gd">-    ```</span>
<span class="linenos"> 89</span><span class="gd">-</span>
<span class="linenos"> 90</span><span class="gd">-    ## Creating a chunked attention mask</span>
<span class="linenos"> 91</span><span class="gd">-</span>
<span class="linenos"> 92</span><span class="gd">-    To create the following chunked attention mask (`chunk_size=3`):</span>
<span class="linenos"> 93</span><span class="gd">-</span>
<span class="linenos"> 94</span><span class="gd">-        0 ■ ⬚ ⬚ ⬚ ⬚</span>
<span class="linenos"> 95</span><span class="gd">-        1 ■ ■ ⬚ ⬚ ⬚</span>
<span class="linenos"> 96</span><span class="gd">-        2 ■ ■ ■ ⬚ ⬚</span>
<span class="linenos"> 97</span><span class="gd">-        3 ⬚ ⬚ ⬚ ■ ⬚</span>
<span class="linenos"> 98</span><span class="gd">-        4 ⬚ ⬚ ⬚ ■ ■</span>
<span class="linenos"> 99</span><span class="gd">-</span>
<span class="linenos">100</span><span class="gd">-    You can do</span>
<span class="linenos">101</span><span class="gd">-</span>
<span class="linenos">102</span><span class="gd">-    ```python</span>
<span class="linenos">103</span><span class="gd">-    &gt;&gt;&gt; sdpa_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=chunked_causal_mask_function(3, torch.zeros(1, dtype=int)))</span>
<span class="linenos">104</span><span class="gd">-    &gt;&gt;&gt; tensor([[[[ True, False, False, False, False],</span>
<span class="linenos">105</span><span class="gd">-                [ True,  True, False, False, False],</span>
<span class="linenos">106</span><span class="gd">-                [ True,  True,  True, False, False],</span>
<span class="linenos">107</span><span class="gd">-                [False, False, False,  True, False],</span>
<span class="linenos">108</span><span class="gd">-                [False, False, False,  True,  True]]]])</span>
<span class="linenos">109</span><span class="gd">-    ```</span>
<span class="linenos">110</span><span class="gd">-</span>
<span class="linenos">111</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">112</span><span class="gi">+    &quot;&quot;&quot;manual patch for function ``transformers.masking_utils.sdpa_mask``.&quot;&quot;&quot;</span>
<span class="linenos">113</span><span class="w"> </span>    q_length = cache_position.shape[0]
<span class="linenos">114</span>
<span class="linenos">115</span><span class="w"> </span>    # Potentially pad the 2D mask
<span class="linenos">116</span><span class="gu">@@ -134,40 +36,44 @@</span>
<span class="linenos">117</span>
<span class="linenos">118</span><span class="w"> </span>    batch_arange = torch.arange(batch_size, device=cache_position.device)
<span class="linenos">119</span><span class="w"> </span>    head_arange = torch.arange(1, device=cache_position.device)
<span class="linenos">120</span><span class="gd">-    # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`</span>
<span class="linenos">121</span><span class="gi">+    # Similar to `kv_arange = torch.arange(start=kv_offset,</span>
<span class="linenos">122</span><span class="gi">+    # end=kv_offset + kv_length, device=cache_position.device)`</span>
<span class="linenos">123</span><span class="w"> </span>    # but without data-dependent slicing (i.e. torch.compile friendly)
<span class="linenos">124</span><span class="w"> </span>    kv_arange = torch.arange(kv_length, device=cache_position.device) + kv_offset
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>    # Actual mask creation
<span class="linenos">127</span><span class="w"> </span>    # Option 1: Fast non-vmap mask creation (default)
<span class="linenos">128</span><span class="gi">+    # PATCHED</span>
<span class="linenos">129</span><span class="gi">+    use_vmap = False</span>
<span class="linenos">130</span><span class="w"> </span>    if not use_vmap:
<span class="linenos">131</span><span class="w"> </span>        # Apply mask function element-wise through broadcasting
<span class="linenos">132</span><span class="w"> </span>        attention_mask = mask_function(
<span class="linenos">133</span><span class="w"> </span>            *_non_vmap_expansion_sdpa(batch_arange, head_arange, cache_position, kv_arange)
<span class="linenos">134</span><span class="w"> </span>        )
<span class="linenos">135</span><span class="gd">-        # Expand the mask to match batch size and query length if they weren&#39;t used in the mask function</span>
<span class="linenos">136</span><span class="gi">+        # Expand the mask to match batch size</span>
<span class="linenos">137</span><span class="gi">+        # and query length if they weren&#39;t used in the mask function</span>
<span class="linenos">138</span><span class="w"> </span>        attention_mask = attention_mask.expand(batch_size, -1, q_length, kv_length)
<span class="linenos">139</span>
<span class="linenos">140</span><span class="w"> </span>    # Option 2: Vmap mask creation (torch&gt;=2.6 and custom patterns)
<span class="linenos">141</span><span class="gd">-    elif _is_torch_greater_or_equal_than_2_6:</span>
<span class="linenos">142</span><span class="gd">-        # This creates the 4D mask easily. Note that we need this context manager as vmap cannot handle slicing a tensor from</span>
<span class="linenos">143</span><span class="gd">-        # scalar tensor (it internally calls `.item()` which vmap does not allow, but this context works around it</span>
<span class="linenos">144</span><span class="gd">-        # We don&#39;t need to add an offset to the mask_function either, as we vmap directly the correct indices for k and kv indices</span>
<span class="linenos">145</span><span class="gd">-        with TransformGetItemToIndex():</span>
<span class="linenos">146</span><span class="gd">-            attention_mask = _vmap_expansion_sdpa(mask_function)(</span>
<span class="linenos">147</span><span class="gd">-                batch_arange, head_arange, cache_position, kv_arange</span>
<span class="linenos">148</span><span class="gd">-            )</span>
<span class="linenos">149</span><span class="gi">+    # elif _is_torch_greater_or_equal_than_2_6:</span>
<span class="linenos">150</span><span class="gi">+    # This creates the 4D mask easily.</span>
<span class="linenos">151</span><span class="gi">+    # Note that we need this context manager as vmap cannot handle slicing a tensor from</span>
<span class="linenos">152</span><span class="gi">+    # scalar tensor (it internally calls `.item()` which vmap does not allow,</span>
<span class="linenos">153</span><span class="gi">+    # but this context works around it</span>
<span class="linenos">154</span><span class="gi">+    # We don&#39;t need to add an offset to the mask_function either,</span>
<span class="linenos">155</span><span class="gi">+    # as we vmap directly the correct indices for k and kv indices</span>
<span class="linenos">156</span><span class="gi">+    #    with TransformGetItemToIndex():</span>
<span class="linenos">157</span><span class="gi">+    #        attention_mask = _vmap_expansion_sdpa(mask_function)(</span>
<span class="linenos">158</span><span class="gi">+    #            batch_arange, head_arange, cache_position, kv_arange</span>
<span class="linenos">159</span><span class="gi">+    #        )</span>
<span class="linenos">160</span>
<span class="linenos">161</span><span class="gd">-    # Option 3: Error out since it indicates that the user did something custom, which they shouldn&#39;t have (torch&lt;2.6)</span>
<span class="linenos">162</span><span class="gi">+    # Option 3: Error out since it indicates that the user did something custom,</span>
<span class="linenos">163</span><span class="gi">+    # which they shouldn&#39;t have (torch&lt;2.6)</span>
<span class="linenos">164</span><span class="w"> </span>    else:
<span class="linenos">165</span><span class="w"> </span>        raise ValueError(
<span class="linenos">166</span><span class="gd">-            &quot;The vmap functionality for mask creation is only supported from torch&gt;=2.6. &quot;</span>
<span class="linenos">167</span><span class="gd">-            &quot;Please update your torch version or use `use_vmap=False` with index-based masks.&quot;</span>
<span class="linenos">168</span><span class="gi">+            &quot;The vmap functionality for mask creation &quot;</span>
<span class="linenos">169</span><span class="gi">+            &quot;is only supported from torch&gt;=2.6. &quot;</span>
<span class="linenos">170</span><span class="gi">+            &quot;Please update your torch version or use &quot;</span>
<span class="linenos">171</span><span class="gi">+            &quot;`use_vmap=False` with index-based masks.&quot;</span>
<span class="linenos">172</span><span class="w"> </span>        )
<span class="linenos">173</span><span class="gd">-</span>
<span class="linenos">174</span><span class="gd">-    # Due to a bug in versions of torch&lt;2.5, we need to update the mask in case a query is not attending to any</span>
<span class="linenos">175</span><span class="gd">-    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213</span>
<span class="linenos">176</span><span class="gd">-    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:</span>
<span class="linenos">177</span><span class="gd">-        attention_mask = attention_mask | torch.all(~attention_mask, dim=-1, keepdim=True)</span>
<span class="linenos">178</span><span class="gd">-</span>
<span class="linenos">179</span><span class="w"> </span>    return attention_mask
</pre></div>
</div>
</section>
<section id="transformers-eager-mask-patched-eager-mask">
<h2>transformers: eager_mask -&gt; patched_eager_mask<a class="headerlink" href="#transformers-eager-mask-patched-eager-mask" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,46 +1,19 @@</span>
<span class="linenos"> 4</span><span class="gd">-def eager_mask(</span>
<span class="linenos"> 5</span><span class="gi">+def patched_eager_mask(</span>
<span class="linenos"> 6</span><span class="w"> </span>    batch_size: int,
<span class="linenos"> 7</span><span class="w"> </span>    cache_position: torch.Tensor,
<span class="linenos"> 8</span><span class="w"> </span>    kv_length: int,
<span class="linenos"> 9</span><span class="w"> </span>    kv_offset: int = 0,
<span class="linenos">10</span><span class="w"> </span>    mask_function: Callable = causal_mask_function,
<span class="linenos">11</span><span class="gd">-    attention_mask: torch.Tensor | None = None,</span>
<span class="linenos">12</span><span class="gi">+    attention_mask: Optional[torch.Tensor] = None,</span>
<span class="linenos">13</span><span class="w"> </span>    dtype: torch.dtype = torch.float32,
<span class="linenos">14</span><span class="gd">-    allow_is_bidirectional_skip: bool = False,</span>
<span class="linenos">15</span><span class="gd">-    use_vmap: bool = False,</span>
<span class="linenos">16</span><span class="w"> </span>    **kwargs,
<span class="linenos">17</span><span class="w"> </span>) -&gt; torch.Tensor:
<span class="linenos">18</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">19</span><span class="gd">-    Create a 4D float mask of shape `(batch_size, 1, query_length, kv_length)` where a value of 0 indicates that</span>
<span class="linenos">20</span><span class="gd">-    the element should take part in the attention computation, and -inf (minimum value for the given `dtype`) that</span>
<span class="linenos">21</span><span class="gd">-    it should not.</span>
<span class="linenos">22</span><span class="gd">-</span>
<span class="linenos">23</span><span class="gd">-    Args:</span>
<span class="linenos">24</span><span class="gd">-        batch_size (`int`):</span>
<span class="linenos">25</span><span class="gd">-            The batch size of the input sequence.</span>
<span class="linenos">26</span><span class="gd">-        cache_position (`torch.Tensor`):</span>
<span class="linenos">27</span><span class="gd">-            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.</span>
<span class="linenos">28</span><span class="gd">-        kv_length (`int`):</span>
<span class="linenos">29</span><span class="gd">-            The size that the key and value states will have during the attention computation.</span>
<span class="linenos">30</span><span class="gd">-        kv_offset (`int`, optional):</span>
<span class="linenos">31</span><span class="gd">-            An optional offset to indicate at which first position the key and values states will refer to.</span>
<span class="linenos">32</span><span class="gd">-        mask_function (`Callable`):</span>
<span class="linenos">33</span><span class="gd">-            The mask factory function describing the mask pattern.</span>
<span class="linenos">34</span><span class="gd">-        attention_mask (`torch.Tensor`, optional):</span>
<span class="linenos">35</span><span class="gd">-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)</span>
<span class="linenos">36</span><span class="gd">-        dtype (`torch.dtype`, optional):</span>
<span class="linenos">37</span><span class="gd">-            The dtype to use for the mask. By default, `torch.float32`.</span>
<span class="linenos">38</span><span class="gd">-        allow_is_bidirectional_skip (`bool`, optional):</span>
<span class="linenos">39</span><span class="gd">-            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,</span>
<span class="linenos">40</span><span class="gd">-            i.e. full attention without any padding. Default to `False`.</span>
<span class="linenos">41</span><span class="gd">-        use_vmap (`bool`, optional):</span>
<span class="linenos">42</span><span class="gd">-            Whether to use `vmap` during the mask construction or not. Allows powerful custom patterns that may not be</span>
<span class="linenos">43</span><span class="gd">-            index-based (for the cost of speed performance). By default `False`.</span>
<span class="linenos">44</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">45</span><span class="gi">+    &quot;&quot;&quot;manual patch for function ``transformers.masking_utils.eager_mask``.&quot;&quot;&quot;</span>
<span class="linenos">46</span><span class="w"> </span>    # The masks for eager attention are simply boolean mask from sdpa, casted to 0 and -inf
<span class="linenos">47</span><span class="w"> </span>    _ = kwargs.pop(&quot;allow_is_causal_skip&quot;, None)
<span class="linenos">48</span><span class="gd">-    _ = kwargs.pop(&quot;allow_torch_fix&quot;, None)</span>
<span class="linenos">49</span><span class="gd">-    mask = sdpa_mask(</span>
<span class="linenos">50</span><span class="gi">+    _ = kwargs.pop(&quot;allow_is_bidirectional_skip&quot;, None)</span>
<span class="linenos">51</span><span class="gi">+    # PATCHED: this line called the patched version of sdpa_mask</span>
<span class="linenos">52</span><span class="gi">+    mask = patched_sdpa_mask_recent_torch(</span>
<span class="linenos">53</span><span class="w"> </span>        batch_size=batch_size,
<span class="linenos">54</span><span class="w"> </span>        cache_position=cache_position,
<span class="linenos">55</span><span class="w"> </span>        kv_length=kv_length,
<span class="linenos">56</span><span class="gu">@@ -48,14 +21,15 @@</span>
<span class="linenos">57</span><span class="w"> </span>        mask_function=mask_function,
<span class="linenos">58</span><span class="w"> </span>        attention_mask=attention_mask,
<span class="linenos">59</span><span class="w"> </span>        allow_is_causal_skip=False,
<span class="linenos">60</span><span class="gd">-        allow_is_bidirectional_skip=allow_is_bidirectional_skip,</span>
<span class="linenos">61</span><span class="gi">+        allow_is_bidirectional_skip=False,</span>
<span class="linenos">62</span><span class="w"> </span>        allow_torch_fix=False,
<span class="linenos">63</span><span class="gd">-        use_vmap=use_vmap,</span>
<span class="linenos">64</span><span class="w"> </span>        **kwargs,
<span class="linenos">65</span><span class="w"> </span>    )
<span class="linenos">66</span><span class="gd">-    # only bidirectional masks can be skipped, otherwise we convert bool -&gt; float</span>
<span class="linenos">67</span><span class="gd">-    if mask is not None:</span>
<span class="linenos">68</span><span class="gd">-        min_dtype = torch.finfo(dtype).min</span>
<span class="linenos">69</span><span class="gd">-        # we need 0s where the tokens should be taken into account, and -inf otherwise (mask is already of boolean type)</span>
<span class="linenos">70</span><span class="gd">-        mask = torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), min_dtype)</span>
<span class="linenos">71</span><span class="gi">+    min_dtype = torch.finfo(dtype).min</span>
<span class="linenos">72</span><span class="gi">+    # PATCHED: the following line</span>
<span class="linenos">73</span><span class="gi">+    # we need 0s where the tokens should be taken into account,</span>
<span class="linenos">74</span><span class="gi">+    # and -inf otherwise (mask is already of boolean type)</span>
<span class="linenos">75</span><span class="gi">+    # mask =</span>
<span class="linenos">76</span><span class="gi">+    #   torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), min_dtype)</span>
<span class="linenos">77</span><span class="gi">+    mask = (~mask).to(dtype) * min_dtype</span>
<span class="linenos">78</span><span class="w"> </span>    return mask
</pre></div>
</div>
</section>
<section id="transformers-sdpa-attention-forward-patched-sdpa-attention-forward">
<h2>transformers: sdpa_attention_forward -&gt; patched_sdpa_attention_forward<a class="headerlink" href="#transformers-sdpa-attention-forward-patched-sdpa-attention-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,65 +1,142 @@</span>
<span class="linenos">  4</span><span class="gd">-def sdpa_attention_forward(</span>
<span class="linenos">  5</span><span class="gi">+def patched_sdpa_attention_forward(</span>
<span class="linenos">  6</span><span class="w"> </span>    module: torch.nn.Module,
<span class="linenos">  7</span><span class="w"> </span>    query: torch.Tensor,
<span class="linenos">  8</span><span class="w"> </span>    key: torch.Tensor,
<span class="linenos">  9</span><span class="w"> </span>    value: torch.Tensor,
<span class="linenos"> 10</span><span class="gd">-    attention_mask: torch.Tensor | None,</span>
<span class="linenos"> 11</span><span class="gi">+    attention_mask: Optional[torch.Tensor],</span>
<span class="linenos"> 12</span><span class="w"> </span>    dropout: float = 0.0,
<span class="linenos"> 13</span><span class="gd">-    scaling: float | None = None,</span>
<span class="linenos"> 14</span><span class="gd">-    is_causal: bool | None = None,</span>
<span class="linenos"> 15</span><span class="gi">+    scaling: Optional[float] = None,</span>
<span class="linenos"> 16</span><span class="gi">+    is_causal: Optional[bool] = None,</span>
<span class="linenos"> 17</span><span class="w"> </span>    **kwargs,
<span class="linenos"> 18</span><span class="w"> </span>) -&gt; tuple[torch.Tensor, None]:
<span class="linenos"> 19</span><span class="gd">-    if kwargs.get(&quot;output_attentions&quot;, False):</span>
<span class="linenos"> 20</span><span class="gd">-        logger.warning_once(</span>
<span class="linenos"> 21</span><span class="gd">-            &quot;`sdpa` attention does not support `output_attentions=True`.&quot;</span>
<span class="linenos"> 22</span><span class="gd">-            &quot; Please set your attention to `eager` if you want any of these features.&quot;</span>
<span class="linenos"> 23</span><span class="gd">-        )</span>
<span class="linenos"> 24</span><span class="gi">+    &quot;&quot;&quot;</span>
<span class="linenos"> 25</span><span class="gi">+    manual patch for function</span>
<span class="linenos"> 26</span><span class="gi">+    ``transformers.integrations.sdpa_attention.sdpa_attention_forward``</span>
<span class="linenos"> 27</span><span class="gi">+    &quot;&quot;&quot;</span>
<span class="linenos"> 28</span><span class="gi">+    assert not kwargs.get(&quot;output_attentions&quot;, False), (</span>
<span class="linenos"> 29</span><span class="gi">+        &quot;`sdpa` attention does not support `output_attentions=True`.&quot;</span>
<span class="linenos"> 30</span><span class="gi">+        &quot; Please set your attention to `eager` if you want any of these features.&quot;</span>
<span class="linenos"> 31</span><span class="gi">+    )</span>
<span class="linenos"> 32</span><span class="gi">+    torch._check(</span>
<span class="linenos"> 33</span><span class="gi">+        query.shape[0] == key.shape[0] or query.shape[0] == 1,</span>
<span class="linenos"> 34</span><span class="gi">+        lambda: (</span>
<span class="linenos"> 35</span><span class="gi">+            f&quot;broadcast issue query (1): {query.shape}, key: {key.shape}, &quot;</span>
<span class="linenos"> 36</span><span class="gi">+            f&quot;value: {value.shape}&quot;</span>
<span class="linenos"> 37</span><span class="gi">+        ),</span>
<span class="linenos"> 38</span><span class="gi">+    )</span>
<span class="linenos"> 39</span><span class="gi">+    torch._check(</span>
<span class="linenos"> 40</span><span class="gi">+        key.shape[0] == value.shape[0] or key.shape[0] == 1,</span>
<span class="linenos"> 41</span><span class="gi">+        lambda: (</span>
<span class="linenos"> 42</span><span class="gi">+            f&quot;broadcast issue query (2): {query.shape}, key: {key.shape}, &quot;</span>
<span class="linenos"> 43</span><span class="gi">+            f&quot;value: {value.shape}&quot;</span>
<span class="linenos"> 44</span><span class="gi">+        ),</span>
<span class="linenos"> 45</span><span class="gi">+    )</span>
<span class="linenos"> 46</span><span class="gi">+</span>
<span class="linenos"> 47</span><span class="w"> </span>    sdpa_kwargs = {}
<span class="linenos"> 48</span><span class="w"> </span>    if hasattr(module, &quot;num_key_value_groups&quot;):
<span class="linenos"> 49</span><span class="gd">-        if not use_gqa_in_sdpa(attention_mask, key):</span>
<span class="linenos"> 50</span><span class="gd">-            key = repeat_kv(key, module.num_key_value_groups)</span>
<span class="linenos"> 51</span><span class="gd">-            value = repeat_kv(value, module.num_key_value_groups)</span>
<span class="linenos"> 52</span><span class="gi">+        if not transformers.integrations.sdpa_attention.use_gqa_in_sdpa(attention_mask, key):</span>
<span class="linenos"> 53</span><span class="gi">+            key = transformers.integrations.sdpa_attention.repeat_kv(</span>
<span class="linenos"> 54</span><span class="gi">+                key, module.num_key_value_groups</span>
<span class="linenos"> 55</span><span class="gi">+            )</span>
<span class="linenos"> 56</span><span class="gi">+            value = transformers.integrations.sdpa_attention.repeat_kv(</span>
<span class="linenos"> 57</span><span class="gi">+                value, module.num_key_value_groups</span>
<span class="linenos"> 58</span><span class="gi">+            )</span>
<span class="linenos"> 59</span><span class="w"> </span>        else:
<span class="linenos"> 60</span><span class="w"> </span>            sdpa_kwargs = {&quot;enable_gqa&quot;: True}
<span class="linenos"> 61</span>
<span class="linenos"> 62</span><span class="gd">-    # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented</span>
<span class="linenos"> 63</span><span class="gd">-    is_causal = is_causal if is_causal is not None else getattr(module, &quot;is_causal&quot;, True)</span>
<span class="linenos"> 64</span><span class="gi">+    if attention_mask is not None and attention_mask.ndim == 4:</span>
<span class="linenos"> 65</span><span class="gi">+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]</span>
<span class="linenos"> 66</span>
<span class="linenos"> 67</span><span class="gd">-    # SDPA&#39;s Flash Attention (and cuDNN) kernels rely on the `is_causal` flag. However, there are certain conditions:</span>
<span class="linenos"> 68</span><span class="gd">-    # - Not in decoding phase (otherwise we want full attention on the single query token)</span>
<span class="linenos"> 69</span><span class="gd">-    # - Attention mask is not to be provided (even if it is a causal pattern)</span>
<span class="linenos"> 70</span><span class="gd">-    # - Internally, we marked this as compatible with causal, i.e. it is a decoder attention type</span>
<span class="linenos"> 71</span><span class="gd">-    #</span>
<span class="linenos"> 72</span><span class="gd">-    # Quirks on the conditionals:</span>
<span class="linenos"> 73</span><span class="gd">-    # - We avoid inline passing this to the SDPA function directly to support both torch.compile&#39;s dynamic shapes and</span>
<span class="linenos"> 74</span><span class="gd">-    #   full graph options. Otherwise, dynamic shapes are prevented from compiling.</span>
<span class="linenos"> 75</span><span class="gd">-    # - It is important to check first for the shape, otherwise compile will fail with</span>
<span class="linenos"> 76</span><span class="gd">-    #   `argument &#39;is_causal&#39; must be bool, not SymBool`.</span>
<span class="linenos"> 77</span><span class="gd">-    is_causal = query.shape[2] &gt; 1 and attention_mask is None and is_causal</span>
<span class="linenos"> 78</span><span class="gi">+    torch._check(</span>
<span class="linenos"> 79</span><span class="gi">+        attention_mask is None or attention_mask.shape[3] == key.shape[2],</span>
<span class="linenos"> 80</span><span class="gi">+        lambda: &quot;Attention mask shape incompatible with key shape.&quot;,</span>
<span class="linenos"> 81</span><span class="gi">+    )</span>
<span class="linenos"> 82</span>
<span class="linenos"> 83</span><span class="gd">-    # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.</span>
<span class="linenos"> 84</span><span class="gd">-    # We convert it to a bool for the SDPA kernel that only accepts bools.</span>
<span class="linenos"> 85</span><span class="gd">-    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):</span>
<span class="linenos"> 86</span><span class="gd">-        is_causal = is_causal.item()</span>
<span class="linenos"> 87</span><span class="gi">+    if patch_sdpa_is_causal:</span>
<span class="linenos"> 88</span><span class="gi">+        # transformers&gt;=4.55</span>
<span class="linenos"> 89</span><span class="gi">+        is_causal = is_causal if is_causal is not None else getattr(module, &quot;is_causal&quot;, True)</span>
<span class="linenos"> 90</span>
<span class="linenos"> 91</span><span class="gd">-    # When `is_causal = False` and the `attention_mask` is not of boolean type, the Ascend NPU&#39;s SDPA interface cannot utilize the FlashAttentionScore operator，</span>
<span class="linenos"> 92</span><span class="gd">-    # and falls back to small-operator concatenation. To invoke the FlashAttentionScore, the attention_mask must be converted to boolean type.</span>
<span class="linenos"> 93</span><span class="gd">-    # This adaptation ensures the `attention_mask` meets the requirement for using FlashAttentionScore.</span>
<span class="linenos"> 94</span><span class="gd">-    if _is_torch_npu_available:</span>
<span class="linenos"> 95</span><span class="gd">-        if attention_mask is not None and attention_mask.dtype != torch.bool:</span>
<span class="linenos"> 96</span><span class="gd">-            # Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.</span>
<span class="linenos"> 97</span><span class="gd">-            attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)</span>
<span class="linenos"> 98</span><span class="gi">+        # PATCHED: remove the test query.shape[2] &gt; 1</span>
<span class="linenos"> 99</span><span class="gi">+        # is_causal = query.shape[2] &gt; 1 and attention_mask is None and is_causal</span>
<span class="linenos">100</span><span class="gi">+        # and we split the test to keep the minimum in torch.cond</span>
<span class="linenos">101</span><span class="gi">+        is_causal = attention_mask is None and is_causal</span>
<span class="linenos">102</span>
<span class="linenos">103</span><span class="gd">-    attn_output = torch.nn.functional.scaled_dot_product_attention(</span>
<span class="linenos">104</span><span class="gd">-        query,</span>
<span class="linenos">105</span><span class="gd">-        key,</span>
<span class="linenos">106</span><span class="gd">-        value,</span>
<span class="linenos">107</span><span class="gd">-        attn_mask=attention_mask,</span>
<span class="linenos">108</span><span class="gd">-        dropout_p=dropout,</span>
<span class="linenos">109</span><span class="gd">-        scale=scaling,</span>
<span class="linenos">110</span><span class="gd">-        is_causal=is_causal,</span>
<span class="linenos">111</span><span class="gd">-        **sdpa_kwargs,</span>
<span class="linenos">112</span><span class="gi">+        if not is_causal:</span>
<span class="linenos">113</span><span class="gi">+            torch._check(query.shape[0] &gt; 0)</span>
<span class="linenos">114</span><span class="gi">+            torch._check(query.shape[1] &gt; 0)</span>
<span class="linenos">115</span><span class="gi">+            torch._check(query.shape[2] &gt; 0)</span>
<span class="linenos">116</span><span class="gi">+            torch._check(query.shape[3] &gt; 0)</span>
<span class="linenos">117</span><span class="gi">+            torch._check(key.shape[0] &gt; 0)</span>
<span class="linenos">118</span><span class="gi">+            torch._check(key.shape[1] &gt; 0)</span>
<span class="linenos">119</span><span class="gi">+            torch._check(key.shape[2] &gt; 0)</span>
<span class="linenos">120</span><span class="gi">+            torch._check(key.shape[3] &gt; 0)</span>
<span class="linenos">121</span><span class="gi">+            torch._check(value.shape[0] &gt; 0)</span>
<span class="linenos">122</span><span class="gi">+            torch._check(value.shape[1] &gt; 0)</span>
<span class="linenos">123</span><span class="gi">+            torch._check(value.shape[2] &gt; 0)</span>
<span class="linenos">124</span><span class="gi">+            torch._check(value.shape[3] &gt; 0)</span>
<span class="linenos">125</span><span class="gi">+</span>
<span class="linenos">126</span><span class="gi">+            return (</span>
<span class="linenos">127</span><span class="gi">+                torch.nn.functional.scaled_dot_product_attention(</span>
<span class="linenos">128</span><span class="gi">+                    query,</span>
<span class="linenos">129</span><span class="gi">+                    key,</span>
<span class="linenos">130</span><span class="gi">+                    value,</span>
<span class="linenos">131</span><span class="gi">+                    attn_mask=attention_mask,</span>
<span class="linenos">132</span><span class="gi">+                    dropout_p=dropout,</span>
<span class="linenos">133</span><span class="gi">+                    scale=scaling,</span>
<span class="linenos">134</span><span class="gi">+                    is_causal=is_causal,</span>
<span class="linenos">135</span><span class="gi">+                    **sdpa_kwargs,</span>
<span class="linenos">136</span><span class="gi">+                )</span>
<span class="linenos">137</span><span class="gi">+                .transpose(1, 2)</span>
<span class="linenos">138</span><span class="gi">+                .contiguous(),</span>
<span class="linenos">139</span><span class="gi">+                None,</span>
<span class="linenos">140</span><span class="gi">+            )</span>
<span class="linenos">141</span><span class="gi">+    else:</span>
<span class="linenos">142</span><span class="gi">+        # transformers&lt;4.55</span>
<span class="linenos">143</span><span class="gi">+        if is_causal is None and attention_mask is not None:</span>
<span class="linenos">144</span><span class="gi">+            is_causal = False</span>
<span class="linenos">145</span><span class="gi">+        if is_causal is not None:</span>
<span class="linenos">146</span><span class="gi">+            return (</span>
<span class="linenos">147</span><span class="gi">+                torch.nn.functional.scaled_dot_product_attention(</span>
<span class="linenos">148</span><span class="gi">+                    query,</span>
<span class="linenos">149</span><span class="gi">+                    key,</span>
<span class="linenos">150</span><span class="gi">+                    value,</span>
<span class="linenos">151</span><span class="gi">+                    attn_mask=attention_mask,</span>
<span class="linenos">152</span><span class="gi">+                    dropout_p=dropout,</span>
<span class="linenos">153</span><span class="gi">+                    scale=scaling,</span>
<span class="linenos">154</span><span class="gi">+                    is_causal=is_causal,</span>
<span class="linenos">155</span><span class="gi">+                    **sdpa_kwargs,</span>
<span class="linenos">156</span><span class="gi">+                )</span>
<span class="linenos">157</span><span class="gi">+                .transpose(1, 2)</span>
<span class="linenos">158</span><span class="gi">+                .contiguous(),</span>
<span class="linenos">159</span><span class="gi">+                None,</span>
<span class="linenos">160</span><span class="gi">+            )</span>
<span class="linenos">161</span><span class="gi">+</span>
<span class="linenos">162</span><span class="gi">+    # To avoid the following errors:</span>
<span class="linenos">163</span><span class="gi">+    # is_causal=query.shape[2] &gt; 1</span>
<span class="linenos">164</span><span class="gi">+    # TypeError: scaled_dot_product_attention(): argument &#39;is_causal&#39; must be bool, not SymBool</span>
<span class="linenos">165</span><span class="gi">+    # is_causal=torch.tensor(query.shape[2] &gt; 1)</span>
<span class="linenos">166</span><span class="gi">+    # TypeError: scaled_dot_product_attention(): argument &#39;is_causal&#39; must be bool, not Tensor</span>
<span class="linenos">167</span><span class="gi">+    attn_output = torch.cond(</span>
<span class="linenos">168</span><span class="gi">+        query.shape[2] &gt; 1,  # distinction between prefill and decoding steps</span>
<span class="linenos">169</span><span class="gi">+        lambda query, key, value: torch.nn.functional.scaled_dot_product_attention(</span>
<span class="linenos">170</span><span class="gi">+            query,</span>
<span class="linenos">171</span><span class="gi">+            key,</span>
<span class="linenos">172</span><span class="gi">+            value,</span>
<span class="linenos">173</span><span class="gi">+            dropout_p=dropout,</span>
<span class="linenos">174</span><span class="gi">+            scale=scaling,</span>
<span class="linenos">175</span><span class="gi">+            is_causal=True,</span>
<span class="linenos">176</span><span class="gi">+            **sdpa_kwargs,</span>
<span class="linenos">177</span><span class="gi">+        ).contiguous(),</span>
<span class="linenos">178</span><span class="gi">+        lambda query, key, value: torch.nn.functional.scaled_dot_product_attention(</span>
<span class="linenos">179</span><span class="gi">+            query,</span>
<span class="linenos">180</span><span class="gi">+            key,</span>
<span class="linenos">181</span><span class="gi">+            value,</span>
<span class="linenos">182</span><span class="gi">+            dropout_p=dropout,</span>
<span class="linenos">183</span><span class="gi">+            scale=scaling,</span>
<span class="linenos">184</span><span class="gi">+            is_causal=False,</span>
<span class="linenos">185</span><span class="gi">+            **sdpa_kwargs,</span>
<span class="linenos">186</span><span class="gi">+        ).contiguous(),</span>
<span class="linenos">187</span><span class="gi">+        [query, key, value],</span>
<span class="linenos">188</span><span class="w"> </span>    )
<span class="linenos">189</span><span class="w"> </span>    attn_output = attn_output.transpose(1, 2).contiguous()
<span class="linenos">190</span><span class="gd">-</span>
<span class="linenos">191</span><span class="w"> </span>    return attn_output, None
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-dynamiclayer-lazy-initialization-patched-dynamiclayer-lazy-initialization">
<h2>auto/patch_transformers: DynamicLayer.lazy_initialization -&gt; patched_DynamicLayer.lazy_initialization<a class="headerlink" href="#auto-patch-transformers-dynamiclayer-lazy-initialization-patched-dynamiclayer-lazy-initialization" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,5 +1,16 @@</span>
<span class="linenos"> 4</span><span class="gd">-def lazy_initialization(self, key_states: torch.Tensor, value_states: torch.Tensor) -&gt; None:</span>
<span class="linenos"> 5</span><span class="gi">+def lazy_initialization(self, key_states: torch.Tensor, value_states: torch.Tensor = None):</span>
<span class="linenos"> 6</span><span class="w"> </span>    self.dtype, self.device = key_states.dtype, key_states.device
<span class="linenos"> 7</span><span class="gd">-    self.keys = torch.tensor([], dtype=self.dtype, device=self.device)</span>
<span class="linenos"> 8</span><span class="gd">-    self.values = torch.tensor([], dtype=self.dtype, device=self.device)</span>
<span class="linenos"> 9</span><span class="gd">-    self.is_initialized = True</span>
<span class="linenos">10</span><span class="gi">+    assert (</span>
<span class="linenos">11</span><span class="gi">+        hasattr(key_states, &quot;shape&quot;) and key_states is not None</span>
<span class="linenos">12</span><span class="gi">+    ), f&quot;Attribute &#39;shape&#39; is wrong for type {type(key_states)}&quot;</span>
<span class="linenos">13</span><span class="gi">+    like = torch.narrow(key_states, dim=-2, start=0, length=0)</span>
<span class="linenos">14</span><span class="gi">+    # PATCHED: used a tensor with an empty shape and not en empty list to initialize</span>
<span class="linenos">15</span><span class="gi">+    if isinstance(key_states, torch._subclasses.fake_tensor.FakeTensor):</span>
<span class="linenos">16</span><span class="gi">+        with key_states.fake_mode:</span>
<span class="linenos">17</span><span class="gi">+            self.keys = torch.empty_like(like, dtype=self.dtype, device=self.device)</span>
<span class="linenos">18</span><span class="gi">+            self.values = torch.empty_like(like, dtype=self.dtype, device=self.device)</span>
<span class="linenos">19</span><span class="gi">+    else:</span>
<span class="linenos">20</span><span class="gi">+        self.keys = torch.empty_like(like, dtype=self.dtype, device=self.device)</span>
<span class="linenos">21</span><span class="gi">+        self.values = torch.empty_like(like, dtype=self.dtype, device=self.device)</span>
<span class="linenos">22</span><span class="gi">+    if patch_is_initialized:</span>
<span class="linenos">23</span><span class="gi">+        self.is_initialized = True</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-funnelattentionstructure-relative-pos-patched-funnelattentionstructure-relative-pos">
<h2>auto/patch_transformers: FunnelAttentionStructure.relative_pos -&gt; patched_FunnelAttentionStructure.relative_pos<a class="headerlink" href="#auto-patch-transformers-funnelattentionstructure-relative-pos-patched-funnelattentionstructure-relative-pos" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,15 +1,17 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>def relative_pos(
<span class="linenos"> 5</span><span class="w"> </span>    self, pos: torch.Tensor, stride: int, pooled_pos=None, shift: int = 1
<span class="linenos"> 6</span><span class="w"> </span>) -&gt; torch.Tensor:
<span class="linenos"> 7</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 8</span><span class="gd">-    Build the relative positional vector between `pos` and `pooled_pos`.</span>
<span class="linenos"> 9</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">10</span><span class="w"> </span>    if pooled_pos is None:
<span class="linenos">11</span><span class="w"> </span>        pooled_pos = pos
<span class="linenos">12</span><span class="gd">-</span>
<span class="linenos">13</span><span class="w"> </span>    ref_point = pooled_pos[0] - pos[0]
<span class="linenos">14</span><span class="gd">-    num_remove = shift * len(pooled_pos)</span>
<span class="linenos">15</span><span class="gi">+    # PATCHED</span>
<span class="linenos">16</span><span class="gi">+    num_remove = shift * pooled_pos.shape[0]</span>
<span class="linenos">17</span><span class="w"> </span>    max_dist = ref_point + num_remove * stride
<span class="linenos">18</span><span class="w"> </span>    min_dist = pooled_pos[0] - pos[-1]
<span class="linenos">19</span><span class="gd">-</span>
<span class="linenos">20</span><span class="gd">-    return torch.arange(max_dist, min_dist - 1, -stride, dtype=torch.long, device=pos.device)</span>
<span class="linenos">21</span><span class="gi">+    return torch.arange(</span>
<span class="linenos">22</span><span class="gi">+        max_dist.to(torch.long),</span>
<span class="linenos">23</span><span class="gi">+        (min_dist - 1).to(torch.long),</span>
<span class="linenos">24</span><span class="gi">+        torch.tensor(-stride, dtype=torch.long),</span>
<span class="linenos">25</span><span class="gi">+        dtype=torch.long,</span>
<span class="linenos">26</span><span class="gi">+        device=pos.device,</span>
<span class="linenos">27</span><span class="gi">+    )</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-funnelrelmultiheadattention-relative-positional-attention-patched-funnelrelmultiheadattention-relative-positional-attention">
<h2>auto/patch_transformers: FunnelRelMultiheadAttention.relative_positional_attention -&gt; patched_FunnelRelMultiheadAttention.relative_positional_attention<a class="headerlink" href="#auto-patch-transformers-funnelrelmultiheadattention-relative-positional-attention-patched-funnelrelmultiheadattention-relative-positional-attention" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -2,8 +2,6 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>    &quot;&quot;&quot;Relative attention score for the positional encodings&quot;&quot;&quot;
<span class="linenos"> 5</span><span class="w"> </span>    # q_head has shape batch_size x sea_len x n_head x d_head
<span class="linenos"> 6</span><span class="w"> </span>    if self.config.attention_type == &quot;factorized&quot;:
<span class="linenos"> 7</span><span class="gd">-        # Notations from the paper, appending A.2.2, final formula (https://huggingface.co/papers/2006.03236)</span>
<span class="linenos"> 8</span><span class="gd">-        # phi and pi have shape seq_len x d_model, psi and omega have shape context_len x d_model</span>
<span class="linenos"> 9</span><span class="w"> </span>        phi, pi, psi, omega = position_embeds
<span class="linenos">10</span><span class="w"> </span>        # Shape n_head x d_head
<span class="linenos">11</span><span class="w"> </span>        u = self.r_r_bias * self.scale
<span class="linenos">12</span><span class="gu">@@ -21,8 +19,6 @@</span>
<span class="linenos">13</span><span class="w"> </span>        )
<span class="linenos">14</span><span class="w"> </span>    else:
<span class="linenos">15</span><span class="w"> </span>        shift = 2 if q_head.shape[1] != context_len else 1
<span class="linenos">16</span><span class="gd">-        # Notations from the paper, appending A.2.1, final formula (https://huggingface.co/papers/2006.03236)</span>
<span class="linenos">17</span><span class="gd">-        # Grab the proper positional encoding, shape max_rel_len x d_model</span>
<span class="linenos">18</span><span class="w"> </span>        r = position_embeds[self.block_index][shift - 1]
<span class="linenos">19</span><span class="w"> </span>        # Shape n_head x d_head
<span class="linenos">20</span><span class="w"> </span>        v = self.r_r_bias * self.scale
<span class="linenos">21</span><span class="gu">@@ -37,5 +33,6 @@</span>
<span class="linenos">22</span><span class="w"> </span>        positional_attn = _relative_shift_gather(positional_attn, context_len, shift)
<span class="linenos">23</span>
<span class="linenos">24</span><span class="w"> </span>    if cls_mask is not None:
<span class="linenos">25</span><span class="gd">-        positional_attn *= cls_mask</span>
<span class="linenos">26</span><span class="gi">+        # PATCHED</span>
<span class="linenos">27</span><span class="gi">+        positional_attn = positional_attn * cls_mask</span>
<span class="linenos">28</span><span class="w"> </span>    return positional_attn
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-gemma2rotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: Gemma2RotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-gemma2rotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-gemma3model-get-placeholder-mask-patched-gemma3model-get-placeholder-mask">
<h2>auto/patch_transformers: Gemma3Model.get_placeholder_mask -&gt; patched_Gemma3Model.get_placeholder_mask<a class="headerlink" href="#auto-patch-transformers-gemma3model-get-placeholder-mask-patched-gemma3model-get-placeholder-mask" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -4,14 +4,12 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>    inputs_embeds: torch.FloatTensor,
<span class="linenos"> 5</span><span class="w"> </span>    image_features: torch.FloatTensor,
<span class="linenos"> 6</span><span class="w"> </span>):
<span class="linenos"> 7</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 8</span><span class="gd">-    Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is</span>
<span class="linenos"> 9</span><span class="gd">-    equal to the length of multimodal features. If the lengths are different, an error is raised.</span>
<span class="linenos">10</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">11</span><span class="w"> </span>    if input_ids is None:
<span class="linenos">12</span><span class="w"> </span>        special_image_mask = inputs_embeds == self.get_input_embeddings()(
<span class="linenos">13</span><span class="w"> </span>            torch.tensor(
<span class="linenos">14</span><span class="gd">-                self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device</span>
<span class="linenos">15</span><span class="gi">+                self.config.image_token_id,</span>
<span class="linenos">16</span><span class="gi">+                dtype=torch.long,</span>
<span class="linenos">17</span><span class="gi">+                device=inputs_embeds.device,</span>
<span class="linenos">18</span><span class="w"> </span>            )
<span class="linenos">19</span><span class="w"> </span>        )
<span class="linenos">20</span><span class="w"> </span>        special_image_mask = special_image_mask.all(-1)
<span class="linenos">21</span><span class="gu">@@ -19,12 +17,18 @@</span>
<span class="linenos">22</span><span class="w"> </span>        special_image_mask = input_ids == self.config.image_token_id
<span class="linenos">23</span>
<span class="linenos">24</span><span class="w"> </span>    n_image_tokens = special_image_mask.sum()
<span class="linenos">25</span><span class="gd">-    n_image_features = image_features.shape[0] * image_features.shape[1]</span>
<span class="linenos">26</span><span class="w"> </span>    special_image_mask = (
<span class="linenos">27</span><span class="w"> </span>        special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
<span class="linenos">28</span><span class="w"> </span>    )
<span class="linenos">29</span><span class="gd">-    torch_compilable_check(</span>
<span class="linenos">30</span><span class="gi">+    n_image_features = image_features.shape[0] * image_features.shape[1]</span>
<span class="linenos">31</span><span class="gi">+    # PATCHED: torch._check</span>
<span class="linenos">32</span><span class="gi">+    # if inputs_embeds[special_image_mask].numel() != image_features.numel():</span>
<span class="linenos">33</span><span class="gi">+    #    raise ValueError( ... )</span>
<span class="linenos">34</span><span class="gi">+    torch._check(</span>
<span class="linenos">35</span><span class="w"> </span>        inputs_embeds[special_image_mask].numel() == image_features.numel(),
<span class="linenos">36</span><span class="gd">-        f&quot;Image features and image tokens do not match, tokens: {n_image_tokens}, features: {n_image_features}&quot;,</span>
<span class="linenos">37</span><span class="gi">+        lambda: (</span>
<span class="linenos">38</span><span class="gi">+            f&quot;Image features and image tokens do not match: tokens: &quot;</span>
<span class="linenos">39</span><span class="gi">+            f&quot;{n_image_tokens}, features {n_image_features}&quot;</span>
<span class="linenos">40</span><span class="gi">+        ),</span>
<span class="linenos">41</span><span class="w"> </span>    )
<span class="linenos">42</span><span class="w"> </span>    return special_image_mask
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-gemma3rotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: Gemma3RotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-gemma3rotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,8 +1,13 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  7</span><span class="w"> </span>def forward(self, x, position_ids, layer_type=None):
<span class="linenos">  8</span><span class="gd">-    inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos">  9</span><span class="gd">-    attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 10</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 11</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 12</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 14</span><span class="gi">+    else:</span>
<span class="linenos"> 15</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 16</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 17</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 18</span>
<span class="linenos"> 19</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 20</span><span class="w"> </span>        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
<span class="linenos"> 21</span><span class="gu">@@ -12,7 +17,7 @@</span>
<span class="linenos"> 22</span><span class="w"> </span>    device_type = (
<span class="linenos"> 23</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 24</span><span class="w"> </span>    )
<span class="linenos"> 25</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 26</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 27</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 28</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 29</span><span class="w"> </span>        cos = emb.cos() * attention_scaling
<span class="linenos"> 30</span>
<span class="linenos"> 31</span><span class="gd">--- original</span>
<span class="linenos"> 32</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 33</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 34</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 35</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 36</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 37</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 38</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 39</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 40</span>
<span class="linenos"> 41</span><span class="gd">-    Args:</span>
<span class="linenos"> 42</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 43</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 44</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 45</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 46</span>
<span class="linenos"> 47</span><span class="gd">-    Returns:</span>
<span class="linenos"> 48</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 49</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 50</span><span class="gi">+</span>
<span class="linenos"> 51</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 52</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 53</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 54</span><span class="gi">+        else:</span>
<span class="linenos"> 55</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 56</span><span class="gi">+</span>
<span class="linenos"> 57</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 58</span><span class="gi">+</span>
<span class="linenos"> 59</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 60</span><span class="gi">+</span>
<span class="linenos"> 61</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 62</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 63</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 64</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 65</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 66</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 67</span><span class="gi">+                else:</span>
<span class="linenos"> 68</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 69</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 70</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 71</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 73</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 74</span><span class="gi">+                        )</span>
<span class="linenos"> 75</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 76</span><span class="gi">+                else:</span>
<span class="linenos"> 77</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 78</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 79</span><span class="gi">+</span>
<span class="linenos"> 80</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 81</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 82</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 83</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 84</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 87</span><span class="gi">+</span>
<span class="linenos"> 88</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 89</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 90</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 91</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos"> 93</span><span class="gi">+</span>
<span class="linenos"> 94</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos"> 95</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos"> 96</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos"> 97</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos"> 98</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos"> 99</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">100</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">101</span><span class="gi">+</span>
<span class="linenos">102</span><span class="gi">+            return wrapper</span>
<span class="linenos">103</span><span class="gi">+</span>
<span class="linenos">104</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">105</span>
<span class="linenos">106</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">107</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">108</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">109</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">110</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">111</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">112</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">113</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">114</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">115</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">116</span><span class="gi">+        else:</span>
<span class="linenos">117</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">118</span>
<span class="linenos">119</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">120</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">121</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">122</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">123</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">124</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">125</span><span class="gd">-            ]</span>
<span class="linenos">126</span><span class="gd">-        else:</span>
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">129</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-</span>
<span class="linenos">134</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">135</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">136</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">137</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">138</span><span class="gd">-                    self.config,</span>
<span class="linenos">139</span><span class="gd">-                    device,</span>
<span class="linenos">140</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">141</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">142</span><span class="gd">-                )</span>
<span class="linenos">143</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">144</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">145</span><span class="gd">-        else:</span>
<span class="linenos">146</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">147</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">148</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">149</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">150</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">151</span><span class="gd">-</span>
<span class="linenos">152</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">153</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">154</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">155</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">156</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">157</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">158</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">159</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">160</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">161</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">162</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">163</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">164</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">165</span><span class="w"> </span>        else:
<span class="linenos">166</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">167</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">168</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gd">-            )</span>
<span class="linenos">170</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">171</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">172</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">173</span>
<span class="linenos">174</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">175</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">176</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">177</span><span class="gd">-                self.config,</span>
<span class="linenos">178</span><span class="gd">-                device,</span>
<span class="linenos">179</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">180</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">181</span><span class="gd">-            )</span>
<span class="linenos">182</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">183</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">184</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">185</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">186</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">187</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">188</span><span class="gi">+        )</span>
<span class="linenos">189</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">190</span>
<span class="linenos">191</span><span class="gd">-        if (</span>
<span class="linenos">192</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">193</span><span class="gd">-        ):  # reset</span>
<span class="linenos">194</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">195</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">196</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">197</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">198</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">199</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">200</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">201</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">202</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">203</span><span class="gi">+            cond,</span>
<span class="linenos">204</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">205</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">206</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">207</span><span class="gi">+        )</span>
<span class="linenos">208</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">209</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">210</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">211</span><span class="gi">+        # else:</span>
<span class="linenos">212</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">213</span><span class="gi">+</span>
<span class="linenos">214</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">215</span><span class="gi">+        # constructor:</span>
<span class="linenos">216</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">217</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">218</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">219</span><span class="gi">+</span>
<span class="linenos">220</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">221</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">222</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">223</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">224</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">225</span><span class="gi">+</span>
<span class="linenos">226</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">227</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">228</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">229</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">230</span><span class="gi">+        #</span>
<span class="linenos">231</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">232</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">233</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">234</span><span class="gi">+        #    )</span>
<span class="linenos">235</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">236</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">239</span><span class="gi">+        #</span>
<span class="linenos">240</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">241</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">242</span><span class="gi">+        #    cond,</span>
<span class="linenos">243</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">244</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">245</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">246</span><span class="gi">+        # )</span>
<span class="linenos">247</span><span class="gi">+</span>
<span class="linenos">248</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">249</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">250</span><span class="gi">+</span>
<span class="linenos">251</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">252</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">253</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">254</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">255</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">256</span><span class="gi">+        else:</span>
<span class="linenos">257</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">258</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">259</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">260</span><span class="gi">+            # )</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">262</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">263</span><span class="gi">+</span>
<span class="linenos">264</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">265</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">266</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">267</span><span class="gi">+</span>
<span class="linenos">268</span><span class="gi">+        # if (</span>
<span class="linenos">269</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">270</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">271</span><span class="gi">+        # ):</span>
<span class="linenos">272</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">273</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">274</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">275</span><span class="gi">+</span>
<span class="linenos">276</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">277</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">278</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">279</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">280</span><span class="gi">+            cond,</span>
<span class="linenos">281</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">282</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">283</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">284</span><span class="gi">+        )</span>
<span class="linenos">285</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">286</span>
<span class="linenos">287</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">288</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">289</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">290</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">291</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">292</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">293</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">294</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">295</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">296</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">297</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">298</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">299</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">300</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">301</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">302</span><span class="gi">+</span>
<span class="linenos">303</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">304</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">305</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">306</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">307</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">308</span>
<span class="linenos">309</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-gemmarotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: GemmaRotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-gemmarotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-generationmixin-cache-dependant-input-preparation-patched-generationmixin-cache-dependant-input-preparation">
<h2>auto/patch_transformers: GenerationMixin._cache_dependant_input_preparation -&gt; patched_GenerationMixin._cache_dependant_input_preparation<a class="headerlink" href="#auto-patch-transformers-generationmixin-cache-dependant-input-preparation-patched-generationmixin-cache-dependant-input-preparation" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,25 +1,31 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>def _cache_dependant_input_preparation(
<span class="linenos"> 5</span><span class="w"> </span>    self,
<span class="linenos"> 6</span><span class="w"> </span>    input_ids: torch.LongTensor,
<span class="linenos"> 7</span><span class="gd">-    inputs_embeds: torch.FloatTensor | None,</span>
<span class="linenos"> 8</span><span class="gd">-    cache_position: torch.LongTensor | None,</span>
<span class="linenos"> 9</span><span class="gd">-) -&gt; tuple[torch.FloatTensor, torch.LongTensor]:</span>
<span class="linenos">10</span><span class="gi">+    inputs_embeds: Optional[torch.FloatTensor],</span>
<span class="linenos">11</span><span class="gi">+    cache_position: Optional[torch.LongTensor],</span>
<span class="linenos">12</span><span class="gi">+) -&gt; Tuple[torch.FloatTensor, torch.LongTensor]:</span>
<span class="linenos">13</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">14</span><span class="w"> </span>    Generic cache-dependent input preparation
<span class="linenos">15</span><span class="w"> </span>    The code is put in a separate function to allow granular unit testing
<span class="linenos">16</span><span class="w"> </span>    as it needs a different implementation to be exportable.
<span class="linenos">17</span>
<span class="linenos">18</span><span class="gd">-    If we have cache: let&#39;s slice `input_ids` through `cache_position`, to keep only the unprocessed tokens</span>
<span class="linenos">19</span><span class="gd">-    - Exception 1: when passing input_embeds, input_ids may be missing entries</span>
<span class="linenos">20</span><span class="gd">-    - Exception 2: some generation methods do special slicing of input_ids, so we don&#39;t need to do it here</span>
<span class="linenos">21</span><span class="gd">-    - Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.</span>
<span class="linenos">22</span><span class="gd">-    - Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and</span>
<span class="linenos">23</span><span class="gd">-      generate the first token for each sequence. Later use the generated Input ids for continuation.</span>
<span class="linenos">24</span><span class="gi">+    If we have cache: let&#39;s slice `input_ids` through `cache_position`,</span>
<span class="linenos">25</span><span class="gi">+    to keep only the unprocessed tokens</span>
<span class="linenos">26</span><span class="gi">+    - Exception 1: when passing input_embeds,</span>
<span class="linenos">27</span><span class="gi">+      input_ids may be missing entries</span>
<span class="linenos">28</span><span class="gi">+    - Exception 2: some generation methods do special slicing of input_ids,</span>
<span class="linenos">29</span><span class="gi">+      so we don&#39;t need to do it here</span>
<span class="linenos">30</span><span class="gi">+    - Exception 3: with synced GPUs cache_position may go out of bounds,</span>
<span class="linenos">31</span><span class="gi">+      but we only want dummy token in that case.</span>
<span class="linenos">32</span><span class="gi">+    - Exception 4: If input_embeds are passed then slice it through</span>
<span class="linenos">33</span><span class="gi">+      `cache_position`, to keep only the unprocessed tokens and</span>
<span class="linenos">34</span><span class="gi">+      generate the first token for each sequence.</span>
<span class="linenos">35</span><span class="gi">+      Later use the generated Input ids for continuation.</span>
<span class="linenos">36</span>
<span class="linenos">37</span><span class="w"> </span>    The current implementation does not rely on ``self`` and could be
<span class="linenos">38</span><span class="w"> </span>    a class method. It is left as a standard method to be easily rewritten.
<span class="linenos">39</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">40</span><span class="gd">-    if is_torchdynamo_exporting():</span>
<span class="linenos">41</span><span class="gi">+    if _is_torchdynamo_exporting():</span>
<span class="linenos">42</span><span class="w"> </span>        return self._cache_dependant_input_preparation_exporting(
<span class="linenos">43</span><span class="w"> </span>            input_ids, inputs_embeds, cache_position
<span class="linenos">44</span><span class="w"> </span>        )
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-generationmixin-cache-dependant-input-preparation-exporting-patched-generationmixin-cache-dependant-input-preparation-exporting">
<h2>auto/patch_transformers: GenerationMixin._cache_dependant_input_preparation_exporting -&gt; patched_GenerationMixin._cache_dependant_input_preparation_exporting<a class="headerlink" href="#auto-patch-transformers-generationmixin-cache-dependant-input-preparation-exporting-patched-generationmixin-cache-dependant-input-preparation-exporting" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,9 +1,9 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>def _cache_dependant_input_preparation_exporting(
<span class="linenos"> 5</span><span class="w"> </span>    self,
<span class="linenos"> 6</span><span class="w"> </span>    input_ids: torch.LongTensor,
<span class="linenos"> 7</span><span class="gd">-    inputs_embeds: torch.FloatTensor | None,</span>
<span class="linenos"> 8</span><span class="gd">-    cache_position: torch.LongTensor | None,</span>
<span class="linenos"> 9</span><span class="gd">-) -&gt; tuple[torch.FloatTensor, torch.LongTensor]:</span>
<span class="linenos">10</span><span class="gi">+    inputs_embeds: Optional[torch.FloatTensor],</span>
<span class="linenos">11</span><span class="gi">+    cache_position: Optional[torch.LongTensor],</span>
<span class="linenos">12</span><span class="gi">+) -&gt; Tuple[torch.FloatTensor, torch.LongTensor]:</span>
<span class="linenos">13</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">14</span><span class="w"> </span>    This method implements method ``_cache_dependant_input_preparation``
<span class="linenos">15</span><span class="w"> </span>    with :func:`torch.cond` to make it exportable with :func:`torch.export.export`.
<span class="linenos">16</span><span class="gu">@@ -21,7 +21,6 @@</span>
<span class="linenos">17</span><span class="w"> </span>        #     else:
<span class="linenos">18</span><span class="w"> </span>        #         if input_ids.shape[1] != cache_position.shape[0]:
<span class="linenos">19</span><span class="w"> </span>        #             input_ids = input_ids[:, cache_position]
<span class="linenos">20</span><span class="gd">-        # We need to clone the outputs to avoid aliasing.</span>
<span class="linenos">21</span><span class="w"> </span>        def branch_1(inputs_embeds, cache_position):
<span class="linenos">22</span><span class="w"> </span>            return inputs_embeds[:, -cache_position.shape[0] :].clone()
<span class="linenos">23</span>
<span class="linenos">24</span><span class="gu">@@ -49,7 +48,7 @@</span>
<span class="linenos">25</span><span class="w"> </span>                            torch.cond(
<span class="linenos">26</span><span class="w"> </span>                                input_ids.shape[1] != cache_position.shape[0],
<span class="linenos">27</span><span class="w"> </span>                                branch_3,
<span class="linenos">28</span><span class="gd">-                                (lambda input_ids, cache_position: input_ids.clone()),</span>
<span class="linenos">29</span><span class="gi">+                                (lambda input_ids, cache_position: input_ids),</span>
<span class="linenos">30</span><span class="w"> </span>                                [input_ids, cache_position],
<span class="linenos">31</span><span class="w"> </span>                            )
<span class="linenos">32</span><span class="w"> </span>                        ),
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-ideficsattention-forward-patched-ideficsattention-forward">
<h2>auto/patch_transformers: IdeficsAttention.forward -&gt; patched_IdeficsAttention.forward<a class="headerlink" href="#auto-patch-transformers-ideficsattention-forward-patched-ideficsattention-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,13 +1,15 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>def forward(
<span class="linenos"> 5</span><span class="w"> </span>    self,
<span class="linenos"> 6</span><span class="w"> </span>    hidden_states: torch.Tensor,
<span class="linenos"> 7</span><span class="gd">-    key_value_states: torch.Tensor | None = None,</span>
<span class="linenos"> 8</span><span class="gd">-    attention_mask: torch.Tensor | None = None,</span>
<span class="linenos"> 9</span><span class="gd">-    position_ids: torch.LongTensor | None = None,</span>
<span class="linenos">10</span><span class="gd">-    past_key_values: Cache | None = None,</span>
<span class="linenos">11</span><span class="gd">-    cache_position: torch.LongTensor | None = None,</span>
<span class="linenos">12</span><span class="gd">-    **kwargs: Unpack[TransformersKwargs],</span>
<span class="linenos">13</span><span class="gd">-) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
<span class="linenos">14</span><span class="gi">+    key_value_states: Optional[torch.Tensor] = None,</span>
<span class="linenos">15</span><span class="gi">+    attention_mask: Optional[torch.Tensor] = None,</span>
<span class="linenos">16</span><span class="gi">+    position_ids: Optional[torch.LongTensor] = None,</span>
<span class="linenos">17</span><span class="gi">+    past_key_value: Optional[Tuple[torch.Tensor]] = None,</span>
<span class="linenos">18</span><span class="gi">+    output_attentions: bool = False,</span>
<span class="linenos">19</span><span class="gi">+    use_cache: bool = False,</span>
<span class="linenos">20</span><span class="gi">+    cache_position: Optional[torch.LongTensor] = None,</span>
<span class="linenos">21</span><span class="gi">+    **kwargs,</span>
<span class="linenos">22</span><span class="gi">+) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:</span>
<span class="linenos">23</span><span class="w"> </span>    # if key_value_states are provided this layer is used as a cross-attention layer
<span class="linenos">24</span><span class="w"> </span>    is_cross_attention = self.is_cross_attention or key_value_states is not None
<span class="linenos">25</span>
<span class="linenos">26</span><span class="gu">@@ -43,20 +45,27 @@</span>
<span class="linenos">27</span><span class="w"> </span>        )
<span class="linenos">28</span>
<span class="linenos">29</span><span class="w"> </span>    kv_seq_len = key_states.shape[-2]
<span class="linenos">30</span><span class="gd">-    if past_key_values is not None:</span>
<span class="linenos">31</span><span class="gi">+    if past_key_value is not None:</span>
<span class="linenos">32</span><span class="w"> </span>        kv_seq_len += cache_position[0]
<span class="linenos">33</span>
<span class="linenos">34</span><span class="w"> </span>    if not is_cross_attention:
<span class="linenos">35</span><span class="gd">-        cos, sin = self.rotary_emb(value_states, seq_len=max(kv_seq_len, q_len))</span>
<span class="linenos">36</span><span class="gd">-        query_states, key_states = apply_rotary_pos_emb(</span>
<span class="linenos">37</span><span class="gd">-            query_states, key_states, cos, sin, position_ids</span>
<span class="linenos">38</span><span class="gi">+        rotary_length = torch.maximum(</span>
<span class="linenos">39</span><span class="gi">+            torch.tensor(kv_seq_len, dtype=torch.int64),</span>
<span class="linenos">40</span><span class="gi">+            torch.tensor(q_len, dtype=torch.int64),</span>
<span class="linenos">41</span><span class="gi">+        )</span>
<span class="linenos">42</span><span class="gi">+        cos, sin = self.rotary_emb(value_states, seq_len=rotary_length)</span>
<span class="linenos">43</span><span class="gi">+        query_states, key_states = (</span>
<span class="linenos">44</span><span class="gi">+            transformers.models.idefics.modeling_idefics.apply_rotary_pos_emb(</span>
<span class="linenos">45</span><span class="gi">+                query_states, key_states, cos, sin, position_ids</span>
<span class="linenos">46</span><span class="gi">+            )</span>
<span class="linenos">47</span><span class="w"> </span>        )
<span class="linenos">48</span><span class="w"> </span>    # [bsz, nh, t, hd]
<span class="linenos">49</span>
<span class="linenos">50</span><span class="gd">-    if past_key_values is not None:</span>
<span class="linenos">51</span><span class="gd">-        # sin and cos are specific to RoPE models; cache_position needed for the static cache</span>
<span class="linenos">52</span><span class="gi">+    if past_key_value is not None:</span>
<span class="linenos">53</span><span class="gi">+        # sin and cos are specific to RoPE models;</span>
<span class="linenos">54</span><span class="gi">+        # cache_position needed for the static cache</span>
<span class="linenos">55</span><span class="w"> </span>        cache_kwargs = {&quot;cache_position&quot;: cache_position}
<span class="linenos">56</span><span class="gd">-        key_states, value_states = past_key_values.update(</span>
<span class="linenos">57</span><span class="gi">+        key_states, value_states = past_key_value.update(</span>
<span class="linenos">58</span><span class="w"> </span>            key_states, value_states, self.layer_idx, cache_kwargs
<span class="linenos">59</span><span class="w"> </span>        )
<span class="linenos">60</span>
<span class="linenos">61</span><span class="gu">@@ -64,9 +73,22 @@</span>
<span class="linenos">62</span><span class="w"> </span>        query_states = self.q_layer_norm(query_states)
<span class="linenos">63</span><span class="w"> </span>        key_states = self.k_layer_norm(key_states)
<span class="linenos">64</span>
<span class="linenos">65</span><span class="gd">-    attention_interface: Callable = ALL_ATTENTION_FUNCTIONS.get_interface(</span>
<span class="linenos">66</span><span class="gd">-        self.config._attn_implementation, eager_attention_forward</span>
<span class="linenos">67</span><span class="gi">+    attention_interface: Callable = (</span>
<span class="linenos">68</span><span class="gi">+        transformers.models.idefics.modeling_idefics.eager_attention_forward</span>
<span class="linenos">69</span><span class="w"> </span>    )
<span class="linenos">70</span><span class="gi">+</span>
<span class="linenos">71</span><span class="gi">+    if self.config._attn_implementation != &quot;eager&quot;:</span>
<span class="linenos">72</span><span class="gi">+        if self.config._attn_implementation == &quot;sdpa&quot; and output_attentions:</span>
<span class="linenos">73</span><span class="gi">+            transformers.models.idefics.modeling_idefics.logger.warning_once(</span>
<span class="linenos">74</span><span class="gi">+                &quot;`torch.nn.functional.scaled_dot_product_attention` does not support &quot;</span>
<span class="linenos">75</span><span class="gi">+                &quot;`output_attentions=True`. Falling back to &quot;</span>
<span class="linenos">76</span><span class="gi">+                &quot;eager attention. This warning can be removed using the argument &quot;</span>
<span class="linenos">77</span><span class="gi">+                &#39;`attn_implementation=&quot;eager&quot;` when loading the model.&#39;</span>
<span class="linenos">78</span><span class="gi">+            )</span>
<span class="linenos">79</span><span class="gi">+        else:</span>
<span class="linenos">80</span><span class="gi">+            attention_interface = transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS[</span>
<span class="linenos">81</span><span class="gi">+                self.config._attn_implementation</span>
<span class="linenos">82</span><span class="gi">+            ]</span>
<span class="linenos">83</span>
<span class="linenos">84</span><span class="w"> </span>    attn_output, attn_weights = attention_interface(
<span class="linenos">85</span><span class="w"> </span>        self,
<span class="linenos">86</span><span class="gu">@@ -82,4 +104,9 @@</span>
<span class="linenos">87</span><span class="w"> </span>    attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
<span class="linenos">88</span><span class="w"> </span>    attn_output = self.o_proj(attn_output)
<span class="linenos">89</span>
<span class="linenos">90</span><span class="gi">+    if output_attentions:</span>
<span class="linenos">91</span><span class="gi">+        attn_weights = None</span>
<span class="linenos">92</span><span class="gi">+</span>
<span class="linenos">93</span><span class="gi">+    if pv.Version(transformers.__version__) &lt; pv.Version(&quot;4.53.99&quot;):</span>
<span class="linenos">94</span><span class="gi">+        return attn_output, attn_weights, past_key_value</span>
<span class="linenos">95</span><span class="w"> </span>    return attn_output, attn_weights
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-ideficsembedding-forward-patched-ideficsembedding-forward">
<h2>auto/patch_transformers: IdeficsEmbedding.forward -&gt; patched_IdeficsEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-ideficsembedding-forward-patched-ideficsembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,9 +1,26 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>def forward(self, x, seq_len=None):
<span class="linenos"> 5</span><span class="w"> </span>    # x: [bs, num_attention_heads, seq_len, head_size]
<span class="linenos"> 6</span><span class="gd">-    if seq_len &gt; self.max_seq_len_cached:</span>
<span class="linenos"> 7</span><span class="gd">-        self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)</span>
<span class="linenos"> 8</span><span class="gi">+    # if seq_len &gt; self.max_seq_len_cached:</span>
<span class="linenos"> 9</span><span class="gi">+    #    self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="gd">-    return (</span>
<span class="linenos">12</span><span class="gd">-        self.cos_cached[:seq_len].to(dtype=x.dtype),</span>
<span class="linenos">13</span><span class="gd">-        self.sin_cached[:seq_len].to(dtype=x.dtype),</span>
<span class="linenos">14</span><span class="gi">+    def _set_cos_sin_cache_then(x, inv_freq, seq_len, _cos_cached, _sin_cached):</span>
<span class="linenos">15</span><span class="gi">+        t = torch.arange(seq_len, device=x.device, dtype=torch.int64).type_as(inv_freq)</span>
<span class="linenos">16</span><span class="gi">+        # freqs = torch.einsum(&quot;i,j-&gt;ij&quot;, t, inv_freq)</span>
<span class="linenos">17</span><span class="gi">+        freqs = t.reshape((-1, 1)) * inv_freq.reshape((1, -1))</span>
<span class="linenos">18</span><span class="gi">+        emb = torch.cat((freqs, freqs), dim=-1)</span>
<span class="linenos">19</span><span class="gi">+        return emb.cos().to(x.dtype), emb.sin().to(x.dtype)</span>
<span class="linenos">20</span><span class="gi">+</span>
<span class="linenos">21</span><span class="gi">+    def _set_cos_sin_cache_else(_x, _inv_freq, _seq_len, cos_cached, sin_cached):</span>
<span class="linenos">22</span><span class="gi">+        torch._check(seq_len.item() &lt;= cos_cached.shape[0])</span>
<span class="linenos">23</span><span class="gi">+        co = cos_cached[: seq_len.item()].detach().clone()</span>
<span class="linenos">24</span><span class="gi">+        torch._check(seq_len.item() &lt;= sin_cached.shape[0])</span>
<span class="linenos">25</span><span class="gi">+        si = sin_cached[: seq_len.item()].detach().clone()</span>
<span class="linenos">26</span><span class="gi">+        return co.to(dtype=x.dtype), si.to(dtype=x.dtype)</span>
<span class="linenos">27</span><span class="gi">+</span>
<span class="linenos">28</span><span class="gi">+    cos_cached, sin_cached = torch.cond(</span>
<span class="linenos">29</span><span class="gi">+        (seq_len &gt; self.max_seq_len_cached).item(),</span>
<span class="linenos">30</span><span class="gi">+        _set_cos_sin_cache_then,</span>
<span class="linenos">31</span><span class="gi">+        _set_cos_sin_cache_else,</span>
<span class="linenos">32</span><span class="gi">+        [x, self.inv_freq, seq_len, self.cos_cached, self.sin_cached],</span>
<span class="linenos">33</span><span class="w"> </span>    )
<span class="linenos">34</span><span class="gi">+    return cos_cached, sin_cached</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-llamarotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: LlamaRotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-llamarotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-mistralrotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: MistralRotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-mistralrotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-mixtralrotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: MixtralRotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-mixtralrotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-phi3rotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: Phi3RotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-phi3rotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-phi4multimodalrotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: Phi4MultimodalRotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-phi4multimodalrotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-phirotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: PhiRotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-phirotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen2-5-vlforconditionalgeneration-prepare-inputs-for-generation-patched-qwen2-5-vlforconditionalgeneration-prepare-inputs-for-generation">
<h2>auto/patch_transformers: Qwen2_5_VLForConditionalGeneration.prepare_inputs_for_generation -&gt; patched_Qwen2_5_VLForConditionalGeneration.prepare_inputs_for_generation<a class="headerlink" href="#auto-patch-transformers-qwen2-5-vlforconditionalgeneration-prepare-inputs-for-generation-patched-qwen2-5-vlforconditionalgeneration-prepare-inputs-for-generation" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -12,12 +12,14 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>    image_grid_thw=None,
<span class="linenos"> 5</span><span class="w"> </span>    video_grid_thw=None,
<span class="linenos"> 6</span><span class="w"> </span>    second_per_grid_ts=None,
<span class="linenos"> 7</span><span class="gd">-    is_first_iteration=False,</span>
<span class="linenos"> 8</span><span class="w"> </span>    **kwargs,
<span class="linenos"> 9</span><span class="w"> </span>):
<span class="linenos">10</span><span class="gd">-    # Overwritten -- in specific circumstances we don&#39;t want to forward image inputs to the model</span>
<span class="linenos">11</span><span class="gi">+    # Overwritten -- in specific circumstances we don&#39;t want to</span>
<span class="linenos">12</span><span class="gi">+    # forward image inputs to the model</span>
<span class="linenos">13</span><span class="gi">+    from transformers.generation import GenerationMixin</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="gd">-    model_inputs = super().prepare_inputs_for_generation(</span>
<span class="linenos">16</span><span class="gi">+    model_inputs = GenerationMixin.prepare_inputs_for_generation(</span>
<span class="linenos">17</span><span class="gi">+        self,</span>
<span class="linenos">18</span><span class="w"> </span>        input_ids,
<span class="linenos">19</span><span class="w"> </span>        past_key_values=past_key_values,
<span class="linenos">20</span><span class="w"> </span>        attention_mask=attention_mask,
<span class="linenos">21</span><span class="gu">@@ -30,7 +32,6 @@</span>
<span class="linenos">22</span><span class="w"> </span>        video_grid_thw=video_grid_thw,
<span class="linenos">23</span><span class="w"> </span>        second_per_grid_ts=second_per_grid_ts,
<span class="linenos">24</span><span class="w"> </span>        use_cache=use_cache,
<span class="linenos">25</span><span class="gd">-        is_first_iteration=is_first_iteration,</span>
<span class="linenos">26</span><span class="w"> </span>        **kwargs,
<span class="linenos">27</span><span class="w"> </span>    )
<span class="linenos">28</span>
<span class="linenos">29</span><span class="gu">@@ -38,9 +39,9 @@</span>
<span class="linenos">30</span><span class="w"> </span>    if position_ids is None:
<span class="linenos">31</span><span class="w"> </span>        # Calculate RoPE index once per generation in the pre-fill stage only.
<span class="linenos">32</span><span class="w"> </span>        # When compiling, we can&#39;t check tensor values thus we check only input length
<span class="linenos">33</span><span class="gd">-        # It is safe to assume that `length!=1` means we&#39;re in pre-fill because compiled</span>
<span class="linenos">34</span><span class="gd">-        # models currently cannot do assisted decoding</span>
<span class="linenos">35</span><span class="gd">-        if (cache_position[0] == 0 or not use_cache) or self.model.rope_deltas is None:</span>
<span class="linenos">36</span><span class="gi">+        # It is safe to assume that `length!=1` means we&#39;re in pre-fill</span>
<span class="linenos">37</span><span class="gi">+        # because compiled models currently cannot do assisted decoding</span>
<span class="linenos">38</span><span class="gi">+        if cache_position[0] == 0 or self.model.rope_deltas is None:</span>
<span class="linenos">39</span><span class="w"> </span>            vision_positions, rope_deltas = self.model.get_rope_index(
<span class="linenos">40</span><span class="w"> </span>                model_inputs.get(&quot;input_ids&quot;, None),
<span class="linenos">41</span><span class="w"> </span>                image_grid_thw=image_grid_thw,
<span class="linenos">42</span><span class="gu">@@ -50,7 +51,7 @@</span>
<span class="linenos">43</span><span class="w"> </span>            )
<span class="linenos">44</span><span class="w"> </span>            self.model.rope_deltas = rope_deltas
<span class="linenos">45</span><span class="w"> </span>        # then use the prev pre-calculated rope-deltas to get the correct position ids
<span class="linenos">46</span><span class="gd">-        elif &quot;position_ids&quot; in model_inputs:</span>
<span class="linenos">47</span><span class="gi">+        elif &quot;position_ids&quot; in model_inputs and model_inputs[&quot;position_ids&quot;] is not None:</span>
<span class="linenos">48</span><span class="w"> </span>            batch_size, seq_length = model_inputs[&quot;position_ids&quot;].shape
<span class="linenos">49</span><span class="w"> </span>            device = model_inputs[&quot;position_ids&quot;].device
<span class="linenos">50</span><span class="w"> </span>            position_ids = torch.arange(seq_length, device=device)
<span class="linenos">51</span><span class="gu">@@ -60,10 +61,17 @@</span>
<span class="linenos">52</span><span class="w"> </span>            vision_positions = position_ids + delta.expand_as(position_ids)
<span class="linenos">53</span>
<span class="linenos">54</span><span class="w"> </span>        # Concatenate &quot;text + vision&quot; positions into [4, bs, seq-len]
<span class="linenos">55</span><span class="gd">-        text_positions = model_inputs[&quot;position_ids&quot;][None, ...]</span>
<span class="linenos">56</span><span class="gi">+        if &quot;position_ids&quot; not in model_inputs or model_inputs[&quot;position_ids&quot;] is None:</span>
<span class="linenos">57</span><span class="gi">+            text_positions = torch.arange(input_ids.shape[1], device=input_ids.device)[</span>
<span class="linenos">58</span><span class="gi">+                None, None, :</span>
<span class="linenos">59</span><span class="gi">+            ]</span>
<span class="linenos">60</span><span class="gi">+        else:</span>
<span class="linenos">61</span><span class="gi">+            text_positions = model_inputs[&quot;position_ids&quot;][None, ...]</span>
<span class="linenos">62</span><span class="gi">+        # text_positions = model_inputs[&quot;position_ids&quot;][None, ...]</span>
<span class="linenos">63</span><span class="gi">+        assert vision_positions is not None, &quot;vision_positions are missing&quot;</span>
<span class="linenos">64</span><span class="w"> </span>        model_inputs[&quot;position_ids&quot;] = torch.cat([text_positions, vision_positions], dim=0)
<span class="linenos">65</span>
<span class="linenos">66</span><span class="gd">-    if not is_first_iteration and use_cache:</span>
<span class="linenos">67</span><span class="gi">+    if cache_position[0] != 0:</span>
<span class="linenos">68</span><span class="w"> </span>        model_inputs[&quot;pixel_values&quot;] = None
<span class="linenos">69</span><span class="w"> </span>        model_inputs[&quot;pixel_values_videos&quot;] = None
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen2-5-vlmodel-get-placeholder-mask-patched-qwen2-5-vlmodel-get-placeholder-mask">
<h2>auto/patch_transformers: Qwen2_5_VLModel.get_placeholder_mask -&gt; patched_Qwen2_5_VLModel.get_placeholder_mask<a class="headerlink" href="#auto-patch-transformers-qwen2-5-vlmodel-get-placeholder-mask-patched-qwen2-5-vlmodel-get-placeholder-mask" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -2,23 +2,23 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>    self,
<span class="linenos"> 5</span><span class="w"> </span>    input_ids: torch.LongTensor,
<span class="linenos"> 6</span><span class="w"> </span>    inputs_embeds: torch.FloatTensor,
<span class="linenos"> 7</span><span class="gd">-    image_features: torch.FloatTensor | None = None,</span>
<span class="linenos"> 8</span><span class="gd">-    video_features: torch.FloatTensor | None = None,</span>
<span class="linenos"> 9</span><span class="gi">+    image_features: Optional[torch.FloatTensor] = None,</span>
<span class="linenos">10</span><span class="gi">+    video_features: Optional[torch.FloatTensor] = None,</span>
<span class="linenos">11</span><span class="w"> </span>):
<span class="linenos">12</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">13</span><span class="gd">-    Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is</span>
<span class="linenos">14</span><span class="gd">-    equal to the length of multimodal features. If the lengths are different, an error is raised.</span>
<span class="linenos">15</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos">16</span><span class="w"> </span>    if input_ids is None:
<span class="linenos">17</span><span class="w"> </span>        special_image_mask = inputs_embeds == self.get_input_embeddings()(
<span class="linenos">18</span><span class="w"> </span>            torch.tensor(
<span class="linenos">19</span><span class="gd">-                self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device</span>
<span class="linenos">20</span><span class="gi">+                self.config.image_token_id,</span>
<span class="linenos">21</span><span class="gi">+                dtype=torch.long,</span>
<span class="linenos">22</span><span class="gi">+                device=inputs_embeds.device,</span>
<span class="linenos">23</span><span class="w"> </span>            )
<span class="linenos">24</span><span class="w"> </span>        )
<span class="linenos">25</span><span class="w"> </span>        special_image_mask = special_image_mask.all(-1)
<span class="linenos">26</span><span class="w"> </span>        special_video_mask = inputs_embeds == self.get_input_embeddings()(
<span class="linenos">27</span><span class="w"> </span>            torch.tensor(
<span class="linenos">28</span><span class="gd">-                self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device</span>
<span class="linenos">29</span><span class="gi">+                self.config.video_token_id,</span>
<span class="linenos">30</span><span class="gi">+                dtype=torch.long,</span>
<span class="linenos">31</span><span class="gi">+                device=inputs_embeds.device,</span>
<span class="linenos">32</span><span class="w"> </span>            )
<span class="linenos">33</span><span class="w"> </span>        )
<span class="linenos">34</span><span class="w"> </span>        special_video_mask = special_video_mask.all(-1)
<span class="linenos">35</span><span class="gu">@@ -26,23 +26,34 @@</span>
<span class="linenos">36</span><span class="w"> </span>        special_image_mask = input_ids == self.config.image_token_id
<span class="linenos">37</span><span class="w"> </span>        special_video_mask = input_ids == self.config.video_token_id
<span class="linenos">38</span>
<span class="linenos">39</span><span class="gd">-    n_image_tokens = special_image_mask.sum()</span>
<span class="linenos">40</span><span class="w"> </span>    special_image_mask = (
<span class="linenos">41</span><span class="w"> </span>        special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
<span class="linenos">42</span><span class="w"> </span>    )
<span class="linenos">43</span><span class="gd">-    if image_features is not None:</span>
<span class="linenos">44</span><span class="gd">-        torch_compilable_check(</span>
<span class="linenos">45</span><span class="gd">-            inputs_embeds[special_image_mask].numel() == image_features.numel(),</span>
<span class="linenos">46</span><span class="gd">-            f&quot;Image features and image tokens do not match, tokens: {n_image_tokens}, features: {image_features.shape[0]}&quot;,</span>
<span class="linenos">47</span><span class="gd">-        )</span>
<span class="linenos">48</span>
<span class="linenos">49</span><span class="gd">-    n_video_tokens = special_video_mask.sum()</span>
<span class="linenos">50</span><span class="gi">+    # PATCHED: we should use torch._check</span>
<span class="linenos">51</span><span class="gi">+    # but this fails for compilation. It cannot be verified with FakeTensors</span>
<span class="linenos">52</span><span class="gi">+    # torch._check(</span>
<span class="linenos">53</span><span class="gi">+    #    image_features is None</span>
<span class="linenos">54</span><span class="gi">+    #    or inputs_embeds[special_image_mask].numel() == image_features.numel(),</span>
<span class="linenos">55</span><span class="gi">+    #    lambda: (</span>
<span class="linenos">56</span><span class="gi">+    #        f&quot;Image features and image tokens do not match: tokens: &quot;</span>
<span class="linenos">57</span><span class="gi">+    #        f&quot;{special_image_mask.sum()}, features {image_features.shape[0]}&quot;</span>
<span class="linenos">58</span><span class="gi">+    #    ),</span>
<span class="linenos">59</span><span class="gi">+    # )</span>
<span class="linenos">60</span><span class="gi">+</span>
<span class="linenos">61</span><span class="w"> </span>    special_video_mask = (
<span class="linenos">62</span><span class="w"> </span>        special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
<span class="linenos">63</span><span class="w"> </span>    )
<span class="linenos">64</span><span class="gd">-    if video_features is not None:</span>
<span class="linenos">65</span><span class="gd">-        torch_compilable_check(</span>
<span class="linenos">66</span><span class="gd">-            inputs_embeds[special_video_mask].numel() == video_features.numel(),</span>
<span class="linenos">67</span><span class="gd">-            f&quot;Video features and video tokens do not match, tokens: {n_video_tokens}, features: {video_features.shape[0]}&quot;,</span>
<span class="linenos">68</span><span class="gd">-        )</span>
<span class="linenos">69</span><span class="gi">+</span>
<span class="linenos">70</span><span class="gi">+    # PATCHED: we should use torch._check</span>
<span class="linenos">71</span><span class="gi">+    # but this fails for compilation. It cannot be verified with FakeTensors</span>
<span class="linenos">72</span><span class="gi">+    # torch._check(</span>
<span class="linenos">73</span><span class="gi">+    #    video_features is None</span>
<span class="linenos">74</span><span class="gi">+    #    or inputs_embeds[special_video_mask].numel() == video_features.numel(),</span>
<span class="linenos">75</span><span class="gi">+    #    lambda: (</span>
<span class="linenos">76</span><span class="gi">+    #        f&quot;Videos features and video tokens do not match: tokens: &quot;</span>
<span class="linenos">77</span><span class="gi">+    #        f&quot;{special_video_mask.sum()}, features {video_features.shape[0]}&quot;</span>
<span class="linenos">78</span><span class="gi">+    #    ),</span>
<span class="linenos">79</span><span class="gi">+    # )</span>
<span class="linenos">80</span><span class="gi">+</span>
<span class="linenos">81</span><span class="w"> </span>    return special_image_mask, special_video_mask
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen2-5-vlvisionattention-forward-patched-qwen2-5-vlvisionattention-forward">
<h2>auto/patch_transformers: Qwen2_5_VLVisionAttention.forward -&gt; patched_Qwen2_5_VLVisionAttention.forward<a class="headerlink" href="#auto-patch-transformers-qwen2-5-vlvisionattention-forward-patched-qwen2-5-vlvisionattention-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -2,30 +2,88 @@</span>
<span class="linenos">  4</span><span class="w"> </span>    self,
<span class="linenos">  5</span><span class="w"> </span>    hidden_states: torch.Tensor,
<span class="linenos">  6</span><span class="w"> </span>    cu_seqlens: torch.Tensor,
<span class="linenos">  7</span><span class="gd">-    rotary_pos_emb: torch.Tensor | None = None,</span>
<span class="linenos">  8</span><span class="gd">-    position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,</span>
<span class="linenos">  9</span><span class="gi">+    rotary_pos_emb: Optional[torch.Tensor] = None,</span>
<span class="linenos"> 10</span><span class="gi">+    position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,</span>
<span class="linenos"> 11</span><span class="w"> </span>    **kwargs,
<span class="linenos"> 12</span><span class="w"> </span>) -&gt; torch.Tensor:
<span class="linenos"> 13</span><span class="w"> </span>    seq_length = hidden_states.shape[0]
<span class="linenos"> 14</span><span class="gd">-    query_states, key_states, value_states = (</span>
<span class="linenos"> 15</span><span class="gd">-        self.qkv(hidden_states)</span>
<span class="linenos"> 16</span><span class="gd">-        .reshape(seq_length, 3, self.num_heads, -1)</span>
<span class="linenos"> 17</span><span class="gd">-        .permute(1, 0, 2, 3)</span>
<span class="linenos"> 18</span><span class="gd">-        .unbind(0)</span>
<span class="linenos"> 19</span><span class="gi">+    # PATCHED: avoid the use of unbind</span>
<span class="linenos"> 20</span><span class="gi">+    qkv = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3)</span>
<span class="linenos"> 21</span><span class="gi">+</span>
<span class="linenos"> 22</span><span class="gi">+    query_states, key_states, value_states = qkv[0], qkv[1], qkv[2]</span>
<span class="linenos"> 23</span><span class="gi">+    cos, sin = position_embeddings</span>
<span class="linenos"> 24</span><span class="gi">+</span>
<span class="linenos"> 25</span><span class="gi">+    # This part should be moved into the loop</span>
<span class="linenos"> 26</span><span class="gi">+    # iteration to enable fusion inside the loop.</span>
<span class="linenos"> 27</span><span class="gi">+    query_states, key_states = (</span>
<span class="linenos"> 28</span><span class="gi">+        transformers.models.qwen2_5_vl.modeling_qwen2_5_vl.apply_rotary_pos_emb_vision(</span>
<span class="linenos"> 29</span><span class="gi">+            query_states, key_states, cos, sin</span>
<span class="linenos"> 30</span><span class="gi">+        )</span>
<span class="linenos"> 31</span><span class="w"> </span>    )
<span class="linenos"> 32</span><span class="gd">-    cos, sin = position_embeddings</span>
<span class="linenos"> 33</span><span class="gd">-    query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)</span>
<span class="linenos"> 34</span>
<span class="linenos"> 35</span><span class="w"> </span>    query_states = query_states.transpose(0, 1).unsqueeze(0)
<span class="linenos"> 36</span><span class="w"> </span>    key_states = key_states.transpose(0, 1).unsqueeze(0)
<span class="linenos"> 37</span><span class="w"> </span>    value_states = value_states.transpose(0, 1).unsqueeze(0)
<span class="linenos"> 38</span>
<span class="linenos"> 39</span><span class="gd">-    attention_interface: Callable = ALL_ATTENTION_FUNCTIONS.get_interface(</span>
<span class="linenos"> 40</span><span class="gd">-        self.config._attn_implementation, eager_attention_forward</span>
<span class="linenos"> 41</span><span class="gi">+    attention_interface: Callable = (</span>
<span class="linenos"> 42</span><span class="gi">+        transformers.models.qwen2_5_vl.modeling_qwen2_5_vl.eager_attention_forward</span>
<span class="linenos"> 43</span><span class="w"> </span>    )
<span class="linenos"> 44</span><span class="gi">+    if self.config._attn_implementation != &quot;eager&quot;:</span>
<span class="linenos"> 45</span><span class="gi">+        # PATCHED</span>
<span class="linenos"> 46</span><span class="gi">+        # attention_interface = ALL_ATTENTION_FUNCTIONS[</span>
<span class="linenos"> 47</span><span class="gi">+        #       self.config._attn_implementation]</span>
<span class="linenos"> 48</span><span class="gi">+        attention_interface = transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS[</span>
<span class="linenos"> 49</span><span class="gi">+            self.config._attn_implementation</span>
<span class="linenos"> 50</span><span class="gi">+        ]</span>
<span class="linenos"> 51</span>
<span class="linenos"> 52</span><span class="gd">-    if is_flash_attention_requested(self.config):</span>
<span class="linenos"> 53</span><span class="gd">-        # Flash Attention: Use cu_seqlens for variable length attention</span>
<span class="linenos"> 54</span><span class="gi">+    is_sdpa_or_eager = (</span>
<span class="linenos"> 55</span><span class="gi">+        attention_interface is transformers.integrations.sdpa_attention.sdpa_attention_forward</span>
<span class="linenos"> 56</span><span class="gi">+        or attention_interface is patched_sdpa_attention_forward</span>
<span class="linenos"> 57</span><span class="gi">+        or attention_interface</span>
<span class="linenos"> 58</span><span class="gi">+        is transformers.models.qwen2_5_vl.modeling_qwen2_5_vl.eager_attention_forward</span>
<span class="linenos"> 59</span><span class="gi">+    )</span>
<span class="linenos"> 60</span><span class="gi">+    if is_sdpa_or_eager:</span>
<span class="linenos"> 61</span><span class="gi">+        attn_output = qwen_sdpa_attention_versatile(</span>
<span class="linenos"> 62</span><span class="gi">+            query_states,</span>
<span class="linenos"> 63</span><span class="gi">+            key_states,</span>
<span class="linenos"> 64</span><span class="gi">+            value_states,</span>
<span class="linenos"> 65</span><span class="gi">+            cu_seqlens,</span>
<span class="linenos"> 66</span><span class="gi">+            self.scaling,</span>
<span class="linenos"> 67</span><span class="gi">+            self.num_heads,</span>
<span class="linenos"> 68</span><span class="gi">+        )</span>
<span class="linenos"> 69</span><span class="gi">+    elif _is_torchdynamo_exporting():</span>
<span class="linenos"> 70</span><span class="gi">+        if self.config._attn_implementation == &quot;flash_attention_2&quot;:</span>
<span class="linenos"> 71</span><span class="gi">+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()</span>
<span class="linenos"> 72</span><span class="gi">+            attn_output = torch.onnx.ops.symbolic(</span>
<span class="linenos"> 73</span><span class="gi">+                &quot;custom::qwen25_flash_attention&quot;,</span>
<span class="linenos"> 74</span><span class="gi">+                (</span>
<span class="linenos"> 75</span><span class="gi">+                    query_states,</span>
<span class="linenos"> 76</span><span class="gi">+                    key_states,</span>
<span class="linenos"> 77</span><span class="gi">+                    value_states,</span>
<span class="linenos"> 78</span><span class="gi">+                    cu_seqlens,</span>
<span class="linenos"> 79</span><span class="gi">+                    cu_seqlens,</span>
<span class="linenos"> 80</span><span class="gi">+                    max_seqlen,</span>
<span class="linenos"> 81</span><span class="gi">+                    max_seqlen,</span>
<span class="linenos"> 82</span><span class="gi">+                    torch.tensor(self.scaling, dtype=torch.float32),</span>
<span class="linenos"> 83</span><span class="gi">+                ),</span>
<span class="linenos"> 84</span><span class="gi">+                dtype=query_states.dtype,</span>
<span class="linenos"> 85</span><span class="gi">+                shape=(</span>
<span class="linenos"> 86</span><span class="gi">+                    query_states.shape[0],  # batch_size</span>
<span class="linenos"> 87</span><span class="gi">+                    query_states.shape[2],  # sequence_length (total patches)</span>
<span class="linenos"> 88</span><span class="gi">+                    query_states.shape[1],  # num_heads</span>
<span class="linenos"> 89</span><span class="gi">+                    query_states.shape[3],  # head_size</span>
<span class="linenos"> 90</span><span class="gi">+                ),</span>
<span class="linenos"> 91</span><span class="gi">+                version=1,</span>
<span class="linenos"> 92</span><span class="gi">+            )</span>
<span class="linenos"> 93</span><span class="gi">+        else:</span>
<span class="linenos"> 94</span><span class="gi">+            raise NotImplementedError(</span>
<span class="linenos"> 95</span><span class="gi">+                f&quot;No corresponding export strategy for implementation &quot;</span>
<span class="linenos"> 96</span><span class="gi">+                f&quot;{self.config._attn_implementation!r}, &quot;</span>
<span class="linenos"> 97</span><span class="gi">+                f&quot;(use QWEN25ATTENTION to change it), and attention_interface=&quot;</span>
<span class="linenos"> 98</span><span class="gi">+                f&quot;{attention_interface!r} (use sdpa)&quot;</span>
<span class="linenos"> 99</span><span class="gi">+            )</span>
<span class="linenos">100</span><span class="gi">+    elif self.config._attn_implementation == &quot;flash_attention_2&quot;:</span>
<span class="linenos">101</span><span class="gi">+        # Flash Attention 2: Use cu_seqlens for variable length attention</span>
<span class="linenos">102</span><span class="w"> </span>        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()
<span class="linenos">103</span><span class="w"> </span>        attn_output, _ = attention_interface(
<span class="linenos">104</span><span class="w"> </span>            self,
<span class="linenos">105</span><span class="gu">@@ -44,6 +102,7 @@</span>
<span class="linenos">106</span><span class="w"> </span>        )
<span class="linenos">107</span><span class="w"> </span>    else:
<span class="linenos">108</span><span class="w"> </span>        # Other implementations: Process each chunk separately
<span class="linenos">109</span><span class="gi">+        # = qwen_sdpa_attention</span>
<span class="linenos">110</span><span class="w"> </span>        lengths = cu_seqlens[1:] - cu_seqlens[:-1]
<span class="linenos">111</span><span class="w"> </span>        splits = [
<span class="linenos">112</span><span class="w"> </span>            torch.split(tensor, lengths.tolist(), dim=2)
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen2-5-visiontransformerpretrainedmodel-get-window-index-patched-qwen2-5-visiontransformerpretrainedmodel-get-window-index">
<h2>auto/patch_transformers: Qwen2_5_VisionTransformerPretrainedModel.get_window_index -&gt; patched_Qwen2_5_VisionTransformerPretrainedModel.get_window_index<a class="headerlink" href="#auto-patch-transformers-qwen2-5-visiontransformerpretrainedmodel-get-window-index-patched-qwen2-5-visiontransformerpretrainedmodel-get-window-index" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,10 +1,15 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>def get_window_index(self, grid_thw):
<span class="linenos"> 5</span><span class="gd">-    window_index: list = []</span>
<span class="linenos"> 6</span><span class="gd">-    cu_window_seqlens: list = [0]</span>
<span class="linenos"> 7</span><span class="gi">+    window_index: list = []  # type: ignore[annotation-unchecked]</span>
<span class="linenos"> 8</span><span class="gi">+    # PATCHED</span>
<span class="linenos"> 9</span><span class="gi">+    cu_window_seqlens: list = [torch.tensor([0], dtype=torch.int64)]  # type: ignore[annotation-unchecked]</span>
<span class="linenos">10</span><span class="w"> </span>    window_index_id = 0
<span class="linenos">11</span><span class="w"> </span>    vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size
<span class="linenos">12</span>
<span class="linenos">13</span><span class="gd">-    for grid_t, grid_h, grid_w in grid_thw:</span>
<span class="linenos">14</span><span class="gi">+    for _thw in grid_thw:</span>
<span class="linenos">15</span><span class="gi">+        # PATCHED: avoid unbind</span>
<span class="linenos">16</span><span class="gi">+        grid_t = _thw[0]</span>
<span class="linenos">17</span><span class="gi">+        grid_h = _thw[1]</span>
<span class="linenos">18</span><span class="gi">+        grid_w = _thw[2]</span>
<span class="linenos">19</span><span class="w"> </span>        llm_grid_h, llm_grid_w = (
<span class="linenos">20</span><span class="w"> </span>            grid_h // self.spatial_merge_size,
<span class="linenos">21</span><span class="w"> </span>            grid_w // self.spatial_merge_size,
<span class="linenos">22</span><span class="gu">@@ -34,9 +39,11 @@</span>
<span class="linenos">23</span><span class="w"> </span>        index_padded = index_padded.reshape(-1)
<span class="linenos">24</span><span class="w"> </span>        index_new = index_padded[index_padded != -100]
<span class="linenos">25</span><span class="w"> </span>        window_index.append(index_new + window_index_id)
<span class="linenos">26</span><span class="gd">-        cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_window_seqlens[-1]</span>
<span class="linenos">27</span><span class="gd">-        cu_window_seqlens.extend(cu_seqlens_tmp.tolist())</span>
<span class="linenos">28</span><span class="gi">+        cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_window_seqlens[-1][-1:]</span>
<span class="linenos">29</span><span class="gi">+        # PATCHED</span>
<span class="linenos">30</span><span class="gi">+        # cu_window_seqlens.extend(cu_seqlens_tmp.tolist())</span>
<span class="linenos">31</span><span class="gi">+        cu_window_seqlens.append(cu_seqlens_tmp)</span>
<span class="linenos">32</span><span class="w"> </span>        window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()
<span class="linenos">33</span><span class="w"> </span>    window_index = torch.cat(window_index, dim=0)
<span class="linenos">34</span>
<span class="linenos">35</span><span class="gd">-    return window_index, cu_window_seqlens</span>
<span class="linenos">36</span><span class="gi">+    return window_index, torch.cat(cu_window_seqlens, dim=0)</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen2-5-visiontransformerpretrainedmodel-forward-patched-qwen2-5-visiontransformerpretrainedmodel-forward">
<h2>auto/patch_transformers: Qwen2_5_VisionTransformerPretrainedModel.forward -&gt; patched_Qwen2_5_VisionTransformerPretrainedModel.forward<a class="headerlink" href="#auto-patch-transformers-qwen2-5-visiontransformerpretrainedmodel-forward-patched-qwen2-5-visiontransformerpretrainedmodel-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,10 +1,4 @@</span>
<span class="linenos"> 4</span><span class="gd">-@check_model_inputs</span>
<span class="linenos"> 5</span><span class="gd">-def forward(</span>
<span class="linenos"> 6</span><span class="gd">-    self,</span>
<span class="linenos"> 7</span><span class="gd">-    hidden_states: torch.Tensor,</span>
<span class="linenos"> 8</span><span class="gd">-    grid_thw: torch.Tensor,</span>
<span class="linenos"> 9</span><span class="gd">-    **kwargs: Unpack[TransformersKwargs]</span>
<span class="linenos">10</span><span class="gd">-) -&gt; tuple | BaseModelOutputWithPooling:</span>
<span class="linenos">11</span><span class="gi">+def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -&gt; torch.Tensor:</span>
<span class="linenos">12</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">13</span><span class="w"> </span>    Args:
<span class="linenos">14</span><span class="w"> </span>        hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
<span class="linenos">15</span><span class="gu">@@ -18,11 +12,13 @@</span>
<span class="linenos">16</span><span class="w"> </span>    hidden_states = self.patch_embed(hidden_states)
<span class="linenos">17</span><span class="w"> </span>    rotary_pos_emb = self.rot_pos_emb(grid_thw)
<span class="linenos">18</span><span class="w"> </span>    window_index, cu_window_seqlens = self.get_window_index(grid_thw)
<span class="linenos">19</span><span class="gd">-    cu_window_seqlens = torch.tensor(</span>
<span class="linenos">20</span><span class="gd">-        cu_window_seqlens,</span>
<span class="linenos">21</span><span class="gd">-        device=hidden_states.device,</span>
<span class="linenos">22</span><span class="gd">-        dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,</span>
<span class="linenos">23</span><span class="gd">-    )</span>
<span class="linenos">24</span><span class="gi">+    # PATCHED</span>
<span class="linenos">25</span><span class="gi">+    # cu_window_seqlens = torch.tensor(</span>
<span class="linenos">26</span><span class="gi">+    #    cu_window_seqlens,</span>
<span class="linenos">27</span><span class="gi">+    #    device=hidden_states.device,</span>
<span class="linenos">28</span><span class="gi">+    #    dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,</span>
<span class="linenos">29</span><span class="gi">+    # )</span>
<span class="linenos">30</span><span class="gi">+    cu_window_seqlens = cu_window_seqlens.to(hidden_states.device).to(grid_thw.dtype)</span>
<span class="linenos">31</span><span class="w"> </span>    cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)
<span class="linenos">32</span>
<span class="linenos">33</span><span class="w"> </span>    seq_len, _ = hidden_states.size()
<span class="linenos">34</span><span class="gu">@@ -43,8 +39,10 @@</span>
<span class="linenos">35</span><span class="w"> </span>        dim=0,
<span class="linenos">36</span><span class="w"> </span>        # Select dtype based on the following factors:
<span class="linenos">37</span><span class="w"> </span>        #  - FA2 requires that cu_seqlens_q must have dtype int32
<span class="linenos">38</span><span class="gd">-        #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw</span>
<span class="linenos">39</span><span class="gd">-        # See https://github.com/huggingface/transformers/pull/34852 for more information</span>
<span class="linenos">40</span><span class="gi">+        #  - torch.onnx.export requires that cu_seqlens_q must have same dtype</span>
<span class="linenos">41</span><span class="gi">+        # as grid_thw</span>
<span class="linenos">42</span><span class="gi">+        # See https://github.com/huggingface/transformers/pull/34852</span>
<span class="linenos">43</span><span class="gi">+        # for more information</span>
<span class="linenos">44</span><span class="w"> </span>        dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
<span class="linenos">45</span><span class="w"> </span>    )
<span class="linenos">46</span><span class="w"> </span>    cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)
<span class="linenos">47</span><span class="gu">@@ -61,12 +59,10 @@</span>
<span class="linenos">48</span><span class="w"> </span>            position_embeddings=position_embeddings,
<span class="linenos">49</span><span class="w"> </span>            **kwargs,
<span class="linenos">50</span><span class="w"> </span>        )
<span class="linenos">51</span><span class="gi">+        if STOPAT is not None and layer_num &gt; STOPAT:</span>
<span class="linenos">52</span><span class="gi">+            break</span>
<span class="linenos">53</span>
<span class="linenos">54</span><span class="gd">-    merged_hidden_states = self.merger(hidden_states)</span>
<span class="linenos">55</span><span class="gi">+    hidden_states = self.merger(hidden_states)</span>
<span class="linenos">56</span><span class="w"> </span>    reverse_indices = torch.argsort(window_index)
<span class="linenos">57</span><span class="gd">-    merged_hidden_states = merged_hidden_states[reverse_indices, :]</span>
<span class="linenos">58</span><span class="gd">-</span>
<span class="linenos">59</span><span class="gd">-    return BaseModelOutputWithPooling(</span>
<span class="linenos">60</span><span class="gd">-        last_hidden_state=hidden_states,</span>
<span class="linenos">61</span><span class="gd">-        pooler_output=merged_hidden_states,</span>
<span class="linenos">62</span><span class="gd">-    )</span>
<span class="linenos">63</span><span class="gi">+    hidden_states = hidden_states[reverse_indices, :]</span>
<span class="linenos">64</span><span class="gi">+    return hidden_states</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen2-5-visiontransformerpretrainedmodel-rot-pos-emb-patched-qwen2-5-visiontransformerpretrainedmodel-rot-pos-emb">
<h2>auto/patch_transformers: Qwen2_5_VisionTransformerPretrainedModel.rot_pos_emb -&gt; patched_Qwen2_5_VisionTransformerPretrainedModel.rot_pos_emb<a class="headerlink" href="#auto-patch-transformers-qwen2-5-visiontransformerpretrainedmodel-rot-pos-emb-patched-qwen2-5-visiontransformerpretrainedmodel-rot-pos-emb" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,6 +1,10 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>def rot_pos_emb(self, grid_thw):
<span class="linenos"> 5</span><span class="w"> </span>    pos_ids = []
<span class="linenos"> 6</span><span class="gd">-    for t, h, w in grid_thw:</span>
<span class="linenos"> 7</span><span class="gi">+    for thw_ in grid_thw:</span>
<span class="linenos"> 8</span><span class="gi">+        # PATCHED: avoid unbind</span>
<span class="linenos"> 9</span><span class="gi">+        t = thw_[0]</span>
<span class="linenos">10</span><span class="gi">+        h = thw_[1]</span>
<span class="linenos">11</span><span class="gi">+        w = thw_[2]</span>
<span class="linenos">12</span><span class="w"> </span>        hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
<span class="linenos">13</span><span class="w"> </span>        hpos_ids = hpos_ids.reshape(
<span class="linenos">14</span><span class="w"> </span>            h // self.spatial_merge_size,
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen3moesparsemoeblock-forward-patched-qwen3moesparsemoeblock-forward">
<h2>auto/patch_transformers: Qwen3MoeSparseMoeBlock.forward -&gt; patched_Qwen3MoeSparseMoeBlock.forward<a class="headerlink" href="#auto-patch-transformers-qwen3moesparsemoeblock-forward-patched-qwen3moesparsemoeblock-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,6 +1,67 @@</span>
<span class="linenos"> 4</span><span class="gd">-def forward(self, hidden_states: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
<span class="linenos"> 5</span><span class="gi">+def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:</span>
<span class="linenos"> 6</span><span class="gi">+    &quot;&quot;&quot; &quot;&quot;&quot;</span>
<span class="linenos"> 7</span><span class="w"> </span>    batch_size, sequence_length, hidden_dim = hidden_states.shape
<span class="linenos"> 8</span><span class="gd">-    hidden_states_reshaped = hidden_states.view(-1, hidden_dim)</span>
<span class="linenos"> 9</span><span class="gd">-    _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)</span>
<span class="linenos">10</span><span class="gd">-    final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)</span>
<span class="linenos">11</span><span class="gd">-    return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)</span>
<span class="linenos">12</span><span class="gi">+    hidden_states = hidden_states.view(-1, hidden_dim)</span>
<span class="linenos">13</span><span class="gi">+    # router_logits: (batch * sequence_length, n_experts)</span>
<span class="linenos">14</span><span class="gi">+    router_logits = self.gate(hidden_states)</span>
<span class="linenos">15</span><span class="gi">+</span>
<span class="linenos">16</span><span class="gi">+    routing_weights = torch.nn.functional.softmax(router_logits, dim=1, dtype=torch.float)</span>
<span class="linenos">17</span><span class="gi">+    routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)</span>
<span class="linenos">18</span><span class="gi">+    if self.norm_topk_prob:  # only diff with mixtral sparse moe block!</span>
<span class="linenos">19</span><span class="gi">+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)</span>
<span class="linenos">20</span><span class="gi">+    # we cast back to the input dtype</span>
<span class="linenos">21</span><span class="gi">+    routing_weights = routing_weights.to(hidden_states.dtype)</span>
<span class="linenos">22</span><span class="gi">+</span>
<span class="linenos">23</span><span class="gi">+    final_hidden_states = torch.zeros(</span>
<span class="linenos">24</span><span class="gi">+        (batch_size * sequence_length, hidden_dim),</span>
<span class="linenos">25</span><span class="gi">+        dtype=hidden_states.dtype,</span>
<span class="linenos">26</span><span class="gi">+        device=hidden_states.device,</span>
<span class="linenos">27</span><span class="gi">+    )</span>
<span class="linenos">28</span><span class="gi">+</span>
<span class="linenos">29</span><span class="gi">+    # One hot encode the selected experts to create an expert mask</span>
<span class="linenos">30</span><span class="gi">+    # this will be used to easily index which expert is going to be sollicitated</span>
<span class="linenos">31</span><span class="gi">+    expert_mask = torch.nn.functional.one_hot(</span>
<span class="linenos">32</span><span class="gi">+        selected_experts, num_classes=self.num_experts</span>
<span class="linenos">33</span><span class="gi">+    ).permute(2, 1, 0)</span>
<span class="linenos">34</span><span class="gi">+</span>
<span class="linenos">35</span><span class="gi">+    # Loop over all available experts in the model</span>
<span class="linenos">36</span><span class="gi">+    # and perform the computation on each expert</span>
<span class="linenos">37</span><span class="gi">+    expert_sum = expert_mask.sum(dim=(-1, -2))</span>
<span class="linenos">38</span><span class="gi">+    # expert_hit = torch.greater(expert_sum, 0).nonzero()</span>
<span class="linenos">39</span><span class="gi">+    # for expert_idx in expert_hit:</span>
<span class="linenos">40</span><span class="gi">+    for expert_idx in range(self.num_experts):</span>
<span class="linenos">41</span><span class="gi">+        # initial code has a squeeze but it is not possible to do that.</span>
<span class="linenos">42</span><span class="gi">+        # expert_mask_idx = expert_mask[expert_idx].squeeze(0)</span>
<span class="linenos">43</span><span class="gi">+        expert_mask_idx = expert_mask[expert_idx]</span>
<span class="linenos">44</span><span class="gi">+        final_hidden_states = torch.cond(</span>
<span class="linenos">45</span><span class="gi">+            (expert_sum[expert_idx] &gt; 0).item(),</span>
<span class="linenos">46</span><span class="gi">+            lambda final_hidden_states, expert_mask, hidden_states, routing_weights, _i=expert_idx: self._forward_expert_loop(  # noqa: E501</span>
<span class="linenos">47</span><span class="gi">+                final_hidden_states,</span>
<span class="linenos">48</span><span class="gi">+                expert_mask,</span>
<span class="linenos">49</span><span class="gi">+                hidden_states,</span>
<span class="linenos">50</span><span class="gi">+                routing_weights,</span>
<span class="linenos">51</span><span class="gi">+                expert_idx=_i,</span>
<span class="linenos">52</span><span class="gi">+            ),</span>
<span class="linenos">53</span><span class="gi">+            lambda final_hidden_states, *args: final_hidden_states.clone(),</span>
<span class="linenos">54</span><span class="gi">+            [final_hidden_states, expert_mask_idx, hidden_states, routing_weights],</span>
<span class="linenos">55</span><span class="gi">+        )</span>
<span class="linenos">56</span><span class="gi">+</span>
<span class="linenos">57</span><span class="gi">+        # if expert_sum[expert_idx] &gt; 0:</span>
<span class="linenos">58</span><span class="gi">+        #    idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))</span>
<span class="linenos">59</span><span class="gi">+</span>
<span class="linenos">60</span><span class="gi">+        # Index the correct hidden states and compute the expert hidden state for</span>
<span class="linenos">61</span><span class="gi">+        # the current expert. We need to make sure to multiply the output hidden</span>
<span class="linenos">62</span><span class="gi">+        # states by `routing_weights` on the corresponding tokens (top-1 and top-2)</span>
<span class="linenos">63</span><span class="gi">+        #    current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)</span>
<span class="linenos">64</span><span class="gi">+        #    current_hidden_states = (</span>
<span class="linenos">65</span><span class="gi">+        #        expert_layer(current_state) * routing_weights[top_x, idx, None]</span>
<span class="linenos">66</span><span class="gi">+        #    )</span>
<span class="linenos">67</span><span class="gi">+</span>
<span class="linenos">68</span><span class="gi">+        # However `index_add_` only support torch tensors for indexing so we&#39;ll use</span>
<span class="linenos">69</span><span class="gi">+        # the `top_x` tensor here.</span>
<span class="linenos">70</span><span class="gi">+        #    final_hidden_states.index_add_(</span>
<span class="linenos">71</span><span class="gi">+        #        0, top_x, current_hidden_states.to(hidden_states.dtype)</span>
<span class="linenos">72</span><span class="gi">+        #    )</span>
<span class="linenos">73</span><span class="gi">+</span>
<span class="linenos">74</span><span class="gi">+    final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)</span>
<span class="linenos">75</span><span class="gi">+    return final_hidden_states, router_logits</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-qwen3moesparsemoeblock-forward-expert-loop-patched-qwen3moesparsemoeblock-forward-expert-loop">
<h2>auto/patch_transformers: ‘Qwen3MoeSparseMoeBlock_forward_expert_loop’ -&gt; patched_Qwen3MoeSparseMoeBlock._forward_expert_loop<a class="headerlink" href="#auto-patch-transformers-qwen3moesparsemoeblock-forward-expert-loop-patched-qwen3moesparsemoeblock-forward-expert-loop" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>def _forward_expert_loop(
<span class="linenos"> 2</span><span class="w"> </span>   self,
<span class="linenos"> 3</span><span class="w"> </span>   final_hidden_states,
<span class="linenos"> 4</span><span class="w"> </span>   expert_mask_idx,
<span class="linenos"> 5</span><span class="w"> </span>   hidden_states,
<span class="linenos"> 6</span><span class="w"> </span>   routing_weights,
<span class="linenos"> 7</span><span class="w"> </span>   expert_idx: int,
<span class="linenos"> 8</span>):
<span class="linenos"> 9</span><span class="w"> </span>   # idx, top_x = torch.where(expert_mask_idx.squeeze(0))
<span class="linenos">10</span><span class="w"> </span>   idx, top_x = torch.nonzero(expert_mask_idx, as_tuple=True)
<span class="linenos">11</span><span class="w"> </span>   hidden_dim = hidden_states.shape[-1]
<span class="linenos">12</span><span class="w"> </span>   current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)
<span class="linenos">13</span><span class="w"> </span>   expert_current_state = self.experts[expert_idx](current_state)
<span class="linenos">14</span><span class="w"> </span>   current_hidden_states = expert_current_state * routing_weights[top_x, idx, None]
<span class="linenos">15</span><span class="w"> </span>   return final_hidden_states.index_add(0, top_x, current_hidden_states.to(hidden_states.dtype))
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-sammaskdecoder-forward-patched-sammaskdecoder-forward">
<h2>auto/patch_transformers: SamMaskDecoder.forward -&gt; patched_SamMaskDecoder.forward<a class="headerlink" href="#auto-patch-transformers-sammaskdecoder-forward-patched-sammaskdecoder-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -5,8 +5,9 @@</span>
<span class="linenos"> 4</span><span class="w"> </span>    sparse_prompt_embeddings: torch.Tensor,
<span class="linenos"> 5</span><span class="w"> </span>    dense_prompt_embeddings: torch.Tensor,
<span class="linenos"> 6</span><span class="w"> </span>    multimask_output: bool,
<span class="linenos"> 7</span><span class="gd">-    attention_similarity: torch.Tensor | None = None,</span>
<span class="linenos"> 8</span><span class="gd">-    target_embedding: torch.Tensor | None = None,</span>
<span class="linenos"> 9</span><span class="gi">+    output_attentions: Optional[bool] = None,</span>
<span class="linenos">10</span><span class="gi">+    attention_similarity: Optional[torch.Tensor] = None,</span>
<span class="linenos">11</span><span class="gi">+    target_embedding: Optional[torch.Tensor] = None,</span>
<span class="linenos">12</span><span class="w"> </span>) -&gt; tuple[torch.Tensor, torch.Tensor]:
<span class="linenos">13</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">14</span><span class="w"> </span>    Predict masks given image and prompt embeddings.
<span class="linenos">15</span><span class="gu">@@ -22,19 +23,31 @@</span>
<span class="linenos">16</span><span class="w"> </span>            the embeddings of the mask inputs
<span class="linenos">17</span><span class="w"> </span>        multimask_output (bool):
<span class="linenos">18</span><span class="w"> </span>            Whether to return multiple masks or a single mask.
<span class="linenos">19</span><span class="gi">+        output_attentions (bool, *optional*):</span>
<span class="linenos">20</span><span class="gi">+            Whether or not to return the attentions tensors of all attention layers.</span>
<span class="linenos">21</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">22</span><span class="w"> </span>    batch_size, num_channels, height, width = image_embeddings.shape
<span class="linenos">23</span><span class="gd">-    point_batch_size = (</span>
<span class="linenos">24</span><span class="gd">-        sparse_prompt_embeddings.shape[1] if sparse_prompt_embeddings is not None else 1</span>
<span class="linenos">25</span><span class="gd">-    )</span>
<span class="linenos">26</span><span class="gi">+    point_batch_size = sparse_prompt_embeddings.shape[1]</span>
<span class="linenos">27</span><span class="w"> </span>    # Concatenate output tokens
<span class="linenos">28</span><span class="w"> </span>    output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)
<span class="linenos">29</span><span class="w"> </span>    output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)
<span class="linenos">30</span>
<span class="linenos">31</span><span class="gd">-    if sparse_prompt_embeddings is not None:</span>
<span class="linenos">32</span><span class="gd">-        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=2)</span>
<span class="linenos">33</span><span class="gd">-    else:</span>
<span class="linenos">34</span><span class="gd">-        tokens = output_tokens</span>
<span class="linenos">35</span><span class="gi">+    # torch.cond rewrites the if-else logic to handle empty sparse_prompt_embeddings</span>
<span class="linenos">36</span><span class="gi">+    # torch.any is needed to avoid data-dependent control flow</span>
<span class="linenos">37</span><span class="gi">+    # with sparse_prompt_embeddings.sum().item() != 0</span>
<span class="linenos">38</span><span class="gi">+    def sparse_prompt_embeddings_is_not_empty(output_tokens, sparse_prompt_embeddings):</span>
<span class="linenos">39</span><span class="gi">+        return torch.cat((output_tokens, sparse_prompt_embeddings), dim=2)</span>
<span class="linenos">40</span><span class="gi">+</span>
<span class="linenos">41</span><span class="gi">+    def sparse_prompt_embeddings_is_empty(output_tokens, sparse_prompt_embeddings):</span>
<span class="linenos">42</span><span class="gi">+        return output_tokens.clone()</span>
<span class="linenos">43</span><span class="gi">+</span>
<span class="linenos">44</span><span class="gi">+    tokens = torch.cond(</span>
<span class="linenos">45</span><span class="gi">+        torch.any(sparse_prompt_embeddings != 0),</span>
<span class="linenos">46</span><span class="gi">+        sparse_prompt_embeddings_is_not_empty,</span>
<span class="linenos">47</span><span class="gi">+        sparse_prompt_embeddings_is_empty,</span>
<span class="linenos">48</span><span class="gi">+        [output_tokens, sparse_prompt_embeddings],</span>
<span class="linenos">49</span><span class="gi">+    )</span>
<span class="linenos">50</span><span class="gi">+</span>
<span class="linenos">51</span><span class="w"> </span>    point_embeddings = tokens.to(self.iou_token.weight.dtype)
<span class="linenos">52</span>
<span class="linenos">53</span><span class="w"> </span>    # Expand per-image data in batch direction to be per-point
<span class="linenos">54</span><span class="gu">@@ -45,15 +58,21 @@</span>
<span class="linenos">55</span><span class="w"> </span>    )
<span class="linenos">56</span>
<span class="linenos">57</span><span class="w"> </span>    # Run the transformer, image_positional_embedding are consumed
<span class="linenos">58</span><span class="gd">-    point_embedding, image_embeddings = self.transformer(</span>
<span class="linenos">59</span><span class="gi">+    torch._check(point_embeddings.shape[0] != 0)</span>
<span class="linenos">60</span><span class="gi">+    torch._check(point_embeddings.shape[1] != 0)</span>
<span class="linenos">61</span><span class="gi">+    torch._check(point_embeddings.shape[2] != 0)</span>
<span class="linenos">62</span><span class="gi">+    torch._check(point_embeddings.shape[3] != 0)</span>
<span class="linenos">63</span><span class="gi">+    embeddings_attentions = self.transformer(</span>
<span class="linenos">64</span><span class="w"> </span>        point_embeddings=point_embeddings,
<span class="linenos">65</span><span class="w"> </span>        image_embeddings=image_embeddings,
<span class="linenos">66</span><span class="w"> </span>        image_positional_embeddings=image_positional_embeddings,
<span class="linenos">67</span><span class="w"> </span>        attention_similarity=attention_similarity,
<span class="linenos">68</span><span class="w"> </span>        target_embedding=target_embedding,
<span class="linenos">69</span><span class="gi">+        output_attentions=output_attentions,</span>
<span class="linenos">70</span><span class="w"> </span>    )
<span class="linenos">71</span><span class="gd">-    iou_token_out = point_embedding[:, :, 0, :]</span>
<span class="linenos">72</span><span class="gd">-    mask_tokens_out = point_embedding[:, :, 1 : (1 + self.num_mask_tokens), :]</span>
<span class="linenos">73</span><span class="gi">+    point_embedding, image_embeddings = embeddings_attentions[:2]</span>
<span class="linenos">74</span><span class="gi">+    iou_token_out = torch.select(point_embedding, dim=2, index=0)</span>
<span class="linenos">75</span><span class="gi">+    mask_tokens_out = torch.narrow(point_embedding, dim=2, start=1, length=self.num_mask_tokens)</span>
<span class="linenos">76</span>
<span class="linenos">77</span><span class="w"> </span>    # Upscale mask embeddings and predict masks using the mask tokens
<span class="linenos">78</span><span class="w"> </span>    image_embeddings = image_embeddings.transpose(2, 3).reshape(
<span class="linenos">79</span><span class="gu">@@ -88,4 +107,15 @@</span>
<span class="linenos">80</span><span class="w"> </span>        mask_slice = slice(0, 1)
<span class="linenos">81</span><span class="w"> </span>    masks = masks[:, :, mask_slice, :, :]
<span class="linenos">82</span><span class="w"> </span>    iou_pred = iou_pred[:, :, mask_slice]
<span class="linenos">83</span><span class="gd">-    return masks, iou_pred</span>
<span class="linenos">84</span><span class="gi">+</span>
<span class="linenos">85</span><span class="gi">+    outputs = (masks, iou_pred)</span>
<span class="linenos">86</span><span class="gi">+</span>
<span class="linenos">87</span><span class="gi">+    if len(embeddings_attentions) == 2:</span>
<span class="linenos">88</span><span class="gi">+        # transformers==4.54</span>
<span class="linenos">89</span><span class="gi">+        return outputs</span>
<span class="linenos">90</span><span class="gi">+</span>
<span class="linenos">91</span><span class="gi">+    if output_attentions and len(embeddings_attentions) &gt; 2:</span>
<span class="linenos">92</span><span class="gi">+        outputs = outputs + (embeddings_attentions[2],)  # noqa: RUF005</span>
<span class="linenos">93</span><span class="gi">+    else:</span>
<span class="linenos">94</span><span class="gi">+        outputs = outputs + (None,)  # noqa: RUF005</span>
<span class="linenos">95</span><span class="gi">+    return outputs</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-smollm3rotaryembedding-forward-common-rotaryembedding-forward">
<h2>auto/patch_transformers: SmolLM3RotaryEmbedding.forward -&gt; common_RotaryEmbedding.forward<a class="headerlink" href="#auto-patch-transformers-smollm3rotaryembedding-forward-common-rotaryembedding-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -1,18 +1,26 @@</span>
<span class="linenos">  4</span><span class="gd">-@torch.no_grad()</span>
<span class="linenos">  5</span><span class="gd">-@dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)</span>
<span class="linenos">  6</span><span class="gd">-def forward(self, x, position_ids):</span>
<span class="linenos">  7</span><span class="gi">+@patched_dynamic_rope_update</span>
<span class="linenos">  8</span><span class="gi">+def forward(self, x, position_ids, layer_type=None):</span>
<span class="linenos">  9</span><span class="gi">+    if layer_type is not None:</span>
<span class="linenos"> 10</span><span class="gi">+        # transformers&gt;=5.0</span>
<span class="linenos"> 11</span><span class="gi">+        inv_freq = getattr(self, f&quot;{layer_type}_inv_freq&quot;)</span>
<span class="linenos"> 12</span><span class="gi">+        attention_scaling = getattr(self, f&quot;{layer_type}_attention_scaling&quot;)</span>
<span class="linenos"> 13</span><span class="gi">+    else:</span>
<span class="linenos"> 14</span><span class="gi">+        # transformers&lt;5.0</span>
<span class="linenos"> 15</span><span class="gi">+        inv_freq = self.inv_freq</span>
<span class="linenos"> 16</span><span class="gi">+        attention_scaling = self.attention_scaling</span>
<span class="linenos"> 17</span><span class="gi">+</span>
<span class="linenos"> 18</span><span class="w"> </span>    inv_freq_expanded = (
<span class="linenos"> 19</span><span class="gd">-        self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 20</span><span class="gi">+        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)</span>
<span class="linenos"> 21</span><span class="w"> </span>    )
<span class="linenos"> 22</span><span class="w"> </span>    position_ids_expanded = position_ids[:, None, :].float()
<span class="linenos"> 23</span>
<span class="linenos"> 24</span><span class="w"> </span>    device_type = (
<span class="linenos"> 25</span><span class="w"> </span>        x.device.type if isinstance(x.device.type, str) and x.device.type != &quot;mps&quot; else &quot;cpu&quot;
<span class="linenos"> 26</span><span class="w"> </span>    )
<span class="linenos"> 27</span><span class="gd">-    with maybe_autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 28</span><span class="gi">+    with torch.autocast(device_type=device_type, enabled=False):  # Force float32</span>
<span class="linenos"> 29</span><span class="w"> </span>        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
<span class="linenos"> 30</span><span class="w"> </span>        emb = torch.cat((freqs, freqs), dim=-1)
<span class="linenos"> 31</span><span class="gd">-        cos = emb.cos() * self.attention_scaling</span>
<span class="linenos"> 32</span><span class="gd">-        sin = emb.sin() * self.attention_scaling</span>
<span class="linenos"> 33</span><span class="gi">+        cos = emb.cos() * attention_scaling</span>
<span class="linenos"> 34</span><span class="gi">+        sin = emb.sin() * attention_scaling</span>
<span class="linenos"> 35</span>
<span class="linenos"> 36</span><span class="w"> </span>    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gd">--- original</span>
<span class="linenos"> 39</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 40</span><span class="gu">@@ -1,103 +1,193 @@</span>
<span class="linenos"> 41</span><span class="gd">-def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 42</span><span class="gd">-    &quot;&quot;&quot;</span>
<span class="linenos"> 43</span><span class="gd">-    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE</span>
<span class="linenos"> 44</span><span class="gd">-    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).</span>
<span class="linenos"> 45</span><span class="gi">+def patched_dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 46</span><span class="gi">+    &quot;&quot;&quot;manual patch: ``[patch:transformers.modeling_rope_utils.dynamic_rope_update]``</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gd">-    Args:</span>
<span class="linenos"> 49</span><span class="gd">-        rope_forward (Callable):</span>
<span class="linenos"> 50</span><span class="gd">-            The forward pass of the RoPE implementation.</span>
<span class="linenos"> 51</span><span class="gi">+    ``rope_type`` is determined in the constructor of class</span>
<span class="linenos"> 52</span><span class="gi">+    :class:`transformers.models.phi3.modeling_phi3.Phi3RotaryEmbedding`.</span>
<span class="linenos"> 53</span>
<span class="linenos"> 54</span><span class="gd">-    Returns:</span>
<span class="linenos"> 55</span><span class="gd">-        The decorated forward pass.</span>
<span class="linenos"> 56</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 57</span><span class="gi">+</span>
<span class="linenos"> 58</span><span class="gi">+        if hasattr(config, &quot;rope_scaling&quot;) and config.rope_scaling is not None:</span>
<span class="linenos"> 59</span><span class="gi">+            self.rope_type = config.rope_scaling.get(</span>
<span class="linenos"> 60</span><span class="gi">+                &quot;rope_type&quot;, config.rope_scaling.get(&quot;type&quot;))</span>
<span class="linenos"> 61</span><span class="gi">+        else:</span>
<span class="linenos"> 62</span><span class="gi">+            self.rope_type = &quot;default&quot;</span>
<span class="linenos"> 63</span><span class="gi">+</span>
<span class="linenos"> 64</span><span class="gi">+    The original code of the patched function:</span>
<span class="linenos"> 65</span><span class="gi">+</span>
<span class="linenos"> 66</span><span class="gi">+    .. code-block:: python</span>
<span class="linenos"> 67</span><span class="gi">+</span>
<span class="linenos"> 68</span><span class="gi">+        def dynamic_rope_update(rope_forward):</span>
<span class="linenos"> 69</span><span class="gi">+            def longrope_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 70</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 71</span><span class="gi">+                if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos"> 72</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 73</span><span class="gi">+                        self.config.original_max_position_embeddings</span>
<span class="linenos"> 74</span><span class="gi">+                else:</span>
<span class="linenos"> 75</span><span class="gi">+                    original_max_position_embeddings =</span>
<span class="linenos"> 76</span><span class="gi">+                        self.config.max_position_embeddings</span>
<span class="linenos"> 77</span><span class="gi">+                if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos"> 78</span><span class="gi">+                    if not hasattr(self, &quot;long_inv_freq&quot;):</span>
<span class="linenos"> 79</span><span class="gi">+                        self.long_inv_freq, _ = self.rope_init_fn(</span>
<span class="linenos"> 80</span><span class="gi">+                            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos"> 81</span><span class="gi">+                        )</span>
<span class="linenos"> 82</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.long_inv_freq, persistent=False)</span>
<span class="linenos"> 83</span><span class="gi">+                else:</span>
<span class="linenos"> 84</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 85</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 86</span><span class="gi">+</span>
<span class="linenos"> 87</span><span class="gi">+            def dynamic_frequency_update(self, position_ids, device):</span>
<span class="linenos"> 88</span><span class="gi">+                seq_len = torch.max(position_ids) + 1</span>
<span class="linenos"> 89</span><span class="gi">+                if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos"> 90</span><span class="gi">+                    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos"> 91</span><span class="gi">+                        self.config, device, seq_len=seq_len)</span>
<span class="linenos"> 92</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos"> 93</span><span class="gi">+                    self.max_seq_len_cached = seq_len</span>
<span class="linenos"> 94</span><span class="gi">+</span>
<span class="linenos"> 95</span><span class="gi">+                if seq_len &lt; self.original_max_seq_len and</span>
<span class="linenos"> 96</span><span class="gi">+                        self.max_seq_len_cached &gt; self.original_max_seq_len:</span>
<span class="linenos"> 97</span><span class="gi">+                    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos"> 98</span><span class="gi">+                    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos"> 99</span><span class="gi">+                    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">100</span><span class="gi">+</span>
<span class="linenos">101</span><span class="gi">+            @wraps(rope_forward)</span>
<span class="linenos">102</span><span class="gi">+            def wrapper(self, x, position_ids):</span>
<span class="linenos">103</span><span class="gi">+                if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">104</span><span class="gi">+                    dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">105</span><span class="gi">+                elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">106</span><span class="gi">+                    longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">107</span><span class="gi">+                return rope_forward(self, x, position_ids)</span>
<span class="linenos">108</span><span class="gi">+</span>
<span class="linenos">109</span><span class="gi">+            return wrapper</span>
<span class="linenos">110</span><span class="gi">+</span>
<span class="linenos">111</span><span class="w"> </span>    &quot;&quot;&quot;
<span class="linenos">112</span>
<span class="linenos">113</span><span class="w"> </span>    def longrope_frequency_update(self, position_ids, device, layer_type=None):
<span class="linenos">114</span><span class="gd">-        &quot;&quot;&quot;Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.&quot;&quot;&quot;</span>
<span class="linenos">115</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">116</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">117</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">118</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">119</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">120</span><span class="w"> </span>        seq_len = torch.max(position_ids) + 1
<span class="linenos">121</span><span class="gi">+        if hasattr(self.config, &quot;original_max_position_embeddings&quot;):</span>
<span class="linenos">122</span><span class="gi">+            original_max_position_embeddings = self.config.original_max_position_embeddings</span>
<span class="linenos">123</span><span class="gi">+        else:</span>
<span class="linenos">124</span><span class="gi">+            original_max_position_embeddings = self.config.max_position_embeddings</span>
<span class="linenos">125</span>
<span class="linenos">126</span><span class="w"> </span>        if layer_type is None:
<span class="linenos">127</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">128</span><span class="gd">-            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">129</span><span class="gd">-            prefix = &quot;&quot;</span>
<span class="linenos">130</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[</span>
<span class="linenos">131</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">132</span><span class="gd">-            ]</span>
<span class="linenos">133</span><span class="gd">-        else:</span>
<span class="linenos">134</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">135</span><span class="gd">-            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">136</span><span class="gd">-            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">137</span><span class="gd">-            original_max_position_embeddings = self.config.rope_parameters[layer_type][</span>
<span class="linenos">138</span><span class="gd">-                &quot;original_max_position_embeddings&quot;</span>
<span class="linenos">139</span><span class="gd">-            ]</span>
<span class="linenos">140</span><span class="gd">-</span>
<span class="linenos">141</span><span class="gd">-        if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">142</span><span class="gd">-            if not hasattr(self, f&quot;{layer_type}_long_inv_freq&quot;):</span>
<span class="linenos">143</span><span class="gd">-                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">144</span><span class="gd">-                long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">145</span><span class="gd">-                    self.config,</span>
<span class="linenos">146</span><span class="gd">-                    device,</span>
<span class="linenos">147</span><span class="gd">-                    seq_len=original_max_position_embeddings + 1,</span>
<span class="linenos">148</span><span class="gd">-                    layer_type=layer_type,</span>
<span class="linenos">149</span><span class="gd">-                )</span>
<span class="linenos">150</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, long_inv_freq, persistent=False)</span>
<span class="linenos">151</span><span class="gd">-            setattr(self, f&quot;{prefix}long_inv_freq&quot;, long_inv_freq)</span>
<span class="linenos">152</span><span class="gd">-        else:</span>
<span class="linenos">153</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">154</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">155</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">156</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">157</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">158</span><span class="gd">-</span>
<span class="linenos">159</span><span class="gd">-    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">160</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">161</span><span class="gd">-        dynamic RoPE layers should recompute `inv_freq` in the following situations:</span>
<span class="linenos">162</span><span class="gd">-        1 - growing beyond the cached sequence length (allow scaling)</span>
<span class="linenos">163</span><span class="gd">-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)</span>
<span class="linenos">164</span><span class="gd">-        &quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="gd">-        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">166</span><span class="gd">-        if layer_type is None:</span>
<span class="linenos">167</span><span class="gd">-            rope_type = self.rope_type</span>
<span class="linenos">168</span><span class="gd">-            max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">169</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">170</span><span class="w"> </span>            original_inv_freq = self.original_inv_freq
<span class="linenos">171</span><span class="w"> </span>            prefix = &quot;&quot;
<span class="linenos">172</span><span class="w"> </span>        else:
<span class="linenos">173</span><span class="gd">-            rope_type = self.rope_type[layer_type]</span>
<span class="linenos">174</span><span class="gd">-            max_seq_len_cached = getattr(</span>
<span class="linenos">175</span><span class="gd">-                self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">176</span><span class="gd">-            )</span>
<span class="linenos">177</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">178</span><span class="w"> </span>            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)
<span class="linenos">179</span><span class="w"> </span>            prefix = f&quot;{layer_type}_&quot;
<span class="linenos">180</span>
<span class="linenos">181</span><span class="gd">-        if seq_len &gt; max_seq_len_cached:  # growth</span>
<span class="linenos">182</span><span class="gd">-            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]</span>
<span class="linenos">183</span><span class="gd">-            inv_freq, self.attention_scaling = rope_init_fn(</span>
<span class="linenos">184</span><span class="gd">-                self.config,</span>
<span class="linenos">185</span><span class="gd">-                device,</span>
<span class="linenos">186</span><span class="gd">-                seq_len=seq_len,</span>
<span class="linenos">187</span><span class="gd">-                layer_type=layer_type,</span>
<span class="linenos">188</span><span class="gd">-            )</span>
<span class="linenos">189</span><span class="gd">-            # TODO joao: may break with compilation</span>
<span class="linenos">190</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">191</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, seq_len)</span>
<span class="linenos">192</span><span class="gi">+        # At export time, seq_len is unknown.</span>
<span class="linenos">193</span><span class="gi">+        long_inv_freq, _ = rope_init_fn(</span>
<span class="linenos">194</span><span class="gi">+            self.config, device, seq_len=original_max_position_embeddings + 1</span>
<span class="linenos">195</span><span class="gi">+        )</span>
<span class="linenos">196</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">197</span>
<span class="linenos">198</span><span class="gd">-        if (</span>
<span class="linenos">199</span><span class="gd">-            seq_len &lt; self.original_max_seq_len and max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">200</span><span class="gd">-        ):  # reset</span>
<span class="linenos">201</span><span class="gd">-            # This .to() is needed if the model has been moved to a device after being initialized (because</span>
<span class="linenos">202</span><span class="gd">-            # the buffer is automatically moved, but not the original copy)</span>
<span class="linenos">203</span><span class="gd">-            original_inv_freq = original_inv_freq.to(device)</span>
<span class="linenos">204</span><span class="gd">-            self.register_buffer(f&quot;{prefix}inv_freq&quot;, original_inv_freq, persistent=False)</span>
<span class="linenos">205</span><span class="gd">-            setattr(self, f&quot;{prefix}original_inv_freq&quot;, original_inv_freq)</span>
<span class="linenos">206</span><span class="gd">-            setattr(self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.original_max_seq_len)</span>
<span class="linenos">207</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">208</span><span class="gi">+        cond = (seq_len &gt; original_max_position_embeddings).item()</span>
<span class="linenos">209</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">210</span><span class="gi">+            cond,</span>
<span class="linenos">211</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">212</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">213</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">214</span><span class="gi">+        )</span>
<span class="linenos">215</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">216</span><span class="gi">+        # if seq_len &gt; original_max_position_embeddings:</span>
<span class="linenos">217</span><span class="gi">+        #    self.inv_freq = self.long_inv_freq</span>
<span class="linenos">218</span><span class="gi">+        # else:</span>
<span class="linenos">219</span><span class="gi">+        #    self.inv_freq = self.original_inv_freq</span>
<span class="linenos">220</span><span class="gi">+</span>
<span class="linenos">221</span><span class="gi">+    def dynamic_frequency_update(self, position_ids, device, layer_type=None):</span>
<span class="linenos">222</span><span class="gi">+        # constructor:</span>
<span class="linenos">223</span><span class="gi">+        # - self.max_seq_len_cached = config.max_position_embeddings</span>
<span class="linenos">224</span><span class="gi">+        # - self.original_max_seq_len = config.max_position_embeddings</span>
<span class="linenos">225</span><span class="gi">+        # - inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)</span>
<span class="linenos">226</span><span class="gi">+</span>
<span class="linenos">227</span><span class="gi">+        # It is no use to patch the function after the model is created</span>
<span class="linenos">228</span><span class="gi">+        # as rope_init_fn is an attribute set to one function when the model</span>
<span class="linenos">229</span><span class="gi">+        # is created and when no patch is applied yet.</span>
<span class="linenos">230</span><span class="gi">+        # So we select the patched version here.</span>
<span class="linenos">231</span><span class="gi">+        rope_init_fn = _get_rope_init_fn(self, layer_type=layer_type)</span>
<span class="linenos">232</span><span class="gi">+</span>
<span class="linenos">233</span><span class="gi">+        # This behaviour is difficult to translate.</span>
<span class="linenos">234</span><span class="gi">+        # The sequence always grows.</span>
<span class="linenos">235</span><span class="gi">+        # The test should always True.</span>
<span class="linenos">236</span><span class="gi">+        # So:  self.max_seq_len_cached = max(self.max_seq_len_cached, seq_len) --&gt; seq_len</span>
<span class="linenos">237</span><span class="gi">+        #</span>
<span class="linenos">238</span><span class="gi">+        # if seq_len &gt; self.max_seq_len_cached:  # growth</span>
<span class="linenos">239</span><span class="gi">+        #    inv_freq, self.attention_scaling = self.rope_init_fn(</span>
<span class="linenos">240</span><span class="gi">+        #        self.config, device, seq_len=seq_len</span>
<span class="linenos">241</span><span class="gi">+        #    )</span>
<span class="linenos">242</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False)</span>
<span class="linenos">243</span><span class="gi">+        #    self.max_seq_len_cached = seq_len</span>
<span class="linenos">244</span><span class="gi">+        #</span>
<span class="linenos">245</span><span class="gi">+        # So we should not need what follows.</span>
<span class="linenos">246</span><span class="gi">+        #</span>
<span class="linenos">247</span><span class="gi">+        # cond = (seq_len &gt; self.max_seq_len_cached).item()</span>
<span class="linenos">248</span><span class="gi">+        # self.attention_scaling = torch.cond(</span>
<span class="linenos">249</span><span class="gi">+        #    cond,</span>
<span class="linenos">250</span><span class="gi">+        #    (lambda x, y: x.clone()),</span>
<span class="linenos">251</span><span class="gi">+        #    (lambda x, y: y.clone()),</span>
<span class="linenos">252</span><span class="gi">+        #    [attention_scaling, self.attention_scaling],</span>
<span class="linenos">253</span><span class="gi">+        # )</span>
<span class="linenos">254</span><span class="gi">+</span>
<span class="linenos">255</span><span class="gi">+        seq_len = torch.max(position_ids) + 1</span>
<span class="linenos">256</span><span class="gi">+        long_inv_freq, self.attention_scaling = rope_init_fn(self.config, device, seq_len=seq_len)</span>
<span class="linenos">257</span><span class="gi">+</span>
<span class="linenos">258</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">259</span><span class="gi">+            # rope_type = self.rope_type</span>
<span class="linenos">260</span><span class="gi">+            # max_seq_len_cached = self.max_seq_len_cached</span>
<span class="linenos">261</span><span class="gi">+            original_inv_freq = self.original_inv_freq</span>
<span class="linenos">262</span><span class="gi">+            prefix = &quot;&quot;</span>
<span class="linenos">263</span><span class="gi">+        else:</span>
<span class="linenos">264</span><span class="gi">+            # rope_type = self.rope_type[layer_type]</span>
<span class="linenos">265</span><span class="gi">+            # max_seq_len_cached = getattr(</span>
<span class="linenos">266</span><span class="gi">+            #     self, f&quot;{layer_type}_max_seq_len_cached&quot;, self.max_seq_len_cached</span>
<span class="linenos">267</span><span class="gi">+            # )</span>
<span class="linenos">268</span><span class="gi">+            original_inv_freq = getattr(self, f&quot;{layer_type}_original_inv_freq&quot;)</span>
<span class="linenos">269</span><span class="gi">+            prefix = f&quot;{layer_type}_&quot;</span>
<span class="linenos">270</span><span class="gi">+</span>
<span class="linenos">271</span><span class="gi">+        # Second test to translate.</span>
<span class="linenos">272</span><span class="gi">+        # Let&#39;s keep in mind, self.max_seq_len_cached = seq_len is likely to be True.</span>
<span class="linenos">273</span><span class="gi">+        # But in that case the following condition is a way to restore the original cache.</span>
<span class="linenos">274</span><span class="gi">+</span>
<span class="linenos">275</span><span class="gi">+        # if (</span>
<span class="linenos">276</span><span class="gi">+        #    seq_len &lt; self.original_max_seq_len</span>
<span class="linenos">277</span><span class="gi">+        #    and self.max_seq_len_cached &gt; self.original_max_seq_len</span>
<span class="linenos">278</span><span class="gi">+        # ):</span>
<span class="linenos">279</span><span class="gi">+        #    self.original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">280</span><span class="gi">+        #    self.register_buffer(&quot;inv_freq&quot;, self.original_inv_freq, persistent=False)</span>
<span class="linenos">281</span><span class="gi">+        #    self.max_seq_len_cached = self.original_max_seq_len</span>
<span class="linenos">282</span><span class="gi">+</span>
<span class="linenos">283</span><span class="gi">+        original_inv_freq = self.original_inv_freq.to(device)</span>
<span class="linenos">284</span><span class="gi">+        cond = (seq_len &gt;= self.original_max_seq_len).item()</span>
<span class="linenos">285</span><span class="gi">+        # PATCHED: uses torch.cond instead of a test</span>
<span class="linenos">286</span><span class="gi">+        inv_freq = torch.cond(</span>
<span class="linenos">287</span><span class="gi">+            cond,</span>
<span class="linenos">288</span><span class="gi">+            (lambda x, y: x.clone()),</span>
<span class="linenos">289</span><span class="gi">+            (lambda x, y: y.clone()),</span>
<span class="linenos">290</span><span class="gi">+            [long_inv_freq.to(original_inv_freq.dtype), original_inv_freq],</span>
<span class="linenos">291</span><span class="gi">+        )</span>
<span class="linenos">292</span><span class="gi">+        setattr(self, f&quot;{prefix}inv_freq&quot;, inv_freq)</span>
<span class="linenos">293</span>
<span class="linenos">294</span><span class="w"> </span>    @wraps(rope_forward)
<span class="linenos">295</span><span class="w"> </span>    def wrapper(self, x, position_ids, layer_type=None):
<span class="linenos">296</span><span class="gd">-        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]</span>
<span class="linenos">297</span><span class="gd">-        kwargs = {&quot;layer_type&quot;: layer_type} if layer_type is not None else {}</span>
<span class="linenos">298</span><span class="gd">-        if &quot;dynamic&quot; in rope_type:</span>
<span class="linenos">299</span><span class="gd">-            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">300</span><span class="gd">-        elif rope_type == &quot;longrope&quot;:</span>
<span class="linenos">301</span><span class="gd">-            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)</span>
<span class="linenos">302</span><span class="gd">-        return rope_forward(self, x, position_ids, **kwargs)</span>
<span class="linenos">303</span><span class="gi">+        if layer_type is None:</span>
<span class="linenos">304</span><span class="gi">+            if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">305</span><span class="gi">+                dynamic_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">306</span><span class="gi">+            elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">307</span><span class="gi">+                longrope_frequency_update(self, position_ids, device=x.device)</span>
<span class="linenos">308</span><span class="gi">+            return rope_forward(self, x, position_ids)</span>
<span class="linenos">309</span><span class="gi">+</span>
<span class="linenos">310</span><span class="gi">+        if &quot;dynamic&quot; in self.rope_type:</span>
<span class="linenos">311</span><span class="gi">+            dynamic_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">312</span><span class="gi">+        elif self.rope_type == &quot;longrope&quot;:</span>
<span class="linenos">313</span><span class="gi">+            longrope_frequency_update(self, position_ids, device=x.device, layer_type=layer_type)</span>
<span class="linenos">314</span><span class="gi">+        return rope_forward(self, x, position_ids, layer_type=layer_type)</span>
<span class="linenos">315</span>
<span class="linenos">316</span><span class="w"> </span>    return wrapper
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-visionattention-forward-patched-visionattention-forward">
<h2>auto/patch_transformers: VisionAttention.forward -&gt; patched_VisionAttention.forward<a class="headerlink" href="#auto-patch-transformers-visionattention-forward-patched-visionattention-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="gd">--- original</span>
<span class="linenos">  2</span><span class="gi">+++ rewritten</span>
<span class="linenos">  3</span><span class="gu">@@ -2,70 +2,56 @@</span>
<span class="linenos">  4</span><span class="w"> </span>    self,
<span class="linenos">  5</span><span class="w"> </span>    hidden_states: torch.Tensor,
<span class="linenos">  6</span><span class="w"> </span>    cu_seqlens: torch.Tensor,
<span class="linenos">  7</span><span class="gd">-    rotary_pos_emb: torch.Tensor | None = None,</span>
<span class="linenos">  8</span><span class="gd">-    position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,</span>
<span class="linenos">  9</span><span class="gd">-    **kwargs,</span>
<span class="linenos"> 10</span><span class="gi">+    rotary_pos_emb: Optional[torch.Tensor] = None,</span>
<span class="linenos"> 11</span><span class="gi">+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,</span>
<span class="linenos"> 12</span><span class="w"> </span>) -&gt; torch.Tensor:
<span class="linenos"> 13</span><span class="w"> </span>    seq_length = hidden_states.shape[0]
<span class="linenos"> 14</span><span class="gd">-    query_states, key_states, value_states = (</span>
<span class="linenos"> 15</span><span class="gi">+    q, k, v = (</span>
<span class="linenos"> 16</span><span class="w"> </span>        self.qkv(hidden_states)
<span class="linenos"> 17</span><span class="w"> </span>        .reshape(seq_length, 3, self.num_heads, -1)
<span class="linenos"> 18</span><span class="w"> </span>        .permute(1, 0, 2, 3)
<span class="linenos"> 19</span><span class="w"> </span>        .unbind(0)
<span class="linenos"> 20</span><span class="w"> </span>    )
<span class="linenos"> 21</span><span class="gd">-    cos, sin = position_embeddings</span>
<span class="linenos"> 22</span><span class="gd">-    query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)</span>
<span class="linenos"> 23</span><span class="gd">-</span>
<span class="linenos"> 24</span><span class="gd">-    query_states = query_states.transpose(0, 1).unsqueeze(0)</span>
<span class="linenos"> 25</span><span class="gd">-    key_states = key_states.transpose(0, 1).unsqueeze(0)</span>
<span class="linenos"> 26</span><span class="gd">-    value_states = value_states.transpose(0, 1).unsqueeze(0)</span>
<span class="linenos"> 27</span><span class="gd">-</span>
<span class="linenos"> 28</span><span class="gd">-    attention_interface: Callable = ALL_ATTENTION_FUNCTIONS.get_interface(</span>
<span class="linenos"> 29</span><span class="gd">-        self.config._attn_implementation, eager_attention_forward</span>
<span class="linenos"> 30</span><span class="gi">+    if position_embeddings is None:</span>
<span class="linenos"> 31</span><span class="gi">+        transformers.models.qwen2_vl.modeling_qwen2_vl.logger.warning_once(</span>
<span class="linenos"> 32</span><span class="gi">+            &quot;The attention layers in this model are transitioning from &quot;</span>
<span class="linenos"> 33</span><span class="gi">+            &quot; computing the RoPE embeddings internally &quot;</span>
<span class="linenos"> 34</span><span class="gi">+            &quot;through `rotary_pos_emb` (2D tensor of RoPE theta values), &quot;</span>
<span class="linenos"> 35</span><span class="gi">+            &quot;to using externally computed &quot;</span>
<span class="linenos"> 36</span><span class="gi">+            &quot;`position_embeddings` (Tuple of tensors, containing cos and sin).&quot;</span>
<span class="linenos"> 37</span><span class="gi">+            &quot; In v4.54 `rotary_pos_emb` will be &quot;</span>
<span class="linenos"> 38</span><span class="gi">+            &quot;removed and `position_embeddings` will be mandatory.&quot;</span>
<span class="linenos"> 39</span><span class="gi">+        )</span>
<span class="linenos"> 40</span><span class="gi">+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)</span>
<span class="linenos"> 41</span><span class="gi">+        cos = emb.cos()</span>
<span class="linenos"> 42</span><span class="gi">+        sin = emb.sin()</span>
<span class="linenos"> 43</span><span class="gi">+    else:</span>
<span class="linenos"> 44</span><span class="gi">+        cos, sin = position_embeddings</span>
<span class="linenos"> 45</span><span class="gi">+    q, k = transformers.models.qwen2_vl.modeling_qwen2_vl.apply_rotary_pos_emb_vision(</span>
<span class="linenos"> 46</span><span class="gi">+        q, k, cos, sin</span>
<span class="linenos"> 47</span><span class="w"> </span>    )
<span class="linenos"> 48</span>
<span class="linenos"> 49</span><span class="gd">-    if is_flash_attention_requested(self.config):</span>
<span class="linenos"> 50</span><span class="gd">-        # Flash Attention: Use cu_seqlens for variable length attention</span>
<span class="linenos"> 51</span><span class="gd">-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()</span>
<span class="linenos"> 52</span><span class="gd">-        attn_output, _ = attention_interface(</span>
<span class="linenos"> 53</span><span class="gd">-            self,</span>
<span class="linenos"> 54</span><span class="gd">-            query_states,</span>
<span class="linenos"> 55</span><span class="gd">-            key_states,</span>
<span class="linenos"> 56</span><span class="gd">-            value_states,</span>
<span class="linenos"> 57</span><span class="gd">-            attention_mask=None,</span>
<span class="linenos"> 58</span><span class="gd">-            scaling=self.scaling,</span>
<span class="linenos"> 59</span><span class="gd">-            dropout=0.0 if not self.training else self.attention_dropout,</span>
<span class="linenos"> 60</span><span class="gd">-            cu_seq_lens_q=cu_seqlens,</span>
<span class="linenos"> 61</span><span class="gd">-            cu_seq_lens_k=cu_seqlens,</span>
<span class="linenos"> 62</span><span class="gd">-            max_length_q=max_seqlen,</span>
<span class="linenos"> 63</span><span class="gd">-            max_length_k=max_seqlen,</span>
<span class="linenos"> 64</span><span class="gd">-            is_causal=False,</span>
<span class="linenos"> 65</span><span class="gd">-            **kwargs,</span>
<span class="linenos"> 66</span><span class="gd">-        )</span>
<span class="linenos"> 67</span><span class="gd">-    else:</span>
<span class="linenos"> 68</span><span class="gd">-        # Other implementations: Process each chunk separately</span>
<span class="linenos"> 69</span><span class="gd">-        lengths = cu_seqlens[1:] - cu_seqlens[:-1]</span>
<span class="linenos"> 70</span><span class="gd">-        splits = [</span>
<span class="linenos"> 71</span><span class="gd">-            torch.split(tensor, lengths.tolist(), dim=2)</span>
<span class="linenos"> 72</span><span class="gd">-            for tensor in (query_states, key_states, value_states)</span>
<span class="linenos"> 73</span><span class="gd">-        ]</span>
<span class="linenos"> 74</span><span class="gi">+    attention_mask = torch.full(</span>
<span class="linenos"> 75</span><span class="gi">+        [1, seq_length, seq_length],</span>
<span class="linenos"> 76</span><span class="gi">+        torch.finfo(q.dtype).min,</span>
<span class="linenos"> 77</span><span class="gi">+        device=q.device,</span>
<span class="linenos"> 78</span><span class="gi">+        dtype=q.dtype,</span>
<span class="linenos"> 79</span><span class="gi">+    )</span>
<span class="linenos"> 80</span><span class="gi">+    # for i in range(1, len(cu_seqlens)):</span>
<span class="linenos"> 81</span><span class="gi">+    #     attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i],</span>
<span class="linenos"> 82</span><span class="gi">+    #                         cu_seqlens[i - 1] : cu_seqlens[i]] = 0</span>
<span class="linenos"> 83</span><span class="gi">+    attention_mask = rewrite_loop_for_square_mask(attention_mask, cu_seqlens)</span>
<span class="linenos"> 84</span>
<span class="linenos"> 85</span><span class="gd">-        attn_outputs = [</span>
<span class="linenos"> 86</span><span class="gd">-            attention_interface(</span>
<span class="linenos"> 87</span><span class="gd">-                self,</span>
<span class="linenos"> 88</span><span class="gd">-                q,</span>
<span class="linenos"> 89</span><span class="gd">-                k,</span>
<span class="linenos"> 90</span><span class="gd">-                v,</span>
<span class="linenos"> 91</span><span class="gd">-                attention_mask=None,</span>
<span class="linenos"> 92</span><span class="gd">-                scaling=self.scaling,</span>
<span class="linenos"> 93</span><span class="gd">-                dropout=0.0 if not self.training else self.attention_dropout,</span>
<span class="linenos"> 94</span><span class="gd">-                is_causal=False,</span>
<span class="linenos"> 95</span><span class="gd">-                **kwargs,</span>
<span class="linenos"> 96</span><span class="gd">-            )[0]</span>
<span class="linenos"> 97</span><span class="gd">-            for q, k, v in zip(*splits)</span>
<span class="linenos"> 98</span><span class="gd">-        ]</span>
<span class="linenos"> 99</span><span class="gd">-        attn_output = torch.cat(attn_outputs, dim=1)</span>
<span class="linenos">100</span><span class="gd">-</span>
<span class="linenos">101</span><span class="gd">-    attn_output = attn_output.reshape(seq_length, -1).contiguous()</span>
<span class="linenos">102</span><span class="gi">+    q = q.transpose(0, 1)</span>
<span class="linenos">103</span><span class="gi">+    k = k.transpose(0, 1)</span>
<span class="linenos">104</span><span class="gi">+    v = v.transpose(0, 1)</span>
<span class="linenos">105</span><span class="gi">+    attn_weights = torch.matmul(q, k.transpose(1, 2)) / (self.head_dim**0.5)</span>
<span class="linenos">106</span><span class="gi">+    attn_weights = attn_weights + attention_mask</span>
<span class="linenos">107</span><span class="gi">+    attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(</span>
<span class="linenos">108</span><span class="gi">+        q.dtype</span>
<span class="linenos">109</span><span class="gi">+    )</span>
<span class="linenos">110</span><span class="gi">+    attn_output = torch.matmul(attn_weights, v)</span>
<span class="linenos">111</span><span class="gi">+    attn_output = attn_output.transpose(0, 1)</span>
<span class="linenos">112</span><span class="gi">+    attn_output = attn_output.reshape(seq_length, -1)</span>
<span class="linenos">113</span><span class="w"> </span>    attn_output = self.proj(attn_output)
<span class="linenos">114</span><span class="w"> </span>    return attn_output
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-eager-attention-forward-patched-model-bart-eager-attention-forward">
<h2>auto/patch_transformers: eager_attention_forward -&gt; patched_model_bart_eager_attention_forward<a class="headerlink" href="#auto-patch-transformers-eager-attention-forward-patched-model-bart-eager-attention-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,27 +1,23 @@</span>
<span class="linenos"> 4</span><span class="gd">-def eager_attention_forward(</span>
<span class="linenos"> 5</span><span class="gd">-    module: nn.Module,</span>
<span class="linenos"> 6</span><span class="gi">+def patched_model_bart_eager_attention_forward(</span>
<span class="linenos"> 7</span><span class="gi">+    module: torch.nn.Module,</span>
<span class="linenos"> 8</span><span class="w"> </span>    query: torch.Tensor,
<span class="linenos"> 9</span><span class="w"> </span>    key: torch.Tensor,
<span class="linenos">10</span><span class="w"> </span>    value: torch.Tensor,
<span class="linenos">11</span><span class="gd">-    attention_mask: torch.Tensor | None,</span>
<span class="linenos">12</span><span class="gd">-    scaling: float | None = None,</span>
<span class="linenos">13</span><span class="gi">+    attention_mask: Optional[torch.Tensor],</span>
<span class="linenos">14</span><span class="gi">+    scaling: Optional[float] = None,</span>
<span class="linenos">15</span><span class="w"> </span>    dropout: float = 0.0,
<span class="linenos">16</span><span class="gd">-    **kwargs: Unpack[TransformersKwargs],</span>
<span class="linenos">17</span><span class="gi">+    head_mask: Optional[torch.Tensor] = None,</span>
<span class="linenos">18</span><span class="gi">+    **kwargs,</span>
<span class="linenos">19</span><span class="w"> </span>):
<span class="linenos">20</span><span class="gd">-    if scaling is None:</span>
<span class="linenos">21</span><span class="gd">-        scaling = query.size(-1) ** -0.5</span>
<span class="linenos">22</span><span class="gd">-</span>
<span class="linenos">23</span><span class="gd">-    # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span>
<span class="linenos">24</span><span class="gd">-    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling</span>
<span class="linenos">25</span><span class="gd">-</span>
<span class="linenos">26</span><span class="gd">-    if attention_mask is not None:</span>
<span class="linenos">27</span><span class="gd">-        attention_mask = attention_mask[:, :, :, : key.shape[-2]]</span>
<span class="linenos">28</span><span class="gd">-        attn_weights = attn_weights + attention_mask</span>
<span class="linenos">29</span><span class="gd">-</span>
<span class="linenos">30</span><span class="gd">-    attn_weights = nn.functional.softmax(attn_weights, dim=-1)</span>
<span class="linenos">31</span><span class="gd">-    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)</span>
<span class="linenos">32</span><span class="gd">-</span>
<span class="linenos">33</span><span class="gd">-    attn_output = torch.matmul(attn_weights, value)</span>
<span class="linenos">34</span><span class="gd">-    attn_output = attn_output.transpose(1, 2).contiguous()</span>
<span class="linenos">35</span><span class="gd">-</span>
<span class="linenos">36</span><span class="gd">-    return attn_output, attn_weights</span>
<span class="linenos">37</span><span class="gi">+    &quot;&quot;&quot;[patch:transformers.models.bart.modeling_bart.eager_attention_forward]&quot;&quot;&quot;</span>
<span class="linenos">38</span><span class="gi">+    return common_eager_attention_forward(</span>
<span class="linenos">39</span><span class="gi">+        module,</span>
<span class="linenos">40</span><span class="gi">+        query,</span>
<span class="linenos">41</span><span class="gi">+        key,</span>
<span class="linenos">42</span><span class="gi">+        value,</span>
<span class="linenos">43</span><span class="gi">+        attention_mask=attention_mask,</span>
<span class="linenos">44</span><span class="gi">+        scaling=scaling,</span>
<span class="linenos">45</span><span class="gi">+        dropout=dropout,</span>
<span class="linenos">46</span><span class="gi">+        head_mask=head_mask,</span>
<span class="linenos">47</span><span class="gi">+        **kwargs,</span>
<span class="linenos">48</span><span class="gi">+    )</span>
</pre></div>
</div>
</section>
<section id="auto-patch-transformers-eager-attention-forward-patched-modeling-marian-eager-attention-forward">
<h2>auto/patch_transformers: eager_attention_forward -&gt; patched_modeling_marian_eager_attention_forward<a class="headerlink" href="#auto-patch-transformers-eager-attention-forward-patched-modeling-marian-eager-attention-forward" title="Link to this heading">¶</a></h2>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="gd">--- original</span>
<span class="linenos"> 2</span><span class="gi">+++ rewritten</span>
<span class="linenos"> 3</span><span class="gu">@@ -1,27 +1,23 @@</span>
<span class="linenos"> 4</span><span class="gd">-def eager_attention_forward(</span>
<span class="linenos"> 5</span><span class="gd">-    module: nn.Module,</span>
<span class="linenos"> 6</span><span class="gi">+def patched_modeling_marian_eager_attention_forward(</span>
<span class="linenos"> 7</span><span class="gi">+    module: torch.nn.Module,</span>
<span class="linenos"> 8</span><span class="w"> </span>    query: torch.Tensor,
<span class="linenos"> 9</span><span class="w"> </span>    key: torch.Tensor,
<span class="linenos">10</span><span class="w"> </span>    value: torch.Tensor,
<span class="linenos">11</span><span class="gd">-    attention_mask: torch.Tensor | None,</span>
<span class="linenos">12</span><span class="gd">-    scaling: float | None = None,</span>
<span class="linenos">13</span><span class="gi">+    attention_mask: Optional[torch.Tensor],</span>
<span class="linenos">14</span><span class="gi">+    scaling: Optional[float] = None,</span>
<span class="linenos">15</span><span class="w"> </span>    dropout: float = 0.0,
<span class="linenos">16</span><span class="gd">-    **kwargs: Unpack[TransformersKwargs],</span>
<span class="linenos">17</span><span class="gi">+    head_mask: Optional[torch.Tensor] = None,</span>
<span class="linenos">18</span><span class="gi">+    **kwargs,</span>
<span class="linenos">19</span><span class="w"> </span>):
<span class="linenos">20</span><span class="gd">-    if scaling is None:</span>
<span class="linenos">21</span><span class="gd">-        scaling = query.size(-1) ** -0.5</span>
<span class="linenos">22</span><span class="gd">-</span>
<span class="linenos">23</span><span class="gd">-    # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span>
<span class="linenos">24</span><span class="gd">-    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling</span>
<span class="linenos">25</span><span class="gd">-</span>
<span class="linenos">26</span><span class="gd">-    if attention_mask is not None:</span>
<span class="linenos">27</span><span class="gd">-        attention_mask = attention_mask[:, :, :, : key.shape[-2]]</span>
<span class="linenos">28</span><span class="gd">-        attn_weights = attn_weights + attention_mask</span>
<span class="linenos">29</span><span class="gd">-</span>
<span class="linenos">30</span><span class="gd">-    attn_weights = nn.functional.softmax(attn_weights, dim=-1)</span>
<span class="linenos">31</span><span class="gd">-    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)</span>
<span class="linenos">32</span><span class="gd">-</span>
<span class="linenos">33</span><span class="gd">-    attn_output = torch.matmul(attn_weights, value)</span>
<span class="linenos">34</span><span class="gd">-    attn_output = attn_output.transpose(1, 2).contiguous()</span>
<span class="linenos">35</span><span class="gd">-</span>
<span class="linenos">36</span><span class="gd">-    return attn_output, attn_weights</span>
<span class="linenos">37</span><span class="gi">+    &quot;&quot;&quot;[patch:transformers.models.marian.modeling_marian.eager_attention_forward]&quot;&quot;&quot;</span>
<span class="linenos">38</span><span class="gi">+    return common_eager_attention_forward(</span>
<span class="linenos">39</span><span class="gi">+        module,</span>
<span class="linenos">40</span><span class="gi">+        query,</span>
<span class="linenos">41</span><span class="gi">+        key,</span>
<span class="linenos">42</span><span class="gi">+        value,</span>
<span class="linenos">43</span><span class="gi">+        attention_mask=attention_mask,</span>
<span class="linenos">44</span><span class="gi">+        scaling=scaling,</span>
<span class="linenos">45</span><span class="gi">+        dropout=dropout,</span>
<span class="linenos">46</span><span class="gi">+        head_mask=head_mask,</span>
<span class="linenos">47</span><span class="gi">+        **kwargs,</span>
<span class="linenos">48</span><span class="gi">+    )</span>
</pre></div>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../api/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">API of onnx_diagnostic</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="patches_coverage.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Coverage of the Patches</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=2a76c96f"></script>
    <script src="../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>