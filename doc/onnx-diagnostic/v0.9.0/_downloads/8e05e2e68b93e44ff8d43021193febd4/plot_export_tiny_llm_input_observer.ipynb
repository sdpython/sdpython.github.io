{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export a LLM with InputObserver (with Tiny-LLM)\n\nThe main issue when exporting a LLM is the example on HuggingFace is\nbased on method generate but we only need to export the forward method.\nExample `l-plot-tiny-llm-export` gives details on how to guess\ndummy inputs and dynamic shapes to do so.\nLet's see how to simplify that.\n\n## Dummy Example\n\nLet's use the example provided on\n[arnir0/Tiny-LLM](https://huggingface.co/arnir0/Tiny-LLM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom onnx_diagnostic import doc\nfrom onnx_diagnostic.helpers import string_type\nfrom onnx_diagnostic.helpers.rt_helper import onnx_generate\nfrom onnx_diagnostic.torch_export_patches import (\n    register_additional_serialization_functions,\n    torch_export_patches,\n)\nfrom onnx_diagnostic.export.api import to_onnx\nfrom onnx_diagnostic.investigate.input_observer import InputObserver\n\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n\ndef generate_text(\n    prompt,\n    model,\n    tokenizer,\n    max_length=50,\n    temperature=0.01,\n    top_k=50,\n    top_p=0.95,\n    do_sample=True,\n):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        do_sample=do_sample,\n    )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n\n# Define your prompt\nprompt = \"Continue: it rains, what should I do?\"\ngenerated_text = generate_text(prompt, model, tokenizer)\nprint(\"-----------------\")\nprint(generated_text)\nprint(\"-----------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace forward method\n\nWe first capture inputs and outputs with an :class`InputObserver\n<onnx_diagnostic.investigate.input_observer>`.\nWe also need to registers additional patches for :epkg:`transformers`.\nThen :epkg:`pytorch` knows how to flatten/unflatten inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "observer = InputObserver()\nwith register_additional_serialization_functions(patch_transformers=True), observer(model):\n    generate_text(prompt, model, tokenizer)\n\nprint(f\"number of stored inputs: {len(observer.info)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exports\n\nThe `InputObserver` has now enough data to infer arguments and dynamic shapes.\nWe need more than serialization but also patches to export the model.\nInferred dynamic shapes looks like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(observer.infer_dynamic_shapes(set_batch_dimension_for=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and inferred arguments:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(string_type(observer.infer_arguments(), with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filenamec = \"plot_export_tiny_llm_input_observer.custom.onnx\"\nwith torch_export_patches(patch_transformers=True):\n    to_onnx(\n        model,\n        (),\n        kwargs=observer.infer_arguments(),\n        dynamic_shapes=observer.infer_dynamic_shapes(set_batch_dimension_for=True),\n        filename=filenamec,\n        exporter=\"custom\",\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check discrepancies\n\nThe model is exported into ONNX. We use again the stored inputs and outputs\nto verify the model produces the same outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = observer.check_discrepancies(filenamec, progress_bar=True)\nprint(pandas.DataFrame(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimal script to export a LLM\n\nThe following lines are a condensed copy with less comments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# from HuggingFace\nprint(\"----------------\")\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n# from HuggingFace again\nprompt = \"Continue: it rains, what should I do?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    do_sample=False,\n)\n\nobserver = InputObserver()\nwith register_additional_serialization_functions(patch_transformers=True), observer(model):\n    generate_text(prompt, model, tokenizer)\n\nfilename = \"plot_export_tiny_llm_input_observer.onnx\"\nwith torch_export_patches(patch_transformers=True):\n    torch.onnx.export(\n        model,\n        (),\n        filename,\n        kwargs=observer.infer_arguments(),\n        dynamic_shapes=observer.infer_dynamic_shapes(set_batch_dimension_for=True),\n    )\n\ndata = observer.check_discrepancies(filename, progress_bar=True)\nprint(pandas.DataFrame(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNX Prompt\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onnx_tokens = onnx_generate(\n    filenamec,\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    eos_token_id=model.config.eos_token_id,\n    max_new_tokens=50,\n)\nonnx_generated_text = tokenizer.decode(onnx_tokens, skip_special_tokens=True)\nprint(\"-----------------\")\nprint(\"\\n\".join(onnx_generated_text))\nprint(\"-----------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc.save_fig(doc.plot_dot(filename), f\"{filename}.png\", dpi=400)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}