{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Measuring performance about Gemm\n\nDifferents types, differents backend, differents\n\n## Onnx Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import platform\nfrom itertools import product\nimport numpy\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame, pivot_table\nfrom onnx import TensorProto\nfrom onnx.helper import (\n    make_model,\n    make_node,\n    make_graph,\n    make_tensor_value_info,\n    make_opsetid,\n)\nfrom onnx.checker import check_model\nfrom onnx.numpy_helper import from_array\nfrom onnxruntime import InferenceSession, get_available_providers\nfrom onnxruntime.capi._pybind_state import (\n    OrtValue as C_OrtValue,\n    OrtDevice as C_OrtDevice,\n)\nfrom onnxruntime.capi.onnxruntime_pybind11_state import (\n    NotImplemented,\n    InvalidGraph,\n    InvalidArgument,\n)\nfrom onnx_extended.reference import CReferenceEvaluator\nfrom onnx_extended.ext_test_case import unit_test_going, measure_time\n\n\ndef create_model(mat_type=TensorProto.FLOAT):\n    I1 = from_array(numpy.array([1], dtype=numpy.float32), name=\"I\")\n    A = make_tensor_value_info(\"A\", mat_type, [None, None])\n    B = make_tensor_value_info(\"B\", mat_type, [None, None])\n    C = make_tensor_value_info(\"C\", mat_type, [None, None])\n    nodes = [\n        make_node(\"CastLike\", [\"I\", \"A\"], [\"Ic\"]),\n        make_node(\"Add\", [\"A\", \"Ic\"], [\"A1\"]),\n        make_node(\"Add\", [\"A1\", \"Ic\"], [\"A2\"]),\n        make_node(\"Add\", [\"A2\", \"Ic\"], [\"A3\"]),\n        make_node(\"MatMul\", [\"A\", \"B\"], [\"M0\"]),\n        make_node(\"MatMul\", [\"A1\", \"B\"], [\"M1\"]),\n        make_node(\"MatMul\", [\"A2\", \"B\"], [\"M2\"]),\n        make_node(\"MatMul\", [\"A3\", \"B\"], [\"M3\"]),\n        make_node(\"Add\", [\"M0\", \"M1\"], [\"M12\"]),\n        make_node(\"Add\", [\"M2\", \"M3\"], [\"M23\"]),\n        make_node(\"Add\", [\"M12\", \"M23\"], [\"C\"]),\n    ]\n    graph = make_graph(nodes, \"a\", [A, B], [C], [I1])\n    if mat_type < 16:\n        # regular type\n        opset, ir = 18, 8\n    else:\n        opset, ir = 19, 9\n    onnx_model = make_model(\n        graph, opset_imports=[make_opsetid(\"\", opset)], ir_version=ir\n    )\n    check_model(onnx_model)\n    return onnx_model\n\n\ncreate_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model to cast\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_cast(to):\n    A = make_tensor_value_info(\"A\", TensorProto.FLOAT, [None, None])\n    C = make_tensor_value_info(\"C\", to, [None, None])\n    node1 = make_node(\"Cast\", [\"A\"], [\"C\"], to=to)\n    graph = make_graph([node1], \"a\", [A], [C])\n    if to < 16:\n        # regular type\n        opset, ir = 18, 8\n    else:\n        opset, ir = 19, 9\n    onnx_model = make_model(\n        graph, opset_imports=[make_opsetid(\"\", opset)], ir_version=ir\n    )\n    check_model(onnx_model)\n    return onnx_model\n\n\ncreate_cast(TensorProto.FLOAT16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance\n\nThe benchmark will run the following configurations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "types = [\n    TensorProto.FLOAT,\n    TensorProto.UINT32,\n    TensorProto.INT32,\n    TensorProto.INT16,\n    TensorProto.INT8,\n    TensorProto.FLOAT16,\n    TensorProto.BFLOAT16,\n    TensorProto.FLOAT8E4M3FN,\n    TensorProto.FLOAT8E5M2,\n]\nengine = [CReferenceEvaluator, InferenceSession]\nproviders = [\n    [\"CPUExecutionProvider\"],\n    [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n]\n# M, N, K\ndims = [\n    (10, 10, 10),\n    (61, 62, 63),\n    (64, 64, 64),\n    (65, 66, 67),\n    (100, 100, 100),\n    (128, 128, 128),\n    # (256, 256, 256),\n    # (400, 400, 400),\n    # (512, 512, 512),\n]\n\n\nmap_type = {TensorProto.FLOAT: numpy.float32, TensorProto.FLOAT16: numpy.float16}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's cache the matrices involved.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def to_ort_value(m):\n    device = C_OrtDevice(C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)\n    ort_value = C_OrtValue.ortvalue_from_numpy(m, device)\n    return ort_value\n\n\nmatrices = {}\nfor m, n, k in dims:\n    for tt in types:\n        for i, j in [(m, k), (k, n)]:\n            try:\n                sess = InferenceSession(\n                    create_cast(tt).SerializeToString(),\n                    providers=[\"CPUExecutionProvider\"],\n                )\n            except (InvalidGraph, InvalidArgument):\n                # not support by this version of onnxruntime\n                continue\n            vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)\n            ov = to_ort_value(vect)\n            ovtt = sess._sess.run_with_ort_values({\"A\": ov}, [\"C\"], None)[0]\n            matrices[tt, i, j] = ovtt\n\nprint(f\"{len(matrices)} matrices were created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\nerrors = []\npbar = tqdm(list(product(types, engine, providers, dims)))\nfor tt, engine, provider, dim in pbar:\n    if max(dim) <= 200:\n        repeat, number = 50, 25\n    elif max(dim) <= 256:\n        repeat, number = 25, 10\n    else:\n        repeat, number = 10, 4\n\n    onx = create_model(tt)\n    with open(f\"plot_bench_gemm_{tt}.onnx\", \"wb\") as f:\n        f.write(onx.SerializeToString())\n    k1 = (tt, dim[0], dim[2])\n    k2 = (tt, dim[2], dim[1])\n    if k1 not in matrices:\n        errors.append(f\"Key k1={k1!r} not in matrices.\")\n        continue\n    if k2 not in matrices:\n        errors.append(f\"Key k2={k2!r} not in matrices.\")\n        continue\n\n    if engine == CReferenceEvaluator:\n        if tt == TensorProto.FLOAT16 and max(dim) > 50:\n            repeat, number = 2, 2\n        if provider != [\"CPUExecutionProvider\"]:\n            continue\n        if tt not in [TensorProto.FLOAT, TensorProto.FLOAT16]:\n            continue\n\n        pbar.set_description(\n            f\"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}\"\n        )\n\n        feeds = {\"A\": matrices[k1].numpy(), \"B\": matrices[k2].numpy()}\n        sess = engine(onx)\n        sess.run(None, feeds)\n        obs = measure_time(lambda: sess.run(None, feeds), repeat=repeat, number=number)\n\n    elif engine == InferenceSession:\n        if provider[0] not in get_available_providers():\n            continue\n        pbar.set_description(\n            f\"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}\"\n        )\n        feeds = {\"A\": matrices[k1], \"B\": matrices[k2]}\n        try:\n            sess = engine(onx.SerializeToString(), providers=provider)\n        except (NotImplemented, InvalidGraph) as e:\n            # not implemented\n            errors.append(e)\n            continue\n\n        if provider == [\"CPUExecutionProvider\"]:\n            the_feeds = feeds\n        else:\n            # moving values to CUDA\n            device = C_OrtDevice(C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0)\n            try:\n                the_feeds = {\n                    k: C_OrtValue.ortvalue_from_numpy(v.numpy(), device)\n                    for k, v in feeds.items()\n                }\n            except RuntimeError as e:\n                errors.append(f\"issue with cuda and type {tt} - {e}\")\n                continue\n\n        sess._sess.run_with_ort_values(the_feeds, [\"C\"], None)[0]\n        obs = measure_time(\n            lambda: sess._sess.run_with_ort_values(the_feeds, [\"C\"], None)[0],\n            repeat=repeat,\n            number=number,\n        )\n\n    else:\n        continue\n\n    obs.update(\n        dict(\n            engine={\"InferenceSession\": \"ort\", \"CReferenceEvaluator\": \"np\"}[\n                engine.__name__\n            ],\n            type={\n                TensorProto.FLOAT: \"f32\",\n                TensorProto.FLOAT16: \"f16\",\n                TensorProto.INT8: \"i8\",\n                TensorProto.INT16: \"i16\",\n                TensorProto.INT32: \"i32\",\n                TensorProto.UINT32: \"u32\",\n            }[tt],\n            M=dim[0],\n            N=dim[1],\n            K=dim[2],\n            cost=numpy.prod(dim) * 4,\n            cost_s=f\"{numpy.prod(dim) * 4}-{dim[0]}x{dim[1]}x{dim[2]}\",\n            repeat=repeat,\n            number=number,\n            provider={\"CPUExecutionProvider\": \"cpu\", \"CUDAExecutionProvider\": \"cuda\"}[\n                provider[0]\n            ],\n            platform=platform.processor(),\n        )\n    )\n    data.append(obs)\n    if unit_test_going() and len(data) >= 2:\n        break\n\n\ndf = DataFrame(data)\ndf.to_excel(\"plot_bench_gemm.xlsx\")\ndf.to_csv(\"plot_bench_gemm.csv\")\ndf.drop([\"min_exec\", \"max_exec\"], axis=1).to_csv(\"plot_bench_gemm_.csv\")\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for e in list(sorted(set(map(str, errors)))):\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pivot_table(\n    df, index=[\"cost\"], columns=[\"engine\", \"type\", \"provider\"], values=\"average\"\n)\npiv.reset_index(drop=False).to_excel(\"plot_bench_gemm_summary.xlsx\")\npiv.reset_index(drop=False).to_csv(\"plot_bench_gemm_summary.csv\")\nprint(piv)\npiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pivs = pivot_table(\n    df, index=[\"cost_s\"], columns=[\"engine\", \"type\", \"provider\"], values=\"average\"\n)\nprint(pivs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfi = df[\n    df.type.isin({\"f32\", \"f16\", \"bf16\", \"f8e4m3\", \"f8e5m2\"}) & df.engine.isin({\"ort\"})\n]\npivi = pivot_table(\n    dfi, index=[\"cost\"], columns=[\"engine\", \"type\", \"provider\"], values=\"average\"\n)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\npiv.plot(ax=ax[0], title=\"Gemm performance\\nlower is better\", logx=True, logy=True)\nif pivi.shape[0] > 0:\n    pivi.plot(\n        ax=ax[1],\n        title=f\"Gemm performance ORT\\n{platform.processor()}\",\n        logx=True,\n        logy=True,\n    )\nfig.tight_layout()\nfig.savefig(\"plot_bench_gemm.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}