{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TreeEnsemble optimization\n\nThe execution of a TreeEnsembleRegressor can lead to very different results\ndepending on how the computation is parallelized. By trees,\nby rows, by both, for only one row, for a short batch of rows, a longer one.\nThe implementation in :epkg:`onnxruntime` does not let the user changed\nthe predetermined settings but a custom kernel might. That's what this example\nis measuring.\n\nThe default set of optimized parameters is very short and is meant to be executed\nfast. Many more parameters can be tried.\n\n::\n\n    python plot_optim_tree_ensemble --scenario=LONG\n\nTo change the training parameters:\n\n::\n\n    python plot_optim_tree_ensemble.py\n        --n_trees=100\n        --max_depth=10\n        --n_features=50\n        --batch_size=100000 \n\n# Training a model\n# ++++++++++++++++\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport timeit\nimport numpy\nimport onnx\nfrom onnx.reference import ReferenceEvaluator\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame, concat\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skl2onnx import to_onnx\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom onnx_array_api.plotting.text_plot import onnx_simple_text_plot\nfrom onnx_extended.reference import CReferenceEvaluator\nfrom onnx_extended.ortops.optim.cpu import get_ort_ext_libs\nfrom onnx_extended.ortops.optim.optimize import (\n    change_onnx_operator_domain,\n    get_node_attribute,\n    optimize_model,\n)\nfrom onnx_extended.ext_test_case import get_parsed_args\n\nscript_args = get_parsed_args(\n    \"plot_optim_tree_ensemble\",\n    description=__doc__,\n    scenarios={\"SHORT\": \"short optimization\", \"LONG\": \"test more options\"},\n    n_features=(5, \"number of features to generate\"),\n    n_trees=(10, \"number of trees to train\"),\n    max_depth=(5, \"max_depth\"),\n    batch_size=(10000, \"batch size\"),\n)\n\nbatch_size = script_args.batch_size\nn_features = script_args.n_features\nn_trees = script_args.n_trees\nmax_depth = script_args.max_depth\n\nfilename = (\n    f\"plot_optim_tree_ensemble_b{batch_size}-f{n_features}-\"\n    f\"t{n_trees}-d{max_depth}.onnx\"\n)\nif not os.path.exists(filename):\n    print(f\"Training to get {filename!r}\")\n    X, y = make_regression(batch_size * 2, n_features=n_features, n_targets=1)\n    X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n    model = RandomForestRegressor(n_trees, max_depth=max_depth, verbose=2)\n    model.fit(X[:batch_size], y[:batch_size])\n    onx = to_onnx(model, X[:1])\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())\nelse:\n    X, y = make_regression(batch_size, n_features=n_features, n_targets=1)\n    X, y = X.astype(numpy.float32), y.astype(numpy.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite the onnx file to use a different kernel\n\nThe custom kernel is mapped to a custom operator with the same name\nthe attributes and domain = `\"onnx_extented.ortops.optim.cpu\"`.\nWe call a function to do that replacement.\nFirst the current model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(filename, \"rb\") as f:\n    onx = onnx.load(f)\nprint(onnx_simple_text_plot(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then the modified model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def transform_model(onx, **kwargs):\n    att = get_node_attribute(onx.graph.node[0], \"nodes_modes\")\n    modes = \",\".join(map(lambda s: s.decode(\"ascii\"), att.strings))\n    return change_onnx_operator_domain(\n        onx,\n        op_type=\"TreeEnsembleRegressor\",\n        op_domain=\"ai.onnx.ml\",\n        new_op_domain=\"onnx_extented.ortops.optim.cpu\",\n        nodes_modes=modes,\n        **kwargs,\n    )\n\n\nprint(\"Tranform model to add a custom node.\")\nonx_modified = transform_model(onx)\nprint(f\"Save into {filename + 'modified.onnx'!r}.\")\nwith open(filename + \"modified.onnx\", \"wb\") as f:\n    f.write(onx_modified.SerializeToString())\nprint(\"done.\")\nprint(onnx_simple_text_plot(onx_modified))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing onnxruntime and the custom kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Loading {filename!r}\")\nsess_ort = InferenceSession(filename, providers=[\"CPUExecutionProvider\"])\n\nr = get_ort_ext_libs()\nprint(f\"Creating SessionOptions with {r!r}\")\nopts = SessionOptions()\nif r is not None:\n    opts.register_custom_ops_library(r[0])\n\nprint(\"Loading modified {filename!r}\")\nsess_cus = InferenceSession(\n    onx_modified.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n)\n\nprint(f\"Running once with shape {X[-batch_size:].shape}.\")\nbase = sess_ort.run(None, {\"X\": X[-batch_size:]})[0]\nprint(f\"Running modified with shape {X[-batch_size:].shape}.\")\ngot = sess_cus.run(None, {\"X\": X[-batch_size:]})[0]\nprint(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discrepancies?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diff = numpy.abs(base - got).max()\nprint(f\"Discrepancies: {diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple verification\n\nBaseline with onnxruntime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t1 = timeit.timeit(lambda: sess_ort.run(None, {\"X\": X[-batch_size:]}), number=50)\nprint(f\"baseline: {t1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t2 = timeit.timeit(lambda: sess_cus.run(None, {\"X\": X[-batch_size:]}), number=50)\nprint(f\"new time: {t2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same implementation but ran from the onnx python backend.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref = CReferenceEvaluator(filename)\nref.run(None, {\"X\": X[-batch_size:]})\nt3 = timeit.timeit(lambda: ref.run(None, {\"X\": X[-batch_size:]}), number=50)\nprint(f\"CReferenceEvaluator: {t3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The python implementation but from the onnx python backend.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref = ReferenceEvaluator(filename)\nref.run(None, {\"X\": X[-batch_size:]})\nt4 = timeit.timeit(lambda: ref.run(None, {\"X\": X[-batch_size:]}), number=5)\nprint(f\"ReferenceEvaluator: {t4} (only 5 times instead of 50)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time for comparison\n\nThe custom kernel supports the same attributes as *TreeEnsembleRegressor*\nplus new ones to tune the parallelization. They can be seen in\n[tree_ensemble.cc](https://github.com/sdpython/onnx-extended/\nblob/main/onnx_extended/ortops/optim/cpu/tree_ensemble.cc#L102).\nLet's try out many possibilities.\nThe default values are the first ones.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if script_args.scenario in (None, \"SHORT\"):\n    optim_params = dict(\n        parallel_tree=[80, 40],  # default is 80\n        parallel_tree_N=[128, 64],  # default is 128\n        parallel_N=[50, 25],  # default is 50\n        batch_size_tree=[2],  # default is 2\n        batch_size_rows=[2],  # default is 2\n        use_node3=[0],  # default is 0\n    )\nelif script_args.scenario in (None, \"LONG\"):\n    optim_params = dict(\n        parallel_tree=[80, 160, 40],\n        parallel_tree_N=[256, 128, 64],\n        parallel_N=[100, 50, 25],\n        batch_size_tree=[2, 4, 8],\n        batch_size_rows=[2, 4, 8],\n        use_node3=[0, 1],\n    )\nelse:\n    raise ValueError(\n        f\"Unknown scenario {script_args.scenario!r}, use --help to get them.\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_session(onx):\n    opts = SessionOptions()\n    r = get_ort_ext_libs()\n    if r is None:\n        raise RuntimeError(\"No custom implementation available.\")\n    opts.register_custom_ops_library(r[0])\n    return InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n\nres = optimize_model(\n    onx,\n    feeds={\"X\": X[-batch_size:]},\n    transform=transform_model,\n    session=create_session,\n    baseline=lambda onx: InferenceSession(\n        onx.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n    ),\n    params=optim_params,\n    verbose=True,\n    number=script_args.number,\n    repeat=script_args.repeat,\n    warmup=script_args.warmup,\n    sleep=script_args.sleep,\n    n_tries=script_args.tries,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataFrame(res)\ndf.to_csv(\"plot_optim_tree_ensemble.csv\", index=False)\ndf.to_excel(\"plot_optim_tree_ensemble.xlsx\", index=False)\nprint(df.columns)\nprint(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sorting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "small_df = df.drop(\n    [\n        \"min_exec\",\n        \"max_exec\",\n        \"repeat\",\n        \"number\",\n        \"context_size\",\n        \"n_exp_name\",\n    ],\n    axis=1,\n).sort_values(\"average\")\nprint(small_df.head(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worst\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(small_df.tail(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfi = df[[\"short_name\", \"average\"]].sort_values(\"average\").reset_index(drop=True)\nbaseline = dfi[dfi.short_name.str.contains(\"baseline\")]\nnot_baseline = dfi[~dfi.short_name.str.contains(\"baseline\")].reset_index(drop=True)\nif not_baseline.shape[0] > 50:\n    not_baseline = not_baseline[:50]\nmerged = concat([baseline, not_baseline], axis=0)\nmerged = merged.sort_values(\"average\").reset_index(drop=True).set_index(\"short_name\")\nskeys = \",\".join(optim_params.keys())\n\nfig, ax = plt.subplots(1, 1, figsize=(10, merged.shape[0] / 4))\nmerged.plot.barh(\n    ax=ax, title=f\"TreeEnsemble tuning, n_tries={script_args.tries}\\n{skeys}\"\n)\nb = df.loc[0, \"average\"]\nax.plot([b, b], [0, df.shape[0]], \"r--\")\nax.set_xlim(\n    [\n        (df[\"min_exec\"].min() + df[\"average\"].min()) / 2,\n        (df[\"max_exec\"].max() + df[\"average\"].max()) / 2,\n    ]\n)\nax.set_xscale(\"log\")\n\nfig.tight_layout()\nfig.savefig(\"plot_optim_tree_ensemble.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}