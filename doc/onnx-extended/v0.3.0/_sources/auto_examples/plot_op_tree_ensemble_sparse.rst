
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_op_tree_ensemble_sparse.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_op_tree_ensemble_sparse.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_op_tree_ensemble_sparse.py:


.. _l-plot-optim-tree-ensemble-sparse:

TreeEnsemble, dense, and sparse
===============================

The example benchmarks the sparse implementation for TreeEnsemble.
The default set of optimized parameters is very short and is meant to be executed
fast. Many more parameters can be tried.

::

    python plot_op_tree_ensemble_sparse --scenario=LONG

To change the training parameters:

::

    python plot_op_tree_ensemble_sparse.py
        --n_trees=100
        --max_depth=10
        --n_features=50
        --sparsity=0.9
        --batch_size=100000
    
Another example with a full list of parameters:

    python plot_op_tree_ensemble_sparse.py
        --n_trees=100
        --max_depth=10
        --n_features=50
        --batch_size=100000
        --sparsity=0.9
        --tries=3
        --scenario=CUSTOM
        --parallel_tree=80,40
        --parallel_tree_N=128,64
        --parallel_N=50,25
        --batch_size_tree=1,2
        --batch_size_rows=1,2
        --use_node3=0

Another example:

::

    python plot_op_tree_ensemble_sparse.py
        --n_trees=100 --n_features=10 --batch_size=10000 --max_depth=8 -s SHORT        

.. GENERATED FROM PYTHON SOURCE LINES 50-103

.. code-block:: Python


    import logging
    import os
    import timeit
    from typing import Tuple
    import numpy
    import onnx
    from onnx import ModelProto, TensorProto
    from onnx.helper import make_graph, make_model, make_tensor_value_info
    from pandas import DataFrame, concat
    from sklearn.datasets import make_regression
    from sklearn.ensemble import RandomForestRegressor
    from skl2onnx import to_onnx
    from onnxruntime import InferenceSession, SessionOptions
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    from onnx_extended.ortops.optim.cpu import get_ort_ext_libs
    from onnx_extended.ortops.optim.optimize import (
        change_onnx_operator_domain,
        get_node_attribute,
        optimize_model,
    )
    from onnx_extended.tools.onnx_nodes import multiply_tree
    from onnx_extended.validation.cpu._validation import dense_to_sparse_struct
    from onnx_extended.plotting.benchmark import hhistograms
    from onnx_extended.args import get_parsed_args
    from onnx_extended.ext_test_case import unit_test_going

    logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)

    script_args = get_parsed_args(
        "plot_op_tree_ensemble_sparse",
        description=__doc__,
        scenarios={
            "SHORT": "short optimization (default)",
            "LONG": "test more options",
            "CUSTOM": "use values specified by the command line",
        },
        sparsity=(0.99, "input sparsity"),
        n_features=(2 if unit_test_going() else 500, "number of features to generate"),
        n_trees=(3 if unit_test_going() else 10, "number of trees to train"),
        max_depth=(2 if unit_test_going() else 10, "max_depth"),
        batch_size=(1000 if unit_test_going() else 1000, "batch size"),
        parallel_tree=("80,160,40", "values to try for parallel_tree"),
        parallel_tree_N=("256,128,64", "values to try for parallel_tree_N"),
        parallel_N=("100,50,25", "values to try for parallel_N"),
        batch_size_tree=("2,4,8", "values to try for batch_size_tree"),
        batch_size_rows=("2,4,8", "values to try for batch_size_rows"),
        use_node3=("0,1", "values to try for use_node3"),
        expose="",
        n_jobs=("-1", "number of jobs to train the RandomForestRegressor"),
    )









.. GENERATED FROM PYTHON SOURCE LINES 104-106

Training a model
++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 106-170

.. code-block:: Python



    def train_model(
        batch_size: int, n_features: int, n_trees: int, max_depth: int, sparsity: float
    ) -> Tuple[str, numpy.ndarray, numpy.ndarray]:
        filename = (
            f"plot_op_tree_ensemble_sparse-f{n_features}-{n_trees}-"
            f"d{max_depth}-s{sparsity}.onnx"
        )
        if not os.path.exists(filename):
            X, y = make_regression(
                batch_size + max(batch_size, 2 ** (max_depth + 1)),
                n_features=n_features,
                n_targets=1,
            )
            mask = numpy.random.rand(*X.shape) <= sparsity
            X[mask] = 0
            X, y = X.astype(numpy.float32), y.astype(numpy.float32)

            print(f"Training to get {filename!r} with X.shape={X.shape}")
            # To be faster, we train only 1 tree.
            model = RandomForestRegressor(
                1, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)
            )
            model.fit(X[:-batch_size], y[:-batch_size])
            onx = to_onnx(model, X[:1], target_opset={"": 18, "ai.onnx.ml": 3})

            # And wd multiply the trees.
            node = multiply_tree(onx.graph.node[0], n_trees)
            onx = make_model(
                make_graph([node], onx.graph.name, onx.graph.input, onx.graph.output),
                domain=onx.domain,
                opset_imports=onx.opset_import,
                ir_version=onx.ir_version,
            )

            with open(filename, "wb") as f:
                f.write(onx.SerializeToString())
        else:
            X, y = make_regression(batch_size, n_features=n_features, n_targets=1)
            mask = numpy.random.rand(*X.shape) <= sparsity
            X[mask] = 0
            X, y = X.astype(numpy.float32), y.astype(numpy.float32)
        Xb, yb = X[-batch_size:].copy(), y[-batch_size:].copy()
        return filename, Xb, yb


    def measure_sparsity(x):
        f = x.flatten()
        return float((f == 0).astype(numpy.int64).sum()) / float(x.size)


    batch_size = script_args.batch_size
    n_features = script_args.n_features
    n_trees = script_args.n_trees
    max_depth = script_args.max_depth
    sparsity = script_args.sparsity

    print(f"batch_size={batch_size}")
    print(f"n_features={n_features}")
    print(f"n_trees={n_trees}")
    print(f"max_depth={max_depth}")
    print(f"sparsity={sparsity}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    batch_size=1000
    n_features=500
    n_trees=10
    max_depth=10
    sparsity=0.99




.. GENERATED FROM PYTHON SOURCE LINES 171-172

training

.. GENERATED FROM PYTHON SOURCE LINES 172-179

.. code-block:: Python


    filename, Xb, yb = train_model(batch_size, n_features, n_trees, max_depth, sparsity)

    print(f"Xb.shape={Xb.shape}")
    print(f"yb.shape={yb.shape}")
    print(f"measured sparsity={measure_sparsity(Xb)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training to get 'plot_op_tree_ensemble_sparse-f500-10-d10-s0.99.onnx' with X.shape=(3048, 500)
    [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 30 concurrent workers.
    building tree 1 of 1
    Xb.shape=(1000, 500)
    yb.shape=(1000,)
    measured sparsity=0.989796




.. GENERATED FROM PYTHON SOURCE LINES 180-187

Rewrite the onnx file to use a different kernel
+++++++++++++++++++++++++++++++++++++++++++++++

The custom kernel is mapped to a custom operator with the same name
the attributes and domain = `"onnx_extended.ortops.optim.cpu"`.
We call a function to do that replacement.
First the current model.

.. GENERATED FROM PYTHON SOURCE LINES 187-192

.. code-block:: Python


    with open(filename, "rb") as f:
        onx = onnx.load(f)
    print(onnx_simple_text_plot(onx))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=18
    input: name='X' type=dtype('float32') shape=['', 500]
    TreeEnsembleRegressor(X, n_targets=1, nodes_falsenodeids=750:[72,63,50...74,0,0], nodes_featureids=750:[157,470,30...0,0,0], nodes_hitrates=750:[1.0,1.0...1.0,1.0], nodes_missing_value_tracks_true=750:[0,0,0...0,0,0], nodes_modes=750:[b'BRANCH_LEQ',b'BRANCH_LEQ'...b'LEAF',b'LEAF'], nodes_nodeids=750:[0,1,2...72,73,74], nodes_treeids=750:[0,0,0...9,9,9], nodes_truenodeids=750:[1,2,3...73,0,0], nodes_values=750:[0.688653290271759,1.3292285203933716...0.0,0.0], post_transform=b'NONE', target_ids=380:[0,0,0...0,0,0], target_nodeids=380:[5,9,10...71,73,74], target_treeids=380:[0,0,0...9,9,9], target_weights=380:[465.1214599609375,-425.57977294921875...-576.3978271484375,-598.8986206054688]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 193-194

And then the modified model.

.. GENERATED FROM PYTHON SOURCE LINES 194-235

.. code-block:: Python



    def transform_model(model, use_sparse=False, **kwargs):
        onx = ModelProto()
        onx.ParseFromString(model.SerializeToString())
        att = get_node_attribute(onx.graph.node[0], "nodes_modes")
        modes = ",".join(map(lambda s: s.decode("ascii"), att.strings)).replace(
            "BRANCH_", ""
        )
        if use_sparse and "new_op_type" not in kwargs:
            kwargs["new_op_type"] = "TreeEnsembleRegressorSparse"
        if use_sparse:
            # with sparse tensor, missing value means 0
            att = get_node_attribute(onx.graph.node[0], "nodes_values")
            thresholds = numpy.array(att.floats, dtype=numpy.float32)
            missing_true = (thresholds >= 0).astype(numpy.int64)
            kwargs["nodes_missing_value_tracks_true"] = missing_true
        new_onx = change_onnx_operator_domain(
            onx,
            op_type="TreeEnsembleRegressor",
            op_domain="ai.onnx.ml",
            new_op_domain="onnx_extended.ortops.optim.cpu",
            nodes_modes=modes,
            **kwargs,
        )
        if use_sparse:
            del new_onx.graph.input[:]
            new_onx.graph.input.append(
                make_tensor_value_info("X", TensorProto.FLOAT, (None,))
            )
        return new_onx


    print("Tranform model to add a custom node.")
    onx_modified = transform_model(onx)
    print(f"Save into {filename + 'modified.onnx'!r}.")
    with open(filename + "modified.onnx", "wb") as f:
        f.write(onx_modified.SerializeToString())
    print("done.")
    print(onnx_simple_text_plot(onx_modified))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Tranform model to add a custom node.
    Save into 'plot_op_tree_ensemble_sparse-f500-10-d10-s0.99.onnxmodified.onnx'.
    done.
    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=18
    opset: domain='onnx_extended.ortops.optim.cpu' version=1
    input: name='X' type=dtype('float32') shape=['', 500]
    TreeEnsembleRegressor[onnx_extended.ortops.optim.cpu](X, nodes_modes=b'LEQ,LEQ,LEQ,LEQ,LEQ,LEAF,LEQ,LEQ,LEQ,L...LEAF,LEAF', n_targets=1, nodes_falsenodeids=750:[72,63,50...74,0,0], nodes_featureids=750:[157,470,30...0,0,0], nodes_hitrates=750:[1.0,1.0...1.0,1.0], nodes_missing_value_tracks_true=750:[0,0,0...0,0,0], nodes_nodeids=750:[0,1,2...72,73,74], nodes_treeids=750:[0,0,0...9,9,9], nodes_truenodeids=750:[1,2,3...73,0,0], nodes_values=750:[0.688653290271759,1.3292285203933716...0.0,0.0], post_transform=b'NONE', target_ids=380:[0,0,0...0,0,0], target_nodeids=380:[5,9,10...71,73,74], target_treeids=380:[0,0,0...9,9,9], target_weights=380:[465.1214599609375,-425.57977294921875...-576.3978271484375,-598.8986206054688]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 236-237

Same with sparse.

.. GENERATED FROM PYTHON SOURCE LINES 237-247

.. code-block:: Python



    print("Same transformation but with sparse.")
    onx_modified_sparse = transform_model(onx, use_sparse=True)
    print(f"Save into {filename + 'modified.sparse.onnx'!r}.")
    with open(filename + "modified.sparse.onnx", "wb") as f:
        f.write(onx_modified_sparse.SerializeToString())
    print("done.")
    print(onnx_simple_text_plot(onx_modified_sparse))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Same transformation but with sparse.
    Save into 'plot_op_tree_ensemble_sparse-f500-10-d10-s0.99.onnxmodified.sparse.onnx'.
    done.
    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=18
    opset: domain='onnx_extended.ortops.optim.cpu' version=1
    input: name='X' type=dtype('float32') shape=['']
    TreeEnsembleRegressorSparse[onnx_extended.ortops.optim.cpu](X, nodes_missing_value_tracks_true=750:[1,1,1...1,1,1], nodes_modes=b'LEQ,LEQ,LEQ,LEQ,LEQ,LEAF,LEQ,LEQ,LEQ,L...LEAF,LEAF', n_targets=1, nodes_falsenodeids=750:[72,63,50...74,0,0], nodes_featureids=750:[157,470,30...0,0,0], nodes_hitrates=750:[1.0,1.0...1.0,1.0], nodes_nodeids=750:[0,1,2...72,73,74], nodes_treeids=750:[0,0,0...9,9,9], nodes_truenodeids=750:[1,2,3...73,0,0], nodes_values=750:[0.688653290271759,1.3292285203933716...0.0,0.0], post_transform=b'NONE', target_ids=380:[0,0,0...0,0,0], target_nodeids=380:[5,9,10...71,73,74], target_treeids=380:[0,0,0...9,9,9], target_weights=380:[465.1214599609375,-425.57977294921875...-576.3978271484375,-598.8986206054688]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 248-250

Comparing onnxruntime and the custom kernel
+++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 250-283

.. code-block:: Python


    print(f"Loading {filename!r}")
    sess_ort = InferenceSession(filename, providers=["CPUExecutionProvider"])

    r = get_ort_ext_libs()
    print(f"Creating SessionOptions with {r!r}")
    opts = SessionOptions()
    if r is not None:
        opts.register_custom_ops_library(r[0])

    print(f"Loading modified {filename!r}")
    sess_cus = InferenceSession(
        onx_modified.SerializeToString(), opts, providers=["CPUExecutionProvider"]
    )

    print(f"Loading modified sparse {filename!r}")
    sess_cus_sparse = InferenceSession(
        onx_modified_sparse.SerializeToString(), opts, providers=["CPUExecutionProvider"]
    )


    print(f"Running once with shape {Xb.shape}.")
    base = sess_ort.run(None, {"X": Xb})[0]

    print(f"Running modified with shape {Xb.shape}.")
    got = sess_cus.run(None, {"X": Xb})[0]
    print("done.")

    Xb_sp = dense_to_sparse_struct(Xb)
    print(f"Running modified sparse with shape {Xb_sp.shape}.")
    got_sparse = sess_cus_sparse.run(None, {"X": Xb_sp})[0]
    print("done.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading 'plot_op_tree_ensemble_sparse-f500-10-d10-s0.99.onnx'
    Creating SessionOptions with ['/home/onyxia/work/github/onnx-extended/onnx_extended/ortops/optim/cpu/libortops_optim_cpu.so']
    Loading modified 'plot_op_tree_ensemble_sparse-f500-10-d10-s0.99.onnx'
    Loading modified sparse 'plot_op_tree_ensemble_sparse-f500-10-d10-s0.99.onnx'
    Running once with shape (1000, 500).
    Running modified with shape (1000, 500).
    done.
    Running modified sparse with shape (10260,).
    done.




.. GENERATED FROM PYTHON SOURCE LINES 284-285

Discrepancies?

.. GENERATED FROM PYTHON SOURCE LINES 285-292

.. code-block:: Python


    diff = numpy.abs(base - got).max()
    print(f"Discrepancies: {diff}")

    diff = numpy.abs(base - got_sparse).max()
    print(f"Discrepancies sparse: {diff}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Discrepancies: 0.00048828125
    Discrepancies sparse: 0.00048828125




.. GENERATED FROM PYTHON SOURCE LINES 293-297

Simple verification
+++++++++++++++++++

Baseline with onnxruntime.

.. GENERATED FROM PYTHON SOURCE LINES 297-300

.. code-block:: Python

    t1 = timeit.timeit(lambda: sess_ort.run(None, {"X": Xb}), number=50)
    print(f"baseline: {t1}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    baseline: 0.07653061486780643




.. GENERATED FROM PYTHON SOURCE LINES 301-302

The custom implementation.

.. GENERATED FROM PYTHON SOURCE LINES 302-305

.. code-block:: Python

    t2 = timeit.timeit(lambda: sess_cus.run(None, {"X": Xb}), number=50)
    print(f"new time: {t2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    new time: 0.11359465308487415




.. GENERATED FROM PYTHON SOURCE LINES 306-307

The custom sparse implementation.

.. GENERATED FROM PYTHON SOURCE LINES 307-310

.. code-block:: Python

    t3 = timeit.timeit(lambda: sess_cus_sparse.run(None, {"X": Xb_sp}), number=50)
    print(f"new time sparse: {t3}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    new time sparse: 0.004443535581231117




.. GENERATED FROM PYTHON SOURCE LINES 311-320

Time for comparison
+++++++++++++++++++

The custom kernel supports the same attributes as *TreeEnsembleRegressor*
plus new ones to tune the parallelization. They can be seen in
`tree_ensemble.cc <https://github.com/sdpython/onnx-extended/
blob/main/onnx_extended/ortops/optim/cpu/tree_ensemble.cc#L102>`_.
Let's try out many possibilities.
The default values are the first ones.

.. GENERATED FROM PYTHON SOURCE LINES 320-368

.. code-block:: Python


    if unit_test_going():
        optim_params = dict(
            parallel_tree=[40],  # default is 80
            parallel_tree_N=[128],  # default is 128
            parallel_N=[50, 25],  # default is 50
            batch_size_tree=[1],  # default is 1
            batch_size_rows=[1],  # default is 1
            use_node3=[0],  # default is 0
        )
    elif script_args.scenario in (None, "SHORT"):
        optim_params = dict(
            parallel_tree=[80, 40],  # default is 80
            parallel_tree_N=[128, 64],  # default is 128
            parallel_N=[50, 25],  # default is 50
            batch_size_tree=[1],  # default is 1
            batch_size_rows=[1],  # default is 1
            use_node3=[0],  # default is 0
        )
    elif script_args.scenario == "LONG":
        optim_params = dict(
            parallel_tree=[80, 160, 40],
            parallel_tree_N=[256, 128, 64],
            parallel_N=[100, 50, 25],
            batch_size_tree=[1, 2, 4, 8],
            batch_size_rows=[1, 2, 4, 8],
            use_node3=[0, 1],
        )
    elif script_args.scenario == "CUSTOM":
        optim_params = dict(
            parallel_tree=list(int(i) for i in script_args.parallel_tree.split(",")),
            parallel_tree_N=list(int(i) for i in script_args.parallel_tree_N.split(",")),
            parallel_N=list(int(i) for i in script_args.parallel_N.split(",")),
            batch_size_tree=list(int(i) for i in script_args.batch_size_tree.split(",")),
            batch_size_rows=list(int(i) for i in script_args.batch_size_rows.split(",")),
            use_node3=list(int(i) for i in script_args.use_node3.split(",")),
        )
    else:
        raise ValueError(
            f"Unknown scenario {script_args.scenario!r}, use --help to get them."
        )

    cmds = []
    for att, value in optim_params.items():
        cmds.append(f"--{att}={','.join(map(str, value))}")
    print("Full list of optimization parameters:")
    print(" ".join(cmds))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Full list of optimization parameters:
    --parallel_tree=80,40 --parallel_tree_N=128,64 --parallel_N=50,25 --batch_size_tree=1 --batch_size_rows=1 --use_node3=0




.. GENERATED FROM PYTHON SOURCE LINES 369-370

Then the optimization for dense

.. GENERATED FROM PYTHON SOURCE LINES 370-400

.. code-block:: Python



    def create_session(onx):
        opts = SessionOptions()
        r = get_ort_ext_libs()
        if r is None:
            raise RuntimeError("No custom implementation available.")
        opts.register_custom_ops_library(r[0])
        return InferenceSession(
            onx.SerializeToString(), opts, providers=["CPUExecutionProvider"]
        )


    res = optimize_model(
        onx,
        feeds={"X": Xb},
        transform=transform_model,
        session=create_session,
        baseline=lambda onx: InferenceSession(
            onx.SerializeToString(), providers=["CPUExecutionProvider"]
        ),
        params=optim_params,
        verbose=True,
        number=script_args.number,
        repeat=script_args.repeat,
        warmup=script_args.warmup,
        sleep=script_args.sleep,
        n_tries=script_args.tries,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:   0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:   6%|▋         | 1/16 [00:00<00:04,  3.51it/s]    i=2/16 TRY=0 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.01x:   6%|▋         | 1/16 [00:00<00:04,  3.51it/s]    i=2/16 TRY=0 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.01x:  12%|█▎        | 2/16 [00:00<00:02,  4.93it/s]    i=3/16 TRY=0 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.01x:  12%|█▎        | 2/16 [00:00<00:02,  4.93it/s]     i=3/16 TRY=0 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.01x:  19%|█▉        | 3/16 [00:00<00:02,  5.68it/s]    i=4/16 TRY=0 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.01x:  19%|█▉        | 3/16 [00:00<00:02,  5.68it/s]    i=4/16 TRY=0 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.01x:  25%|██▌       | 4/16 [00:00<00:01,  6.14it/s]    i=5/16 TRY=0 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  25%|██▌       | 4/16 [00:00<00:01,  6.14it/s]    i=5/16 TRY=0 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  31%|███▏      | 5/16 [00:00<00:01,  6.33it/s]    i=6/16 TRY=0 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  31%|███▏      | 5/16 [00:00<00:01,  6.33it/s]    i=6/16 TRY=0 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  38%|███▊      | 6/16 [00:01<00:01,  6.54it/s]    i=7/16 TRY=0 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  38%|███▊      | 6/16 [00:01<00:01,  6.54it/s]     i=7/16 TRY=0 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  44%|████▍     | 7/16 [00:01<00:01,  6.57it/s]    i=8/16 TRY=0 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  44%|████▍     | 7/16 [00:01<00:01,  6.57it/s]    i=8/16 TRY=0 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.37x:  50%|█████     | 8/16 [00:01<00:01,  6.60it/s]    i=9/16 TRY=1 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  50%|█████     | 8/16 [00:01<00:01,  6.60it/s]    i=9/16 TRY=1 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  56%|█████▋    | 9/16 [00:01<00:01,  6.74it/s]    i=10/16 TRY=1 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  56%|█████▋    | 9/16 [00:01<00:01,  6.74it/s]    i=10/16 TRY=1 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  62%|██████▎   | 10/16 [00:01<00:00,  6.80it/s]    i=11/16 TRY=1 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  62%|██████▎   | 10/16 [00:01<00:00,  6.80it/s]     i=11/16 TRY=1 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  69%|██████▉   | 11/16 [00:01<00:00,  6.85it/s]    i=12/16 TRY=1 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  69%|██████▉   | 11/16 [00:01<00:00,  6.85it/s]    i=12/16 TRY=1 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  75%|███████▌  | 12/16 [00:01<00:00,  6.85it/s]    i=13/16 TRY=1 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  75%|███████▌  | 12/16 [00:01<00:00,  6.85it/s]    i=13/16 TRY=1 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  81%|████████▏ | 13/16 [00:02<00:00,  6.86it/s]    i=14/16 TRY=1 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  81%|████████▏ | 13/16 [00:02<00:00,  6.86it/s]    i=14/16 TRY=1 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  88%|████████▊ | 14/16 [00:02<00:00,  6.80it/s]    i=15/16 TRY=1 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  88%|████████▊ | 14/16 [00:02<00:00,  6.80it/s]     i=15/16 TRY=1 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  94%|█████████▍| 15/16 [00:02<00:00,  6.85it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x:  94%|█████████▍| 15/16 [00:02<00:00,  6.85it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x: 100%|██████████| 16/16 [00:02<00:00,  6.89it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=3.11x: 100%|██████████| 16/16 [00:02<00:00,  6.48it/s]




.. GENERATED FROM PYTHON SOURCE LINES 401-402

Then the optimization for sparse

.. GENERATED FROM PYTHON SOURCE LINES 402-418

.. code-block:: Python


    res_sparse = optimize_model(
        onx,
        feeds={"X": Xb_sp},
        transform=lambda *args, **kwargs: transform_model(*args, use_sparse=True, **kwargs),
        session=create_session,
        params=optim_params,
        verbose=True,
        number=script_args.number,
        repeat=script_args.repeat,
        warmup=script_args.warmup,
        sleep=script_args.sleep,
        n_tries=script_args.tries,
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:   0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:   6%|▋         | 1/16 [00:00<00:02,  6.87it/s]    i=2/16 TRY=0 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:   6%|▋         | 1/16 [00:00<00:02,  6.87it/s]    i=2/16 TRY=0 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  12%|█▎        | 2/16 [00:00<00:02,  6.70it/s]    i=3/16 TRY=0 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  12%|█▎        | 2/16 [00:00<00:02,  6.70it/s]     i=3/16 TRY=0 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  19%|█▉        | 3/16 [00:00<00:01,  6.66it/s]    i=4/16 TRY=0 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  19%|█▉        | 3/16 [00:00<00:01,  6.66it/s]    i=4/16 TRY=0 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  25%|██▌       | 4/16 [00:00<00:01,  6.68it/s]    i=5/16 TRY=0 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  25%|██▌       | 4/16 [00:00<00:01,  6.68it/s]    i=5/16 TRY=0 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  31%|███▏      | 5/16 [00:00<00:01,  6.67it/s]    i=6/16 TRY=0 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  31%|███▏      | 5/16 [00:00<00:01,  6.67it/s]    i=6/16 TRY=0 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  38%|███▊      | 6/16 [00:00<00:01,  6.60it/s]    i=7/16 TRY=0 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  38%|███▊      | 6/16 [00:00<00:01,  6.60it/s]     i=7/16 TRY=0 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  44%|████▍     | 7/16 [00:01<00:01,  6.45it/s]    i=8/16 TRY=0 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  44%|████▍     | 7/16 [00:01<00:01,  6.45it/s]    i=8/16 TRY=0 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  50%|█████     | 8/16 [00:01<00:01,  6.56it/s]    i=9/16 TRY=1 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  50%|█████     | 8/16 [00:01<00:01,  6.56it/s]    i=9/16 TRY=1 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  56%|█████▋    | 9/16 [00:01<00:01,  6.46it/s]    i=10/16 TRY=1 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  56%|█████▋    | 9/16 [00:01<00:01,  6.46it/s]    i=10/16 TRY=1 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  62%|██████▎   | 10/16 [00:01<00:00,  6.57it/s]    i=11/16 TRY=1 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  62%|██████▎   | 10/16 [00:01<00:00,  6.57it/s]     i=11/16 TRY=1 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  69%|██████▉   | 11/16 [00:01<00:00,  6.64it/s]    i=12/16 TRY=1 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  69%|██████▉   | 11/16 [00:01<00:00,  6.64it/s]    i=12/16 TRY=1 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  75%|███████▌  | 12/16 [00:01<00:00,  6.32it/s]    i=13/16 TRY=1 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  75%|███████▌  | 12/16 [00:01<00:00,  6.32it/s]    i=13/16 TRY=1 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  81%|████████▏ | 13/16 [00:01<00:00,  6.40it/s]    i=14/16 TRY=1 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  81%|████████▏ | 13/16 [00:01<00:00,  6.40it/s]    i=14/16 TRY=1 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  88%|████████▊ | 14/16 [00:02<00:00,  6.41it/s]    i=15/16 TRY=1 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  88%|████████▊ | 14/16 [00:02<00:00,  6.41it/s]     i=15/16 TRY=1 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0:  94%|█████████▍| 15/16 [00:02<00:00,  6.15it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0:  94%|█████████▍| 15/16 [00:02<00:00,  6.15it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0: 100%|██████████| 16/16 [00:02<00:00,  6.09it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0: 100%|██████████| 16/16 [00:02<00:00,  6.42it/s]




.. GENERATED FROM PYTHON SOURCE LINES 419-420

And the results.

.. GENERATED FROM PYTHON SOURCE LINES 420-431

.. code-block:: Python


    df_dense = DataFrame(res)
    df_dense["input"] = "dense"
    df_sparse = DataFrame(res_sparse)
    df_sparse["input"] = "sparse"
    df = concat([df_dense, df_sparse], axis=0)
    df.to_csv("plot_op_tree_ensemble_sparse.csv", index=False)
    df.to_excel("plot_op_tree_ensemble_sparse.xlsx", index=False)
    print(df.columns)
    print(df.head(5))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Index(['average', 'deviation', 'min_exec', 'max_exec', 'repeat', 'number',
           'ttime', 'context_size', 'warmup_time', 'n_exp', 'n_exp_name',
           'short_name', 'TRY', 'name', 'parallel_tree', 'parallel_tree_N',
           'parallel_N', 'batch_size_tree', 'batch_size_rows', 'use_node3',
           'input'],
          dtype='object')
        average  deviation  min_exec  max_exec  repeat  number  ...  parallel_tree_N  parallel_N  batch_size_tree  batch_size_rows use_node3  input
    0  0.000135   0.000020  0.000105  0.000162      10      10  ...              NaN         NaN              NaN              NaN       NaN  dense
    1  0.000067   0.000007  0.000059  0.000081      10      10  ...            128.0        50.0              1.0              1.0       0.0  dense
    2  0.000069   0.000013  0.000049  0.000086      10      10  ...            128.0        25.0              1.0              1.0       0.0  dense
    3  0.000071   0.000010  0.000048  0.000085      10      10  ...             64.0        50.0              1.0              1.0       0.0  dense
    4  0.000057   0.000007  0.000044  0.000064      10      10  ...             64.0        25.0              1.0              1.0       0.0  dense

    [5 rows x 21 columns]




.. GENERATED FROM PYTHON SOURCE LINES 432-434

Sorting
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 434-449

.. code-block:: Python


    small_df = df.drop(
        [
            "min_exec",
            "max_exec",
            "repeat",
            "number",
            "context_size",
            "n_exp_name",
        ],
        axis=1,
    ).sort_values("average")
    print(small_df.head(n=10))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

         average  deviation     ttime  warmup_time  n_exp         short_name  ...  parallel_tree_N parallel_N  batch_size_tree  batch_size_rows  use_node3  input
    8   0.000044   0.000005  0.000435     0.000795      7   0,40,64,25,1,1,0  ...             64.0       25.0              1.0              1.0        0.0  dense
    4   0.000057   0.000007  0.000571     0.000687      3   0,80,64,25,1,1,0  ...             64.0       25.0              1.0              1.0        0.0  dense
    9   0.000058   0.000012  0.000583     0.000808      8  1,80,128,50,1,1,0  ...            128.0       50.0              1.0              1.0        0.0  dense
    6   0.000060   0.000006  0.000604     0.000910      5  0,40,128,25,1,1,0  ...            128.0       25.0              1.0              1.0        0.0  dense
    7   0.000061   0.000014  0.000606     0.000870      6   0,40,64,50,1,1,0  ...             64.0       50.0              1.0              1.0        0.0  dense
    11  0.000061   0.000003  0.000610     0.000772     10   1,80,64,50,1,1,0  ...             64.0       50.0              1.0              1.0        0.0  dense
    1   0.000067   0.000007  0.000672     0.000895      0  0,80,128,50,1,1,0  ...            128.0       50.0              1.0              1.0        0.0  dense
    12  0.000067   0.000010  0.000674     0.000795     11   1,80,64,25,1,1,0  ...             64.0       25.0              1.0              1.0        0.0  dense
    13  0.000068   0.000009  0.000675     0.000761     12  1,40,128,50,1,1,0  ...            128.0       50.0              1.0              1.0        0.0  dense
    2   0.000069   0.000013  0.000691     0.000871      1  0,80,128,25,1,1,0  ...            128.0       25.0              1.0              1.0        0.0  dense

    [10 rows x 15 columns]




.. GENERATED FROM PYTHON SOURCE LINES 450-452

Worst
+++++

.. GENERATED FROM PYTHON SOURCE LINES 452-456

.. code-block:: Python


    print(small_df.tail(n=10))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

         average  deviation     ttime  warmup_time  n_exp         short_name  ...  parallel_tree_N parallel_N  batch_size_tree  batch_size_rows  use_node3   input
    2   0.000132   0.000039  0.001318     0.001476      2   0,80,64,50,1,1,0  ...             64.0       50.0              1.0              1.0        0.0  sparse
    3   0.000132   0.000032  0.001321     0.001577      3   0,80,64,25,1,1,0  ...             64.0       25.0              1.0              1.0        0.0  sparse
    6   0.000133   0.000048  0.001327     0.001717      6   0,40,64,50,1,1,0  ...             64.0       50.0              1.0              1.0        0.0  sparse
    0   0.000135   0.000020  0.001355     0.001262      0         0,baseline  ...              NaN        NaN              NaN              NaN        NaN   dense
    1   0.000139   0.000031  0.001393     0.001477      1  0,80,128,25,1,1,0  ...            128.0       25.0              1.0              1.0        0.0  sparse
    10  0.000141   0.000154  0.001410     0.001431     10   1,80,64,50,1,1,0  ...             64.0       50.0              1.0              1.0        0.0  sparse
    4   0.000144   0.000043  0.001441     0.001662      4  0,40,128,50,1,1,0  ...            128.0       50.0              1.0              1.0        0.0  sparse
    5   0.000145   0.000058  0.001453     0.001673      5  0,40,128,25,1,1,0  ...            128.0       25.0              1.0              1.0        0.0  sparse
    11  0.000174   0.000113  0.001738     0.001588     11   1,80,64,25,1,1,0  ...             64.0       25.0              1.0              1.0        0.0  sparse
    14  0.000185   0.000263  0.001853     0.001699     14   1,40,64,50,1,1,0  ...             64.0       50.0              1.0              1.0        0.0  sparse

    [10 rows x 15 columns]




.. GENERATED FROM PYTHON SOURCE LINES 457-459

Plot
++++

.. GENERATED FROM PYTHON SOURCE LINES 459-465

.. code-block:: Python


    skeys = ",".join(optim_params.keys())
    title = f"TreeEnsemble tuning, n_tries={script_args.tries}\n{skeys}\nlower is better"
    ax = hhistograms(df, title=title, keys=("input", "name"))
    fig = ax.get_figure()
    fig.savefig("plot_op_tree_ensemble_sparse.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_op_tree_ensemble_sparse_001.png
   :alt: TreeEnsemble tuning, n_tries=2 parallel_tree,parallel_tree_N,parallel_N,batch_size_tree,batch_size_rows,use_node3 lower is better
   :srcset: /auto_examples/images/sphx_glr_plot_op_tree_ensemble_sparse_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 6.507 seconds)


.. _sphx_glr_download_auto_examples_plot_op_tree_ensemble_sparse.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_op_tree_ensemble_sparse.ipynb <plot_op_tree_ensemble_sparse.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_op_tree_ensemble_sparse.py <plot_op_tree_ensemble_sparse.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
