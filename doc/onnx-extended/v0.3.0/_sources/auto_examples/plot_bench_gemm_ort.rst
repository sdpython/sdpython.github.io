
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_bench_gemm_ort.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_bench_gemm_ort.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_bench_gemm_ort.py:


.. _l-example-gemm-ort-f8:


Measuring performance about Gemm with onnxruntime
=================================================

The benchmark measures the performance of Gemm for different
types and configuration. That includes a custom operator
only available on CUDA calling function :epkg:`cublasLtMatmul`.
This function offers many options.

.. GENERATED FROM PYTHON SOURCE LINES 13-99

.. code-block:: Python


    import pprint
    import platform
    from itertools import product
    import numpy
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    from pandas import DataFrame, pivot_table
    from onnx import TensorProto
    from onnx.helper import (
        make_model,
        make_node,
        make_graph,
        make_tensor_value_info,
        make_opsetid,
    )
    from onnx.checker import check_model
    from onnx.numpy_helper import from_array
    from onnx.reference import ReferenceEvaluator
    from onnxruntime import InferenceSession, SessionOptions, get_available_providers
    from onnxruntime.capi._pybind_state import (
        OrtValue as C_OrtValue,
        OrtDevice as C_OrtDevice,
    )
    from onnxruntime.capi.onnxruntime_pybind11_state import (
        Fail,
        NotImplemented,
        InvalidGraph,
        InvalidArgument,
    )

    try:
        from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    except ImportError:
        onnx_simple_text_plot = str
    try:
        from onnx_extended.reference import CReferenceEvaluator
    except ImportError:
        CReferenceEvaluator = ReferenceEvaluator
    from onnx_extended.args import get_parsed_args
    from onnx_extended.ext_test_case import unit_test_going, measure_time

    try:
        from onnx_extended.validation.cuda.cuda_example_py import get_device_prop
        from onnx_extended.ortops.tutorial.cuda import get_ort_ext_libs

        has_cuda = True
    except ImportError:

        def get_device_prop():
            return {"name": "CPU"}

        def get_ort_ext_libs():
            return None

        has_cuda = False

    default_dims = (
        "32,32,32;64,64,64;128,128,128;256,256,256;"
        "400,400,400;512,512,512;1024,1024,1024"
    )
    if has_cuda:
        prop = get_device_prop()
        if prop.get("major", 0) >= 7:
            default_dims += ";2048,2048,2048;4096,4096,4096"
        if prop.get("major", 0) >= 9:
            default_dims += ";16384,16384,16384"


    script_args = get_parsed_args(
        "plot_bench_gemm_ort",
        description=__doc__,
        dims=(
            "32,32,32;64,64,64" if unit_test_going() else default_dims,
            "square matrix dimensions to try, comma separated values",
        ),
        types=(
            "FLOAT" if unit_test_going() else "FLOAT8E4M3FN,FLOAT,FLOAT16,BFLOAT16",
            "element type to teest",
        ),
        number=2 if unit_test_going() else 4,
        repeat=2 if unit_test_going() else 10,
        warmup=2 if unit_test_going() else 5,
        expose="repeat,number,warmup",
    )








.. GENERATED FROM PYTHON SOURCE LINES 100-102

Device properties
+++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 102-111

.. code-block:: Python



    if has_cuda:
        properties = get_device_prop()
        pprint.pprint(properties)
    else:
        properties = {"major": 0}






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'clockRate': 1569000,
     'computeMode': 0,
     'concurrentKernels': 1,
     'isMultiGpuBoard': 0,
     'major': 6,
     'maxThreadsPerBlock': 1024,
     'minor': 1,
     'multiProcessorCount': 10,
     'name': 'NVIDIA GeForce GTX 1060',
     'sharedMemPerBlock': 49152,
     'totalConstMem': 65536,
     'totalGlobalMem': 6442319872}




.. GENERATED FROM PYTHON SOURCE LINES 112-119

Model to benchmark
++++++++++++++++++

It includes one Gemm. The operator changes.
It can the regular Gemm, a custom Gemm from domain `com.microsoft`
or a custom implementation from domain
`onnx_extented.ortops.tutorial.cuda`.

.. GENERATED FROM PYTHON SOURCE LINES 119-199

.. code-block:: Python



    def create_model(
        mat_type=TensorProto.FLOAT, provider="CUDAExecutionProvider", domain="com.microsoft"
    ):
        A = make_tensor_value_info("A", mat_type, [None, None])
        B = make_tensor_value_info("B", mat_type, [None, None])
        outputs = [make_tensor_value_info("C", mat_type, [None, None])]
        inits = []
        if domain != "":
            if provider != "CUDAExecutionProvider":
                return None
            f8 = False
            if domain == "com.microsoft":
                op_name = "GemmFloat8"
                computeType = "CUBLAS_COMPUTE_32F"
                node_output = ["C"]
            elif mat_type == TensorProto.FLOAT:
                op_name = "CustomGemmFloat"
                computeType = "CUBLAS_COMPUTE_32F_FAST_TF32"
                node_output = ["C"]
            elif mat_type == TensorProto.FLOAT16:
                op_name = "CustomGemmFloat16"
                computeType = "CUBLAS_COMPUTE_16F"
                node_output = ["C"]
            elif mat_type in (TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2):
                f8 = True
                op_name = "CustomGemmFloat8E4M3FN"
                computeType = "CUBLAS_COMPUTE_32F"
                node_output = ["C"]
                outputs = [
                    make_tensor_value_info("C", TensorProto.FLOAT16, [None, None]),
                ]
                inits.append(from_array(numpy.array([1], dtype=numpy.float32), name="I"))
            else:
                return None
            node_kw = dict(
                alpha=1.0,
                transB=1,
                domain=domain,
                computeType=computeType,
                fastAccumulationMode=1,
                rowMajor=0 if op_name.startswith("CustomGemmFloat") else 1,
            )
            node_kw["name"] = (
                f"{mat_type}.{len(node_output)}.{len(outputs)}."
                f"{domain}..{node_kw['rowMajor']}.."
                f"{node_kw['fastAccumulationMode']}..{node_kw['computeType']}.."
                f"{f8}"
            )
            node_inputs = ["A", "B"]
            if f8:
                node_inputs.append("")
                node_inputs.extend(["I"] * 3)
            nodes = [make_node(op_name, node_inputs, node_output, **node_kw)]
        else:
            nodes = [
                make_node("Gemm", ["A", "B"], ["C"], transA=1, beta=0.0),
            ]
        graph = make_graph(nodes, "a", [A, B], outputs, inits)
        if mat_type < 16:
            # regular type
            opset, ir = 18, 8
        else:
            opset, ir = 19, 9
        onnx_model = make_model(
            graph,
            opset_imports=[
                make_opsetid("", opset),
                make_opsetid("com.microsoft", 1),
                make_opsetid("onnx_extented.ortops.tutorial.cuda", 1),
            ],
            ir_version=ir,
        )
        check_model(onnx_model)
        return onnx_model


    print(onnx_simple_text_plot(create_model()))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    opset: domain='com.microsoft' version=1
    opset: domain='onnx_extented.ortops.tutorial.cuda' version=1
    input: name='A' type=dtype('float32') shape=['', '']
    input: name='B' type=dtype('float32') shape=['', '']
    GemmFloat8[com.microsoft](A, B, alpha=1.00, computeType=b'CUBLAS_COMPUTE_32F', fastAccumulationMode=1, rowMajor=1, transB=1) -> C
    output: name='C' type=dtype('float32') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 200-204

A model to cast into anytype.
numpy does not support float 8. onnxruntime is used
to cast a float array into any type.
It must be called with tensor of type `OrtValue`.

.. GENERATED FROM PYTHON SOURCE LINES 204-234

.. code-block:: Python



    def create_cast(to, cuda=False):
        A = make_tensor_value_info("A", TensorProto.FLOAT, [None, None])
        C = make_tensor_value_info("C", to, [None, None])
        if cuda:
            nodes = [
                make_node("Cast", ["A"], ["Cc"], to=to),
                make_node("MemcpyFromHost", ["Cc"], ["C"]),
            ]
        else:
            nodes = [make_node("Cast", ["A"], ["C"], to=to)]
        graph = make_graph(nodes, "a", [A], [C])
        if to < 16:
            # regular type
            opset, ir = 18, 8
        else:
            opset, ir = 19, 9
        onnx_model = make_model(
            graph, opset_imports=[make_opsetid("", opset)], ir_version=ir
        )
        if not cuda:
            # OpType: MemcpyFromHost
            check_model(onnx_model)
        return onnx_model


    print(onnx_simple_text_plot(create_cast(TensorProto.FLOAT16)))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    input: name='A' type=dtype('float32') shape=['', '']
    Cast(A, to=10) -> C
    output: name='C' type=dtype('float16') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 235-239

Performance
+++++++++++

The benchmark will run the following configurations.

.. GENERATED FROM PYTHON SOURCE LINES 239-252

.. code-block:: Python


    types = list(getattr(TensorProto, a) for a in script_args.types.split(","))
    engine = [InferenceSession, CReferenceEvaluator]
    providers = [
        ["CUDAExecutionProvider", "CPUExecutionProvider"],
        ["CPUExecutionProvider"],
    ]
    # M, N, K
    # we use multiple of 8, otherwise, float8 does not work.
    dims = [list(int(i) for i in line.split(",")) for line in script_args.dims.split(";")]
    domains = ["onnx_extented.ortops.tutorial.cuda", "", "com.microsoft"]









.. GENERATED FROM PYTHON SOURCE LINES 253-254

Let's cache the matrices involved.

.. GENERATED FROM PYTHON SOURCE LINES 254-309

.. code-block:: Python



    def to_ort_value(m):
        device = C_OrtDevice(C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)
        ort_value = C_OrtValue.ortvalue_from_numpy(m, device)
        return ort_value


    def cached_inputs(dims, types):
        matrices = {}
        matrices_cuda = {}
        pbar = tqdm(list(product(dims, types)))
        for dim, tt in pbar:
            m, n, k = dim
            pbar.set_description(f"t={tt} dim={dim}")
            for i, j in [(m, k), (k, n), (k, m)]:
                if (tt, i, j) in matrices:
                    continue
                # CPU
                try:
                    sess = InferenceSession(
                        create_cast(tt).SerializeToString(),
                        providers=["CPUExecutionProvider"],
                    )
                    cpu = True
                except (InvalidGraph, InvalidArgument, NotImplemented):
                    # not support by this version of onnxruntime
                    cpu = False

                if cpu:
                    vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)
                    ov = to_ort_value(vect)
                    ovtt = sess._sess.run_with_ort_values({"A": ov}, ["C"], None)[0]
                    matrices[tt, i, j] = ovtt
                else:
                    continue

                # CUDA
                if "CUDAExecutionProvider" not in get_available_providers():
                    # No CUDA
                    continue
                sess = InferenceSession(
                    create_cast(tt, cuda=True).SerializeToString(),
                    providers=["CUDAExecutionProvider", "CPUExecutionProvider"],
                )
                vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)
                ov = to_ort_value(vect)
                ovtt = sess._sess.run_with_ort_values({"A": ov}, ["C"], None)[0]
                matrices_cuda[tt, i, j] = ovtt
        return matrices, matrices_cuda


    matrices, matrices_cuda = cached_inputs(dims, types)
    print(f"{len(matrices)} matrices were created.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/28 [00:00<?, ?it/s]    t=17 dim=[32, 32, 32]:   0%|          | 0/28 [00:00<?, ?it/s]    t=1 dim=[32, 32, 32]:   0%|          | 0/28 [00:00<?, ?it/s]     t=10 dim=[32, 32, 32]:   0%|          | 0/28 [00:00<?, ?it/s]    t=16 dim=[32, 32, 32]:   0%|          | 0/28 [00:00<?, ?it/s]    t=17 dim=[64, 64, 64]:   0%|          | 0/28 [00:00<?, ?it/s]    t=1 dim=[64, 64, 64]:   0%|          | 0/28 [00:00<?, ?it/s]     t=10 dim=[64, 64, 64]:   0%|          | 0/28 [00:00<?, ?it/s]    t=16 dim=[64, 64, 64]:   0%|          | 0/28 [00:00<?, ?it/s]    t=16 dim=[64, 64, 64]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]    t=17 dim=[128, 128, 128]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]    t=1 dim=[128, 128, 128]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]     t=10 dim=[128, 128, 128]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]    t=16 dim=[128, 128, 128]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]    t=17 dim=[256, 256, 256]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]    t=1 dim=[256, 256, 256]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]     t=10 dim=[256, 256, 256]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]    t=16 dim=[256, 256, 256]:  29%|██▊       | 8/28 [00:00<00:00, 65.78it/s]    t=16 dim=[256, 256, 256]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]    t=17 dim=[400, 400, 400]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]    t=1 dim=[400, 400, 400]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]     t=10 dim=[400, 400, 400]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]    t=16 dim=[400, 400, 400]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]    t=17 dim=[512, 512, 512]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]    t=1 dim=[512, 512, 512]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]     t=10 dim=[512, 512, 512]:  57%|█████▋    | 16/28 [00:00<00:00, 69.10it/s]    t=10 dim=[512, 512, 512]:  82%|████████▏ | 23/28 [00:00<00:00, 39.76it/s]    t=16 dim=[512, 512, 512]:  82%|████████▏ | 23/28 [00:00<00:00, 39.76it/s]    t=17 dim=[1024, 1024, 1024]:  82%|████████▏ | 23/28 [00:00<00:00, 39.76it/s]    t=1 dim=[1024, 1024, 1024]:  82%|████████▏ | 23/28 [00:00<00:00, 39.76it/s]     t=10 dim=[1024, 1024, 1024]:  82%|████████▏ | 23/28 [00:00<00:00, 39.76it/s]    t=16 dim=[1024, 1024, 1024]:  82%|████████▏ | 23/28 [00:00<00:00, 39.76it/s]    t=16 dim=[1024, 1024, 1024]: 100%|██████████| 28/28 [00:00<00:00, 21.87it/s]    t=16 dim=[1024, 1024, 1024]: 100%|██████████| 28/28 [00:00<00:00, 28.63it/s]
    28 matrices were created.




.. GENERATED FROM PYTHON SOURCE LINES 310-311

Let's run the benchmark

.. GENERATED FROM PYTHON SOURCE LINES 311-471

.. code-block:: Python



    def rendering_obs(obs, dim, number, repeat, domain, provider, internal_time):
        stype = {
            TensorProto.FLOAT: "f32",
            TensorProto.FLOAT16: "f16",
            TensorProto.BFLOAT16: "bf16",
            TensorProto.INT8: "i8",
            TensorProto.INT16: "i16",
            TensorProto.INT32: "i32",
            TensorProto.UINT32: "u32",
            TensorProto.FLOAT8E4M3FN: "e4m3fn",
            TensorProto.FLOAT8E5M2: "e5m2",
        }[tt]
        obs.update(
            dict(
                engine={"InferenceSession": "ort", "CReferenceEvaluator": "np"}[
                    engine.__name__
                ],
                stype=stype,
                type=f"{stype}",
                M=dim[0],
                N=dim[1],
                K=dim[2],
                cost=numpy.prod(dim) * 4,
                cost_s=f"{numpy.prod(dim) * 4}-{dim[0]}x{dim[1]}x{dim[2]}",
                repeat=repeat,
                number=number,
                domain={
                    "": "ORT",
                    "com.microsoft": "COM",
                    "onnx_extented.ortops.tutorial.cuda": "EXT",
                }[domain],
                provider={
                    "CPUExecutionProvider": "cpu",
                    "CUDAExecutionProvider": "cuda",
                }[provider[0]],
                platform=platform.processor(),
                intime=internal_time,
            )
        )
        return obs


    opts = SessionOptions()
    r = get_ort_ext_libs()
    if r is not None:
        opts.register_custom_ops_library(r[0])


    data = []
    errors = []
    pbar = tqdm(list(product(types, engine, providers, dims, domains)))
    for tt, engine, provider, dim, domain in pbar:
        if (
            tt in {TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2}
            and properties.get("major", 0) < 9
        ):
            # f8 not available
            if provider[0] == "CPUExecutionProvider":
                continue
            errors.append(
                f"f8 not available, major={properties.get('major', 0)}, "
                f"tt={tt}, provider={provider!r}, domain={domain!r}."
            )
            continue
        elif provider[0] == "CPUExecutionProvider" and max(dim) > 2000:
            # too long
            continue
        if max(dim) <= 200:
            repeat, number = script_args.repeat * 4, script_args.number * 4
        elif max(dim) <= 256:
            repeat, number = script_args.repeat * 2, script_args.number * 2
        else:
            repeat, number = script_args.repeat, script_args.number

        onx = create_model(tt, provider=provider[0], domain=domain)
        if onx is None:
            if provider[0] == "CPUExecutionProvider":
                continue
            errors.append(
                f"No model for tt={tt}, provider={provider!r}, domain={domain!r}."
            )
            continue
        with open(f"plot_bench_gemm_ort_{tt}_{domain}.onnx", "wb") as f:
            f.write(onx.SerializeToString())
        k1 = (tt, dim[2], dim[0])
        k2 = (tt, dim[2], dim[1])
        if k1 not in matrices:
            errors.append(f"Key k1={k1!r} not in matrices.")
            continue
        if k2 not in matrices:
            errors.append(f"Key k2={k2!r} not in matrices.")
            continue

        pbar.set_description(f"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}")

        if engine == CReferenceEvaluator:
            if (
                domain != ""
                or max(dim) > 256
                or provider != ["CPUExecutionProvider"]
                or tt not in [TensorProto.FLOAT, TensorProto.FLOAT16]
            ):
                # All impossible or slow cases.
                continue
            if tt == TensorProto.FLOAT16 and max(dim) > 50:
                repeat, number = 2, 2

            feeds = {"A": matrices[k1].numpy(), "B": matrices[k2].numpy()}
            sess = engine(onx)
            sess.run(None, feeds)
            obs = measure_time(lambda: sess.run(None, feeds), repeat=repeat, number=number)

        elif engine == InferenceSession:
            if provider[0] not in get_available_providers():
                errors.append(f"provider={provider[0]} is missing")
                continue
            try:
                sess = engine(onx.SerializeToString(), opts, providers=provider)
            except (NotImplemented, InvalidGraph, Fail) as e:
                # not implemented
                errors.append((tt, engine.__class__.__name__, provider, domain, e))
                continue

            the_feeds = (
                {"A": matrices[k1], "B": matrices[k2]}
                if provider == ["CPUExecutionProvider"]
                else {"A": matrices_cuda[k1], "B": matrices_cuda[k2]}
            )
            out_names = ["C"]

            # warmup
            for i in range(script_args.warmup):
                sess._sess.run_with_ort_values(the_feeds, out_names, None)[0]

            # benchamrk
            times = []

            def fct_benchmarked():
                got = sess._sess.run_with_ort_values(the_feeds, out_names, None)
                if len(got) > 1:
                    times.append(got[1])

            obs = measure_time(fct_benchmarked, repeat=repeat, number=number)
            internal_time = None
            if times:
                np_times = [t.numpy() for t in times]
                internal_time = (sum(np_times) / len(times))[0]

        else:
            errors.append(f"unknown engine={engine}")
            continue

        # improves the rendering
        obs = rendering_obs(obs, dim, number, repeat, domain, provider, internal_time)
        data.append(obs)
        if unit_test_going() and len(data) >= 2:
            break





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/336 [00:00<?, ?it/s]    t=1 e=InferenceSession p=CUDA dim=[32, 32, 32]:   0%|          | 0/336 [00:00<?, ?it/s]    t=1 e=InferenceSession p=CUDA dim=[32, 32, 32]:  25%|██▌       | 85/336 [00:03<00:09, 25.24it/s]    t=1 e=InferenceSession p=CUDA dim=[32, 32, 32]:  25%|██▌       | 85/336 [00:03<00:09, 25.24it/s]    t=1 e=InferenceSession p=CUDA dim=[32, 32, 32]:  25%|██▌       | 85/336 [00:03<00:09, 25.24it/s]    t=1 e=InferenceSession p=CUDA dim=[64, 64, 64]:  25%|██▌       | 85/336 [00:03<00:09, 25.24it/s]    t=1 e=InferenceSession p=CUDA dim=[64, 64, 64]:  26%|██▌       | 88/336 [00:06<00:22, 11.23it/s]    t=1 e=InferenceSession p=CUDA dim=[64, 64, 64]:  26%|██▌       | 88/336 [00:06<00:22, 11.23it/s]    t=1 e=InferenceSession p=CUDA dim=[64, 64, 64]:  26%|██▌       | 88/336 [00:06<00:22, 11.23it/s]    t=1 e=InferenceSession p=CUDA dim=[64, 64, 64]:  27%|██▋       | 90/336 [00:06<00:22, 11.13it/s]    t=1 e=InferenceSession p=CUDA dim=[128, 128, 128]:  27%|██▋       | 90/336 [00:06<00:22, 11.13it/s]    t=1 e=InferenceSession p=CUDA dim=[128, 128, 128]:  27%|██▋       | 90/336 [00:10<00:22, 11.13it/s]    t=1 e=InferenceSession p=CUDA dim=[128, 128, 128]:  27%|██▋       | 92/336 [00:10<00:49,  4.98it/s]    t=1 e=InferenceSession p=CUDA dim=[128, 128, 128]:  27%|██▋       | 92/336 [00:10<00:49,  4.98it/s]    t=1 e=InferenceSession p=CUDA dim=[256, 256, 256]:  27%|██▋       | 92/336 [00:10<00:49,  4.98it/s]    t=1 e=InferenceSession p=CUDA dim=[256, 256, 256]:  28%|██▊       | 94/336 [00:11<00:55,  4.32it/s]    t=1 e=InferenceSession p=CUDA dim=[256, 256, 256]:  28%|██▊       | 94/336 [00:11<00:55,  4.32it/s]    t=1 e=InferenceSession p=CUDA dim=[256, 256, 256]:  28%|██▊       | 95/336 [00:12<00:57,  4.22it/s]    t=1 e=InferenceSession p=CUDA dim=[256, 256, 256]:  28%|██▊       | 95/336 [00:12<00:57,  4.22it/s]    t=1 e=InferenceSession p=CUDA dim=[400, 400, 400]:  28%|██▊       | 95/336 [00:12<00:57,  4.22it/s]    t=1 e=InferenceSession p=CUDA dim=[400, 400, 400]:  29%|██▉       | 97/336 [00:12<00:55,  4.28it/s]    t=1 e=InferenceSession p=CUDA dim=[400, 400, 400]:  29%|██▉       | 97/336 [00:12<00:55,  4.28it/s]    t=1 e=InferenceSession p=CUDA dim=[400, 400, 400]:  29%|██▉       | 98/336 [00:12<00:55,  4.31it/s]    t=1 e=InferenceSession p=CUDA dim=[400, 400, 400]:  29%|██▉       | 98/336 [00:12<00:55,  4.31it/s]    t=1 e=InferenceSession p=CUDA dim=[512, 512, 512]:  29%|██▉       | 98/336 [00:12<00:55,  4.31it/s]    t=1 e=InferenceSession p=CUDA dim=[512, 512, 512]:  30%|██▉       | 100/336 [00:13<00:56,  4.18it/s]    t=1 e=InferenceSession p=CUDA dim=[512, 512, 512]:  30%|██▉       | 100/336 [00:13<00:56,  4.18it/s]    t=1 e=InferenceSession p=CUDA dim=[512, 512, 512]:  30%|███       | 101/336 [00:13<00:58,  4.01it/s]    t=1 e=InferenceSession p=CUDA dim=[512, 512, 512]:  30%|███       | 101/336 [00:13<00:58,  4.01it/s]    t=1 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  30%|███       | 101/336 [00:13<00:58,  4.01it/s]    t=1 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  31%|███       | 103/336 [00:15<01:32,  2.53it/s]    t=1 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  31%|███       | 103/336 [00:15<01:32,  2.53it/s]    t=1 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  31%|███       | 104/336 [00:16<02:05,  1.84it/s]    t=1 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  31%|███       | 104/336 [00:16<02:05,  1.84it/s]    t=1 e=InferenceSession p=CPUE dim=[32, 32, 32]:  31%|███       | 104/336 [00:16<02:05,  1.84it/s]          t=1 e=InferenceSession p=CPUE dim=[64, 64, 64]:  31%|███       | 104/336 [00:16<02:05,  1.84it/s]    t=1 e=InferenceSession p=CPUE dim=[128, 128, 128]:  31%|███       | 104/336 [00:16<02:05,  1.84it/s]    t=1 e=InferenceSession p=CPUE dim=[256, 256, 256]:  31%|███       | 104/336 [00:16<02:05,  1.84it/s]    t=1 e=InferenceSession p=CPUE dim=[256, 256, 256]:  35%|███▍      | 116/336 [00:16<00:33,  6.47it/s]    t=1 e=InferenceSession p=CPUE dim=[400, 400, 400]:  35%|███▍      | 116/336 [00:16<00:33,  6.47it/s]    t=1 e=InferenceSession p=CPUE dim=[512, 512, 512]:  35%|███▍      | 116/336 [00:16<00:33,  6.47it/s]    t=1 e=InferenceSession p=CPUE dim=[512, 512, 512]:  36%|███▋      | 122/336 [00:17<00:23,  9.02it/s]    t=1 e=InferenceSession p=CPUE dim=[1024, 1024, 1024]:  36%|███▋      | 122/336 [00:17<00:23,  9.02it/s]    t=1 e=InferenceSession p=CPUE dim=[1024, 1024, 1024]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]       t=1 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[32, 32, 32]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]          t=1 e=CReferenceEvaluator p=CPUE dim=[64, 64, 64]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[128, 128, 128]:  38%|███▊      | 126/336 [00:17<00:27,  7.74it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[128, 128, 128]:  46%|████▌     | 155/336 [00:20<00:16, 10.74it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[256, 256, 256]:  46%|████▌     | 155/336 [00:20<00:16, 10.74it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[256, 256, 256]:  47%|████▋     | 158/336 [00:20<00:18,  9.75it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[400, 400, 400]:  47%|████▋     | 158/336 [00:20<00:18,  9.75it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[512, 512, 512]:  47%|████▋     | 158/336 [00:20<00:18,  9.75it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=[1024, 1024, 1024]:  47%|████▋     | 158/336 [00:20<00:18,  9.75it/s]    t=10 e=InferenceSession p=CUDA dim=[32, 32, 32]:  47%|████▋     | 158/336 [00:20<00:18,  9.75it/s]            t=10 e=InferenceSession p=CUDA dim=[32, 32, 32]:  50%|█████     | 169/336 [00:25<00:38,  4.32it/s]    t=10 e=InferenceSession p=CUDA dim=[32, 32, 32]:  50%|█████     | 169/336 [00:25<00:38,  4.32it/s]    t=10 e=InferenceSession p=CUDA dim=[32, 32, 32]:  50%|█████     | 169/336 [00:26<00:38,  4.32it/s]    t=10 e=InferenceSession p=CUDA dim=[32, 32, 32]:  51%|█████     | 171/336 [00:26<00:36,  4.55it/s]    t=10 e=InferenceSession p=CUDA dim=[64, 64, 64]:  51%|█████     | 171/336 [00:26<00:36,  4.55it/s]    t=10 e=InferenceSession p=CUDA dim=[64, 64, 64]:  51%|█████     | 171/336 [00:34<00:36,  4.55it/s]    t=10 e=InferenceSession p=CUDA dim=[64, 64, 64]:  51%|█████▏    | 173/336 [00:34<01:43,  1.57it/s]    t=10 e=InferenceSession p=CUDA dim=[64, 64, 64]:  51%|█████▏    | 173/336 [00:34<01:43,  1.57it/s]    t=10 e=InferenceSession p=CUDA dim=[128, 128, 128]:  51%|█████▏    | 173/336 [00:34<01:43,  1.57it/s]    t=10 e=InferenceSession p=CUDA dim=[128, 128, 128]:  52%|█████▏    | 175/336 [00:41<02:41,  1.00s/it]    t=10 e=InferenceSession p=CUDA dim=[128, 128, 128]:  52%|█████▏    | 175/336 [00:41<02:41,  1.00s/it]    t=10 e=InferenceSession p=CUDA dim=[128, 128, 128]:  52%|█████▏    | 176/336 [00:42<02:31,  1.06it/s]    t=10 e=InferenceSession p=CUDA dim=[128, 128, 128]:  52%|█████▏    | 176/336 [00:42<02:31,  1.06it/s]    t=10 e=InferenceSession p=CUDA dim=[256, 256, 256]:  52%|█████▏    | 176/336 [00:42<02:31,  1.06it/s]    t=10 e=InferenceSession p=CUDA dim=[256, 256, 256]:  53%|█████▎    | 178/336 [00:45<02:47,  1.06s/it]    t=10 e=InferenceSession p=CUDA dim=[256, 256, 256]:  53%|█████▎    | 178/336 [00:45<02:47,  1.06s/it]    t=10 e=InferenceSession p=CUDA dim=[256, 256, 256]:  53%|█████▎    | 179/336 [00:45<02:31,  1.04it/s]    t=10 e=InferenceSession p=CUDA dim=[256, 256, 256]:  53%|█████▎    | 179/336 [00:45<02:31,  1.04it/s]    t=10 e=InferenceSession p=CUDA dim=[400, 400, 400]:  53%|█████▎    | 179/336 [00:45<02:31,  1.04it/s]    t=10 e=InferenceSession p=CUDA dim=[400, 400, 400]:  54%|█████▍    | 181/336 [00:48<02:58,  1.15s/it]    t=10 e=InferenceSession p=CUDA dim=[400, 400, 400]:  54%|█████▍    | 181/336 [00:48<02:58,  1.15s/it]    t=10 e=InferenceSession p=CUDA dim=[400, 400, 400]:  54%|█████▍    | 182/336 [00:48<02:32,  1.01it/s]    t=10 e=InferenceSession p=CUDA dim=[400, 400, 400]:  54%|█████▍    | 182/336 [00:48<02:32,  1.01it/s]    t=10 e=InferenceSession p=CUDA dim=[512, 512, 512]:  54%|█████▍    | 182/336 [00:48<02:32,  1.01it/s]    t=10 e=InferenceSession p=CUDA dim=[512, 512, 512]:  55%|█████▍    | 184/336 [00:54<03:49,  1.51s/it]    t=10 e=InferenceSession p=CUDA dim=[512, 512, 512]:  55%|█████▍    | 184/336 [00:54<03:49,  1.51s/it]    t=10 e=InferenceSession p=CUDA dim=[512, 512, 512]:  55%|█████▌    | 185/336 [00:54<03:12,  1.28s/it]    t=10 e=InferenceSession p=CUDA dim=[512, 512, 512]:  55%|█████▌    | 185/336 [00:54<03:12,  1.28s/it]    t=10 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  55%|█████▌    | 185/336 [00:54<03:12,  1.28s/it]    t=10 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  56%|█████▌    | 187/336 [01:28<16:22,  6.59s/it]    t=10 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  56%|█████▌    | 187/336 [01:28<16:22,  6.59s/it]    t=10 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  56%|█████▌    | 188/336 [01:29<13:33,  5.50s/it]    t=10 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  56%|█████▌    | 188/336 [01:29<13:33,  5.50s/it]    t=10 e=InferenceSession p=CPUE dim=[32, 32, 32]:  56%|█████▌    | 188/336 [01:29<13:33,  5.50s/it]          t=10 e=InferenceSession p=CPUE dim=[64, 64, 64]:  56%|█████▌    | 188/336 [01:29<13:33,  5.50s/it]    t=10 e=InferenceSession p=CPUE dim=[64, 64, 64]:  58%|█████▊    | 194/336 [01:29<04:53,  2.07s/it]    t=10 e=InferenceSession p=CPUE dim=[128, 128, 128]:  58%|█████▊    | 194/336 [01:29<04:53,  2.07s/it]    t=10 e=InferenceSession p=CPUE dim=[128, 128, 128]:  59%|█████▊    | 197/336 [01:29<03:21,  1.45s/it]    t=10 e=InferenceSession p=CPUE dim=[256, 256, 256]:  59%|█████▊    | 197/336 [01:29<03:21,  1.45s/it]    t=10 e=InferenceSession p=CPUE dim=[256, 256, 256]:  60%|█████▉    | 200/336 [01:29<02:20,  1.03s/it]    t=10 e=InferenceSession p=CPUE dim=[400, 400, 400]:  60%|█████▉    | 200/336 [01:29<02:20,  1.03s/it]    t=10 e=InferenceSession p=CPUE dim=[400, 400, 400]:  60%|██████    | 203/336 [01:30<01:37,  1.37it/s]    t=10 e=InferenceSession p=CPUE dim=[512, 512, 512]:  60%|██████    | 203/336 [01:30<01:37,  1.37it/s]    t=10 e=InferenceSession p=CPUE dim=[512, 512, 512]:  61%|██████▏   | 206/336 [01:30<01:12,  1.78it/s]    t=10 e=InferenceSession p=CPUE dim=[1024, 1024, 1024]:  61%|██████▏   | 206/336 [01:30<01:12,  1.78it/s]    t=10 e=InferenceSession p=CPUE dim=[1024, 1024, 1024]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]       t=10 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[32, 32, 32]:  62%|██████▏   | 209/336 [01:32<01:08,  1.85it/s]          t=10 e=CReferenceEvaluator p=CPUE dim=[32, 32, 32]:  69%|██████▉   | 233/336 [01:32<00:13,  7.67it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[64, 64, 64]:  69%|██████▉   | 233/336 [01:32<00:13,  7.67it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[128, 128, 128]:  69%|██████▉   | 233/336 [01:32<00:13,  7.67it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[128, 128, 128]:  71%|███████   | 239/336 [01:32<00:10,  9.42it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[256, 256, 256]:  71%|███████   | 239/336 [01:32<00:10,  9.42it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[256, 256, 256]:  72%|███████▏  | 243/336 [01:33<00:11,  8.33it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[400, 400, 400]:  72%|███████▏  | 243/336 [01:33<00:11,  8.33it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[512, 512, 512]:  72%|███████▏  | 243/336 [01:33<00:11,  8.33it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=[1024, 1024, 1024]:  72%|███████▏  | 243/336 [01:33<00:11,  8.33it/s]    t=16 e=InferenceSession p=CUDA dim=[32, 32, 32]:  72%|███████▏  | 243/336 [01:33<00:11,  8.33it/s]             t=16 e=InferenceSession p=CUDA dim=[32, 32, 32]:  76%|███████▌  | 254/336 [01:33<00:06, 12.15it/s]    t=16 e=InferenceSession p=CUDA dim=[32, 32, 32]:  76%|███████▌  | 254/336 [01:33<00:06, 12.15it/s]    t=16 e=InferenceSession p=CUDA dim=[64, 64, 64]:  76%|███████▌  | 254/336 [01:33<00:06, 12.15it/s]    t=16 e=InferenceSession p=CUDA dim=[64, 64, 64]:  76%|███████▌  | 254/336 [01:33<00:06, 12.15it/s]    t=16 e=InferenceSession p=CUDA dim=[64, 64, 64]:  77%|███████▋  | 258/336 [01:33<00:06, 12.10it/s]    t=16 e=InferenceSession p=CUDA dim=[128, 128, 128]:  77%|███████▋  | 258/336 [01:33<00:06, 12.10it/s]    t=16 e=InferenceSession p=CUDA dim=[128, 128, 128]:  77%|███████▋  | 258/336 [01:34<00:06, 12.10it/s]    t=16 e=InferenceSession p=CUDA dim=[128, 128, 128]:  78%|███████▊  | 261/336 [01:34<00:07, 10.32it/s]    t=16 e=InferenceSession p=CUDA dim=[256, 256, 256]:  78%|███████▊  | 261/336 [01:34<00:07, 10.32it/s]    t=16 e=InferenceSession p=CUDA dim=[256, 256, 256]:  78%|███████▊  | 263/336 [01:34<00:07,  9.91it/s]    t=16 e=InferenceSession p=CUDA dim=[256, 256, 256]:  78%|███████▊  | 263/336 [01:34<00:07,  9.91it/s]    t=16 e=InferenceSession p=CUDA dim=[400, 400, 400]:  78%|███████▊  | 263/336 [01:34<00:07,  9.91it/s]    t=16 e=InferenceSession p=CUDA dim=[400, 400, 400]:  79%|███████▉  | 266/336 [01:34<00:06, 10.85it/s]    t=16 e=InferenceSession p=CUDA dim=[400, 400, 400]:  79%|███████▉  | 266/336 [01:34<00:06, 10.85it/s]    t=16 e=InferenceSession p=CUDA dim=[512, 512, 512]:  79%|███████▉  | 266/336 [01:34<00:06, 10.85it/s]    t=16 e=InferenceSession p=CUDA dim=[512, 512, 512]:  80%|████████  | 269/336 [01:35<00:06, 10.84it/s]    t=16 e=InferenceSession p=CUDA dim=[512, 512, 512]:  80%|████████  | 269/336 [01:35<00:06, 10.84it/s]    t=16 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  80%|████████  | 269/336 [01:35<00:06, 10.84it/s]    t=16 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=InferenceSession p=CUDA dim=[1024, 1024, 1024]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=InferenceSession p=CPUE dim=[32, 32, 32]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]          t=16 e=InferenceSession p=CPUE dim=[64, 64, 64]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=InferenceSession p=CPUE dim=[128, 128, 128]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=InferenceSession p=CPUE dim=[256, 256, 256]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=InferenceSession p=CPUE dim=[400, 400, 400]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=InferenceSession p=CPUE dim=[512, 512, 512]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=InferenceSession p=CPUE dim=[1024, 1024, 1024]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]       t=16 e=CReferenceEvaluator p=CUDA dim=[32, 32, 32]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[64, 64, 64]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[128, 128, 128]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[256, 256, 256]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[400, 400, 400]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[512, 512, 512]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CUDA dim=[1024, 1024, 1024]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CPUE dim=[32, 32, 32]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]          t=16 e=CReferenceEvaluator p=CPUE dim=[64, 64, 64]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CPUE dim=[128, 128, 128]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CPUE dim=[256, 256, 256]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CPUE dim=[400, 400, 400]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CPUE dim=[512, 512, 512]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CPUE dim=[1024, 1024, 1024]:  81%|████████  | 272/336 [01:36<00:11,  5.65it/s]    t=16 e=CReferenceEvaluator p=CPUE dim=[1024, 1024, 1024]: 100%|██████████| 336/336 [01:36<00:00,  3.49it/s]




.. GENERATED FROM PYTHON SOURCE LINES 472-474

Results
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 474-484

.. code-block:: Python


    df = DataFrame(data)
    df.to_excel("plot_bench_gemm_ort.xlsx")
    df.to_csv("plot_bench_gemm_ort.csv")
    df.drop(["min_exec", "max_exec", "cost_s", "cost"], axis=1).to_csv(
        "plot_bench_gemm_ort.csv", index=False
    )
    print(df.head().T)
    df





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                0                1                 2                 3                    4
    average               0.00489         0.000179          0.004755           0.00032             0.005273
    deviation             0.00023         0.000016          0.000191          0.000052             0.000415
    min_exec              0.00466         0.000168          0.004512          0.000244             0.004865
    max_exec             0.005993          0.00026          0.005431          0.000431             0.006488
    repeat                     40               40                40                40                   40
    number                     16               16                16                16                   16
    ttime                0.195605         0.007143           0.19019          0.012782             0.210911
    context_size               64               64                64                64                   64
    warmup_time          0.005092         0.000672          0.005076          0.000362             0.005676
    engine                    ort              ort               ort               ort                  ort
    stype                     f32              f32               f32               f32                  f32
    type                      f32              f32               f32               f32                  f32
    M                          32               32                64                64                  128
    N                          32               32                64                64                  128
    K                          32               32                64                64                  128
    cost                   131072           131072           1048576           1048576              8388608
    cost_s        131072-32x32x32  131072-32x32x32  1048576-64x64x64  1048576-64x64x64  8388608-128x128x128
    domain                    EXT              ORT               EXT               ORT                  EXT
    provider                 cuda             cuda              cuda              cuda                 cuda
    platform               x86_64           x86_64            x86_64            x86_64               x86_64
    intime                   None             None              None              None                 None


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>average</th>
          <th>deviation</th>
          <th>min_exec</th>
          <th>max_exec</th>
          <th>repeat</th>
          <th>number</th>
          <th>ttime</th>
          <th>context_size</th>
          <th>warmup_time</th>
          <th>engine</th>
          <th>stype</th>
          <th>type</th>
          <th>M</th>
          <th>N</th>
          <th>K</th>
          <th>cost</th>
          <th>cost_s</th>
          <th>domain</th>
          <th>provider</th>
          <th>platform</th>
          <th>intime</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.004890</td>
          <td>0.000230</td>
          <td>0.004660</td>
          <td>0.005993</td>
          <td>40</td>
          <td>16</td>
          <td>0.195605</td>
          <td>64</td>
          <td>0.005092</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.000179</td>
          <td>0.000016</td>
          <td>0.000168</td>
          <td>0.000260</td>
          <td>40</td>
          <td>16</td>
          <td>0.007143</td>
          <td>64</td>
          <td>0.000672</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.004755</td>
          <td>0.000191</td>
          <td>0.004512</td>
          <td>0.005431</td>
          <td>40</td>
          <td>16</td>
          <td>0.190190</td>
          <td>64</td>
          <td>0.005076</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.000320</td>
          <td>0.000052</td>
          <td>0.000244</td>
          <td>0.000431</td>
          <td>40</td>
          <td>16</td>
          <td>0.012782</td>
          <td>64</td>
          <td>0.000362</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.005273</td>
          <td>0.000415</td>
          <td>0.004865</td>
          <td>0.006488</td>
          <td>40</td>
          <td>16</td>
          <td>0.210911</td>
          <td>64</td>
          <td>0.005676</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>5</th>
          <td>0.000817</td>
          <td>0.000221</td>
          <td>0.000556</td>
          <td>0.001555</td>
          <td>40</td>
          <td>16</td>
          <td>0.032660</td>
          <td>64</td>
          <td>0.000851</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>6</th>
          <td>0.007044</td>
          <td>0.000401</td>
          <td>0.006537</td>
          <td>0.007853</td>
          <td>20</td>
          <td>8</td>
          <td>0.140886</td>
          <td>64</td>
          <td>0.007007</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>7</th>
          <td>0.001910</td>
          <td>0.000162</td>
          <td>0.001621</td>
          <td>0.002349</td>
          <td>20</td>
          <td>8</td>
          <td>0.038203</td>
          <td>64</td>
          <td>0.001914</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>8</th>
          <td>0.009082</td>
          <td>0.000386</td>
          <td>0.008605</td>
          <td>0.010057</td>
          <td>10</td>
          <td>4</td>
          <td>0.090820</td>
          <td>64</td>
          <td>0.009130</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>400</td>
          <td>400</td>
          <td>400</td>
          <td>256000000</td>
          <td>256000000-400x400x400</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>9</th>
          <td>0.004567</td>
          <td>0.000199</td>
          <td>0.004277</td>
          <td>0.004889</td>
          <td>10</td>
          <td>4</td>
          <td>0.045672</td>
          <td>64</td>
          <td>0.004253</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>400</td>
          <td>400</td>
          <td>400</td>
          <td>256000000</td>
          <td>256000000-400x400x400</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>10</th>
          <td>0.011514</td>
          <td>0.000239</td>
          <td>0.011129</td>
          <td>0.011965</td>
          <td>10</td>
          <td>4</td>
          <td>0.115140</td>
          <td>64</td>
          <td>0.011324</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>512</td>
          <td>512</td>
          <td>512</td>
          <td>536870912</td>
          <td>536870912-512x512x512</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>11</th>
          <td>0.006779</td>
          <td>0.000129</td>
          <td>0.006587</td>
          <td>0.006994</td>
          <td>10</td>
          <td>4</td>
          <td>0.067790</td>
          <td>64</td>
          <td>0.006846</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>512</td>
          <td>512</td>
          <td>512</td>
          <td>536870912</td>
          <td>536870912-512x512x512</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>12</th>
          <td>0.035128</td>
          <td>0.000139</td>
          <td>0.034889</td>
          <td>0.035432</td>
          <td>10</td>
          <td>4</td>
          <td>0.351280</td>
          <td>64</td>
          <td>0.035979</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>1024</td>
          <td>1024</td>
          <td>1024</td>
          <td>4294967296</td>
          <td>4294967296-1024x1024x1024</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>13</th>
          <td>0.028854</td>
          <td>0.000046</td>
          <td>0.028789</td>
          <td>0.028946</td>
          <td>10</td>
          <td>4</td>
          <td>0.288543</td>
          <td>64</td>
          <td>0.028995</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>1024</td>
          <td>1024</td>
          <td>1024</td>
          <td>4294967296</td>
          <td>4294967296-1024x1024x1024</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>14</th>
          <td>0.000012</td>
          <td>0.000001</td>
          <td>0.000010</td>
          <td>0.000018</td>
          <td>40</td>
          <td>16</td>
          <td>0.000461</td>
          <td>64</td>
          <td>0.000030</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>15</th>
          <td>0.000023</td>
          <td>0.000003</td>
          <td>0.000017</td>
          <td>0.000032</td>
          <td>40</td>
          <td>16</td>
          <td>0.000911</td>
          <td>64</td>
          <td>0.000042</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>16</th>
          <td>0.000047</td>
          <td>0.000006</td>
          <td>0.000040</td>
          <td>0.000069</td>
          <td>40</td>
          <td>16</td>
          <td>0.001866</td>
          <td>64</td>
          <td>0.000071</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>17</th>
          <td>0.000271</td>
          <td>0.000029</td>
          <td>0.000228</td>
          <td>0.000367</td>
          <td>20</td>
          <td>8</td>
          <td>0.005422</td>
          <td>64</td>
          <td>0.000245</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>18</th>
          <td>0.001023</td>
          <td>0.000084</td>
          <td>0.000875</td>
          <td>0.001174</td>
          <td>10</td>
          <td>4</td>
          <td>0.010228</td>
          <td>64</td>
          <td>0.000738</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>400</td>
          <td>400</td>
          <td>400</td>
          <td>256000000</td>
          <td>256000000-400x400x400</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>19</th>
          <td>0.002013</td>
          <td>0.000087</td>
          <td>0.001782</td>
          <td>0.002142</td>
          <td>10</td>
          <td>4</td>
          <td>0.020126</td>
          <td>64</td>
          <td>0.001949</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>512</td>
          <td>512</td>
          <td>512</td>
          <td>536870912</td>
          <td>536870912-512x512x512</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>20</th>
          <td>0.016095</td>
          <td>0.001203</td>
          <td>0.014631</td>
          <td>0.018449</td>
          <td>10</td>
          <td>4</td>
          <td>0.160953</td>
          <td>64</td>
          <td>0.014230</td>
          <td>ort</td>
          <td>f32</td>
          <td>f32</td>
          <td>1024</td>
          <td>1024</td>
          <td>1024</td>
          <td>4294967296</td>
          <td>4294967296-1024x1024x1024</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>21</th>
          <td>0.000044</td>
          <td>0.000015</td>
          <td>0.000035</td>
          <td>0.000084</td>
          <td>40</td>
          <td>16</td>
          <td>0.001772</td>
          <td>64</td>
          <td>0.000078</td>
          <td>np</td>
          <td>f32</td>
          <td>f32</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>22</th>
          <td>0.000071</td>
          <td>0.000021</td>
          <td>0.000048</td>
          <td>0.000123</td>
          <td>40</td>
          <td>16</td>
          <td>0.002835</td>
          <td>64</td>
          <td>0.000103</td>
          <td>np</td>
          <td>f32</td>
          <td>f32</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>23</th>
          <td>0.003370</td>
          <td>0.003987</td>
          <td>0.000111</td>
          <td>0.015590</td>
          <td>40</td>
          <td>16</td>
          <td>0.134783</td>
          <td>64</td>
          <td>0.011207</td>
          <td>np</td>
          <td>f32</td>
          <td>f32</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>24</th>
          <td>0.003486</td>
          <td>0.002434</td>
          <td>0.000422</td>
          <td>0.009248</td>
          <td>20</td>
          <td>8</td>
          <td>0.069720</td>
          <td>64</td>
          <td>0.000528</td>
          <td>np</td>
          <td>f32</td>
          <td>f32</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>25</th>
          <td>0.008238</td>
          <td>0.000238</td>
          <td>0.007839</td>
          <td>0.008848</td>
          <td>40</td>
          <td>16</td>
          <td>0.329519</td>
          <td>64</td>
          <td>0.010102</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>26</th>
          <td>0.000241</td>
          <td>0.000038</td>
          <td>0.000209</td>
          <td>0.000415</td>
          <td>40</td>
          <td>16</td>
          <td>0.009632</td>
          <td>64</td>
          <td>0.000382</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>27</th>
          <td>0.012193</td>
          <td>0.001524</td>
          <td>0.010824</td>
          <td>0.015947</td>
          <td>40</td>
          <td>16</td>
          <td>0.487702</td>
          <td>64</td>
          <td>0.010899</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>28</th>
          <td>0.001076</td>
          <td>0.000454</td>
          <td>0.000449</td>
          <td>0.002011</td>
          <td>40</td>
          <td>16</td>
          <td>0.043057</td>
          <td>64</td>
          <td>0.000863</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>29</th>
          <td>0.010802</td>
          <td>0.001165</td>
          <td>0.009682</td>
          <td>0.015130</td>
          <td>40</td>
          <td>16</td>
          <td>0.432093</td>
          <td>64</td>
          <td>0.009645</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>30</th>
          <td>0.000500</td>
          <td>0.000061</td>
          <td>0.000417</td>
          <td>0.000709</td>
          <td>40</td>
          <td>16</td>
          <td>0.019991</td>
          <td>64</td>
          <td>0.000704</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>31</th>
          <td>0.018378</td>
          <td>0.000373</td>
          <td>0.017770</td>
          <td>0.019396</td>
          <td>20</td>
          <td>8</td>
          <td>0.367556</td>
          <td>64</td>
          <td>0.017944</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>32</th>
          <td>0.001512</td>
          <td>0.000108</td>
          <td>0.001343</td>
          <td>0.001705</td>
          <td>20</td>
          <td>8</td>
          <td>0.030246</td>
          <td>64</td>
          <td>0.001472</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>33</th>
          <td>0.073036</td>
          <td>0.002474</td>
          <td>0.070877</td>
          <td>0.079000</td>
          <td>10</td>
          <td>4</td>
          <td>0.730364</td>
          <td>64</td>
          <td>0.070677</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>400</td>
          <td>400</td>
          <td>400</td>
          <td>256000000</td>
          <td>256000000-400x400x400</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>34</th>
          <td>0.002754</td>
          <td>0.000114</td>
          <td>0.002580</td>
          <td>0.002951</td>
          <td>10</td>
          <td>4</td>
          <td>0.027536</td>
          <td>64</td>
          <td>0.002850</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>400</td>
          <td>400</td>
          <td>400</td>
          <td>256000000</td>
          <td>256000000-400x400x400</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>35</th>
          <td>0.115094</td>
          <td>0.004889</td>
          <td>0.106854</td>
          <td>0.123440</td>
          <td>10</td>
          <td>4</td>
          <td>1.150936</td>
          <td>64</td>
          <td>0.123673</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>512</td>
          <td>512</td>
          <td>512</td>
          <td>536870912</td>
          <td>536870912-512x512x512</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>36</th>
          <td>0.004500</td>
          <td>0.000123</td>
          <td>0.004315</td>
          <td>0.004685</td>
          <td>10</td>
          <td>4</td>
          <td>0.044998</td>
          <td>64</td>
          <td>0.004542</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>512</td>
          <td>512</td>
          <td>512</td>
          <td>536870912</td>
          <td>536870912-512x512x512</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>37</th>
          <td>0.732097</td>
          <td>0.017199</td>
          <td>0.706991</td>
          <td>0.755061</td>
          <td>10</td>
          <td>4</td>
          <td>7.320970</td>
          <td>64</td>
          <td>0.778394</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>1024</td>
          <td>1024</td>
          <td>1024</td>
          <td>4294967296</td>
          <td>4294967296-1024x1024x1024</td>
          <td>EXT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>38</th>
          <td>0.020952</td>
          <td>0.000149</td>
          <td>0.020830</td>
          <td>0.021370</td>
          <td>10</td>
          <td>4</td>
          <td>0.209521</td>
          <td>64</td>
          <td>0.021117</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>1024</td>
          <td>1024</td>
          <td>1024</td>
          <td>4294967296</td>
          <td>4294967296-1024x1024x1024</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>39</th>
          <td>0.000041</td>
          <td>0.000014</td>
          <td>0.000026</td>
          <td>0.000072</td>
          <td>40</td>
          <td>16</td>
          <td>0.001653</td>
          <td>64</td>
          <td>0.000055</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>40</th>
          <td>0.000088</td>
          <td>0.000006</td>
          <td>0.000065</td>
          <td>0.000101</td>
          <td>40</td>
          <td>16</td>
          <td>0.003515</td>
          <td>64</td>
          <td>0.000121</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>41</th>
          <td>0.000295</td>
          <td>0.000081</td>
          <td>0.000153</td>
          <td>0.000659</td>
          <td>40</td>
          <td>16</td>
          <td>0.011809</td>
          <td>64</td>
          <td>0.000303</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>42</th>
          <td>0.001413</td>
          <td>0.000509</td>
          <td>0.000963</td>
          <td>0.002738</td>
          <td>20</td>
          <td>8</td>
          <td>0.028258</td>
          <td>64</td>
          <td>0.002137</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>43</th>
          <td>0.003152</td>
          <td>0.000339</td>
          <td>0.002451</td>
          <td>0.003683</td>
          <td>10</td>
          <td>4</td>
          <td>0.031521</td>
          <td>64</td>
          <td>0.002008</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>400</td>
          <td>400</td>
          <td>400</td>
          <td>256000000</td>
          <td>256000000-400x400x400</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>44</th>
          <td>0.010407</td>
          <td>0.003035</td>
          <td>0.006079</td>
          <td>0.017162</td>
          <td>10</td>
          <td>4</td>
          <td>0.104070</td>
          <td>64</td>
          <td>0.014414</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>512</td>
          <td>512</td>
          <td>512</td>
          <td>536870912</td>
          <td>536870912-512x512x512</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>45</th>
          <td>0.031043</td>
          <td>0.001915</td>
          <td>0.027471</td>
          <td>0.034875</td>
          <td>10</td>
          <td>4</td>
          <td>0.310432</td>
          <td>64</td>
          <td>0.032101</td>
          <td>ort</td>
          <td>f16</td>
          <td>f16</td>
          <td>1024</td>
          <td>1024</td>
          <td>1024</td>
          <td>4294967296</td>
          <td>4294967296-1024x1024x1024</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>46</th>
          <td>0.000398</td>
          <td>0.000105</td>
          <td>0.000281</td>
          <td>0.000827</td>
          <td>40</td>
          <td>16</td>
          <td>0.015919</td>
          <td>64</td>
          <td>0.000351</td>
          <td>np</td>
          <td>f16</td>
          <td>f16</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>47</th>
          <td>0.001907</td>
          <td>0.000027</td>
          <td>0.001881</td>
          <td>0.001934</td>
          <td>2</td>
          <td>2</td>
          <td>0.003815</td>
          <td>64</td>
          <td>0.001931</td>
          <td>np</td>
          <td>f16</td>
          <td>f16</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>48</th>
          <td>0.017289</td>
          <td>0.002794</td>
          <td>0.014495</td>
          <td>0.020084</td>
          <td>2</td>
          <td>2</td>
          <td>0.034579</td>
          <td>64</td>
          <td>0.014279</td>
          <td>np</td>
          <td>f16</td>
          <td>f16</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>49</th>
          <td>0.115596</td>
          <td>0.003265</td>
          <td>0.112331</td>
          <td>0.118860</td>
          <td>2</td>
          <td>2</td>
          <td>0.231191</td>
          <td>64</td>
          <td>0.148986</td>
          <td>np</td>
          <td>f16</td>
          <td>f16</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>ORT</td>
          <td>cpu</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>50</th>
          <td>0.000430</td>
          <td>0.000070</td>
          <td>0.000338</td>
          <td>0.000605</td>
          <td>40</td>
          <td>16</td>
          <td>0.017214</td>
          <td>64</td>
          <td>0.000499</td>
          <td>ort</td>
          <td>bf16</td>
          <td>bf16</td>
          <td>32</td>
          <td>32</td>
          <td>32</td>
          <td>131072</td>
          <td>131072-32x32x32</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>51</th>
          <td>0.000500</td>
          <td>0.000076</td>
          <td>0.000418</td>
          <td>0.000748</td>
          <td>40</td>
          <td>16</td>
          <td>0.020017</td>
          <td>64</td>
          <td>0.000527</td>
          <td>ort</td>
          <td>bf16</td>
          <td>bf16</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>52</th>
          <td>0.000755</td>
          <td>0.000126</td>
          <td>0.000627</td>
          <td>0.001136</td>
          <td>40</td>
          <td>16</td>
          <td>0.030213</td>
          <td>64</td>
          <td>0.000876</td>
          <td>ort</td>
          <td>bf16</td>
          <td>bf16</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>53</th>
          <td>0.001434</td>
          <td>0.000104</td>
          <td>0.001308</td>
          <td>0.001694</td>
          <td>20</td>
          <td>8</td>
          <td>0.028672</td>
          <td>64</td>
          <td>0.001525</td>
          <td>ort</td>
          <td>bf16</td>
          <td>bf16</td>
          <td>256</td>
          <td>256</td>
          <td>256</td>
          <td>67108864</td>
          <td>67108864-256x256x256</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>54</th>
          <td>0.003611</td>
          <td>0.000176</td>
          <td>0.003354</td>
          <td>0.003970</td>
          <td>10</td>
          <td>4</td>
          <td>0.036113</td>
          <td>64</td>
          <td>0.004095</td>
          <td>ort</td>
          <td>bf16</td>
          <td>bf16</td>
          <td>400</td>
          <td>400</td>
          <td>400</td>
          <td>256000000</td>
          <td>256000000-400x400x400</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>55</th>
          <td>0.005800</td>
          <td>0.000090</td>
          <td>0.005692</td>
          <td>0.005974</td>
          <td>10</td>
          <td>4</td>
          <td>0.058003</td>
          <td>64</td>
          <td>0.006251</td>
          <td>ort</td>
          <td>bf16</td>
          <td>bf16</td>
          <td>512</td>
          <td>512</td>
          <td>512</td>
          <td>536870912</td>
          <td>536870912-512x512x512</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
        <tr>
          <th>56</th>
          <td>0.027627</td>
          <td>0.000088</td>
          <td>0.027478</td>
          <td>0.027772</td>
          <td>10</td>
          <td>4</td>
          <td>0.276274</td>
          <td>64</td>
          <td>0.027796</td>
          <td>ort</td>
          <td>bf16</td>
          <td>bf16</td>
          <td>1024</td>
          <td>1024</td>
          <td>1024</td>
          <td>4294967296</td>
          <td>4294967296-1024x1024x1024</td>
          <td>ORT</td>
          <td>cuda</td>
          <td>x86_64</td>
          <td>None</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 485-487

The errors
++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 487-490

.. code-block:: Python

    for i, e in enumerate(errors):
        print(f"{i+1}/{len(errors)}-{e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    2/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    3/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    4/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    5/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    6/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    7/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    8/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    9/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    10/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    11/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    12/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    13/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    14/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    15/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    16/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    17/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    18/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    19/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    20/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    21/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    22/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    23/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    24/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    25/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    26/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    27/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    28/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    29/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    30/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    31/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    32/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    33/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    34/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    35/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    36/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    37/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    38/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    39/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    40/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    41/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain=''.
    42/84-f8 not available, major=6, tt=17, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='com.microsoft'.
    43/84-(1, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("1.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float),"B": tensor(float),) -> ("C": tensor(float),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    44/84-(1, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("1.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float),"B": tensor(float),) -> ("C": tensor(float),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    45/84-(1, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("1.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float),"B": tensor(float),) -> ("C": tensor(float),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    46/84-(1, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("1.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float),"B": tensor(float),) -> ("C": tensor(float),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    47/84-(1, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("1.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float),"B": tensor(float),) -> ("C": tensor(float),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    48/84-(1, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("1.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float),"B": tensor(float),) -> ("C": tensor(float),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    49/84-(1, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("1.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float),"B": tensor(float),) -> ("C": tensor(float),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    50/84-(10, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("10.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float16),"B": tensor(float16),) -> ("C": tensor(float16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    51/84-(10, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("10.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float16),"B": tensor(float16),) -> ("C": tensor(float16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    52/84-(10, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("10.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float16),"B": tensor(float16),) -> ("C": tensor(float16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    53/84-(10, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("10.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float16),"B": tensor(float16),) -> ("C": tensor(float16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    54/84-(10, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("10.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float16),"B": tensor(float16),) -> ("C": tensor(float16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    55/84-(10, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("10.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float16),"B": tensor(float16),) -> ("C": tensor(float16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    56/84-(10, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("10.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(float16),"B": tensor(float16),) -> ("C": tensor(float16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    57/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    58/84-(16, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("16.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(bfloat16),"B": tensor(bfloat16),) -> ("C": tensor(bfloat16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    59/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    60/84-(16, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("16.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(bfloat16),"B": tensor(bfloat16),) -> ("C": tensor(bfloat16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    61/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    62/84-(16, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("16.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(bfloat16),"B": tensor(bfloat16),) -> ("C": tensor(bfloat16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    63/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    64/84-(16, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("16.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(bfloat16),"B": tensor(bfloat16),) -> ("C": tensor(bfloat16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    65/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    66/84-(16, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("16.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(bfloat16),"B": tensor(bfloat16),) -> ("C": tensor(bfloat16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    67/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    68/84-(16, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("16.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(bfloat16),"B": tensor(bfloat16),) -> ("C": tensor(bfloat16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    69/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    70/84-(16, 'type', ['CUDAExecutionProvider', 'CPUExecutionProvider'], 'com.microsoft', InvalidGraph('[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. In Node, ("16.1.1.com.microsoft..1..1..CUBLAS_COMPUTE_32F..False", GemmFloat8, "com.microsoft", -1) : ("A": tensor(bfloat16),"B": tensor(bfloat16),) -> ("C": tensor(bfloat16),) , Error Unrecognized attribute: rowMajor for operator GemmFloat8'))
    71/84-(16, 'type', ['CPUExecutionProvider'], '', NotImplemented("[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(13) node with name ''"))
    72/84-(16, 'type', ['CPUExecutionProvider'], '', NotImplemented("[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(13) node with name ''"))
    73/84-(16, 'type', ['CPUExecutionProvider'], '', NotImplemented("[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(13) node with name ''"))
    74/84-(16, 'type', ['CPUExecutionProvider'], '', NotImplemented("[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(13) node with name ''"))
    75/84-(16, 'type', ['CPUExecutionProvider'], '', NotImplemented("[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(13) node with name ''"))
    76/84-(16, 'type', ['CPUExecutionProvider'], '', NotImplemented("[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(13) node with name ''"))
    77/84-(16, 'type', ['CPUExecutionProvider'], '', NotImplemented("[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(13) node with name ''"))
    78/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    79/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    80/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    81/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    82/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    83/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.
    84/84-No model for tt=16, provider=['CUDAExecutionProvider', 'CPUExecutionProvider'], domain='onnx_extented.ortops.tutorial.cuda'.




.. GENERATED FROM PYTHON SOURCE LINES 491-493

Summary
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 493-508

.. code-block:: Python


    piv = pivot_table(
        df,
        index=["cost"],
        columns=["provider", "type", "domain", "engine"],
        values=["average", "intime"],
    )
    piv.reset_index(drop=False).to_excel("plot_bench_gemm_ort_summary.xlsx")
    piv.reset_index(drop=False).to_csv("plot_bench_gemm_ort_summary.csv")


    print("summary")
    print(piv)
    piv





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    summary
                 average                                                                                
    provider         cpu                                    cuda                                        
    type             f16                 f32                bf16       f16                 f32          
    domain           ORT                 ORT                 ORT       EXT       ORT       EXT       ORT
    engine            np       ort        np       ort       ort       ort       ort       ort       ort
    cost                                                                                                
    131072      0.000398  0.000041  0.000044  0.000012  0.000430  0.008238  0.000241  0.004890  0.000179
    1048576     0.001907  0.000088  0.000071  0.000023  0.000500  0.012193  0.001076  0.004755  0.000320
    8388608     0.017289  0.000295  0.003370  0.000047  0.000755  0.010802  0.000500  0.005273  0.000817
    67108864    0.115596  0.001413  0.003486  0.000271  0.001434  0.018378  0.001512  0.007044  0.001910
    256000000        NaN  0.003152       NaN  0.001023  0.003611  0.073036  0.002754  0.009082  0.004567
    536870912        NaN  0.010407       NaN  0.002013  0.005800  0.115094  0.004500  0.011514  0.006779
    4294967296       NaN  0.031043       NaN  0.016095  0.027627  0.732097  0.020952  0.035128  0.028854


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead tr th {
            text-align: left;
        }

        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th colspan="9" halign="left">average</th>
        </tr>
        <tr>
          <th>provider</th>
          <th colspan="4" halign="left">cpu</th>
          <th colspan="5" halign="left">cuda</th>
        </tr>
        <tr>
          <th>type</th>
          <th colspan="2" halign="left">f16</th>
          <th colspan="2" halign="left">f32</th>
          <th>bf16</th>
          <th colspan="2" halign="left">f16</th>
          <th colspan="2" halign="left">f32</th>
        </tr>
        <tr>
          <th>domain</th>
          <th colspan="2" halign="left">ORT</th>
          <th colspan="2" halign="left">ORT</th>
          <th>ORT</th>
          <th>EXT</th>
          <th>ORT</th>
          <th>EXT</th>
          <th>ORT</th>
        </tr>
        <tr>
          <th>engine</th>
          <th>np</th>
          <th>ort</th>
          <th>np</th>
          <th>ort</th>
          <th>ort</th>
          <th>ort</th>
          <th>ort</th>
          <th>ort</th>
          <th>ort</th>
        </tr>
        <tr>
          <th>cost</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>131072</th>
          <td>0.000398</td>
          <td>0.000041</td>
          <td>0.000044</td>
          <td>0.000012</td>
          <td>0.000430</td>
          <td>0.008238</td>
          <td>0.000241</td>
          <td>0.004890</td>
          <td>0.000179</td>
        </tr>
        <tr>
          <th>1048576</th>
          <td>0.001907</td>
          <td>0.000088</td>
          <td>0.000071</td>
          <td>0.000023</td>
          <td>0.000500</td>
          <td>0.012193</td>
          <td>0.001076</td>
          <td>0.004755</td>
          <td>0.000320</td>
        </tr>
        <tr>
          <th>8388608</th>
          <td>0.017289</td>
          <td>0.000295</td>
          <td>0.003370</td>
          <td>0.000047</td>
          <td>0.000755</td>
          <td>0.010802</td>
          <td>0.000500</td>
          <td>0.005273</td>
          <td>0.000817</td>
        </tr>
        <tr>
          <th>67108864</th>
          <td>0.115596</td>
          <td>0.001413</td>
          <td>0.003486</td>
          <td>0.000271</td>
          <td>0.001434</td>
          <td>0.018378</td>
          <td>0.001512</td>
          <td>0.007044</td>
          <td>0.001910</td>
        </tr>
        <tr>
          <th>256000000</th>
          <td>NaN</td>
          <td>0.003152</td>
          <td>NaN</td>
          <td>0.001023</td>
          <td>0.003611</td>
          <td>0.073036</td>
          <td>0.002754</td>
          <td>0.009082</td>
          <td>0.004567</td>
        </tr>
        <tr>
          <th>536870912</th>
          <td>NaN</td>
          <td>0.010407</td>
          <td>NaN</td>
          <td>0.002013</td>
          <td>0.005800</td>
          <td>0.115094</td>
          <td>0.004500</td>
          <td>0.011514</td>
          <td>0.006779</td>
        </tr>
        <tr>
          <th>4294967296</th>
          <td>NaN</td>
          <td>0.031043</td>
          <td>NaN</td>
          <td>0.016095</td>
          <td>0.027627</td>
          <td>0.732097</td>
          <td>0.020952</td>
          <td>0.035128</td>
          <td>0.028854</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 509-510

With the dimensions.

.. GENERATED FROM PYTHON SOURCE LINES 510-519

.. code-block:: Python


    pivs = pivot_table(
        df,
        index=["cost_s"],
        columns=["provider", "type", "domain", "engine"],
        values=["average", "intime"],
    )
    print(pivs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                average                                                                                
    provider                        cpu                                    cuda                                        
    type                            f16                 f32                bf16       f16                 f32          
    domain                          ORT                 ORT                 ORT       EXT       ORT       EXT       ORT
    engine                           np       ort        np       ort       ort       ort       ort       ort       ort
    cost_s                                                                                                             
    1048576-64x64x64           0.001907  0.000088  0.000071  0.000023  0.000500  0.012193  0.001076  0.004755  0.000320
    131072-32x32x32            0.000398  0.000041  0.000044  0.000012  0.000430  0.008238  0.000241  0.004890  0.000179
    256000000-400x400x400           NaN  0.003152       NaN  0.001023  0.003611  0.073036  0.002754  0.009082  0.004567
    4294967296-1024x1024x1024       NaN  0.031043       NaN  0.016095  0.027627  0.732097  0.020952  0.035128  0.028854
    536870912-512x512x512           NaN  0.010407       NaN  0.002013  0.005800  0.115094  0.004500  0.011514  0.006779
    67108864-256x256x256       0.115596  0.001413  0.003486  0.000271  0.001434  0.018378  0.001512  0.007044  0.001910
    8388608-128x128x128        0.017289  0.000295  0.003370  0.000047  0.000755  0.010802  0.000500  0.005273  0.000817




.. GENERATED FROM PYTHON SOURCE LINES 520-521

plot

.. GENERATED FROM PYTHON SOURCE LINES 521-543

.. code-block:: Python


    dfi = df[
        df.type.isin({"f32", "f16", "bf16", "e4m3fn", "e5m2"}) & df.engine.isin({"ort"})
    ]
    pivi = pivot_table(
        dfi,
        index=["cost"],
        columns=["type", "domain", "provider", "engine"],
        values="average",
    )

    fig, ax = plt.subplots(1, 2, figsize=(12, 6))
    piv.plot(ax=ax[0], title="Gemm performance\nlower is better", logx=True, logy=True)
    if pivi.shape[0] > 0:
        pivi.plot(
            ax=ax[1],
            title=f"Gemm performance ORT\n{platform.processor()}",
            logx=True,
            logy=True,
        )
    fig.tight_layout()
    fig.savefig("plot_bench_gemm_ort.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_bench_gemm_ort_001.png
   :alt: Gemm performance lower is better, Gemm performance ORT x86_64
   :srcset: /auto_examples/images/sphx_glr_plot_bench_gemm_ort_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 40.217 seconds)


.. _sphx_glr_download_auto_examples_plot_bench_gemm_ort.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bench_gemm_ort.ipynb <plot_bench_gemm_ort.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bench_gemm_ort.py <plot_bench_gemm_ort.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
