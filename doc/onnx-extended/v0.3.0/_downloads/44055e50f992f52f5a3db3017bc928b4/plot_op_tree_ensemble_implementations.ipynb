{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Evaluate different implementation of TreeEnsemble\n\nThis is a simplified benchmark to compare TreeEnsemble implementations (see below)\nRun `python plot_op_tree_ensemble_implementations.py --help` to change the tree\ndimension. Here are the following implementation:\n\n* **ort**: current onnxruntime implementations\n* **custom**: very close implementation of TreeEnsemble from onnxruntime,\n  it allows more options to parallelize. The default is to use the parallelization\n  settings as onnxruntime.\n* **cusopt**: it calls the same implementations as *custom* but\n  with parallelization settings defined through the command line.\n  These settings can be optimized\n  with function :func:`onnx_extended.ortops.optim.optimize.optimize_model`.\n  It is usually possible to gain 10% to 20%.\n* **sparse**: the input matrix used for this test can be as sparse as desired.\n  The *custom* implementations can leverage this sparsity. It reduces the memory\n  peak but it is usually slower and a dense representation of the features.\n* **assembly**: the tree is compiled with\n  [TreeBeard](https://github.com/asprasad/treebeard) and this assembly\n  is called though a custom kernel implemented for this only purpose.\n  The tree is compiled for a particular machine and once it is compiled,\n  the batch size cannot be changed any more. That's why this benchmark\n  only compares one configuration specified in the command line arguments.\n\n## Sparse Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport pickle\nimport os\nimport subprocess\nimport multiprocessing\nfrom typing import Any, Dict, Iterator, Optional, Tuple, Union\nimport warnings\nimport numpy\nimport matplotlib.pyplot as plt\nimport onnx\nfrom onnx import ModelProto, TensorProto\nfrom onnx.helper import make_attribute, make_graph, make_model, make_tensor_value_info\nfrom onnx.reference import ReferenceEvaluator\nfrom pandas import DataFrame\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skl2onnx import to_onnx\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom onnx_extended.ortops.optim.cpu import get_ort_ext_libs\nfrom onnx_extended.ortops.optim.optimize import (\n    change_onnx_operator_domain,\n    get_node_attribute,\n)\nfrom onnx_extended.tools.onnx_nodes import multiply_tree\nfrom onnx_extended.validation.cpu._validation import dense_to_sparse_struct\n\n# from onnx_extended.plotting.benchmark import hhistograms\nfrom onnx_extended.args import get_parsed_args\nfrom onnx_extended.ext_test_case import unit_test_going\nfrom onnx_extended.ext_test_case import measure_time\n\nlogging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n\nscript_args = get_parsed_args(\n    \"plot_op_tree_ensemble_sparse\",\n    description=__doc__,\n    scenarios={\n        \"SHORT\": \"short optimization (default)\",\n        \"LONG\": \"test more options\",\n        \"CUSTOM\": \"use values specified by the command line\",\n    },\n    sparsity=(0.99, \"input sparsity\"),\n    n_features=(2 if unit_test_going() else 512, \"number of features to generate\"),\n    n_trees=(3 if unit_test_going() else 512, \"number of trees to train\"),\n    max_depth=(2 if unit_test_going() else 12, \"max_depth\"),\n    batch_size=(1024 if unit_test_going() else 2048, \"batch size\"),\n    warmup=1 if unit_test_going() else 3,\n    parallel_tree=(128, \"values to try for parallel_tree\"),\n    parallel_tree_N=(256, \"values to try for parallel_tree_N\"),\n    parallel_N=(64, \"values to try for parallel_N\"),\n    batch_size_tree=(4, \"values to try for batch_size_tree\"),\n    batch_size_rows=(4, \"values to try for batch_size_rows\"),\n    train_all_trees=(\n        False,\n        \"train all trees or replicate the first tree with a \"\n        \"random permutation of the threshold\",\n    ),\n    use_node3=(0, \"values to try for use_node3\"),\n    expose=\"\",\n    n_jobs=(\"-1\", \"number of jobs to train the RandomForestRegressor\"),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(\n    batch_size: int,\n    n_features: int,\n    n_trees: int,\n    max_depth: int,\n    sparsity: float,\n    train_all_trees: bool = False,\n) -> Tuple[str, numpy.ndarray, numpy.ndarray]:\n    filename = (\n        f\"plot_op_tree_ensemble_sparse-f{n_features}-{n_trees}-\"\n        f\"d{max_depth}-s{sparsity}-{1 if train_all_trees else 0}.onnx\"\n    )\n    if not os.path.exists(filename):\n        X, y = make_regression(\n            batch_size + 2 ** (max_depth + 1),\n            n_features=n_features,\n            n_targets=1,\n        )\n        y -= y.mean()\n        y /= y.std()\n        mask = numpy.random.rand(*X.shape) <= sparsity\n        X[mask] = 0\n        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n\n        print(f\"Training to get {filename!r} with X.shape={X.shape}\")\n        # To be faster, we train only 1 tree.\n        if train_all_trees:\n            model = RandomForestRegressor(\n                n_trees, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)\n            )\n            model.fit(X[:-batch_size], y[:-batch_size])\n            onx = to_onnx(model, X[:1])\n            skl_name = filename + \".pkl\"\n            with open(skl_name, \"wb\") as f:\n                pickle.dump(model, f)\n        else:\n            model = RandomForestRegressor(\n                1, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)\n            )\n            model.fit(X[:-batch_size], y[:-batch_size])\n            onx = to_onnx(model, X[:1])\n\n            # And wd multiply the trees.\n            node = multiply_tree(onx.graph.node[0], n_trees)\n            onx = make_model(\n                make_graph([node], onx.graph.name, onx.graph.input, onx.graph.output),\n                domain=onx.domain,\n                opset_imports=onx.opset_import,\n            )\n            model = None\n\n        with open(filename, \"wb\") as f:\n            f.write(onx.SerializeToString())\n    else:\n        X, y = make_regression(batch_size, n_features=n_features, n_targets=1)\n        mask = numpy.random.rand(*X.shape) <= sparsity\n        X[mask] = 0\n        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n        skl_name = filename + \".pkl\"\n        if os.path.exists(skl_name):\n            with open(skl_name, \"rb\") as f:\n                model = pickle.load(f)\n        else:\n            model = None\n\n    Xb, yb = X[-batch_size:].copy(), y[-batch_size:].copy()\n    return filename, Xb, yb, model\n\n\ndef measure_sparsity(x):\n    f = x.flatten()\n    return float((f == 0).astype(numpy.int64).sum()) / float(x.size)\n\n\nbatch_size = script_args.batch_size\nn_features = script_args.n_features\nn_trees = script_args.n_trees\nmax_depth = script_args.max_depth\nsparsity = script_args.sparsity\nwarmup = script_args.warmup\ntrain_all_trees = script_args.train_all_trees in (1, \"1\", True, \"True\")\n\nprint(f\"batch_size={batch_size}\")\nprint(f\"n_features={n_features}\")\nprint(f\"n_trees={n_trees}\")\nprint(f\"max_depth={max_depth}\")\nprint(f\"sparsity={sparsity}\")\nprint(f\"warmup={warmup}\")\nprint(f\"train_all_trees={train_all_trees} - {script_args.train_all_trees!r}\")\n\nfilename, Xb, yb, model_skl = train_model(\n    batch_size,\n    n_features,\n    n_trees,\n    max_depth,\n    sparsity,\n    train_all_trees=train_all_trees,\n)\n\nprint(f\"Xb.shape={Xb.shape}\")\nprint(f\"yb.shape={yb.shape}\")\nprint(f\"measured sparsity={measure_sparsity(Xb)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compile_tree(\n    llc_exe: str,\n    filename: str,\n    onx: ModelProto,\n    batch_size: int,\n    n_features: int,\n    tree_tile_size: int = 8,\n    pipeline_width: int = 8,\n    reorder_tree_by_depth: bool = True,\n    representation_type: str = \"sparse\",\n    n_cores: Optional[int] = None,\n    verbose: int = 0,\n) -> str:\n    \"\"\"\n    Compiles a tree with `TreeBeard <https://github.com/asprasad/treebeard>`_.\n\n    :param llc_exe: path to `llc <https://llvm.org/docs/CommandGuide/llc.html>`_\n        executable\n    :param filename: assembly name, the outcome of the compilation\n    :param onx: model to compile, it should contain only one node with a\n        TreeEssembleRegressor.\n    :param batch_size: batch size\n    :param n_features: number of features as it cannot be guessed only from the\n        tree definition\n    :param tree_tile_size: compilation parameters\n    :param pipeline_width: compilation parameters\n    :param reorder_tree_by_depth: compilation parameters\n    :param representation_type: compilation parameters\n    :param n_cores: optimized for this number of cores,\n        if unspecified, it uses `multiprocessing.cpu_count()`\n    :param verbose: to show some progress\n    :return: path to the generated assembly\n    \"\"\"\n    if verbose:\n        print(\"[compile_tree] import treebeard\")\n    import treebeard\n\n    if verbose:\n        print(\n            f\"[compile_tree] treebeard set options, \"\n            f\"batch_size={batch_size}, tree_tile_size={tree_tile_size}\"\n        )\n    compiler_options = treebeard.CompilerOptions(batch_size, tree_tile_size)\n\n    compiler_options.SetNumberOfCores(n_cores or multiprocessing.cpu_count())\n    compiler_options.SetMakeAllLeavesSameDepth(pipeline_width)\n    compiler_options.SetReorderTreesByDepth(reorder_tree_by_depth)\n    compiler_options.SetNumberOfFeatures(n_features)\n    assert 8 < batch_size\n    compiler_options.SetPipelineWidth(8)\n\n    if verbose:\n        print(f\"[compile_tree] write filename={filename!r}\")\n\n    # let's remove nodes_hitrates to avoid a warning before saving the model\n    for node in onx.graph.node:\n        if node.op_type == \"TreeEnsembleRegressor\":\n            found = -1\n            for i in range(len(node.attribute)):\n                if node.attribute[i].name == \"nodes_hitrates\":\n                    found = i\n            if found >= 0:\n                del node.attribute[found]\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())\n\n    onnx_model_path = os.path.abspath(filename)\n    if verbose:\n        print(\n            f\"[compile_tree] treebeard context with onnx_model_path={onnx_model_path!r}\"\n        )\n    tbContext = treebeard.TreebeardContext(onnx_model_path, \"\", compiler_options)\n    tbContext.SetRepresentationType(representation_type)\n    tbContext.SetInputFiletype(\"onnx_file\")\n\n    llvm_file_path = f\"{os.path.splitext(onnx_model_path)[0]}.ll\"\n    if verbose:\n        print(f\"[compile_tree] LLVM dump into {llvm_file_path!r}\")\n    error = tbContext.DumpLLVMIR(llvm_file_path)\n    if error:\n        raise RuntimeError(\n            f\"Failed to dump LLVM IR in {llvm_file_path!r}, error={error}.\"\n        )\n    if not os.path.exists(llvm_file_path):\n        raise FileNotFoundError(f\"Unable to find {llvm_file_path!r}.\")\n\n    # Run LLC\n    asm_file_path = f\"{os.path.splitext(onnx_model_path)[0]}.s\"\n    if verbose:\n        print(f\"[compile_tree] llc={llc_exe!r}\")\n        print(f\"[compile_tree] run LLC into {llvm_file_path!r}\")\n    subprocess.run(\n        [\n            llc_exe,\n            llvm_file_path,\n            \"-O3\",\n            \"-march=x86-64\",\n            \"-mcpu=native\",\n            \"--relocation-model=pic\",\n            \"-o\",\n            asm_file_path,\n        ]\n    )\n\n    # Run CLANG\n    so_file_path = f\"{os.path.splitext(onnx_model_path)[0]}.so\"\n    if verbose:\n        print(f\"[compile_tree] run clang into {so_file_path!r}\")\n    subprocess.run(\n        [\"clang\", \"-shared\", asm_file_path, \"-fopenmp=libomp\", \"-o\", so_file_path]\n    )\n    if verbose:\n        print(\"[compile_tree] done.\")\n    return so_file_path\n\n\ndef make_ort_assembly_session(\n    onx: ModelProto, batch_size: int, n_features: int, verbose: bool = False, **kwargs\n) -> Any:\n    \"\"\"\n    Creates an instance of `onnxruntime.InferenceSession` using an assembly generated\n    by `TreeBeard <https://github.com/asprasad/treebeard>`_.\n\n    :param onx: model to compile\n    :param batch_size: batch size\n    :param n_features: number of features as it cannot be guessed only from the\n        tree definition\n    :param verbose: verbosity\n    :param kwargs: any additional parameters sent to function `compile_tree`\n    :return: `onnxruntime.InferenceSession`\n    \"\"\"\n    from onnxruntime import InferenceSession, SessionOptions\n    from onnx_extended.ortops.tutorial.cpu import get_ort_ext_libs as lib_tuto\n\n    llc_exe = os.environ.get(\"TEST_LLC_EXE\", \"SKIP\")\n    if llc_exe == \"SKIP\":\n        warnings.warn(\"Unable to find environment variable 'TEST_LLC_EXE'.\")\n        return None\n\n    filename = \"plot_op_tree_ensemble_implementation.onnx\"\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())\n    onx = onnx.load(filename)\n    assembly_name = compile_tree(\n        llc_exe, filename, onx, batch_size, n_features, verbose=verbose, **kwargs\n    )\n\n    # assembly\n    print(\"change\")\n    for node in onx.graph.node:\n        if node.op_type == \"TreeEnsembleRegressor\":\n            node.op_type = \"TreeEnsembleAssemblyRegressor\"\n            node.domain = \"onnx_extented.ortops.tutorial.cpu\"\n            del node.attribute[:]\n            new_add = make_attribute(\"assembly\", assembly_name)\n            node.attribute.append(new_add)\n\n    d = onx.opset_import.add()\n    d.domain = \"onnx_extented.ortops.tutorial.cpu\"\n    d.version = 1\n\n    r = lib_tuto()\n    opts = SessionOptions()\n    opts.register_custom_ops_library(r[0])\n    sess_assembly = InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n    return sess_assembly\n\n\ndef transform_model(model, use_sparse=False, **kwargs):\n    onx = ModelProto()\n    onx.ParseFromString(model.SerializeToString())\n    att = get_node_attribute(onx.graph.node[0], \"nodes_modes\")\n    modes = \",\".join(map(lambda s: s.decode(\"ascii\"), att.strings)).replace(\n        \"BRANCH_\", \"\"\n    )\n    if use_sparse and \"new_op_type\" not in kwargs:\n        kwargs[\"new_op_type\"] = \"TreeEnsembleRegressorSparse\"\n    if use_sparse:\n        # with sparse tensor, missing value means 0\n        att = get_node_attribute(onx.graph.node[0], \"nodes_values\")\n        thresholds = numpy.array(att.floats, dtype=numpy.float32)\n        missing_true = (thresholds >= 0).astype(numpy.int64)\n        kwargs[\"nodes_missing_value_tracks_true\"] = missing_true\n    new_onx = change_onnx_operator_domain(\n        onx,\n        op_type=\"TreeEnsembleRegressor\",\n        op_domain=\"ai.onnx.ml\",\n        new_op_domain=\"onnx_extented.ortops.optim.cpu\",\n        nodes_modes=modes,\n        **kwargs,\n    )\n    if use_sparse:\n        del new_onx.graph.input[:]\n        new_onx.graph.input.append(\n            make_tensor_value_info(\"X\", TensorProto.FLOAT, (None,))\n        )\n    return new_onx\n\n\ndef enumerate_implementations(\n    onx: ModelProto,\n    X: \"Tensor\",  # noqa: F821\n    parallel_settings: Optional[Dict[str, int]] = None,\n    treebeard_settings: Optional[Dict[str, Union[int, str]]] = None,\n    verbose: bool = False,\n) -> Iterator[\n    Tuple[str, ModelProto, \"onnxruntime.InferenceSession\", \"Tensor\"]  # noqa: F821\n]:\n    \"\"\"\n    Creates all the InferenceSession.\n\n    :param onx: model\n    :param X: example of an input tensor, dimension should not change\n    :param parallel_settings: parallelisation settings for *cusopt*, *sparse*\n    :param treebeard_settings: settings for treebeard compilation\n    :return: see annotation\n    \"\"\"\n    providers = [\"CPUExecutionProvider\"]\n    yield (\n        \"ort\",\n        onx,\n        InferenceSession(onx.SerializeToString(), providers=providers),\n        X,\n    )\n\n    r = get_ort_ext_libs()\n    opts = SessionOptions()\n    if r is not None:\n        opts.register_custom_ops_library(r[0])\n\n    tr = transform_model(onx)\n    yield (\n        \"custom\",\n        tr,\n        InferenceSession(tr.SerializeToString(), opts, providers=providers),\n        X,\n    )\n\n    tr = transform_model(onx, **parallel_settings)\n    yield (\n        \"cusopt\",\n        tr,\n        InferenceSession(tr.SerializeToString(), opts, providers=providers),\n        X,\n    )\n\n    Xsp = dense_to_sparse_struct(X)\n    tr = transform_model(onx, use_sparse=True, **parallel_settings)\n    yield (\n        \"sparse\",\n        tr,\n        InferenceSession(tr.SerializeToString(), opts, providers=providers),\n        Xsp,\n    )\n\n    sess = make_ort_assembly_session(\n        onx,\n        batch_size=X.shape[0],\n        n_features=X.shape[1],\n        verbose=verbose,\n        **treebeard_settings,\n    )\n    yield (\"assembly\", onx, sess, X)\n\n\nparallel_settings = dict(\n    parallel_tree=40,\n    parallel_tree_N=128,\n    parallel_N=50,\n    batch_size_tree=4,\n    batch_size_rows=4,\n    use_node3=0,\n)\ntreebeard_settings = dict()\n\n\nonx = onnx.load(filename)\nsessions = []\n\nprint(\"----- warmup\")\nfor name, onx2, sess, tensor in enumerate_implementations(\n    onx,\n    Xb,\n    parallel_settings=parallel_settings,\n    treebeard_settings=treebeard_settings,\n    verbose=1 if __name__ == \"__main__\" else 0,\n):\n    if sess is None:\n        continue\n    with open(f\"plot_op_tree_ensemble_implementations_{name}.onnx\", \"wb\") as f:\n        f.write(onx2.SerializeToString())\n    sessions.append((name, sess, tensor))\n    print(f\"run {name!r} - shape={tensor.shape}\")\n    feeds = {\"X\": tensor}\n    sess.run(None, feeds)\nprint(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark implementations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\nbaseline = None\nif model_skl:\n    print(\"computing the expected values with scikit-learn\")\n    expected_values = model_skl.predict(Xb)\nelse:\n    print(\"computing the expected values with ReferenceEvaluator\")\n    ref = ReferenceEvaluator(onx)\n    expected_values = ref.run(None, {\"X\": Xb})[0]\n\nprint(\"----- measure time\")\nfor name, sess, tensor in sessions:\n    print(f\"run {name!r}\")\n    feeds = {\"X\": tensor}\n    output = sess.run(None, feeds)[0]\n    if baseline is None:\n        baseline = output\n        disc = 0\n        max_disc = 0\n    else:\n        diff = numpy.abs(output - baseline).ravel()\n        disc = diff.mean()\n        max_disc = diff.max()\n    obs = measure_time(\n        lambda: sess.run(None, feeds),\n        repeat=script_args.repeat,\n        number=script_args.number,\n        warmup=script_args.warmup,\n    )\n    obs[\"name\"] = name\n    obs[\"disc_mean\"] = disc\n    obs[\"disc_max\"] = max_disc\n    diff = numpy.abs(output.ravel() - expected_values.ravel())\n    obs[\"err_mean\"] = diff.mean()\n    obs[\"err_max\"] = diff.max()\n    data.append(obs)\n\nprint(\"done.\")\n\ndf = DataFrame(data)\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plots.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "has_skl = \"err_mean\" in df.columns\nfig, ax = plt.subplots(1, 3 if has_skl else 2, figsize=(10, 4), sharey=True)\ndf[[\"name\", \"average\"]].set_index(\"name\").plot.barh(\n    ax=ax[0],\n    title=\"Compare implementations of TreeEnsemble\\nlower is better\",\n    xerr=[df[\"min_exec\"], df[\"max_exec\"]],\n)\ndf[[\"name\", \"disc_mean\"]].set_index(\"name\").plot.barh(\n    ax=ax[1],\n    title=\"Average discrepancies with ORT (L1)\\nlower is better\",\n    xerr=[df[\"disc_max\"].values * 0, df[\"disc_max\"].values],\n)\nif has_skl:\n    df[[\"name\", \"err_mean\"]].set_index(\"name\").plot.barh(\n        ax=ax[2],\n        title=\"Average discrepancies with SKL (L1)\\nlower is better\",\n        xerr=[df[\"err_max\"].values * 0, df[\"err_max\"].values],\n    )\nfig.tight_layout()\nfig.savefig(\"plot_tree_ensemble_implementations.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}