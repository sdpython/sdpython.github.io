{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TreeEnsemble, dense, and sparse\n\nThe example benchmarks the sparse implementation for TreeEnsemble.\nThe default set of optimized parameters is very short and is meant to be executed\nfast. Many more parameters can be tried.\n\n::\n\n    python plot_op_tree_ensemble_sparse --scenario=LONG\n\nTo change the training parameters:\n\n::\n\n    python plot_op_tree_ensemble_sparse.py\n        --n_trees=100\n        --max_depth=10\n        --n_features=50\n        --sparsity=0.9\n        --batch_size=100000\n    \nAnother example with a full list of parameters:\n\n    python plot_op_tree_ensemble_sparse.py\n        --n_trees=100\n        --max_depth=10\n        --n_features=50\n        --batch_size=100000\n        --sparsity=0.9\n        --tries=3\n        --scenario=CUSTOM\n        --parallel_tree=80,40\n        --parallel_tree_N=128,64\n        --parallel_N=50,25\n        --batch_size_tree=1,2\n        --batch_size_rows=1,2\n        --use_node3=0\n\nAnother example:\n\n::\n\n    python plot_op_tree_ensemble_sparse.py\n        --n_trees=100 --n_features=10 --batch_size=10000 --max_depth=8 -s SHORT        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport os\nimport timeit\nfrom typing import Tuple\nimport numpy\nimport onnx\nfrom onnx import ModelProto, TensorProto\nfrom onnx.helper import make_graph, make_model, make_tensor_value_info\nfrom pandas import DataFrame, concat\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skl2onnx import to_onnx\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom onnx_array_api.plotting.text_plot import onnx_simple_text_plot\nfrom onnx_extended.ortops.optim.cpu import get_ort_ext_libs\nfrom onnx_extended.ortops.optim.optimize import (\n    change_onnx_operator_domain,\n    get_node_attribute,\n    optimize_model,\n)\nfrom onnx_extended.tools.onnx_nodes import multiply_tree\nfrom onnx_extended.validation.cpu._validation import dense_to_sparse_struct\nfrom onnx_extended.plotting.benchmark import hhistograms\nfrom onnx_extended.args import get_parsed_args\nfrom onnx_extended.ext_test_case import unit_test_going\n\nlogging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n\nscript_args = get_parsed_args(\n    \"plot_op_tree_ensemble_sparse\",\n    description=__doc__,\n    scenarios={\n        \"SHORT\": \"short optimization (default)\",\n        \"LONG\": \"test more options\",\n        \"CUSTOM\": \"use values specified by the command line\",\n    },\n    sparsity=(0.99, \"input sparsity\"),\n    n_features=(2 if unit_test_going() else 500, \"number of features to generate\"),\n    n_trees=(3 if unit_test_going() else 10, \"number of trees to train\"),\n    max_depth=(2 if unit_test_going() else 10, \"max_depth\"),\n    batch_size=(1000 if unit_test_going() else 1000, \"batch size\"),\n    parallel_tree=(\"80,160,40\", \"values to try for parallel_tree\"),\n    parallel_tree_N=(\"256,128,64\", \"values to try for parallel_tree_N\"),\n    parallel_N=(\"100,50,25\", \"values to try for parallel_N\"),\n    batch_size_tree=(\"2,4,8\", \"values to try for batch_size_tree\"),\n    batch_size_rows=(\"2,4,8\", \"values to try for batch_size_rows\"),\n    use_node3=(\"0,1\", \"values to try for use_node3\"),\n    expose=\"\",\n    n_jobs=(\"-1\", \"number of jobs to train the RandomForestRegressor\"),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(\n    batch_size: int, n_features: int, n_trees: int, max_depth: int, sparsity: float\n) -> Tuple[str, numpy.ndarray, numpy.ndarray]:\n    filename = (\n        f\"plot_op_tree_ensemble_sparse-f{n_features}-{n_trees}-\"\n        f\"d{max_depth}-s{sparsity}.onnx\"\n    )\n    if not os.path.exists(filename):\n        X, y = make_regression(\n            batch_size + max(batch_size, 2 ** (max_depth + 1)),\n            n_features=n_features,\n            n_targets=1,\n        )\n        mask = numpy.random.rand(*X.shape) <= sparsity\n        X[mask] = 0\n        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n\n        print(f\"Training to get {filename!r} with X.shape={X.shape}\")\n        # To be faster, we train only 1 tree.\n        model = RandomForestRegressor(\n            1, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)\n        )\n        model.fit(X[:-batch_size], y[:-batch_size])\n        onx = to_onnx(model, X[:1])\n\n        # And wd multiply the trees.\n        node = multiply_tree(onx.graph.node[0], n_trees)\n        onx = make_model(\n            make_graph([node], onx.graph.name, onx.graph.input, onx.graph.output),\n            domain=onx.domain,\n            opset_imports=onx.opset_import,\n        )\n\n        with open(filename, \"wb\") as f:\n            f.write(onx.SerializeToString())\n    else:\n        X, y = make_regression(batch_size, n_features=n_features, n_targets=1)\n        mask = numpy.random.rand(*X.shape) <= sparsity\n        X[mask] = 0\n        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n    Xb, yb = X[-batch_size:].copy(), y[-batch_size:].copy()\n    return filename, Xb, yb\n\n\ndef measure_sparsity(x):\n    f = x.flatten()\n    return float((f == 0).astype(numpy.int64).sum()) / float(x.size)\n\n\nbatch_size = script_args.batch_size\nn_features = script_args.n_features\nn_trees = script_args.n_trees\nmax_depth = script_args.max_depth\nsparsity = script_args.sparsity\n\nprint(f\"batch_size={batch_size}\")\nprint(f\"n_features={n_features}\")\nprint(f\"n_trees={n_trees}\")\nprint(f\"max_depth={max_depth}\")\nprint(f\"sparsity={sparsity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename, Xb, yb = train_model(batch_size, n_features, n_trees, max_depth, sparsity)\n\nprint(f\"Xb.shape={Xb.shape}\")\nprint(f\"yb.shape={yb.shape}\")\nprint(f\"measured sparsity={measure_sparsity(Xb)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite the onnx file to use a different kernel\n\nThe custom kernel is mapped to a custom operator with the same name\nthe attributes and domain = `\"onnx_extented.ortops.optim.cpu\"`.\nWe call a function to do that replacement.\nFirst the current model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(filename, \"rb\") as f:\n    onx = onnx.load(f)\nprint(onnx_simple_text_plot(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then the modified model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def transform_model(model, use_sparse=False, **kwargs):\n    onx = ModelProto()\n    onx.ParseFromString(model.SerializeToString())\n    att = get_node_attribute(onx.graph.node[0], \"nodes_modes\")\n    modes = \",\".join(map(lambda s: s.decode(\"ascii\"), att.strings)).replace(\n        \"BRANCH_\", \"\"\n    )\n    if use_sparse and \"new_op_type\" not in kwargs:\n        kwargs[\"new_op_type\"] = \"TreeEnsembleRegressorSparse\"\n    if use_sparse:\n        # with sparse tensor, missing value means 0\n        att = get_node_attribute(onx.graph.node[0], \"nodes_values\")\n        thresholds = numpy.array(att.floats, dtype=numpy.float32)\n        missing_true = (thresholds >= 0).astype(numpy.int64)\n        kwargs[\"nodes_missing_value_tracks_true\"] = missing_true\n    new_onx = change_onnx_operator_domain(\n        onx,\n        op_type=\"TreeEnsembleRegressor\",\n        op_domain=\"ai.onnx.ml\",\n        new_op_domain=\"onnx_extented.ortops.optim.cpu\",\n        nodes_modes=modes,\n        **kwargs,\n    )\n    if use_sparse:\n        del new_onx.graph.input[:]\n        new_onx.graph.input.append(\n            make_tensor_value_info(\"X\", TensorProto.FLOAT, (None,))\n        )\n    return new_onx\n\n\nprint(\"Tranform model to add a custom node.\")\nonx_modified = transform_model(onx)\nprint(f\"Save into {filename + 'modified.onnx'!r}.\")\nwith open(filename + \"modified.onnx\", \"wb\") as f:\n    f.write(onx_modified.SerializeToString())\nprint(\"done.\")\nprint(onnx_simple_text_plot(onx_modified))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same with sparse.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Same transformation but with sparse.\")\nonx_modified_sparse = transform_model(onx, use_sparse=True)\nprint(f\"Save into {filename + 'modified.sparse.onnx'!r}.\")\nwith open(filename + \"modified.sparse.onnx\", \"wb\") as f:\n    f.write(onx_modified_sparse.SerializeToString())\nprint(\"done.\")\nprint(onnx_simple_text_plot(onx_modified_sparse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing onnxruntime and the custom kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Loading {filename!r}\")\nsess_ort = InferenceSession(filename, providers=[\"CPUExecutionProvider\"])\n\nr = get_ort_ext_libs()\nprint(f\"Creating SessionOptions with {r!r}\")\nopts = SessionOptions()\nif r is not None:\n    opts.register_custom_ops_library(r[0])\n\nprint(f\"Loading modified {filename!r}\")\nsess_cus = InferenceSession(\n    onx_modified.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n)\n\nprint(f\"Loading modified sparse {filename!r}\")\nsess_cus_sparse = InferenceSession(\n    onx_modified_sparse.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n)\n\n\nprint(f\"Running once with shape {Xb.shape}.\")\nbase = sess_ort.run(None, {\"X\": Xb})[0]\n\nprint(f\"Running modified with shape {Xb.shape}.\")\ngot = sess_cus.run(None, {\"X\": Xb})[0]\nprint(\"done.\")\n\nXb_sp = dense_to_sparse_struct(Xb)\nprint(f\"Running modified sparse with shape {Xb_sp.shape}.\")\ngot_sparse = sess_cus_sparse.run(None, {\"X\": Xb_sp})[0]\nprint(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discrepancies?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diff = numpy.abs(base - got).max()\nprint(f\"Discrepancies: {diff}\")\n\ndiff = numpy.abs(base - got_sparse).max()\nprint(f\"Discrepancies sparse: {diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple verification\n\nBaseline with onnxruntime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t1 = timeit.timeit(lambda: sess_ort.run(None, {\"X\": Xb}), number=50)\nprint(f\"baseline: {t1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t2 = timeit.timeit(lambda: sess_cus.run(None, {\"X\": Xb}), number=50)\nprint(f\"new time: {t2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom sparse implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t3 = timeit.timeit(lambda: sess_cus_sparse.run(None, {\"X\": Xb_sp}), number=50)\nprint(f\"new time sparse: {t3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time for comparison\n\nThe custom kernel supports the same attributes as *TreeEnsembleRegressor*\nplus new ones to tune the parallelization. They can be seen in\n[tree_ensemble.cc](https://github.com/sdpython/onnx-extended/\nblob/main/onnx_extended/ortops/optim/cpu/tree_ensemble.cc#L102).\nLet's try out many possibilities.\nThe default values are the first ones.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if unit_test_going():\n    optim_params = dict(\n        parallel_tree=[40],  # default is 80\n        parallel_tree_N=[128],  # default is 128\n        parallel_N=[50, 25],  # default is 50\n        batch_size_tree=[1],  # default is 1\n        batch_size_rows=[1],  # default is 1\n        use_node3=[0],  # default is 0\n    )\nelif script_args.scenario in (None, \"SHORT\"):\n    optim_params = dict(\n        parallel_tree=[80, 40],  # default is 80\n        parallel_tree_N=[128, 64],  # default is 128\n        parallel_N=[50, 25],  # default is 50\n        batch_size_tree=[1],  # default is 1\n        batch_size_rows=[1],  # default is 1\n        use_node3=[0],  # default is 0\n    )\nelif script_args.scenario == \"LONG\":\n    optim_params = dict(\n        parallel_tree=[80, 160, 40],\n        parallel_tree_N=[256, 128, 64],\n        parallel_N=[100, 50, 25],\n        batch_size_tree=[1, 2, 4, 8],\n        batch_size_rows=[1, 2, 4, 8],\n        use_node3=[0, 1],\n    )\nelif script_args.scenario == \"CUSTOM\":\n    optim_params = dict(\n        parallel_tree=list(int(i) for i in script_args.parallel_tree.split(\",\")),\n        parallel_tree_N=list(int(i) for i in script_args.parallel_tree_N.split(\",\")),\n        parallel_N=list(int(i) for i in script_args.parallel_N.split(\",\")),\n        batch_size_tree=list(int(i) for i in script_args.batch_size_tree.split(\",\")),\n        batch_size_rows=list(int(i) for i in script_args.batch_size_rows.split(\",\")),\n        use_node3=list(int(i) for i in script_args.use_node3.split(\",\")),\n    )\nelse:\n    raise ValueError(\n        f\"Unknown scenario {script_args.scenario!r}, use --help to get them.\"\n    )\n\ncmds = []\nfor att, value in optim_params.items():\n    cmds.append(f\"--{att}={','.join(map(str, value))}\")\nprint(\"Full list of optimization parameters:\")\nprint(\" \".join(cmds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the optimization for dense\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_session(onx):\n    opts = SessionOptions()\n    r = get_ort_ext_libs()\n    if r is None:\n        raise RuntimeError(\"No custom implementation available.\")\n    opts.register_custom_ops_library(r[0])\n    return InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n\nres = optimize_model(\n    onx,\n    feeds={\"X\": Xb},\n    transform=transform_model,\n    session=create_session,\n    baseline=lambda onx: InferenceSession(\n        onx.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n    ),\n    params=optim_params,\n    verbose=True,\n    number=script_args.number,\n    repeat=script_args.repeat,\n    warmup=script_args.warmup,\n    sleep=script_args.sleep,\n    n_tries=script_args.tries,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the optimization for sparse\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res_sparse = optimize_model(\n    onx,\n    feeds={\"X\": Xb_sp},\n    transform=lambda *args, **kwargs: transform_model(*args, use_sparse=True, **kwargs),\n    session=create_session,\n    params=optim_params,\n    verbose=True,\n    number=script_args.number,\n    repeat=script_args.repeat,\n    warmup=script_args.warmup,\n    sleep=script_args.sleep,\n    n_tries=script_args.tries,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_dense = DataFrame(res)\ndf_dense[\"input\"] = \"dense\"\ndf_sparse = DataFrame(res_sparse)\ndf_sparse[\"input\"] = \"sparse\"\ndf = concat([df_dense, df_sparse], axis=0)\ndf.to_csv(\"plot_op_tree_ensemble_sparse.csv\", index=False)\ndf.to_excel(\"plot_op_tree_ensemble_sparse.xlsx\", index=False)\nprint(df.columns)\nprint(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sorting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "small_df = df.drop(\n    [\n        \"min_exec\",\n        \"max_exec\",\n        \"repeat\",\n        \"number\",\n        \"context_size\",\n        \"n_exp_name\",\n    ],\n    axis=1,\n).sort_values(\"average\")\nprint(small_df.head(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worst\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(small_df.tail(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skeys = \",\".join(optim_params.keys())\ntitle = f\"TreeEnsemble tuning, n_tries={script_args.tries}\\n{skeys}\\nlower is better\"\nax = hhistograms(df, title=title, keys=(\"input\", \"name\"))\nfig = ax.get_figure()\nfig.savefig(\"plot_op_tree_ensemble_sparse.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}