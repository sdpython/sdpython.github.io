
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_bench_gemm_ort.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_bench_gemm_ort.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_bench_gemm_ort.py:


.. _l-example-gemm-ort-f8:


Measuring performance about Gemm with onnxruntime
=================================================

The benchmark measures the performance of Gemm for different
types and configuration. That includes a custom operator
only available on CUDA calling function :epkg:`cublasLtMatmul`.
This function offers many options.

.. GENERATED FROM PYTHON SOURCE LINES 13-99

.. code-block:: Python


    import pprint
    import platform
    from itertools import product
    import numpy
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    from pandas import DataFrame, pivot_table
    from onnx import TensorProto
    from onnx.helper import (
        make_model,
        make_node,
        make_graph,
        make_tensor_value_info,
        make_opsetid,
    )
    from onnx.checker import check_model
    from onnx.numpy_helper import from_array
    from onnx.reference import ReferenceEvaluator
    from onnxruntime import InferenceSession, SessionOptions, get_available_providers
    from onnxruntime.capi._pybind_state import (
        OrtValue as C_OrtValue,
        OrtDevice as C_OrtDevice,
    )
    from onnxruntime.capi.onnxruntime_pybind11_state import (
        Fail,
        NotImplemented,
        InvalidGraph,
        InvalidArgument,
    )

    try:
        from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    except ImportError:
        onnx_simple_text_plot = str
    try:
        from onnx_extended.reference import CReferenceEvaluator
    except ImportError:
        CReferenceEvaluator = ReferenceEvaluator
    from onnx_extended.args import get_parsed_args
    from onnx_extended.ext_test_case import unit_test_going, measure_time

    try:
        from onnx_extended.validation.cuda.cuda_example_py import get_device_prop
        from onnx_extended.ortops.tutorial.cuda import get_ort_ext_libs

        has_cuda = True
    except ImportError:

        def get_device_prop():
            return {"name": "CPU"}

        def get_ort_ext_libs():
            return None

        has_cuda = False

    default_dims = (
        "32,32,32;64,64,64;128,128,128;256,256,256;"
        "400,400,400;512,512,512;1024,1024,1024"
    )
    if has_cuda:
        prop = get_device_prop()
        if prop.get("major", 0) >= 7:
            default_dims += ";2048,2048,2048;4096,4096,4096"
        if prop.get("major", 0) >= 9:
            default_dims += ";16384,16384,16384"


    script_args = get_parsed_args(
        "plot_bench_gemm_ort",
        description=__doc__,
        dims=(
            "32,32,32;64,64,64" if unit_test_going() else default_dims,
            "square matrix dimensions to try, comma separated values",
        ),
        types=(
            "FLOAT" if unit_test_going() else "FLOAT8E4M3FN,FLOAT,FLOAT16,BFLOAT16",
            "element type to teest",
        ),
        number=2 if unit_test_going() else 4,
        repeat=2 if unit_test_going() else 10,
        warmup=2 if unit_test_going() else 5,
        expose="repeat,number,warmup",
    )








.. GENERATED FROM PYTHON SOURCE LINES 100-102

Device properties
+++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 102-111

.. code-block:: Python



    if has_cuda:
        properties = get_device_prop()
        pprint.pprint(properties)
    else:
        properties = {"major": 0}






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'clockRate': 2010000,
     'computeMode': 0,
     'concurrentKernels': 1,
     'isMultiGpuBoard': 0,
     'major': 8,
     'maxThreadsPerBlock': 1024,
     'minor': 9,
     'multiProcessorCount': 24,
     'name': 'NVIDIA GeForce RTX 4060 Laptop GPU',
     'sharedMemPerBlock': 49152,
     'totalConstMem': 65536,
     'totalGlobalMem': 8585281536}




.. GENERATED FROM PYTHON SOURCE LINES 112-119

Model to benchmark
++++++++++++++++++

It includes one Gemm. The operator changes.
It can the regular Gemm, a custom Gemm from domain `com.microsoft`
or a custom implementation from domain
`onnx_extended.ortops.tutorial.cuda`.

.. GENERATED FROM PYTHON SOURCE LINES 119-199

.. code-block:: Python



    def create_model(
        mat_type=TensorProto.FLOAT, provider="CUDAExecutionProvider", domain="com.microsoft"
    ):
        A = make_tensor_value_info("A", mat_type, [None, None])
        B = make_tensor_value_info("B", mat_type, [None, None])
        outputs = [make_tensor_value_info("C", mat_type, [None, None])]
        inits = []
        if domain != "":
            if provider != "CUDAExecutionProvider":
                return None
            f8 = False
            if domain == "com.microsoft":
                op_name = "GemmFloat8"
                computeType = "CUBLAS_COMPUTE_32F"
                node_output = ["C"]
            elif mat_type == TensorProto.FLOAT:
                op_name = "CustomGemmFloat"
                computeType = "CUBLAS_COMPUTE_32F_FAST_TF32"
                node_output = ["C"]
            elif mat_type == TensorProto.FLOAT16:
                op_name = "CustomGemmFloat16"
                computeType = "CUBLAS_COMPUTE_16F"
                node_output = ["C"]
            elif mat_type in (TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2):
                f8 = True
                op_name = "CustomGemmFloat8E4M3FN"
                computeType = "CUBLAS_COMPUTE_32F"
                node_output = ["C"]
                outputs = [
                    make_tensor_value_info("C", TensorProto.FLOAT16, [None, None]),
                ]
                inits.append(from_array(numpy.array([1], dtype=numpy.float32), name="I"))
            else:
                return None
            node_kw = dict(
                alpha=1.0,
                transB=1,
                domain=domain,
                computeType=computeType,
                fastAccumulationMode=1,
                rowMajor=0 if op_name.startswith("CustomGemmFloat") else 1,
            )
            node_kw["name"] = (
                f"{mat_type}.{len(node_output)}.{len(outputs)}."
                f"{domain}..{node_kw['rowMajor']}.."
                f"{node_kw['fastAccumulationMode']}..{node_kw['computeType']}.."
                f"{f8}"
            )
            node_inputs = ["A", "B"]
            if f8:
                node_inputs.append("")
                node_inputs.extend(["I"] * 3)
            nodes = [make_node(op_name, node_inputs, node_output, **node_kw)]
        else:
            nodes = [
                make_node("Gemm", ["A", "B"], ["C"], transA=1, beta=0.0),
            ]
        graph = make_graph(nodes, "a", [A, B], outputs, inits)
        if mat_type < 16:
            # regular type
            opset, ir = 18, 8
        else:
            opset, ir = 19, 9
        onnx_model = make_model(
            graph,
            opset_imports=[
                make_opsetid("", opset),
                make_opsetid("com.microsoft", 1),
                make_opsetid("onnx_extended.ortops.tutorial.cuda", 1),
            ],
            ir_version=ir,
        )
        check_model(onnx_model)
        return onnx_model


    print(onnx_simple_text_plot(create_model()))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    opset: domain='com.microsoft' version=1
    opset: domain='onnx_extended.ortops.tutorial.cuda' version=1
    input: name='A' type=dtype('float32') shape=['', '']
    input: name='B' type=dtype('float32') shape=['', '']
    GemmFloat8[com.microsoft](A, B, alpha=1.00, computeType=b'CUBLAS_COMPUTE_32F', fastAccumulationMode=1, rowMajor=1, transB=1) -> C
    output: name='C' type=dtype('float32') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 200-204

A model to cast into anytype.
numpy does not support float 8. onnxruntime is used
to cast a float array into any type.
It must be called with tensor of type `OrtValue`.

.. GENERATED FROM PYTHON SOURCE LINES 204-234

.. code-block:: Python



    def create_cast(to, cuda=False):
        A = make_tensor_value_info("A", TensorProto.FLOAT, [None, None])
        C = make_tensor_value_info("C", to, [None, None])
        if cuda:
            nodes = [
                make_node("Cast", ["A"], ["Cc"], to=to),
                make_node("MemcpyFromHost", ["Cc"], ["C"]),
            ]
        else:
            nodes = [make_node("Cast", ["A"], ["C"], to=to)]
        graph = make_graph(nodes, "a", [A], [C])
        if to < 16:
            # regular type
            opset, ir = 18, 8
        else:
            opset, ir = 19, 9
        onnx_model = make_model(
            graph, opset_imports=[make_opsetid("", opset)], ir_version=ir
        )
        if not cuda:
            # OpType: MemcpyFromHost
            check_model(onnx_model)
        return onnx_model


    print(onnx_simple_text_plot(create_cast(TensorProto.FLOAT16)))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    input: name='A' type=dtype('float32') shape=['', '']
    Cast(A, to=10) -> C
    output: name='C' type=dtype('float16') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 235-239

Performance
+++++++++++

The benchmark will run the following configurations.

.. GENERATED FROM PYTHON SOURCE LINES 239-252

.. code-block:: Python


    types = [getattr(TensorProto, a) for a in script_args.types.split(",")]
    engine = [InferenceSession, CReferenceEvaluator]
    providers = [
        ["CUDAExecutionProvider", "CPUExecutionProvider"],
        ["CPUExecutionProvider"],
    ]
    # M, N, K
    # we use multiple of 8, otherwise, float8 does not work.
    dims = [[int(i) for i in line.split(",")] for line in script_args.dims.split(";")]
    domains = ["onnx_extended.ortops.tutorial.cuda", "", "com.microsoft"]









.. GENERATED FROM PYTHON SOURCE LINES 253-254

Let's cache the matrices involved.

.. GENERATED FROM PYTHON SOURCE LINES 254-309

.. code-block:: Python



    def to_ort_value(m):
        device = C_OrtDevice(C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)
        ort_value = C_OrtValue.ortvalue_from_numpy(m, device)
        return ort_value


    def cached_inputs(dims, types):
        matrices = {}
        matrices_cuda = {}
        pbar = tqdm(list(product(dims, types)))
        for dim, tt in pbar:
            m, n, k = dim
            pbar.set_description(f"t={tt} dim={dim}")
            for i, j in [(m, k), (k, n), (k, m)]:
                if (tt, i, j) in matrices:
                    continue
                # CPU
                try:
                    sess = InferenceSession(
                        create_cast(tt).SerializeToString(),
                        providers=["CPUExecutionProvider"],
                    )
                    cpu = True
                except (InvalidGraph, InvalidArgument, NotImplemented):
                    # not support by this version of onnxruntime
                    cpu = False

                if cpu:
                    vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)
                    ov = to_ort_value(vect)
                    ovtt = sess._sess.run_with_ort_values({"A": ov}, ["C"], None)[0]
                    matrices[tt, i, j] = ovtt
                else:
                    continue

                # CUDA
                if "CUDAExecutionProvider" not in get_available_providers():
                    # No CUDA
                    continue
                sess = InferenceSession(
                    create_cast(tt, cuda=True).SerializeToString(),
                    providers=["CUDAExecutionProvider", "CPUExecutionProvider"],
                )
                vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)
                ov = to_ort_value(vect)
                ovtt = sess._sess.run_with_ort_values({"A": ov}, ["C"], None)[0]
                matrices_cuda[tt, i, j] = ovtt
        return matrices, matrices_cuda


    matrices, matrices_cuda = cached_inputs(dims, types)
    print(f"{len(matrices)} matrices were created.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/36 [00:00<?, ?it/s]    t=17 dim=[32, 32, 32]:   0%|          | 0/36 [00:00<?, ?it/s]    t=17 dim=[32, 32, 32]:   3%|▎         | 1/36 [00:07<04:07,  7.07s/it]    t=1 dim=[32, 32, 32]:   3%|▎         | 1/36 [00:07<04:07,  7.07s/it]     t=10 dim=[32, 32, 32]:   3%|▎         | 1/36 [00:07<04:07,  7.07s/it]    t=16 dim=[32, 32, 32]:   3%|▎         | 1/36 [00:07<04:07,  7.07s/it]    t=16 dim=[32, 32, 32]:  11%|█         | 4/36 [00:07<00:43,  1.37s/it]    t=17 dim=[64, 64, 64]:  11%|█         | 4/36 [00:07<00:43,  1.37s/it]    t=1 dim=[64, 64, 64]:  11%|█         | 4/36 [00:07<00:43,  1.37s/it]     t=10 dim=[64, 64, 64]:  11%|█         | 4/36 [00:07<00:43,  1.37s/it]    t=16 dim=[64, 64, 64]:  11%|█         | 4/36 [00:07<00:43,  1.37s/it]    t=17 dim=[128, 128, 128]:  11%|█         | 4/36 [00:07<00:43,  1.37s/it]    t=17 dim=[128, 128, 128]:  25%|██▌       | 9/36 [00:07<00:13,  2.07it/s]    t=1 dim=[128, 128, 128]:  25%|██▌       | 9/36 [00:07<00:13,  2.07it/s]     t=10 dim=[128, 128, 128]:  25%|██▌       | 9/36 [00:07<00:13,  2.07it/s]    t=16 dim=[128, 128, 128]:  25%|██▌       | 9/36 [00:07<00:13,  2.07it/s]    t=17 dim=[256, 256, 256]:  25%|██▌       | 9/36 [00:07<00:13,  2.07it/s]    t=1 dim=[256, 256, 256]:  25%|██▌       | 9/36 [00:07<00:13,  2.07it/s]     t=1 dim=[256, 256, 256]:  39%|███▉      | 14/36 [00:07<00:05,  3.85it/s]    t=10 dim=[256, 256, 256]:  39%|███▉      | 14/36 [00:07<00:05,  3.85it/s]    t=16 dim=[256, 256, 256]:  39%|███▉      | 14/36 [00:07<00:05,  3.85it/s]    t=17 dim=[400, 400, 400]:  39%|███▉      | 14/36 [00:07<00:05,  3.85it/s]    t=1 dim=[400, 400, 400]:  39%|███▉      | 14/36 [00:07<00:05,  3.85it/s]     t=10 dim=[400, 400, 400]:  39%|███▉      | 14/36 [00:07<00:05,  3.85it/s]    t=16 dim=[400, 400, 400]:  39%|███▉      | 14/36 [00:07<00:05,  3.85it/s]    t=16 dim=[400, 400, 400]:  56%|█████▌    | 20/36 [00:07<00:02,  6.62it/s]    t=17 dim=[512, 512, 512]:  56%|█████▌    | 20/36 [00:07<00:02,  6.62it/s]    t=1 dim=[512, 512, 512]:  56%|█████▌    | 20/36 [00:07<00:02,  6.62it/s]     t=10 dim=[512, 512, 512]:  56%|█████▌    | 20/36 [00:07<00:02,  6.62it/s]    t=16 dim=[512, 512, 512]:  56%|█████▌    | 20/36 [00:07<00:02,  6.62it/s]    t=16 dim=[512, 512, 512]:  67%|██████▋   | 24/36 [00:07<00:01,  8.71it/s]    t=17 dim=[1024, 1024, 1024]:  67%|██████▋   | 24/36 [00:07<00:01,  8.71it/s]    t=1 dim=[1024, 1024, 1024]:  67%|██████▋   | 24/36 [00:07<00:01,  8.71it/s]     t=10 dim=[1024, 1024, 1024]:  67%|██████▋   | 24/36 [00:07<00:01,  8.71it/s]    t=16 dim=[1024, 1024, 1024]:  67%|██████▋   | 24/36 [00:07<00:01,  8.71it/s]    t=16 dim=[1024, 1024, 1024]:  78%|███████▊  | 28/36 [00:08<00:00,  9.19it/s]    t=17 dim=[2048, 2048, 2048]:  78%|███████▊  | 28/36 [00:08<00:00,  9.19it/s]    t=1 dim=[2048, 2048, 2048]:  78%|███████▊  | 28/36 [00:08<00:00,  9.19it/s]     t=10 dim=[2048, 2048, 2048]:  78%|███████▊  | 28/36 [00:08<00:00,  9.19it/s]    t=10 dim=[2048, 2048, 2048]:  86%|████████▌ | 31/36 [00:09<00:00,  5.99it/s]    t=16 dim=[2048, 2048, 2048]:  86%|████████▌ | 31/36 [00:09<00:00,  5.99it/s]    t=17 dim=[4096, 4096, 4096]:  86%|████████▌ | 31/36 [00:09<00:00,  5.99it/s]    t=1 dim=[4096, 4096, 4096]:  86%|████████▌ | 31/36 [00:10<00:00,  5.99it/s]     t=1 dim=[4096, 4096, 4096]:  94%|█████████▍| 34/36 [00:12<00:00,  2.61it/s]    t=10 dim=[4096, 4096, 4096]:  94%|█████████▍| 34/36 [00:12<00:00,  2.61it/s]    t=16 dim=[4096, 4096, 4096]:  94%|█████████▍| 34/36 [00:13<00:00,  2.61it/s]    t=16 dim=[4096, 4096, 4096]: 100%|██████████| 36/36 [00:14<00:00,  1.74it/s]    t=16 dim=[4096, 4096, 4096]: 100%|██████████| 36/36 [00:14<00:00,  2.43it/s]
    36 matrices were created.




.. GENERATED FROM PYTHON SOURCE LINES 310-311

Let's run the benchmark

.. GENERATED FROM PYTHON SOURCE LINES 311-477

.. code-block:: Python



    def rendering_obs(obs, dim, number, repeat, domain, provider, internal_time):
        stype = {
            TensorProto.FLOAT: "f32",
            TensorProto.FLOAT16: "f16",
            TensorProto.BFLOAT16: "bf16",
            TensorProto.INT8: "i8",
            TensorProto.INT16: "i16",
            TensorProto.INT32: "i32",
            TensorProto.UINT32: "u32",
            TensorProto.FLOAT8E4M3FN: "e4m3fn",
            TensorProto.FLOAT8E5M2: "e5m2",
        }[tt]
        obs.update(
            dict(
                engine={"InferenceSession": "ort", "CReferenceEvaluator": "np"}[
                    engine.__name__
                ],
                stype=stype,
                type=f"{stype}",
                M=dim[0],
                N=dim[1],
                K=dim[2],
                cost=numpy.prod(dim) * 4,
                cost_s=f"{numpy.prod(dim) * 4}-{dim[0]}x{dim[1]}x{dim[2]}",
                repeat=repeat,
                number=number,
                domain={
                    "": "ORT",
                    "com.microsoft": "COM",
                    "onnx_extended.ortops.tutorial.cuda": "EXT",
                }[domain],
                provider={
                    "CPUExecutionProvider": "cpu",
                    "CUDAExecutionProvider": "cuda",
                }[provider[0]],
                platform=platform.processor(),
                intime=internal_time,
            )
        )
        return obs


    opts = SessionOptions()
    r = get_ort_ext_libs()
    if r is not None:
        opts.register_custom_ops_library(r[0])


    data = []
    errors = []
    pbar = tqdm(list(product(types, engine, providers, dims, domains)))
    for tt, engine, provider, dim, domain in pbar:
        if (
            tt in {TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2}
            and properties.get("major", 0) < 9
        ):
            # f8 not available
            if provider[0] == "CPUExecutionProvider":
                continue
            errors.append(
                f"f8 not available, major={properties.get('major', 0)}, "
                f"tt={tt}, provider={provider!r}, domain={domain!r}."
            )
            continue
        elif provider[0] == "CPUExecutionProvider" and max(dim) > 2000:
            # too long
            continue
        if max(dim) <= 200:
            repeat, number = script_args.repeat * 4, script_args.number * 4
        elif max(dim) <= 256:
            repeat, number = script_args.repeat * 2, script_args.number * 2
        else:
            repeat, number = script_args.repeat, script_args.number

        onx = create_model(tt, provider=provider[0], domain=domain)
        if onx is None:
            if provider[0] == "CPUExecutionProvider":
                continue
            errors.append(
                f"No model for tt={tt}, provider={provider!r}, domain={domain!r}."
            )
            continue
        with open(f"plot_bench_gemm_ort_{tt}_{domain}.onnx", "wb") as f:
            f.write(onx.SerializeToString())
        k1 = (tt, dim[2], dim[0])
        k2 = (tt, dim[2], dim[1])
        if k1 not in matrices:
            errors.append(f"Key k1={k1!r} not in matrices.")
            continue
        if k2 not in matrices:
            errors.append(f"Key k2={k2!r} not in matrices.")
            continue

        pbar.set_description(f"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}")

        if engine == CReferenceEvaluator:
            if (
                domain != ""
                or max(dim) > 256
                or provider != ["CPUExecutionProvider"]
                or tt not in [TensorProto.FLOAT, TensorProto.FLOAT16]
            ):
                # All impossible or slow cases.
                continue
            if tt == TensorProto.FLOAT16 and max(dim) > 50:
                repeat, number = 2, 2

            feeds = {"A": matrices[k1].numpy(), "B": matrices[k2].numpy()}
            sess = engine(onx)
            sess.run(None, feeds)
            obs = measure_time(
                lambda sess=sess, feeds=feeds: sess.run(None, feeds),
                repeat=repeat,
                number=number,
            )

        elif engine == InferenceSession:
            if provider[0] not in get_available_providers():
                errors.append(f"provider={provider[0]} is missing")
                continue
            try:
                sess = engine(onx.SerializeToString(), opts, providers=provider)
            except (NotImplemented, InvalidGraph, Fail) as e:
                # not implemented
                errors.append((tt, engine.__class__.__name__, provider, domain, e))
                continue

            the_feeds = (
                {"A": matrices[k1], "B": matrices[k2]}
                if provider == ["CPUExecutionProvider"]
                else {"A": matrices_cuda[k1], "B": matrices_cuda[k2]}
            )
            out_names = ["C"]

            # warmup
            for _i in range(script_args.warmup):
                sess._sess.run_with_ort_values(the_feeds, out_names, None)[0]

            # benchamrk
            times = []

            def fct_benchmarked(
                sess=sess, times=times, out_names=out_names, the_feeds=the_feeds
            ):
                got = sess._sess.run_with_ort_values(the_feeds, out_names, None)
                if len(got) > 1:
                    times.append(got[1])

            obs = measure_time(fct_benchmarked, repeat=repeat, number=number)
            internal_time = None
            if times:
                np_times = [t.numpy() for t in times]
                internal_time = (sum(np_times) / len(times))[0]

        else:
            errors.append(f"unknown engine={engine}")
            continue

        # improves the rendering
        obs = rendering_obs(obs, dim, number, repeat, domain, provider, internal_time)
        data.append(obs)
        if unit_test_going() and len(data) >= 2:
            break



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/home/xadupre/github/onnx-extended/_doc/examples/plot_bench_gemm_ort.py", line 449, in <module>
        sess._sess.run_with_ort_values(the_feeds, out_names, None)[0]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running CustomGemmFloat16 node. Name:'10.1.1.onnx_extended.ortops.tutorial.cuda..0..1..CUBLAS_COMPUTE_16F..False' Status Message: `returnedResults > 0 && cuda_status == CUBLAS_STATUS_SUCCESS` failed. [onnx-extended]  Unable to find any suitable algorithm due to CUBLAS_STATUS_INVALID_VALUE, returnedResults=0, alpha=1, beta=0, n_inputs=2, A_type=CUDA_R_16F-2, B_type=CUDA_R_16F-2, C_type=CUDA_R_32F-0, result_type=CUDA_R_16F-2, bias_type=CUDA_R_32F-0, scale_type=CUDA_R_32F-0, computeType=CUBLAS_COMPUTE_16F-64, epilogue=1, smCount=0, transA=1, transB=0, fastAccumulationMode=1, shape_A=32x32, shape_B=32x32, shape_C=0x0, M=32, N=32, K=32, lda=32, ldb=32, ldd=32, workspaceSize=33554432, rowMajor=0. Check NVIDIA documentation to see what combination is valid: https://docs.nvidia.com/cuda/cublas/index.html?highlight=cublasLtMatmulAlgoGetHeuristic#cublasltmatmulalgogetheuristic.




.. GENERATED FROM PYTHON SOURCE LINES 478-480

Results
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 480-490

.. code-block:: Python


    df = DataFrame(data)
    df.to_excel("plot_bench_gemm_ort.xlsx")
    df.to_csv("plot_bench_gemm_ort.csv")
    df.drop(["min_exec", "max_exec", "cost_s", "cost"], axis=1).to_csv(
        "plot_bench_gemm_ort.csv", index=False
    )
    print(df.head().T)
    df


.. GENERATED FROM PYTHON SOURCE LINES 491-493

The errors
++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 493-496

.. code-block:: Python

    for i, e in enumerate(errors):
        print(f"{i+1}/{len(errors)}-{e}")


.. GENERATED FROM PYTHON SOURCE LINES 497-499

Summary
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 499-514

.. code-block:: Python


    piv = pivot_table(
        df,
        index=["cost"],
        columns=["provider", "type", "domain", "engine"],
        values=["average", "intime"],
    )
    piv.reset_index(drop=False).to_excel("plot_bench_gemm_ort_summary.xlsx")
    piv.reset_index(drop=False).to_csv("plot_bench_gemm_ort_summary.csv")


    print("summary")
    print(piv)
    piv


.. GENERATED FROM PYTHON SOURCE LINES 515-516

With the dimensions.

.. GENERATED FROM PYTHON SOURCE LINES 516-525

.. code-block:: Python


    pivs = pivot_table(
        df,
        index=["cost_s"],
        columns=["provider", "type", "domain", "engine"],
        values=["average", "intime"],
    )
    print(pivs)


.. GENERATED FROM PYTHON SOURCE LINES 526-527

plot

.. GENERATED FROM PYTHON SOURCE LINES 527-549

.. code-block:: Python


    dfi = df[
        df.type.isin({"f32", "f16", "bf16", "e4m3fn", "e5m2"}) & df.engine.isin({"ort"})
    ]
    pivi = pivot_table(
        dfi,
        index=["cost"],
        columns=["type", "domain", "provider", "engine"],
        values="average",
    )

    fig, ax = plt.subplots(1, 2, figsize=(12, 6))
    piv.plot(ax=ax[0], title="Gemm performance\nlower is better", logx=True, logy=True)
    if pivi.shape[0] > 0:
        pivi.plot(
            ax=ax[1],
            title=f"Gemm performance ORT\n{platform.processor()}",
            logx=True,
            logy=True,
        )
    fig.tight_layout()
    fig.savefig("plot_bench_gemm_ort.png")


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 39.993 seconds)


.. _sphx_glr_download_auto_examples_plot_bench_gemm_ort.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bench_gemm_ort.ipynb <plot_bench_gemm_ort.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bench_gemm_ort.py <plot_bench_gemm_ort.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_bench_gemm_ort.zip <plot_bench_gemm_ort.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
