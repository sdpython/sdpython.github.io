
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_op_tree_ensemble_sparse.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_op_tree_ensemble_sparse.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_op_tree_ensemble_sparse.py:


.. _l-plot-optim-tree-ensemble-sparse:

TreeEnsemble, dense, and sparse
===============================

The example benchmarks the sparse implementation for TreeEnsemble.
The default set of optimized parameters is very short and is meant to be executed
fast. Many more parameters can be tried.

::

    python plot_op_tree_ensemble_sparse --scenario=LONG

To change the training parameters:

::

    python plot_op_tree_ensemble_sparse.py
        --n_trees=100
        --max_depth=10
        --n_features=50
        --sparsity=0.9
        --batch_size=100000
    
Another example with a full list of parameters:

    python plot_op_tree_ensemble_sparse.py
        --n_trees=100
        --max_depth=10
        --n_features=50
        --batch_size=100000
        --sparsity=0.9
        --tries=3
        --scenario=CUSTOM
        --parallel_tree=80,40
        --parallel_tree_N=128,64
        --parallel_N=50,25
        --batch_size_tree=1,2
        --batch_size_rows=1,2
        --use_node3=0

Another example:

::

    python plot_op_tree_ensemble_sparse.py
        --n_trees=100 --n_features=10 --batch_size=10000 --max_depth=8 -s SHORT        

.. GENERATED FROM PYTHON SOURCE LINES 50-101

.. code-block:: Python

    import logging
    import os
    import timeit
    from typing import Tuple
    import numpy
    import onnx
    from onnx import ModelProto, TensorProto
    from onnx.helper import make_graph, make_model, make_tensor_value_info
    from pandas import DataFrame, concat
    from sklearn.datasets import make_regression
    from sklearn.ensemble import RandomForestRegressor
    from skl2onnx import to_onnx
    from onnxruntime import InferenceSession, SessionOptions
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    from onnx_extended.ortops.optim.cpu import get_ort_ext_libs
    from onnx_extended.ortops.optim.optimize import (
        change_onnx_operator_domain,
        get_node_attribute,
        optimize_model,
    )
    from onnx_extended.tools.onnx_nodes import multiply_tree
    from onnx_extended.validation.cpu._validation import dense_to_sparse_struct
    from onnx_extended.plotting.benchmark import hhistograms
    from onnx_extended.ext_test_case import get_parsed_args, unit_test_going

    logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)

    script_args = get_parsed_args(
        "plot_op_tree_ensemble_sparse",
        description=__doc__,
        scenarios={
            "SHORT": "short optimization (default)",
            "LONG": "test more options",
            "CUSTOM": "use values specified by the command line",
        },
        sparsity=(0.99, "input sparsity"),
        n_features=(2 if unit_test_going() else 50, "number of features to generate"),
        n_trees=(3 if unit_test_going() else 10, "number of trees to train"),
        max_depth=(2 if unit_test_going() else 5, "max_depth"),
        batch_size=(1000 if unit_test_going() else 10000, "batch size"),
        parallel_tree=("80,160,40", "values to try for parallel_tree"),
        parallel_tree_N=("256,128,64", "values to try for parallel_tree_N"),
        parallel_N=("100,50,25", "values to try for parallel_N"),
        batch_size_tree=("2,4,8", "values to try for batch_size_tree"),
        batch_size_rows=("2,4,8", "values to try for batch_size_rows"),
        use_node3=("0,1", "values to try for use_node3"),
        expose="",
        n_jobs=("-1", "number of jobs to train the RandomForestRegressor"),
    )









.. GENERATED FROM PYTHON SOURCE LINES 102-104

Training a model
++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 104-158

.. code-block:: Python



    def train_model(
        batch_size: int, n_features: int, n_trees: int, max_depth: int, sparsity: float
    ) -> Tuple[str, numpy.ndarray, numpy.ndarray]:
        filename = (
            f"plot_op_tree_ensemble_sparse-f{n_features}-{n_trees}-"
            f"d{max_depth}-s{sparsity}.onnx"
        )
        if not os.path.exists(filename):
            X, y = make_regression(
                batch_size + max(batch_size, 2 ** (max_depth + 1)),
                n_features=n_features,
                n_targets=1,
            )
            mask = numpy.random.rand(*X.shape) <= sparsity
            X[mask] = 0
            X, y = X.astype(numpy.float32), y.astype(numpy.float32)

            print(f"Training to get {filename!r} with X.shape={X.shape}")
            # To be faster, we train only 1 tree.
            model = RandomForestRegressor(
                1, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)
            )
            model.fit(X[:-batch_size], y[:-batch_size])
            onx = to_onnx(model, X[:1])

            # And wd multiply the trees.
            node = multiply_tree(onx.graph.node[0], n_trees)
            onx = make_model(
                make_graph([node], onx.graph.name, onx.graph.input, onx.graph.output),
                domain=onx.domain,
                opset_imports=onx.opset_import,
            )

            with open(filename, "wb") as f:
                f.write(onx.SerializeToString())
        else:
            X, y = make_regression(batch_size, n_features=n_features, n_targets=1)
            mask = numpy.random.rand(*X.shape) <= sparsity
            X[mask] = 0
            X, y = X.astype(numpy.float32), y.astype(numpy.float32)
        Xb, yb = X[-batch_size:].copy(), y[-batch_size:].copy()
        return filename, Xb, yb


    batch_size = script_args.batch_size
    n_features = script_args.n_features
    n_trees = script_args.n_trees
    max_depth = script_args.max_depth
    sparsity = script_args.sparsity

    filename, Xb, yb = train_model(batch_size, n_features, n_trees, max_depth, sparsity)








.. GENERATED FROM PYTHON SOURCE LINES 159-166

Rewrite the onnx file to use a different kernel
+++++++++++++++++++++++++++++++++++++++++++++++

The custom kernel is mapped to a custom operator with the same name
the attributes and domain = `"onnx_extented.ortops.optim.cpu"`.
We call a function to do that replacement.
First the current model.

.. GENERATED FROM PYTHON SOURCE LINES 166-171

.. code-block:: Python


    with open(filename, "rb") as f:
        onx = onnx.load(f)
    print(onnx_simple_text_plot(onx))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=19
    input: name='X' type=dtype('float32') shape=['', 50]
    TreeEnsembleRegressor(X, n_targets=1, nodes_falsenodeids=430:[20,11,4...42,0,0], nodes_featureids=430:[45,45,24...24,0,0], nodes_hitrates=430:[1.0,1.0...1.0,1.0], nodes_missing_value_tracks_true=430:[0,0,0...0,0,0], nodes_modes=430:[b'BRANCH_LEQ',b'BRANCH_LEQ'...b'LEAF',b'LEAF'], nodes_nodeids=430:[0,1,2...40,41,42], nodes_treeids=430:[0,0,0...9,9,9], nodes_truenodeids=430:[1,2,3...41,0,0], nodes_values=430:[-0.2748222053050995,-0.506583034992218...0.0,0.0], post_transform=b'NONE', target_ids=220:[0,0,0...0,0,0], target_nodeids=220:[3,6,7...39,41,42], target_treeids=220:[0,0,0...9,9,9], target_weights=220:[46.077239990234375,-41.48093795776367...297.275634765625,109.99082946777344]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 172-173

And then the modified model.

.. GENERATED FROM PYTHON SOURCE LINES 173-214

.. code-block:: Python



    def transform_model(model, use_sparse=False, **kwargs):
        onx = ModelProto()
        onx.ParseFromString(model.SerializeToString())
        att = get_node_attribute(onx.graph.node[0], "nodes_modes")
        modes = ",".join(map(lambda s: s.decode("ascii"), att.strings)).replace(
            "BRANCH_", ""
        )
        if use_sparse and "new_op_type" not in kwargs:
            kwargs["new_op_type"] = "TreeEnsembleRegressorSparse"
        if use_sparse:
            # with sparse tensor, missing value means 0
            att = get_node_attribute(onx.graph.node[0], "nodes_values")
            thresholds = numpy.array(att.floats, dtype=numpy.float32)
            missing_true = (thresholds >= 0).astype(numpy.int64)
            kwargs["nodes_missing_value_tracks_true"] = missing_true
        new_onx = change_onnx_operator_domain(
            onx,
            op_type="TreeEnsembleRegressor",
            op_domain="ai.onnx.ml",
            new_op_domain="onnx_extented.ortops.optim.cpu",
            nodes_modes=modes,
            **kwargs,
        )
        if use_sparse:
            del new_onx.graph.input[:]
            new_onx.graph.input.append(
                make_tensor_value_info("X", TensorProto.FLOAT, (None,))
            )
        return new_onx


    print("Tranform model to add a custom node.")
    onx_modified = transform_model(onx)
    print(f"Save into {filename + 'modified.onnx'!r}.")
    with open(filename + "modified.onnx", "wb") as f:
        f.write(onx_modified.SerializeToString())
    print("done.")
    print(onnx_simple_text_plot(onx_modified))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Tranform model to add a custom node.
    Save into 'plot_op_tree_ensemble_sparse-f50-10-d5-s0.99.onnxmodified.onnx'.
    done.
    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=19
    opset: domain='onnx_extented.ortops.optim.cpu' version=1
    input: name='X' type=dtype('float32') shape=['', 50]
    TreeEnsembleRegressor[onnx_extented.ortops.optim.cpu](X, nodes_modes=b'LEQ,LEQ,LEQ,LEAF,LEQ,LEQ,LEAF,LEAF,LEQ...LEAF,LEAF', n_targets=1, nodes_falsenodeids=430:[20,11,4...42,0,0], nodes_featureids=430:[45,45,24...24,0,0], nodes_hitrates=430:[1.0,1.0...1.0,1.0], nodes_missing_value_tracks_true=430:[0,0,0...0,0,0], nodes_nodeids=430:[0,1,2...40,41,42], nodes_treeids=430:[0,0,0...9,9,9], nodes_truenodeids=430:[1,2,3...41,0,0], nodes_values=430:[-0.2748222053050995,-0.506583034992218...0.0,0.0], post_transform=b'NONE', target_ids=220:[0,0,0...0,0,0], target_nodeids=220:[3,6,7...39,41,42], target_treeids=220:[0,0,0...9,9,9], target_weights=220:[46.077239990234375,-41.48093795776367...297.275634765625,109.99082946777344]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 215-216

Same with sparse.

.. GENERATED FROM PYTHON SOURCE LINES 216-226

.. code-block:: Python



    print("Same transformation but with sparse.")
    onx_modified_sparse = transform_model(onx, use_sparse=True)
    print(f"Save into {filename + 'modified.sparse.onnx'!r}.")
    with open(filename + "modified.sparse.onnx", "wb") as f:
        f.write(onx_modified_sparse.SerializeToString())
    print("done.")
    print(onnx_simple_text_plot(onx_modified_sparse))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Same transformation but with sparse.
    Save into 'plot_op_tree_ensemble_sparse-f50-10-d5-s0.99.onnxmodified.sparse.onnx'.
    done.
    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=19
    opset: domain='onnx_extented.ortops.optim.cpu' version=1
    input: name='X' type=dtype('float32') shape=['']
    TreeEnsembleRegressorSparse[onnx_extented.ortops.optim.cpu](X, nodes_missing_value_tracks_true=430:[0,0,0...1,1,1], nodes_modes=b'LEQ,LEQ,LEQ,LEAF,LEQ,LEQ,LEAF,LEAF,LEQ...LEAF,LEAF', n_targets=1, nodes_falsenodeids=430:[20,11,4...42,0,0], nodes_featureids=430:[45,45,24...24,0,0], nodes_hitrates=430:[1.0,1.0...1.0,1.0], nodes_nodeids=430:[0,1,2...40,41,42], nodes_treeids=430:[0,0,0...9,9,9], nodes_truenodeids=430:[1,2,3...41,0,0], nodes_values=430:[-0.2748222053050995,-0.506583034992218...0.0,0.0], post_transform=b'NONE', target_ids=220:[0,0,0...0,0,0], target_nodeids=220:[3,6,7...39,41,42], target_treeids=220:[0,0,0...9,9,9], target_weights=220:[46.077239990234375,-41.48093795776367...297.275634765625,109.99082946777344]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 227-229

Comparing onnxruntime and the custom kernel
+++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 229-262

.. code-block:: Python


    print(f"Loading {filename!r}")
    sess_ort = InferenceSession(filename, providers=["CPUExecutionProvider"])

    r = get_ort_ext_libs()
    print(f"Creating SessionOptions with {r!r}")
    opts = SessionOptions()
    if r is not None:
        opts.register_custom_ops_library(r[0])

    print(f"Loading modified {filename!r}")
    sess_cus = InferenceSession(
        onx_modified.SerializeToString(), opts, providers=["CPUExecutionProvider"]
    )

    print(f"Loading modified sparse {filename!r}")
    sess_cus_sparse = InferenceSession(
        onx_modified_sparse.SerializeToString(), opts, providers=["CPUExecutionProvider"]
    )


    print(f"Running once with shape {Xb.shape}.")
    base = sess_ort.run(None, {"X": Xb})[0]

    print(f"Running modified with shape {Xb.shape}.")
    got = sess_cus.run(None, {"X": Xb})[0]
    print("done.")

    Xb_sp = dense_to_sparse_struct(Xb)
    print(f"Running modified sparse with shape {Xb_sp.shape}.")
    got_sparse = sess_cus_sparse.run(None, {"X": Xb_sp})[0]
    print("done.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading 'plot_op_tree_ensemble_sparse-f50-10-d5-s0.99.onnx'
    Creating SessionOptions with ['/home/xadupre/github/onnx-extended/onnx_extended/ortops/optim/cpu/libortops_optim_cpu.so']
    Loading modified 'plot_op_tree_ensemble_sparse-f50-10-d5-s0.99.onnx'
    Loading modified sparse 'plot_op_tree_ensemble_sparse-f50-10-d5-s0.99.onnx'
    Running once with shape (10000, 50).
    Running modified with shape (10000, 50).
    done.
    Running modified sparse with shape (9992,).
    done.




.. GENERATED FROM PYTHON SOURCE LINES 263-264

Discrepancies?

.. GENERATED FROM PYTHON SOURCE LINES 264-271

.. code-block:: Python


    diff = numpy.abs(base - got).max()
    print(f"Discrepancies: {diff}")

    diff = numpy.abs(base - got_sparse).max()
    print(f"Discrepancies sparse: {diff}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Discrepancies: 0.000244140625
    Discrepancies sparse: 0.000244140625




.. GENERATED FROM PYTHON SOURCE LINES 272-276

Simple verification
+++++++++++++++++++

Baseline with onnxruntime.

.. GENERATED FROM PYTHON SOURCE LINES 276-279

.. code-block:: Python

    t1 = timeit.timeit(lambda: sess_ort.run(None, {"X": Xb}), number=50)
    print(f"baseline: {t1}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    baseline: 0.1085624999996071




.. GENERATED FROM PYTHON SOURCE LINES 280-281

The custom implementation.

.. GENERATED FROM PYTHON SOURCE LINES 281-284

.. code-block:: Python

    t2 = timeit.timeit(lambda: sess_cus.run(None, {"X": Xb}), number=50)
    print(f"new time: {t2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    new time: 0.06001219999961904




.. GENERATED FROM PYTHON SOURCE LINES 285-286

The custom sparse implementation.

.. GENERATED FROM PYTHON SOURCE LINES 286-289

.. code-block:: Python

    t3 = timeit.timeit(lambda: sess_cus_sparse.run(None, {"X": Xb_sp}), number=50)
    print(f"new time sparse: {t3}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    new time sparse: 0.16779729999962




.. GENERATED FROM PYTHON SOURCE LINES 290-299

Time for comparison
+++++++++++++++++++

The custom kernel supports the same attributes as *TreeEnsembleRegressor*
plus new ones to tune the parallelization. They can be seen in
`tree_ensemble.cc <https://github.com/sdpython/onnx-extended/
blob/main/onnx_extended/ortops/optim/cpu/tree_ensemble.cc#L102>`_.
Let's try out many possibilities.
The default values are the first ones.

.. GENERATED FROM PYTHON SOURCE LINES 299-347

.. code-block:: Python


    if unit_test_going():
        optim_params = dict(
            parallel_tree=[40],  # default is 80
            parallel_tree_N=[128],  # default is 128
            parallel_N=[50, 25],  # default is 50
            batch_size_tree=[1],  # default is 1
            batch_size_rows=[1],  # default is 1
            use_node3=[0],  # default is 0
        )
    elif script_args.scenario in (None, "SHORT"):
        optim_params = dict(
            parallel_tree=[80, 40],  # default is 80
            parallel_tree_N=[128, 64],  # default is 128
            parallel_N=[50, 25],  # default is 50
            batch_size_tree=[1],  # default is 1
            batch_size_rows=[1],  # default is 1
            use_node3=[0],  # default is 0
        )
    elif script_args.scenario == "LONG":
        optim_params = dict(
            parallel_tree=[80, 160, 40],
            parallel_tree_N=[256, 128, 64],
            parallel_N=[100, 50, 25],
            batch_size_tree=[1, 2, 4, 8],
            batch_size_rows=[1, 2, 4, 8],
            use_node3=[0, 1],
        )
    elif script_args.scenario == "CUSTOM":
        optim_params = dict(
            parallel_tree=list(int(i) for i in script_args.parallel_tree.split(",")),
            parallel_tree_N=list(int(i) for i in script_args.parallel_tree_N.split(",")),
            parallel_N=list(int(i) for i in script_args.parallel_N.split(",")),
            batch_size_tree=list(int(i) for i in script_args.batch_size_tree.split(",")),
            batch_size_rows=list(int(i) for i in script_args.batch_size_rows.split(",")),
            use_node3=list(int(i) for i in script_args.use_node3.split(",")),
        )
    else:
        raise ValueError(
            f"Unknown scenario {script_args.scenario!r}, use --help to get them."
        )

    cmds = []
    for att, value in optim_params.items():
        cmds.append(f"--{att}={','.join(map(str, value))}")
    print("Full list of optimization parameters:")
    print(" ".join(cmds))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Full list of optimization parameters:
    --parallel_tree=80,40 --parallel_tree_N=128,64 --parallel_N=50,25 --batch_size_tree=1 --batch_size_rows=1 --use_node3=0




.. GENERATED FROM PYTHON SOURCE LINES 348-349

Then the optimization for dense

.. GENERATED FROM PYTHON SOURCE LINES 349-379

.. code-block:: Python



    def create_session(onx):
        opts = SessionOptions()
        r = get_ort_ext_libs()
        if r is None:
            raise RuntimeError("No custom implementation available.")
        opts.register_custom_ops_library(r[0])
        return InferenceSession(
            onx.SerializeToString(), opts, providers=["CPUExecutionProvider"]
        )


    res = optimize_model(
        onx,
        feeds={"X": Xb},
        transform=transform_model,
        session=create_session,
        baseline=lambda onx: InferenceSession(
            onx.SerializeToString(), providers=["CPUExecutionProvider"]
        ),
        params=optim_params,
        verbose=True,
        number=script_args.number,
        repeat=script_args.repeat,
        warmup=script_args.warmup,
        sleep=script_args.sleep,
        n_tries=script_args.tries,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:   0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:   6%|▋         | 1/16 [00:00<00:07,  1.95it/s]    i=2/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:   6%|▋         | 1/16 [00:00<00:07,  1.95it/s]    i=2/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  12%|█▎        | 2/16 [00:00<00:04,  2.92it/s]    i=3/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  12%|█▎        | 2/16 [00:00<00:04,  2.92it/s]     i=3/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  19%|█▉        | 3/16 [00:00<00:03,  3.55it/s]    i=4/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  19%|█▉        | 3/16 [00:00<00:03,  3.55it/s]    i=4/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  25%|██▌       | 4/16 [00:01<00:03,  3.74it/s]    i=5/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  25%|██▌       | 4/16 [00:01<00:03,  3.74it/s]    i=5/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  31%|███▏      | 5/16 [00:01<00:03,  3.65it/s]    i=6/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  31%|███▏      | 5/16 [00:01<00:03,  3.65it/s]    i=6/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  38%|███▊      | 6/16 [00:01<00:02,  3.93it/s]    i=7/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  38%|███▊      | 6/16 [00:01<00:02,  3.93it/s]     i=7/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  44%|████▍     | 7/16 [00:01<00:02,  4.13it/s]    i=8/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  44%|████▍     | 7/16 [00:01<00:02,  4.13it/s]    i=8/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  50%|█████     | 8/16 [00:02<00:01,  4.35it/s]    i=9/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  50%|█████     | 8/16 [00:02<00:01,  4.35it/s]    i=9/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  56%|█████▋    | 9/16 [00:02<00:01,  4.49it/s]    i=10/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  56%|█████▋    | 9/16 [00:02<00:01,  4.49it/s]    i=10/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  62%|██████▎   | 10/16 [00:02<00:01,  4.53it/s]    i=11/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  62%|██████▎   | 10/16 [00:02<00:01,  4.53it/s]     i=11/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  69%|██████▉   | 11/16 [00:02<00:01,  4.64it/s]    i=12/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  69%|██████▉   | 11/16 [00:02<00:01,  4.64it/s]    i=12/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  75%|███████▌  | 12/16 [00:02<00:00,  4.44it/s]    i=13/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  75%|███████▌  | 12/16 [00:02<00:00,  4.44it/s]    i=13/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  81%|████████▏ | 13/16 [00:03<00:00,  4.35it/s]    i=14/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  81%|████████▏ | 13/16 [00:03<00:00,  4.35it/s]    i=14/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  88%|████████▊ | 14/16 [00:03<00:00,  4.26it/s]    i=15/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  88%|████████▊ | 14/16 [00:03<00:00,  4.26it/s]     i=15/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  94%|█████████▍| 15/16 [00:03<00:00,  4.36it/s]    i=16/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  94%|█████████▍| 15/16 [00:03<00:00,  4.36it/s]    i=16/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0: 100%|██████████| 16/16 [00:03<00:00,  4.44it/s]    i=16/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0: 100%|██████████| 16/16 [00:03<00:00,  4.09it/s]




.. GENERATED FROM PYTHON SOURCE LINES 380-381

Then the optimization for sparse

.. GENERATED FROM PYTHON SOURCE LINES 381-397

.. code-block:: Python


    res_sparse = optimize_model(
        onx,
        feeds={"X": Xb_sp},
        transform=lambda *args, **kwargs: transform_model(*args, use_sparse=True, **kwargs),
        session=create_session,
        params=optim_params,
        verbose=True,
        number=script_args.number,
        repeat=script_args.repeat,
        warmup=script_args.warmup,
        sleep=script_args.sleep,
        n_tries=script_args.tries,
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:   0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:   6%|▋         | 1/16 [00:00<00:07,  2.13it/s]    i=2/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:   6%|▋         | 1/16 [00:00<00:07,  2.13it/s]    i=2/16 TRY=0 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  12%|█▎        | 2/16 [00:00<00:06,  2.10it/s]    i=3/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  12%|█▎        | 2/16 [00:00<00:06,  2.10it/s]     i=3/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  19%|█▉        | 3/16 [00:01<00:06,  2.11it/s]    i=4/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  19%|█▉        | 3/16 [00:01<00:06,  2.11it/s]    i=4/16 TRY=0 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  25%|██▌       | 4/16 [00:01<00:06,  1.95it/s]    i=5/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  25%|██▌       | 4/16 [00:01<00:06,  1.95it/s]    i=5/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  31%|███▏      | 5/16 [00:02<00:05,  1.92it/s]    i=6/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  31%|███▏      | 5/16 [00:02<00:05,  1.92it/s]    i=6/16 TRY=0 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  38%|███▊      | 6/16 [00:02<00:04,  2.01it/s]    i=7/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  38%|███▊      | 6/16 [00:02<00:04,  2.01it/s]     i=7/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  44%|████▍     | 7/16 [00:03<00:04,  2.06it/s]    i=8/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  44%|████▍     | 7/16 [00:03<00:04,  2.06it/s]    i=8/16 TRY=0 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  50%|█████     | 8/16 [00:03<00:03,  2.10it/s]    i=9/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  50%|█████     | 8/16 [00:03<00:03,  2.10it/s]    i=9/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  56%|█████▋    | 9/16 [00:04<00:03,  2.14it/s]    i=10/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  56%|█████▋    | 9/16 [00:04<00:03,  2.14it/s]    i=10/16 TRY=1 parallel_tree=80 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  62%|██████▎   | 10/16 [00:04<00:02,  2.16it/s]    i=11/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  62%|██████▎   | 10/16 [00:04<00:02,  2.16it/s]     i=11/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  69%|██████▉   | 11/16 [00:05<00:02,  2.17it/s]    i=12/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  69%|██████▉   | 11/16 [00:05<00:02,  2.17it/s]    i=12/16 TRY=1 parallel_tree=80 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  75%|███████▌  | 12/16 [00:05<00:01,  2.18it/s]    i=13/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  75%|███████▌  | 12/16 [00:05<00:01,  2.18it/s]    i=13/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  81%|████████▏ | 13/16 [00:06<00:01,  2.17it/s]    i=14/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  81%|████████▏ | 13/16 [00:06<00:01,  2.17it/s]    i=14/16 TRY=1 parallel_tree=40 parallel_tree_N=128 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  88%|████████▊ | 14/16 [00:06<00:00,  2.16it/s]    i=15/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  88%|████████▊ | 14/16 [00:06<00:00,  2.16it/s]     i=15/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=50 batch_size_tree=1 batch_size_rows=1 use_node3=0:  94%|█████████▍| 15/16 [00:07<00:00,  2.16it/s]    i=16/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0:  94%|█████████▍| 15/16 [00:07<00:00,  2.16it/s]    i=16/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0: 100%|██████████| 16/16 [00:07<00:00,  2.18it/s]    i=16/16 TRY=1 parallel_tree=40 parallel_tree_N=64 parallel_N=25 batch_size_tree=1 batch_size_rows=1 use_node3=0: 100%|██████████| 16/16 [00:07<00:00,  2.12it/s]




.. GENERATED FROM PYTHON SOURCE LINES 398-399

And the results.

.. GENERATED FROM PYTHON SOURCE LINES 399-410

.. code-block:: Python


    df_dense = DataFrame(res)
    df_dense["input"] = "dense"
    df_sparse = DataFrame(res_sparse)
    df_sparse["input"] = "sparse"
    df = concat([df_dense, df_sparse], axis=0)
    df.to_csv("plot_op_tree_ensemble_sparse.csv", index=False)
    df.to_excel("plot_op_tree_ensemble_sparse.xlsx", index=False)
    print(df.columns)
    print(df.head(5))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Index(['average', 'deviation', 'min_exec', 'max_exec', 'repeat', 'number',
           'ttime', 'context_size', 'warmup_time', 'n_exp', 'n_exp_name',
           'short_name', 'TRY', 'name', 'parallel_tree', 'parallel_tree_N',
           'parallel_N', 'batch_size_tree', 'batch_size_rows', 'use_node3',
           'input'],
          dtype='object')
        average  deviation  min_exec  ...  batch_size_rows  use_node3  input
    0  0.001634   0.000102  0.001470  ...              NaN        NaN  dense
    1  0.001098   0.000081  0.000997  ...              1.0        0.0  dense
    2  0.001085   0.000056  0.001010  ...              1.0        0.0  dense
    3  0.000959   0.000020  0.000928  ...              1.0        0.0  dense
    4  0.001318   0.000353  0.000897  ...              1.0        0.0  dense

    [5 rows x 21 columns]




.. GENERATED FROM PYTHON SOURCE LINES 411-413

Sorting
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 413-428

.. code-block:: Python


    small_df = df.drop(
        [
            "min_exec",
            "max_exec",
            "repeat",
            "number",
            "context_size",
            "n_exp_name",
        ],
        axis=1,
    ).sort_values("average")
    print(small_df.head(n=10))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

         average  deviation     ttime  ...  batch_size_rows  use_node3  input
    8   0.000902   0.000064  0.009022  ...              1.0        0.0  dense
    11  0.000927   0.000047  0.009271  ...              1.0        0.0  dense
    10  0.000930   0.000023  0.009304  ...              1.0        0.0  dense
    9   0.000947   0.000077  0.009468  ...              1.0        0.0  dense
    3   0.000959   0.000020  0.009588  ...              1.0        0.0  dense
    16  0.000989   0.000044  0.009887  ...              1.0        0.0  dense
    6   0.001006   0.000073  0.010060  ...              1.0        0.0  dense
    7   0.001020   0.000047  0.010200  ...              1.0        0.0  dense
    15  0.001040   0.000076  0.010404  ...              1.0        0.0  dense
    2   0.001085   0.000056  0.010850  ...              1.0        0.0  dense

    [10 rows x 15 columns]




.. GENERATED FROM PYTHON SOURCE LINES 429-431

Worst
+++++

.. GENERATED FROM PYTHON SOURCE LINES 431-435

.. code-block:: Python


    print(small_df.tail(n=10))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

         average  deviation     ttime  ...  batch_size_rows  use_node3   input
    10  0.003291   0.000104  0.032913  ...              1.0        0.0  sparse
    14  0.003344   0.000065  0.033443  ...              1.0        0.0  sparse
    6   0.003354   0.000188  0.033537  ...              1.0        0.0  sparse
    13  0.003392   0.000188  0.033920  ...              1.0        0.0  sparse
    2   0.003404   0.000161  0.034038  ...              1.0        0.0  sparse
    12  0.003440   0.000141  0.034403  ...              1.0        0.0  sparse
    0   0.003460   0.000554  0.034601  ...              1.0        0.0  sparse
    1   0.003567   0.000854  0.035668  ...              1.0        0.0  sparse
    4   0.003985   0.001110  0.039849  ...              1.0        0.0  sparse
    3   0.004441   0.002404  0.044413  ...              1.0        0.0  sparse

    [10 rows x 15 columns]




.. GENERATED FROM PYTHON SOURCE LINES 436-438

Plot
++++

.. GENERATED FROM PYTHON SOURCE LINES 438-444

.. code-block:: Python


    skeys = ",".join(optim_params.keys())
    title = f"TreeEnsemble tuning, n_tries={script_args.tries}\n{skeys}\nlower is better"
    ax = hhistograms(df, title=title, keys=("input", "name"))
    fig = ax.get_figure()
    fig.savefig("plot_op_tree_ensemble_sparse.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_op_tree_ensemble_sparse_001.png
   :alt: TreeEnsemble tuning, n_tries=2 parallel_tree,parallel_tree_N,parallel_N,batch_size_tree,batch_size_rows,use_node3 lower is better
   :srcset: /auto_examples/images/sphx_glr_plot_op_tree_ensemble_sparse_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 13.505 seconds)


.. _sphx_glr_download_auto_examples_plot_op_tree_ensemble_sparse.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_op_tree_ensemble_sparse.ipynb <plot_op_tree_ensemble_sparse.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_op_tree_ensemble_sparse.py <plot_op_tree_ensemble_sparse.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
