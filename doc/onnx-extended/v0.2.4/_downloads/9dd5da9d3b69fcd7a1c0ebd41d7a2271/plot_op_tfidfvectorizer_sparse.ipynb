{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Measuring performance of TfIdfVectorizer\n\nThe banchmark measures the performance of a TfIdfVectizer along two\nparameters, the vocabulary size, the batch size whether. It measures\nthe benefit of using sparse implementation through the computation\ntime and the memory peak.\n\n## A simple model\n\nWe start with a model including only one node TfIdfVectorizer.\nIt only contains unigram. The model processes only sequences of 10\nintegers. The sparsity of the results is then 10 divided by the size of\nvocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gc\nimport time\nimport itertools\nfrom typing import Tuple\nimport numpy as np\nimport pandas\nfrom onnx import ModelProto\nfrom onnx.helper import make_attribute\nfrom tqdm import tqdm\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom onnx_extended.ext_test_case import measure_time, unit_test_going\nfrom onnx_extended.memory_peak import start_spying_on\nfrom onnx_extended.reference import CReferenceEvaluator\nfrom onnx_extended.ortops.optim.cpu import get_ort_ext_libs\nfrom onnx_extended.plotting.benchmark import vhistograms\n\n\ndef make_onnx(n_words: int) -> ModelProto:\n    from skl2onnx.common.data_types import Int64TensorType, FloatTensorType\n    from skl2onnx.algebra.onnx_ops import OnnxTfIdfVectorizer\n\n    # from onnx_array_api.light_api import start\n    # onx = (\n    #     start(opset=19, opsets={\"ai.onnx.ml\": 3})\n    #     .vin(\"X\", elem_type=TensorProto.INT64)\n    #     .ai.onnx.TfIdfVectorizer(\n    #     ...\n    #     )\n    #     .rename(Y)\n    #     .vout(elem_type=TensorProto.FLOAT)\n    #     .to_onnx()\n    # )\n    onx = OnnxTfIdfVectorizer(\n        \"X\",\n        mode=\"TF\",\n        min_gram_length=1,\n        max_gram_length=1,\n        max_skip_count=0,\n        ngram_counts=[0],\n        ngram_indexes=np.arange(n_words).tolist(),\n        pool_int64s=np.arange(n_words).tolist(),\n        output_names=[\"Y\"],\n    ).to_onnx(inputs=[(\"X\", Int64TensorType())], outputs=[(\"Y\", FloatTensorType())])\n    #     .rename(Y)\n    #     .vout(elem_type=TensorProto.FLOAT)\n    #     .to_onnx()\n    # )\n    return onx\n\n\nonx = make_onnx(7)\nref = CReferenceEvaluator(onx)\ngot = ref.run(None, {\"X\": np.array([[0, 1], [2, 3]], dtype=np.int64)})\nprint(got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works as expected. Let's now compare the execution\nwith onnxruntime for different batch size and vocabulary size.\n\n## Benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_sessions(\n    onx: ModelProto,\n) -> Tuple[InferenceSession, InferenceSession, InferenceSession]:\n    # first: onnxruntime\n    ref = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\n    # second: custom kernel equivalent to the onnxruntime implementation\n    for node in onx.graph.node:\n        if node.op_type == \"TfIdfVectorizer\":\n            node.domain = \"onnx_extented.ortops.optim.cpu\"\n            # new_add = make_attribute(\"sparse\", 1)\n            # node.attribute.append(new_add)\n\n    d = onx.opset_import.add()\n    d.domain = \"onnx_extented.ortops.optim.cpu\"\n    d.version = 1\n\n    r = get_ort_ext_libs()\n    opts = SessionOptions()\n    opts.register_custom_ops_library(r[0])\n    cus = InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n    # third: with sparse\n    for node in onx.graph.node:\n        if node.op_type == \"TfIdfVectorizer\":\n            new_add = make_attribute(\"sparse\", 1)\n            node.attribute.append(new_add)\n    cussp = InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n    return ref, cus, cussp\n\n\nif unit_test_going():\n    vocabulary_sizes = [10, 20]\n    batch_sizes = [5, 10]\nelse:\n    vocabulary_sizes = [100, 1000, 5000, 10000]\n    batch_sizes = [1, 10, 500, 1000, 2000]\nconfs = list(itertools.product(vocabulary_sizes, batch_sizes))\n\ndata = []\nfor voc_size, batch_size in tqdm(confs):\n    onx = make_onnx(voc_size)\n    ref, cus, sparse = make_sessions(onx)\n    gc.collect()\n\n    feeds = dict(\n        X=(np.arange(batch_size * 10) % voc_size)\n        .reshape((batch_size, -1))\n        .astype(np.int64)\n    )\n\n    # sparse\n    p = start_spying_on(delay=0.0001)\n    sparse.run(None, feeds)\n    obs = measure_time(\n        lambda sparse=sparse, feeds=feeds: sparse.run(None, feeds), max_time=1\n    )\n    mem = p.stop()\n    obs[\"peak\"] = mem[\"cpu\"].max_peak - mem[\"cpu\"].begin\n    obs[\"name\"] = \"sparse\"\n    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n    data.append(obs)\n    time.sleep(0.1)\n\n    # reference\n    p = start_spying_on(delay=0.0001)\n    ref.run(None, feeds)\n    obs = measure_time(lambda ref=ref, feeds=feeds: ref.run(None, feeds), max_time=1)\n    mem = p.stop()\n    obs[\"peak\"] = mem[\"cpu\"].max_peak - mem[\"cpu\"].begin\n    obs[\"name\"] = \"ref\"\n    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n    data.append(obs)\n    time.sleep(0.1)\n\n    # custom\n    p = start_spying_on(delay=0.0001)\n    cus.run(None, feeds)\n    obs = measure_time(lambda cus=cus, feeds=feeds: cus.run(None, feeds), max_time=1)\n    mem = p.stop()\n    obs[\"peak\"] = mem[\"cpu\"].max_peak - mem[\"cpu\"].begin\n    obs[\"name\"] = \"custom\"\n    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n    data.append(obs)\n    time.sleep(0.1)\n\n    del sparse\n    del cus\n    del ref\n    del feeds\n\ndf = pandas.DataFrame(data)\ndf[\"time\"] = df[\"average\"]\ndf.to_csv(\"plot_op_tfidfvectorizer_sparse.csv\", index=False)\nprint(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing time\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pandas.pivot_table(\n    df, index=[\"voc_size\", \"name\"], columns=\"batch_size\", values=\"average\"\n)\nprint(piv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory peak\n\nIt is always difficult to estimate. A second process is started to measure\nthe physical memory peak during the execution every ms. The figures\nis the difference between this peak and the memory when the measurement\nbegan.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pandas.pivot_table(\n    df, index=[\"voc_size\", \"name\"], columns=\"batch_size\", values=\"peak\"\n)\nprint(piv / 2**20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = vhistograms(df)\nfig = ax[0, 0].get_figure()\nfig.savefig(\"plot_op_tfidfvectorizer_sparse.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Take away\n\nSparse works better when the sparsity is big enough and the batch size as well.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}