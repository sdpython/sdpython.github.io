{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Measuring performance of TfIdfVectorizer\n\nThe banchmark measures the performance of a TfIdfVectizer along two\nparameters, the vocabulary size, the batch size whether. It measures\nthe benefit of using sparse implementation.\n\n## A simple model\n\nWe start with a model including only one node TfIdfVectorizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nfrom typing import Tuple\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nfrom onnx import ModelProto\nfrom onnx.helper import make_attribute\nfrom tqdm import tqdm\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom onnx_extended.ext_test_case import measure_time, unit_test_going\nfrom onnx_extended.reference import CReferenceEvaluator\nfrom onnx_extended.ortops.optim.cpu import get_ort_ext_libs\n\n\ndef make_onnx(n_words: int) -> ModelProto:\n    from skl2onnx.common.data_types import Int64TensorType, FloatTensorType\n    from skl2onnx.algebra.onnx_ops import OnnxTfIdfVectorizer\n\n    # from onnx_array_api.light_api import start\n    # onx = (\n    #     start(opset=19, opsets={\"ai.onnx.ml\": 3})\n    #     .vin(\"X\", elem_type=TensorProto.INT64)\n    #     .ai.onnx.TfIdfVectorizer(\n    #     ...\n    #     )\n    #     .rename(Y)\n    #     .vout(elem_type=TensorProto.FLOAT)\n    #     .to_onnx()\n    # )\n    onx = OnnxTfIdfVectorizer(\n        \"X\",\n        mode=\"TF\",\n        min_gram_length=1,\n        max_gram_length=1,\n        max_skip_count=0,\n        ngram_counts=[0],\n        ngram_indexes=np.arange(n_words).tolist(),\n        pool_int64s=np.arange(n_words).tolist(),\n        output_names=[\"Y\"],\n    ).to_onnx(inputs=[(\"X\", Int64TensorType())], outputs=[(\"Y\", FloatTensorType())])\n    #     .rename(Y)\n    #     .vout(elem_type=TensorProto.FLOAT)\n    #     .to_onnx()\n    # )\n    return onx\n\n\nonx = make_onnx(7)\nref = CReferenceEvaluator(onx)\ngot = ref.run(None, {\"X\": np.array([[0, 1], [2, 3]], dtype=np.int64)})\nprint(got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works as expected. Let's now compare the execution\nwith onnxruntime for different batch size and vocabulary size.\n\n## Benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_sessions(\n    onx: ModelProto,\n) -> Tuple[InferenceSession, InferenceSession, InferenceSession]:\n    # first: onnxruntime\n    ref = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n\n    # second: custom kernel equivalent to the onnxruntime implementation\n    for node in onx.graph.node:\n        if node.op_type == \"TfIdfVectorizer\":\n            node.domain = \"onnx_extented.ortops.optim.cpu\"\n            # new_add = make_attribute(\"sparse\", 1)\n            # node.attribute.append(new_add)\n\n    d = onx.opset_import.add()\n    d.domain = \"onnx_extented.ortops.optim.cpu\"\n    d.version = 1\n\n    r = get_ort_ext_libs()\n    opts = SessionOptions()\n    opts.register_custom_ops_library(r[0])\n    cus = InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n    # third: with sparse\n    for node in onx.graph.node:\n        if node.op_type == \"TfIdfVectorizer\":\n            new_add = make_attribute(\"sparse\", 1)\n            node.attribute.append(new_add)\n    cussp = InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n    return ref, cus, cussp\n\n\nif unit_test_going():\n    vocabulary_sizes = [10, 20]\n    batch_sizes = [10, 20]\nelse:\n    vocabulary_sizes = [100, 1000, 5000, 10000]\n    batch_sizes = [500, 1000, 2000]\nconfs = list(itertools.product(vocabulary_sizes, batch_sizes))\n\ndata = []\nfor voc_size, batch_size in tqdm(confs):\n    onx = make_onnx(voc_size)\n    ref, cus, sparse = make_sessions(onx)\n\n    feeds = dict(\n        X=(np.arange(batch_size * 10) % voc_size)\n        .reshape((batch_size, -1))\n        .astype(np.int64)\n    )\n\n    # reference\n    ref.run(None, feeds)\n    obs = measure_time(lambda: ref.run(None, feeds), max_time=1)\n    obs[\"name\"] = \"ref\"\n    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n    data.append(obs)\n\n    # custom\n    cus.run(None, feeds)\n    obs = measure_time(lambda: cus.run(None, feeds), max_time=1)\n    obs[\"name\"] = \"custom\"\n    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n    data.append(obs)\n\n    # sparse\n    sparse.run(None, feeds)\n    obs = measure_time(lambda: sparse.run(None, feeds), max_time=1)\n    obs[\"name\"] = \"sparse\"\n    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n    data.append(obs)\n\ndf = pandas.DataFrame(data)\ndf.to_csv(\"plot_optim_tfidf.csv\", index=False)\nprint(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pandas.pivot_table(\n    df, index=[\"voc_size\", \"name\"], columns=\"batch_size\", values=\"average\"\n)\nprint(piv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def histograms(df):\n    batch_sizes = list(sorted(set(df.batch_size)))\n    voc_sizes = list(sorted(set(df.voc_size)))\n    B = len(batch_sizes)\n    V = len(voc_sizes)\n\n    fig, ax = plt.subplots(V, B, figsize=(B * 2, V * 2), sharex=True, sharey=True)\n    fig.suptitle(\"Compares Implementations of TfIdfVectorizer\")\n\n    for b in range(B):\n        for v in range(V):\n            aa = ax[v, b]\n            sub = df[(df.batch_size == batch_sizes[b]) & (df.voc_size == voc_sizes[v])][\n                [\"name\", \"average\"]\n            ].set_index(\"name\")\n            if 0 in sub.shape:\n                continue\n            sub.columns = [\"time\"]\n            sub[\"time\"].plot.bar(\n                ax=aa, logy=True, rot=0, color=[\"blue\", \"orange\", \"green\"]\n            )\n            if b == 0:\n                aa.set_ylabel(f\"vocabulary={voc_sizes[v]}\")\n            if v == V - 1:\n                aa.set_xlabel(f\"batch_size={batch_sizes[b]}\")\n            aa.grid(True)\n\n    fig.tight_layout()\n    return fig\n\n\nfig = histograms(df)\nfig.savefig(\"plot_optim_tfidf.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}