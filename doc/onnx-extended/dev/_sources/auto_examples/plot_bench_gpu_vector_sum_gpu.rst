
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_bench_gpu_vector_sum_gpu.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_bench_gpu_vector_sum_gpu.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_bench_gpu_vector_sum_gpu.py:


Measuring CPU/GPU performance with a vector sum
===============================================

The examples compares multiple versions of a vector sum,
CPU, GPU.

Vector Sum
++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 11-131

.. code-block:: Python

    from tqdm import tqdm
    import numpy
    import matplotlib.pyplot as plt
    from pandas import DataFrame
    from onnx_extended.ext_test_case import measure_time, unit_test_going
    from onnx_extended.validation.cpu._validation import (
        vector_sum_array_avx as vector_sum_avx,
        vector_sum_array_avx_parallel as vector_sum_avx_parallel,
    )

    try:
        from onnx_extended.validation.cuda.cuda_example_py import (
            vector_sum0,
            vector_sum6,
            vector_sum_atomic,
        )
    except ImportError:
        # CUDA is not available
        vector_sum0 = None

    obs = []
    dims = [500, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 2000]
    if unit_test_going():
        dims = dims[:3]
    for dim in tqdm(dims):
        values = numpy.ones((dim, dim), dtype=numpy.float32).ravel()

        diff = abs(vector_sum_avx(dim, values) - dim**2)
        res = measure_time(lambda: vector_sum_avx(dim, values), max_time=0.5)

        obs.append(
            dict(
                dim=dim,
                size=values.size,
                time=res["average"],
                direction="avx",
                time_per_element=res["average"] / dim**2,
                diff=diff,
            )
        )

        diff = abs(vector_sum_avx_parallel(dim, values) - dim**2)
        res = measure_time(lambda: vector_sum_avx_parallel(dim, values), max_time=0.5)

        obs.append(
            dict(
                dim=dim,
                size=values.size,
                time=res["average"],
                direction="avx//",
                time_per_element=res["average"] / dim**2,
                diff=diff,
            )
        )

        if vector_sum0 is None:
            # CUDA is not available
            continue

        diff = abs(vector_sum0(values, 32) - dim**2)
        res = measure_time(lambda: vector_sum0(values, 32), max_time=0.5)

        obs.append(
            dict(
                dim=dim,
                size=values.size,
                time=res["average"],
                direction="0cuda32",
                time_per_element=res["average"] / dim**2,
                diff=diff,
            )
        )

        diff = abs(vector_sum_atomic(values, 32) - dim**2)
        res = measure_time(lambda: vector_sum_atomic(values, 32), max_time=0.5)

        obs.append(
            dict(
                dim=dim,
                size=values.size,
                time=res["average"],
                direction="Acuda32",
                time_per_element=res["average"] / dim**2,
                diff=diff,
            )
        )

        diff = abs(vector_sum6(values, 32) - dim**2)
        res = measure_time(lambda: vector_sum6(values, 32), max_time=0.5)

        obs.append(
            dict(
                dim=dim,
                size=values.size,
                time=res["average"],
                direction="6cuda32",
                time_per_element=res["average"] / dim**2,
                diff=diff,
            )
        )

        diff = abs(vector_sum6(values, 256) - dim**2)
        res = measure_time(lambda: vector_sum6(values, 256), max_time=0.5)

        obs.append(
            dict(
                dim=dim,
                size=values.size,
                time=res["average"],
                direction="6cuda256",
                time_per_element=res["average"] / dim**2,
                diff=diff,
            )
        )

    df = DataFrame(obs)
    piv = df.pivot(index="dim", columns="direction", values="time_per_element")
    print(piv)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/14 [00:00<?, ?it/s]      7%|▋         | 1/14 [00:05<01:16,  5.88s/it]     14%|█▍        | 2/14 [00:10<00:59,  4.92s/it]     21%|██▏       | 3/14 [00:14<00:49,  4.52s/it]     29%|██▊       | 4/14 [00:18<00:44,  4.44s/it]     36%|███▌      | 5/14 [00:22<00:38,  4.31s/it]     43%|████▎     | 6/14 [00:26<00:34,  4.26s/it]     50%|█████     | 7/14 [00:31<00:30,  4.32s/it]     57%|█████▋    | 8/14 [00:35<00:26,  4.46s/it]     64%|██████▍   | 9/14 [00:40<00:23,  4.63s/it]     71%|███████▏  | 10/14 [00:46<00:19,  4.87s/it]     79%|███████▊  | 11/14 [00:51<00:15,  5.08s/it]     86%|████████▌ | 12/14 [00:56<00:10,  5.05s/it]     93%|█████████▎| 13/14 [01:01<00:05,  5.03s/it]    100%|██████████| 14/14 [01:07<00:00,  5.19s/it]    100%|██████████| 14/14 [01:07<00:00,  4.82s/it]
    direction       0cuda32      6cuda256  ...           avx         avx//
    dim                                    ...                            
    500        2.210692e-08  2.550990e-08  ...  2.188802e-10  9.092623e-11
    700        1.932894e-08  1.205494e-08  ...  3.562228e-10  3.076543e-10
    800        1.698927e-08  1.713068e-08  ...  3.209673e-10  2.419496e-10
    900        1.552545e-08  1.267153e-08  ...  1.852793e-10  5.729328e-11
    1000       1.424730e-08  1.197979e-08  ...  6.307524e-10  3.051893e-10
    1100       1.551665e-08  1.374250e-08  ...  4.551578e-10  1.039930e-10
    1200       1.475451e-08  1.018128e-08  ...  5.249982e-10  7.492875e-11
    1300       1.388219e-08  1.025067e-08  ...  4.514088e-10  1.259182e-10
    1400       1.143642e-08  8.831154e-09  ...  7.347121e-10  3.122432e-10
    1500       1.151020e-08  8.256674e-09  ...  3.399112e-10  1.839408e-10
    1600       1.086210e-08  8.406561e-09  ...  3.735489e-10  1.882680e-10
    1700       1.058916e-08  7.676374e-09  ...  3.567149e-10  1.864266e-10
    1800       1.007331e-08  8.346193e-09  ...  2.987019e-10  8.451852e-10
    2000       9.859610e-09  7.656553e-09  ...  4.472584e-10  2.472167e-10

    [14 rows x 6 columns]




.. GENERATED FROM PYTHON SOURCE LINES 132-134

Plots
+++++

.. GENERATED FROM PYTHON SOURCE LINES 134-145

.. code-block:: Python


    piv_diff = df.pivot(index="dim", columns="direction", values="diff")
    piv_time = df.pivot(index="dim", columns="direction", values="time")

    fig, ax = plt.subplots(1, 3, figsize=(12, 6))
    piv.plot(ax=ax[0], logx=True, title="Comparison between two summation")
    piv_diff.plot(ax=ax[1], logx=True, logy=True, title="Summation errors")
    piv_time.plot(ax=ax[2], logx=True, logy=True, title="Total time")
    fig.tight_layout()
    fig.savefig("plot_bench_gpu_vector_sum_gpu.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_bench_gpu_vector_sum_gpu_001.png
   :alt: Comparison between two summation, Summation errors, Total time
   :srcset: /auto_examples/images/sphx_glr_plot_bench_gpu_vector_sum_gpu_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 146-185

The results should look like the following.

.. image:: ../_static/vector_sum6_results.png

AVX is still faster. Let's try to understand why.

Profiling
+++++++++

The profiling indicates where the program is most of the time.
It shows when the GPU is waiting and when the memory is copied from
from host (CPU) to device (GPU) and the other way around. There are
the two steps we need to reduce or avoid to make use of the GPU.

Profiling with `nsight-compute <https://developer.nvidia.com/nsight-compute>`_:

::

    nsys profile --trace=cuda,cudnn,cublas,osrt,nvtx,openmp python <file>

If `nsys` fails to find `python`, the command `which python` should locate it.
`<file> can be `plot_bench_gpu_vector_sum_gpu.py` for example.

Then command `nsys-ui` starts the Visual Interface interface of the profiling.
A screen shot shows the following after loading the profiling.

.. image:: ../_static/vector_sum6.png

Most of time is spent in copy the data from CPU memory to GPU memory.
In our case, GPU is not really useful because just copying the data from CPU
to GPU takes more time than processing it with CPU and AVX instructions.

GPU is useful for deep learning because many operations can be chained and
the data stays on GPU memory until the very end. When multiple tools are involved,
torch, numpy, onnxruntime, the `DLPack <https://github.com/dmlc/dlpack>`_
avoids copying the data when switching.

The copy of a big tensor can happens by block. The computation may start
before the data is fully copied.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 9.901 seconds)


.. _sphx_glr_download_auto_examples_plot_bench_gpu_vector_sum_gpu.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bench_gpu_vector_sum_gpu.ipynb <plot_bench_gpu_vector_sum_gpu.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bench_gpu_vector_sum_gpu.py <plot_bench_gpu_vector_sum_gpu.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
