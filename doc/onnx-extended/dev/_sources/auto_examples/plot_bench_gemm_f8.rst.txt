
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_bench_gemm_f8.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_bench_gemm_f8.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_bench_gemm_f8.py:


Measuring Gemm performance with different input and output types
================================================================

This benchmark looks into various combinations allowed by functions
:epkg:`cublasLtMatmul`.

.. GENERATED FROM PYTHON SOURCE LINES 8-26

.. code-block:: default

    import pprint
    from itertools import product
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    from pandas import DataFrame
    from onnx_extended.ext_test_case import unit_test_going

    try:
        from onnx_extended.validation.cuda.cuda_example_py import (
            gemm_benchmark_test,
            get_device_prop,
        )

        has_cuda = True
    except ImportError:
        # CUDA not available.
        has_cuda = False








.. GENERATED FROM PYTHON SOURCE LINES 27-29

Device
++++++

.. GENERATED FROM PYTHON SOURCE LINES 29-38

.. code-block:: default


    if has_cuda:
        prop = get_device_prop()
        pprint.pprint(prop)
    else:
        print("CUDA is not available")
        prop = dict(major=0)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'clockRate': 1569000,
     'computeMode': 0,
     'concurrentKernels': 1,
     'isMultiGpuBoard': 0,
     'major': 6,
     'maxThreadsPerBlock': 1024,
     'minor': 1,
     'multiProcessorCount': 10,
     'name': 'NVIDIA GeForce GTX 1060',
     'sharedMemPerBlock': 49152,
     'totalConstMem': 65536,
     'totalGlobalMem': 6442319872}




.. GENERATED FROM PYTHON SOURCE LINES 39-41

Configurations
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 41-58

.. code-block:: default


    if prop["major"] <= 0:
        # No CUDA.
        tests = []
        dims = []
    elif prop["major"] < 7:
        # No float 8.
        tests = list(range(5))
        dims = [16, 32, 64]
    elif prop["major"] < 9:  # T100, A100
        # No float 8.
        tests = list(range(5))
        dims = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
    else:
        tests = list(range(15))  # H100
        dims = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]








.. GENERATED FROM PYTHON SOURCE LINES 59-61

Benchmark
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 61-117

.. code-block:: default



    def type2string(dt):
        dtypes = {0: "F32", 2: "F16", 14: "BF16", 28: "E4M3", 29: "E5M2"}
        return dtypes[int(dt)]


    pbar = tqdm(list(product(tests, dims)))
    obs = []
    for test, dim in pbar:
        pbar.set_description(f"test={test} dim={dim}")
        if test in {8, 9, 10, 12, 13}:
            # not valid yet
            continue
        if dim < 128:
            n, N = 20, 100
        elif dim < 512:
            n, N = 20, 50
        elif dim < 8192:
            n, N = 10, 25
        else:
            n, N = 3, 5

        # warmup
        gemm_benchmark_test(test, n, dim)

        # benchmark
        res = gemm_benchmark_test(test, N, dim)

        # better rendering
        res["test"] = test
        update = {}
        for k, v in res.items():
            if "type_" in k:
                update[k] = type2string(v)
            if k.startswith("t-"):
                update[k] = res[k] / res["N"]
        update["compute_type"] = f"C{int(res['compute_type'])}"
        update["N"] = int(res["N"])
        update["dim"] = int(res["dim"])
        update["name"] = (
            f"{update['type_a']}x{update['type_b']}->"
            f"{update['type_d']}{update['compute_type']}"
        )
        res.update(update)
        obs.append(res)
        if unit_test_going() and len(obs) > 2:
            break

    df = DataFrame(obs)
    df.to_csv("plot_bench_gemm_f8.csv", index=False)
    df.to_excel("plot_bench_gemm_f8.xlsx", index=False)
    print(df.head().T)

    df.head().T





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/15 [00:00<?, ?it/s]    test=0 dim=16:   0%|          | 0/15 [00:00<?, ?it/s]    test=0 dim=16:   7%|6         | 1/15 [00:06<01:31,  6.54s/it]    test=0 dim=32:   7%|6         | 1/15 [00:06<01:31,  6.54s/it]    test=0 dim=64:   7%|6         | 1/15 [00:06<01:31,  6.54s/it]    test=0 dim=64:  20%|##        | 3/15 [00:06<00:21,  1.76s/it]    test=1 dim=16:  20%|##        | 3/15 [00:06<00:21,  1.76s/it]    test=1 dim=32:  20%|##        | 3/15 [00:06<00:21,  1.76s/it]    test=1 dim=32:  33%|###3      | 5/15 [00:06<00:08,  1.13it/s]    test=1 dim=64:  33%|###3      | 5/15 [00:06<00:08,  1.13it/s]    test=2 dim=16:  33%|###3      | 5/15 [00:06<00:08,  1.13it/s]    test=2 dim=16:  47%|####6     | 7/15 [00:06<00:04,  1.85it/s]    test=2 dim=32:  47%|####6     | 7/15 [00:06<00:04,  1.85it/s]    test=2 dim=64:  47%|####6     | 7/15 [00:07<00:04,  1.85it/s]    test=2 dim=64:  60%|######    | 9/15 [00:07<00:02,  2.76it/s]    test=3 dim=16:  60%|######    | 9/15 [00:07<00:02,  2.76it/s]    test=3 dim=32:  60%|######    | 9/15 [00:07<00:02,  2.76it/s]    test=3 dim=32:  73%|#######3  | 11/15 [00:07<00:01,  2.97it/s]    test=3 dim=64:  73%|#######3  | 11/15 [00:07<00:01,  2.97it/s]    test=3 dim=64:  80%|########  | 12/15 [00:08<00:01,  2.35it/s]    test=4 dim=16:  80%|########  | 12/15 [00:08<00:01,  2.35it/s]    test=4 dim=32:  80%|########  | 12/15 [00:08<00:01,  2.35it/s]    test=4 dim=32:  93%|#########3| 14/15 [00:08<00:00,  3.42it/s]    test=4 dim=64:  93%|#########3| 14/15 [00:08<00:00,  3.42it/s]    test=4 dim=64: 100%|##########| 15/15 [00:08<00:00,  1.73it/s]
                                    0                1                2                3                4
    t-total                  0.000309         0.000369         0.000344         0.000215         0.000292
    t-clean                  0.000004         0.000003         0.000002         0.000002         0.000002
    t-gemm_in                 0.00005         0.000046         0.000046         0.000029         0.000033
    t-setup                  0.000025         0.000039         0.000025         0.000015          0.00003
    epiloque                      1.0              1.0              1.0              1.0              1.0
    compute_type                  C68              C68              C68              C77              C77
    dim                            16               32               64               16               32
    type_a                        F32              F32              F32              F32              F32
    t-gemm                   0.000081         0.000091         0.000076         0.000048         0.000068
    type_b                        F32              F32              F32              F32              F32
    t-workspace_new          0.000016         0.000014          0.00001         0.000012          0.00001
    type_d                        F32              F32              F32              F32              F32
    N                             100              100              100              100              100
    algo                         11.0              0.0              0.0             11.0              0.0
    t-workspace_free         0.000019         0.000019         0.000013         0.000013         0.000014
    t-stream_create               0.0              0.0              0.0              0.0              0.0
    t-gemm_sync              0.000252         0.000317         0.000305         0.000179         0.000255
    workspace_size          1048576.0        1048576.0        1048576.0        1048576.0        1048576.0
    t-stream_destroy         0.000008         0.000007         0.000008         0.000005         0.000006
    test                            0                0                0                1                1
    name              F32xF32->F32C68  F32xF32->F32C68  F32xF32->F32C68  F32xF32->F32C77  F32xF32->F32C77


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>0</th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>4</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>t-total</th>
          <td>0.000309</td>
          <td>0.000369</td>
          <td>0.000344</td>
          <td>0.000215</td>
          <td>0.000292</td>
        </tr>
        <tr>
          <th>t-clean</th>
          <td>0.000004</td>
          <td>0.000003</td>
          <td>0.000002</td>
          <td>0.000002</td>
          <td>0.000002</td>
        </tr>
        <tr>
          <th>t-gemm_in</th>
          <td>0.00005</td>
          <td>0.000046</td>
          <td>0.000046</td>
          <td>0.000029</td>
          <td>0.000033</td>
        </tr>
        <tr>
          <th>t-setup</th>
          <td>0.000025</td>
          <td>0.000039</td>
          <td>0.000025</td>
          <td>0.000015</td>
          <td>0.00003</td>
        </tr>
        <tr>
          <th>epiloque</th>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>1.0</td>
        </tr>
        <tr>
          <th>compute_type</th>
          <td>C68</td>
          <td>C68</td>
          <td>C68</td>
          <td>C77</td>
          <td>C77</td>
        </tr>
        <tr>
          <th>dim</th>
          <td>16</td>
          <td>32</td>
          <td>64</td>
          <td>16</td>
          <td>32</td>
        </tr>
        <tr>
          <th>type_a</th>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
        </tr>
        <tr>
          <th>t-gemm</th>
          <td>0.000081</td>
          <td>0.000091</td>
          <td>0.000076</td>
          <td>0.000048</td>
          <td>0.000068</td>
        </tr>
        <tr>
          <th>type_b</th>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
        </tr>
        <tr>
          <th>t-workspace_new</th>
          <td>0.000016</td>
          <td>0.000014</td>
          <td>0.00001</td>
          <td>0.000012</td>
          <td>0.00001</td>
        </tr>
        <tr>
          <th>type_d</th>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
          <td>F32</td>
        </tr>
        <tr>
          <th>N</th>
          <td>100</td>
          <td>100</td>
          <td>100</td>
          <td>100</td>
          <td>100</td>
        </tr>
        <tr>
          <th>algo</th>
          <td>11.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>11.0</td>
          <td>0.0</td>
        </tr>
        <tr>
          <th>t-workspace_free</th>
          <td>0.000019</td>
          <td>0.000019</td>
          <td>0.000013</td>
          <td>0.000013</td>
          <td>0.000014</td>
        </tr>
        <tr>
          <th>t-stream_create</th>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
        </tr>
        <tr>
          <th>t-gemm_sync</th>
          <td>0.000252</td>
          <td>0.000317</td>
          <td>0.000305</td>
          <td>0.000179</td>
          <td>0.000255</td>
        </tr>
        <tr>
          <th>workspace_size</th>
          <td>1048576.0</td>
          <td>1048576.0</td>
          <td>1048576.0</td>
          <td>1048576.0</td>
          <td>1048576.0</td>
        </tr>
        <tr>
          <th>t-stream_destroy</th>
          <td>0.000008</td>
          <td>0.000007</td>
          <td>0.000008</td>
          <td>0.000005</td>
          <td>0.000006</td>
        </tr>
        <tr>
          <th>test</th>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>1</td>
        </tr>
        <tr>
          <th>name</th>
          <td>F32xF32-&gt;F32C68</td>
          <td>F32xF32-&gt;F32C68</td>
          <td>F32xF32-&gt;F32C68</td>
          <td>F32xF32-&gt;F32C77</td>
          <td>F32xF32-&gt;F32C77</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 118-120

Test definition
+++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 120-127

.. code-block:: default


    col_def = ["name", "test", "type_a", "type_b", "type_d", "compute_type"]
    if df.shape[0] > 0:
        deft = df.copy()
        gr = deft[col_def].groupby(col_def, as_index=False).count()
        print(gr)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                     name  test type_a type_b type_d compute_type
    0  BF16xBF16->BF16C68     4   BF16   BF16   BF16          C68
    1     F16xF16->F16C64     3    F16    F16    F16          C64
    2     F32xF32->F32C68     0    F32    F32    F32          C68
    3     F32xF32->F32C75     2    F32    F32    F32          C75
    4     F32xF32->F32C77     1    F32    F32    F32          C77




.. GENERATED FROM PYTHON SOURCE LINES 128-130

Total time and only gemm
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 130-135

.. code-block:: default


    if df.shape[0] > 0:
        dfi = df[col_def + ["dim", "t-total", "t-gemm_sync"]]
        print(dfi)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                      name  test type_a type_b type_d compute_type  dim   t-total  t-gemm_sync
    0      F32xF32->F32C68     0    F32    F32    F32          C68   16  0.000309     0.000252
    1      F32xF32->F32C68     0    F32    F32    F32          C68   32  0.000369     0.000317
    2      F32xF32->F32C68     0    F32    F32    F32          C68   64  0.000344     0.000305
    3      F32xF32->F32C77     1    F32    F32    F32          C77   16  0.000215     0.000179
    4      F32xF32->F32C77     1    F32    F32    F32          C77   32  0.000292     0.000255
    5      F32xF32->F32C77     1    F32    F32    F32          C77   64  0.000341     0.000298
    6      F32xF32->F32C75     2    F32    F32    F32          C75   16  0.000225     0.000185
    7      F32xF32->F32C75     2    F32    F32    F32          C75   32  0.000193     0.000176
    8      F32xF32->F32C75     2    F32    F32    F32          C75   64  0.000305     0.000263
    9      F16xF16->F16C64     3    F16    F16    F16          C64   16  0.001072     0.001042
    10     F16xF16->F16C64     3    F16    F16    F16          C64   32  0.003353     0.003325
    11     F16xF16->F16C64     3    F16    F16    F16          C64   64  0.006337     0.006303
    12  BF16xBF16->BF16C68     4   BF16   BF16   BF16          C68   16  0.000211     0.000191
    13  BF16xBF16->BF16C68     4   BF16   BF16   BF16          C68   32  0.000352     0.000325
    14  BF16xBF16->BF16C68     4   BF16   BF16   BF16          C68   64  0.000530     0.000502




.. GENERATED FROM PYTHON SOURCE LINES 136-138

Smaller sets
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 138-152

.. code-block:: default


    if df.shape[0] > 0:
        subset = {1, 3, 4, 5, 7}
        dfis = dfi[dfi.test.isin(subset)]
        print()
        print("t-gemm_sync")
        pivi = dfis.pivot_table(index="dim", columns="name", values="t-gemm_sync")
        print(pivi)
        print()
        print("t-total")
        pivi = dfis.pivot_table(index="dim", columns="name", values="t-total")
        print(pivi)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    t-gemm_sync
    name  BF16xBF16->BF16C68  F16xF16->F16C64  F32xF32->F32C77
    dim                                                       
    16              0.000191         0.001042         0.000179
    32              0.000325         0.003325         0.000255
    64              0.000502         0.006303         0.000298

    t-total
    name  BF16xBF16->BF16C68  F16xF16->F16C64  F32xF32->F32C77
    dim                                                       
    16              0.000211         0.001072         0.000215
    32              0.000352         0.003353         0.000292
    64              0.000530         0.006337         0.000341




.. GENERATED FROM PYTHON SOURCE LINES 153-155

Plots
+++++

.. GENERATED FROM PYTHON SOURCE LINES 155-173

.. code-block:: default


    if df.shape[0] > 0:
        piv = df.pivot_table(index="dim", columns="name", values="t-gemm_sync")
        piv.plot(title="MatMul performances")

        fig, ax = plt.subplots(1, 2, figsize=(12, 6))
        piv.plot(ax=ax[0], title="Gemm performance\nlower is better", logx=True, logy=True)

        piv = df[df.test.isin(subset)].pivot_table(
            index="dim", columns="name", values="t-gemm_sync"
        )
        if piv.shape[0] > 0:
            piv.plot(
                ax=ax[1], title="Gemm performance\nlower is better", logx=True, logy=True
            )

        fig.tight_layout()
        fig.savefig("plot_bench_gemm_f8.png")



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_bench_gemm_f8_001.png
         :alt: MatMul performances
         :srcset: /auto_examples/images/sphx_glr_plot_bench_gemm_f8_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_bench_gemm_f8_002.png
         :alt: Gemm performance lower is better, Gemm performance lower is better
         :srcset: /auto_examples/images/sphx_glr_plot_bench_gemm_f8_002.png
         :class: sphx-glr-multi-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  10.597 seconds)


.. _sphx_glr_download_auto_examples_plot_bench_gemm_f8.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bench_gemm_f8.py <plot_bench_gemm_f8.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bench_gemm_f8.ipynb <plot_bench_gemm_f8.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
