
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_optim_tfidf.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_optim_tfidf.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_optim_tfidf.py:


.. _l-plot-optim-tfidf:

Measuring performance of TfIdfVectorizer
========================================

The banchmark measures the performance of a TfIdfVectizer along two
parameters, the vocabulary size, the batch size whether. It measures
the benefit of using sparse implementation.

A simple model
++++++++++++++

We start with a model including only one node TfIdfVectorizer.

.. GENERATED FROM PYTHON SOURCE LINES 16-68

.. code-block:: Python

    import itertools
    from typing import Tuple
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas
    from onnx import ModelProto
    from onnx.helper import make_attribute
    from tqdm import tqdm
    from onnxruntime import InferenceSession, SessionOptions
    from onnx_extended.ext_test_case import measure_time, unit_test_going
    from onnx_extended.reference import CReferenceEvaluator
    from onnx_extended.ortops.optim.cpu import get_ort_ext_libs


    def make_onnx(n_words: int) -> ModelProto:
        from skl2onnx.common.data_types import Int64TensorType, FloatTensorType
        from skl2onnx.algebra.onnx_ops import OnnxTfIdfVectorizer

        # from onnx_array_api.light_api import start
        # onx = (
        #     start(opset=19, opsets={"ai.onnx.ml": 3})
        #     .vin("X", elem_type=TensorProto.INT64)
        #     .ai.onnx.TfIdfVectorizer(
        #     ...
        #     )
        #     .rename(Y)
        #     .vout(elem_type=TensorProto.FLOAT)
        #     .to_onnx()
        # )
        onx = OnnxTfIdfVectorizer(
            "X",
            mode="TF",
            min_gram_length=1,
            max_gram_length=1,
            max_skip_count=0,
            ngram_counts=[0],
            ngram_indexes=np.arange(n_words).tolist(),
            pool_int64s=np.arange(n_words).tolist(),
            output_names=["Y"],
        ).to_onnx(inputs=[("X", Int64TensorType())], outputs=[("Y", FloatTensorType())])
        #     .rename(Y)
        #     .vout(elem_type=TensorProto.FLOAT)
        #     .to_onnx()
        # )
        return onx


    onx = make_onnx(7)
    ref = CReferenceEvaluator(onx)
    got = ref.run(None, {"X": np.array([[0, 1], [2, 3]], dtype=np.int64)})
    print(got)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [2023-11-27 15:07:00,265] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
    [array([[1., 1., 0., 0., 0., 0., 0.],
           [0., 0., 1., 1., 0., 0., 0.]], dtype=float32)]




.. GENERATED FROM PYTHON SOURCE LINES 69-74

It works as expected. Let's now compare the execution
with onnxruntime for different batch size and vocabulary size.

Benchmark
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 74-157

.. code-block:: Python



    def make_sessions(
        onx: ModelProto,
    ) -> Tuple[InferenceSession, InferenceSession, InferenceSession]:
        # first: onnxruntime
        ref = InferenceSession(onx.SerializeToString(), providers=["CPUExecutionProvider"])

        # second: custom kernel equivalent to the onnxruntime implementation
        for node in onx.graph.node:
            if node.op_type == "TfIdfVectorizer":
                node.domain = "onnx_extented.ortops.optim.cpu"
                # new_add = make_attribute("sparse", 1)
                # node.attribute.append(new_add)

        d = onx.opset_import.add()
        d.domain = "onnx_extented.ortops.optim.cpu"
        d.version = 1

        r = get_ort_ext_libs()
        opts = SessionOptions()
        opts.register_custom_ops_library(r[0])
        cus = InferenceSession(
            onx.SerializeToString(), opts, providers=["CPUExecutionProvider"]
        )

        # third: with sparse
        for node in onx.graph.node:
            if node.op_type == "TfIdfVectorizer":
                new_add = make_attribute("sparse", 1)
                node.attribute.append(new_add)
        cussp = InferenceSession(
            onx.SerializeToString(), opts, providers=["CPUExecutionProvider"]
        )

        return ref, cus, cussp


    if unit_test_going():
        vocabulary_sizes = [10, 20]
        batch_sizes = [10, 20]
    else:
        vocabulary_sizes = [100, 1000, 5000, 10000]
        batch_sizes = [500, 1000, 2000]
    confs = list(itertools.product(vocabulary_sizes, batch_sizes))

    data = []
    for voc_size, batch_size in tqdm(confs):
        onx = make_onnx(voc_size)
        ref, cus, sparse = make_sessions(onx)

        feeds = dict(
            X=(np.arange(batch_size * 10) % voc_size)
            .reshape((batch_size, -1))
            .astype(np.int64)
        )

        # reference
        ref.run(None, feeds)
        obs = measure_time(lambda: ref.run(None, feeds), max_time=1)
        obs["name"] = "ref"
        obs.update(dict(voc_size=voc_size, batch_size=batch_size))
        data.append(obs)

        # custom
        cus.run(None, feeds)
        obs = measure_time(lambda: cus.run(None, feeds), max_time=1)
        obs["name"] = "custom"
        obs.update(dict(voc_size=voc_size, batch_size=batch_size))
        data.append(obs)

        # sparse
        sparse.run(None, feeds)
        obs = measure_time(lambda: sparse.run(None, feeds), max_time=1)
        obs["name"] = "sparse"
        obs.update(dict(voc_size=voc_size, batch_size=batch_size))
        data.append(obs)

    df = pandas.DataFrame(data)
    df.to_csv("plot_optim_tfidf.csv", index=False)
    print(df.head())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/12 [00:00<?, ?it/s]      8%|▊         | 1/12 [00:03<00:42,  3.87s/it]     17%|█▋        | 2/12 [00:07<00:37,  3.71s/it]     25%|██▌       | 3/12 [00:11<00:35,  3.93s/it]     33%|███▎      | 4/12 [00:15<00:32,  4.06s/it]     42%|████▏     | 5/12 [00:19<00:27,  3.88s/it]     50%|█████     | 6/12 [00:23<00:22,  3.77s/it]     58%|█████▊    | 7/12 [00:27<00:19,  3.97s/it]     67%|██████▋   | 8/12 [00:31<00:15,  3.88s/it]     75%|███████▌  | 9/12 [00:35<00:12,  4.11s/it]     83%|████████▎ | 10/12 [00:40<00:08,  4.23s/it]     92%|█████████▏| 11/12 [00:44<00:04,  4.18s/it]    100%|██████████| 12/12 [00:51<00:00,  5.17s/it]    100%|██████████| 12/12 [00:51<00:00,  4.31s/it]
        average  deviation  min_exec  ...    name  voc_size  batch_size
    0  0.000128   0.000039  0.000105  ...     ref       100         500
    1  0.000097   0.000019  0.000058  ...  custom       100         500
    2  0.000253   0.000004  0.000235  ...  sparse       100         500
    3  0.000217   0.000019  0.000205  ...     ref       100        1000
    4  0.000118   0.000020  0.000102  ...  custom       100        1000

    [5 rows x 12 columns]




.. GENERATED FROM PYTHON SOURCE LINES 158-160

Plots
+++++

.. GENERATED FROM PYTHON SOURCE LINES 160-167

.. code-block:: Python


    piv = pandas.pivot_table(
        df, index=["voc_size", "name"], columns="batch_size", values="average"
    )
    print(piv)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    batch_size           500       1000      2000
    voc_size name                                
    100      custom  0.000097  0.000118  0.000186
             ref     0.000128  0.000217  0.000389
             sparse  0.000253  0.000477  0.000974
    1000     custom  0.000215  0.000377  0.001005
             ref     0.000590  0.001418  0.003193
             sparse  0.000285  0.000505  0.000921
    5000     custom  0.001305  0.003876  0.011228
             ref     0.004234  0.008521  0.021832
             sparse  0.000312  0.000549  0.000915
    10000    custom  0.006140  0.012380  0.023083
             ref     0.008022  0.018877  0.045320
             sparse  0.000222  0.000624  0.002724




.. GENERATED FROM PYTHON SOURCE LINES 168-169

Graphs.

.. GENERATED FROM PYTHON SOURCE LINES 169-204

.. code-block:: Python



    def histograms(df):
        batch_sizes = list(sorted(set(df.batch_size)))
        voc_sizes = list(sorted(set(df.voc_size)))
        B = len(batch_sizes)
        V = len(voc_sizes)

        fig, ax = plt.subplots(V, B, figsize=(B * 2, V * 2), sharex=True, sharey=True)
        fig.suptitle("Compares Implementations of TfIdfVectorizer")

        for b in range(B):
            for v in range(V):
                aa = ax[v, b]
                sub = df[(df.batch_size == batch_sizes[b]) & (df.voc_size == voc_sizes[v])][
                    ["name", "average"]
                ].set_index("name")
                if 0 in sub.shape:
                    continue
                sub.columns = ["time"]
                sub["time"].plot.bar(
                    ax=aa, logy=True, rot=0, color=["blue", "orange", "green"]
                )
                if b == 0:
                    aa.set_ylabel(f"vocabulary={voc_sizes[v]}")
                if v == V - 1:
                    aa.set_xlabel(f"batch_size={batch_sizes[b]}")
                aa.grid(True)

        fig.tight_layout()
        return fig


    fig = histograms(df)
    fig.savefig("plot_optim_tfidf.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_optim_tfidf_001.png
   :alt: Compares Implementations of TfIdfVectorizer
   :srcset: /auto_examples/images/sphx_glr_plot_optim_tfidf_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 18.712 seconds)


.. _sphx_glr_download_auto_examples_plot_optim_tfidf.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_optim_tfidf.ipynb <plot_optim_tfidf.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_optim_tfidf.py <plot_optim_tfidf.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
