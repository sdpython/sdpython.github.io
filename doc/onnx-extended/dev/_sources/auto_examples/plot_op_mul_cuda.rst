
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_op_mul_cuda.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_op_mul_cuda.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_op_mul_cuda.py:


.. _l-example-op-mul_cuda:

Fusing multiplication operators on CUDA
=======================================

The examples compare the performaance of two fused operators Mul
with the unfused sequence.

Cache Performance
+++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 13-83

.. code-block:: Python


    from onnx_extended.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_op_mul_cuda",
        description=__doc__,
        config=(
            "small",
            "small, short optimization (default), "
            "medium for medium sizes, "
            "large for big sizes",
        ),
        warmup=3,
        repeat=5,
        itype=(1, "1 or 10 for float or float16"),
        expose="config,itype,warmup,repeat",
    )

    itype = script_args.itype
    config = script_args.config
    print(f"config={config}")
    print(f"itype={itype}")

    if config == "small":
        sizes = (256, 512, 1024)
    elif config == "medium":
        sizes = (512, 1024, 2048)
    elif config == "large":
        sizes = (1024, 2048, 4096, 8192)
    else:
        try:
            sizes = list(map(int, config.split(",")))
        except (ValueError, TypeError) as e:
            raise AssertionError(f"Unexpected config value {config!r}.") from e

    import time
    import numpy as np
    import onnx.helper as oh
    from tqdm import tqdm
    from pandas import DataFrame
    from onnxruntime import InferenceSession, SessionOptions, get_available_providers
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    from onnx_extended.ortops.optim.cuda import get_ort_ext_libs


    def get_model1(itype):
        return oh.make_model(
            oh.make_graph(
                [
                    oh.make_node("Mul", ["X", "Y"], ["xy"]),
                    oh.make_node("Mul", ["xy", "Z"], ["xyz"]),
                    oh.make_node("Mul", ["Y", "X"], ["yx"]),
                    oh.make_node("Mul", ["xyz", "yx"], ["final"]),
                ],
                "nd",
                [
                    oh.make_tensor_value_info("X", itype, [None, None]),
                    oh.make_tensor_value_info("Y", itype, [None, None]),
                    oh.make_tensor_value_info("Z", itype, [None, None]),
                ],
                [oh.make_tensor_value_info("final", itype, [None, None])],
            ),
            opset_imports=[oh.make_opsetid("", 18)],
            ir_version=9,
        )


    print(onnx_simple_text_plot(get_model1(itype)))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    config=small
    itype=1
    opset: domain='' version=18
    input: name='X' type=dtype('float32') shape=['', '']
    input: name='Y' type=dtype('float32') shape=['', '']
    input: name='Z' type=dtype('float32') shape=['', '']
    Mul(X, Y) -> xy
      Mul(xy, Z) -> xyz
    Mul(Y, X) -> yx
      Mul(xyz, yx) -> final
    output: name='final' type=dtype('float32') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 84-85

And the other model

.. GENERATED FROM PYTHON SOURCE LINES 85-122

.. code-block:: Python



    def get_model2(itype):
        return oh.make_model(
            oh.make_graph(
                [
                    oh.make_node(
                        "MulMul",
                        ["X", "Y", "Z"],
                        ["xyz"],
                        domain="onnx_extended.ortops.optim.cuda",
                    ),
                    oh.make_node(
                        "MulMul",
                        ["Y", "X", "xyz"],
                        ["final"],
                        domain="onnx_extended.ortops.optim.cuda",
                    ),
                ],
                "nd",
                [
                    oh.make_tensor_value_info("X", itype, [None, None]),
                    oh.make_tensor_value_info("Y", itype, [None, None]),
                    oh.make_tensor_value_info("Z", itype, [None, None]),
                ],
                [oh.make_tensor_value_info("final", itype, [None, None])],
            ),
            opset_imports=[
                oh.make_opsetid("", 18),
                oh.make_opsetid("onnx_extended.ortops.optim.cuda", 1),
            ],
            ir_version=9,
        )


    print(onnx_simple_text_plot(get_model2(itype)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    opset: domain='onnx_extended.ortops.optim.cuda' version=1
    input: name='X' type=dtype('float32') shape=['', '']
    input: name='Y' type=dtype('float32') shape=['', '']
    input: name='Z' type=dtype('float32') shape=['', '']
    MulMul[onnx_extended.ortops.optim.cuda](X, Y, Z) -> xyz
      MulMul[onnx_extended.ortops.optim.cuda](Y, X, xyz) -> final
    output: name='final' type=dtype('float32') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 123-125

InferenceSession
++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 125-142

.. code-block:: Python


    has_cuda = "CUDAExecutionProvider" in get_available_providers()

    if has_cuda:

        dtype = np.float32 if itype == 1 else np.float16

        x = np.random.randn(16, 16).astype(dtype)
        y = np.random.randn(16, 16).astype(dtype)
        z = np.random.randn(16, 16).astype(dtype)
        feeds = dict(X=x, Y=y, Z=z)

        sess1 = InferenceSession(
            get_model1(itype).SerializeToString(), providers=["CUDAExecutionProvider"]
        )
        expected = sess1.run(None, feeds)[0]








.. GENERATED FROM PYTHON SOURCE LINES 143-144

The other model.

.. GENERATED FROM PYTHON SOURCE LINES 144-155

.. code-block:: Python


    if has_cuda:

        opts = SessionOptions()
        opts.register_custom_ops_library(get_ort_ext_libs()[0])

        sess2 = InferenceSession(
            get_model2(itype).SerializeToString(), opts, providers=["CUDAExecutionProvider"]
        )
        got = sess2.run(None, feeds)[0]








.. GENERATED FROM PYTHON SOURCE LINES 156-157

Discrepancies

.. GENERATED FROM PYTHON SOURCE LINES 157-164

.. code-block:: Python


    if has_cuda:

        diff = np.abs(got - expected).max()
        print(f"diff={diff}")









.. GENERATED FROM PYTHON SOURCE LINES 165-169

Benchmark
+++++++++

some code to avoid measuring copying the data from host to device

.. GENERATED FROM PYTHON SOURCE LINES 169-197

.. code-block:: Python



    def move_inputs(sess, feeds):
        from onnxruntime.capi._pybind_state import (
            SessionIOBinding,
            OrtDevice as C_OrtDevice,
            OrtValue as C_OrtValue,
        )

        input_names = [i.name for i in sess.get_inputs()]

        ort_device = C_OrtDevice(C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0)

        feed_ort_value = [
            (name, C_OrtValue.ortvalue_from_numpy(feeds[name], ort_device))
            for name in input_names
        ]

        bind = SessionIOBinding(sess._sess)
        for name, value in feed_ort_value:
            bind.bind_input(
                name, ort_device, feeds[name].dtype, value.shape(), value.data_ptr()
            )
        for o in sess.get_outputs():
            bind.bind_output(o.name, ort_device)
        return bind, feed_ort_value









.. GENERATED FROM PYTHON SOURCE LINES 198-199

Benchmark function

.. GENERATED FROM PYTHON SOURCE LINES 199-240

.. code-block:: Python



    def benchmark(sess, sizes, label):

        data = []
        for size in tqdm(sizes):

            x = np.random.randn(size, size).astype(dtype)
            y = np.random.randn(size, size).astype(dtype)
            z = np.random.randn(size, size).astype(dtype)
            feeds = dict(X=x, Y=y, Z=z)
            bind, cuda_feeds = move_inputs(sess, feeds)

            begin = time.perf_counter()
            for i in range(script_args.warmup):
                # sess.run(None, feeds)
                sess._sess.run_with_iobinding(bind, None)
            warmup = time.perf_counter() - begin

            times = []
            for i in range(script_args.repeat):
                begin = time.perf_counter()
                # sess.run(None, feeds)
                sess._sess.run_with_iobinding(bind, None)
                times.append(time.perf_counter() - begin)

            npt = np.array(times)
            obs = dict(
                warmup=warmup,
                time=npt.mean(),
                std=npt.std(),
                min=npt.min(),
                max=npt.max(),
                repeat=script_args.repeat,
                size=size,
                label=label,
            )
            data.append(obs)
        return data









.. GENERATED FROM PYTHON SOURCE LINES 241-242

Not Fused.

.. GENERATED FROM PYTHON SOURCE LINES 242-249

.. code-block:: Python


    if has_cuda:

        print(f"sizes={sizes}")

        data_mul = benchmark(sess1, sizes, "Not Fused")








.. GENERATED FROM PYTHON SOURCE LINES 250-251

Fused.

.. GENERATED FROM PYTHON SOURCE LINES 251-257

.. code-block:: Python


    if has_cuda:

        data_mulmul = benchmark(sess2, sizes, "Fused")









.. GENERATED FROM PYTHON SOURCE LINES 258-260

Data
++++

.. GENERATED FROM PYTHON SOURCE LINES 260-268

.. code-block:: Python


    if has_cuda:

        df = DataFrame(data_mul + data_mulmul)
        df.to_csv("plot_op_mul_cuda.csv", index=False)
        df.to_csv("plot_op_mul_cuda.xlsx", index=False)
        print(df.head())








.. GENERATED FROM PYTHON SOURCE LINES 269-270

Pivot.

.. GENERATED FROM PYTHON SOURCE LINES 270-284

.. code-block:: Python


    if has_cuda:

        pivot = df.pivot(index="size", columns="label", values="time")
        pivot["ratio"] = pivot["Fused"] / pivot["Not Fused"]
        print(pivot)

        ax = pivot[["Not Fused", "Fused"]].plot(
            logx=True,
            logy=True,
            title=f"Fused/Unfused element wise multiplication on CUDA\nitype={itype}",
        )
        ax.get_figure().savefig("plot_op_mul_cuda.png")








.. GENERATED FROM PYTHON SOURCE LINES 285-286

It seems the fused operator is 33% faster.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.114 seconds)


.. _sphx_glr_download_auto_examples_plot_op_mul_cuda.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_op_mul_cuda.ipynb <plot_op_mul_cuda.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_op_mul_cuda.py <plot_op_mul_cuda.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
