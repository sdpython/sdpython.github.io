
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_bench_sparse_access.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_bench_sparse_access.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_bench_sparse_access.py:


Evaluating random access for sparse
===================================

Whenever computing the prediction of a tree with a sparse tensor,
is it faster to density first and then to compute the prediction or to
keep the tensor in its sparse representation and do look up?
The parameter *nrnd* can be seen as the depth of a tree.

.. GENERATED FROM PYTHON SOURCE LINES 11-45

.. code-block:: Python


    import itertools
    import numpy as np
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    from pandas import DataFrame
    from onnx_extended.ext_test_case import unit_test_going
    from onnx_extended.args import get_parsed_args
    from onnx_extended.validation.cpu._validation import evaluate_sparse


    expose = "repeat,warmup,nrows,ncols,sparsity,nrnd,ntimes"
    script_args = get_parsed_args(
        "plot_bench_sparse_access",
        description=__doc__,
        nrows=(10 if unit_test_going() else 100, "number of rows"),
        ncols=(10 if unit_test_going() else 100000, "number of columns"),
        ntimes=(
            "1" if unit_test_going() else "2,4,8",
            "number of times to do nrnd random accesses per row",
        ),
        sparsity=(
            "0.1,0.2" if unit_test_going() else "0.75,0.8,0.9,0.95,0.99,0.999,0.9999",
            "sparsities to try",
        ),
        repeat=2 if unit_test_going() else 5,
        warmup=1 if unit_test_going() else 3,
        nrnd=(10, "number of random features to access"),
        expose=expose,
    )

    for att in sorted(expose.split(",")):
        print(f"{att}={getattr(script_args, att)}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ncols=100000
    nrnd=10
    nrows=100
    ntimes=2,4,8
    repeat=5
    sparsity=0.75,0.8,0.9,0.95,0.99,0.999,0.9999
    warmup=3




.. GENERATED FROM PYTHON SOURCE LINES 46-48

Sparse tensor
+++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 48-67

.. code-block:: Python



    def make_sparse_random_tensor(n_rows: int, n_cols: int, sparsity: float):
        t = np.random.rand(n_rows, n_cols).astype(np.float32)
        m = np.random.rand(n_rows, n_cols).astype(np.float32)
        t[m <= sparsity] = 0
        return t


    sparsity = list(map(float, script_args.sparsity.split(",")))
    ntimes = list(map(int, script_args.ntimes.split(",")))
    t = make_sparse_random_tensor(script_args.nrows, script_args.ncols, sparsity[0])
    ev = evaluate_sparse(t, script_args.nrnd, ntimes[0], script_args.repeat, 3)
    print(f"dense:  initialization:{ev[0][0]:1.3g}")
    print(f"                access:{ev[0][1]:1.3g}")
    print(f"sparse: initialization:{ev[1][0]:1.3g}")
    print(f"                access:{ev[1][1]:1.3g}")
    print(f"Ratio sparse/dense: {ev[1][1] / ev[0][1]}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dense:  initialization:0.0108
                    access:2.63e-05
    sparse: initialization:0.00563
                    access:0.00034
    Ratio sparse/dense: 12.96138440450807




.. GENERATED FROM PYTHON SOURCE LINES 68-69

If > 1, sparse is slower.

.. GENERATED FROM PYTHON SOURCE LINES 71-74

Try sparsity
++++++++++++


.. GENERATED FROM PYTHON SOURCE LINES 74-100

.. code-block:: Python


    tries = list(itertools.product(ntimes, sparsity))

    data = []
    for nt, sp in tqdm(tries):
        t = make_sparse_random_tensor(script_args.nrows, script_args.ncols, sp)
        ev = evaluate_sparse(t, script_args.nrnd, nt, script_args.repeat, 3)
        obs = dict(
            dense0=ev[0][0],
            dense1=ev[0][1],
            dense=ev[0][0] + ev[0][1],
            sparse0=ev[1][0],
            sparse1=ev[1][1],
            sparse=ev[1][0] + ev[1][1],
            sparsity=sp,
            rows=t.shape[0],
            cols=t.shape[1],
            repeat=script_args.repeat,
            random=script_args.nrnd,
            ntimes=nt,
        )
        data.append(obs)

    df = DataFrame(data)
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/21 [00:00<?, ?it/s]      5%|▍         | 1/21 [00:00<00:07,  2.58it/s]     10%|▉         | 2/21 [00:00<00:06,  2.76it/s]     14%|█▍        | 3/21 [00:01<00:06,  2.89it/s]     19%|█▉        | 4/21 [00:01<00:05,  2.95it/s]     24%|██▍       | 5/21 [00:01<00:05,  3.07it/s]     29%|██▊       | 6/21 [00:02<00:05,  2.98it/s]     33%|███▎      | 7/21 [00:02<00:04,  3.21it/s]     38%|███▊      | 8/21 [00:02<00:04,  3.09it/s]     43%|████▎     | 9/21 [00:03<00:04,  2.99it/s]     48%|████▊     | 10/21 [00:03<00:03,  3.11it/s]     52%|█████▏    | 11/21 [00:03<00:03,  3.17it/s]     57%|█████▋    | 12/21 [00:03<00:02,  3.09it/s]     62%|██████▏   | 13/21 [00:04<00:02,  3.27it/s]     67%|██████▋   | 14/21 [00:04<00:02,  3.43it/s]     71%|███████▏  | 15/21 [00:04<00:01,  3.24it/s]     76%|███████▌  | 16/21 [00:05<00:01,  3.18it/s]     81%|████████  | 17/21 [00:05<00:01,  2.94it/s]     86%|████████▌ | 18/21 [00:05<00:00,  3.07it/s]     90%|█████████ | 19/21 [00:06<00:00,  3.19it/s]     95%|█████████▌| 20/21 [00:06<00:00,  3.50it/s]    100%|██████████| 21/21 [00:06<00:00,  3.70it/s]    100%|██████████| 21/21 [00:06<00:00,  3.19it/s]
          dense0    dense1     dense   sparse0   sparse1    sparse  sparsity  rows    cols  repeat  random  ntimes
    0   0.009865  0.000026  0.009891  0.007750  0.000513  0.008264    0.7500   100  100000       5      10       2
    1   0.011080  0.000029  0.011109  0.005077  0.000280  0.005357    0.8000   100  100000       5      10       2
    2   0.014681  0.000040  0.014721  0.003232  0.000308  0.003539    0.9000   100  100000       5      10       2
    3   0.013472  0.000034  0.013507  0.002504  0.000473  0.002977    0.9500   100  100000       5      10       2
    4   0.009057  0.000021  0.009078  0.000225  0.000127  0.000352    0.9900   100  100000       5      10       2
    5   0.016370  0.000068  0.016438  0.000053  0.000187  0.000240    0.9990   100  100000       5      10       2
    6   0.006080  0.000019  0.006098  0.000005  0.000039  0.000044    0.9999   100  100000       5      10       2
    7   0.010253  0.000043  0.010296  0.005514  0.000618  0.006132    0.7500   100  100000       5      10       4
    8   0.010225  0.000047  0.010272  0.004311  0.000511  0.004822    0.8000   100  100000       5      10       4
    9   0.009613  0.000040  0.009653  0.002160  0.000427  0.002587    0.9000   100  100000       5      10       4
    10  0.009015  0.000043  0.009058  0.001030  0.000343  0.001373    0.9500   100  100000       5      10       4
    11  0.017157  0.000088  0.017245  0.000218  0.000245  0.000463    0.9900   100  100000       5      10       4
    12  0.011787  0.000077  0.011864  0.000097  0.000603  0.000700    0.9990   100  100000       5      10       4
    13  0.006192  0.000033  0.006225  0.000005  0.000082  0.000087    0.9999   100  100000       5      10       4
    14  0.009956  0.000070  0.010026  0.005431  0.001043  0.006474    0.7500   100  100000       5      10       8
    15  0.010456  0.000082  0.010538  0.004420  0.001025  0.005445    0.8000   100  100000       5      10       8
    16  0.009433  0.000072  0.009506  0.002143  0.000762  0.002905    0.9000   100  100000       5      10       8
    17  0.014880  0.000057  0.014937  0.001067  0.000689  0.001756    0.9500   100  100000       5      10       8
    18  0.006612  0.000051  0.006663  0.000213  0.000511  0.000724    0.9900   100  100000       5      10       8
    19  0.006272  0.000063  0.006335  0.000024  0.000332  0.000355    0.9990   100  100000       5      10       8
    20  0.005804  0.000053  0.005856  0.000005  0.000161  0.000166    0.9999   100  100000       5      10       8




.. GENERATED FROM PYTHON SOURCE LINES 101-102

Plots

.. GENERATED FROM PYTHON SOURCE LINES 102-120

.. code-block:: Python


    nts = list(sorted(set(df.ntimes)))

    fig, ax = plt.subplots(len(nts), 2, figsize=(3 * len(nts), 10))
    for i, nt in enumerate(nts):
        sub = df[df.ntimes == nt]
        sub[["sparsity", "dense", "sparse"]].set_index("sparsity").plot(
            title=f"Dense vs Sparsity, ntimes={nt}",
            logy=True,
            ax=ax[0] if len(ax.shape) == 1 else ax[i, 0],
        )
        sub[["sparsity", "dense1", "sparse1"]].set_index("sparsity").plot(
            title="Dense vs Sparsity (access only)",
            logy=True,
            ax=ax[1] if len(ax.shape) == 1 else ax[i, 0],
        )
    fig.tight_layout()
    fig.savefig("plot_bench_sparse_access.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_bench_sparse_access_001.png
   :alt: Dense vs Sparsity (access only), Dense vs Sparsity (access only), Dense vs Sparsity (access only)
   :srcset: /auto_examples/images/sphx_glr_plot_bench_sparse_access_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 7.851 seconds)


.. _sphx_glr_download_auto_examples_plot_bench_sparse_access.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bench_sparse_access.ipynb <plot_bench_sparse_access.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bench_sparse_access.py <plot_bench_sparse_access.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_bench_sparse_access.zip <plot_bench_sparse_access.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
