
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_op_tree_ensemble_optim.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_op_tree_ensemble_optim.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_op_tree_ensemble_optim.py:


.. _l-plot-optim-tree-ensemble:

TreeEnsemble optimization
=========================

The execution of a TreeEnsembleRegressor can lead to very different results
depending on how the computation is parallelized. By trees,
by rows, by both, for only one row, for a short batch of rows, a longer one.
The implementation in :epkg:`onnxruntime` does not let the user changed
the predetermined settings but a custom kernel might. That's what this example
is measuring.

The default set of optimized parameters is very short and is meant to be executed
fast. Many more parameters can be tried.

::

    python plot_op_tree_ensemble_optim --scenario=LONG

To change the training parameters:

::

    python plot_op_tree_ensemble_optim.py
        --n_trees=100
        --max_depth=10
        --n_features=50
        --batch_size=100000
    
Another example with a full list of parameters:

    python plot_op_tree_ensemble_optim.py
        --n_trees=100
        --max_depth=10
        --n_features=50
        --batch_size=100000
        --tries=3
        --scenario=CUSTOM
        --parallel_tree=80,40
        --parallel_tree_N=128,64
        --parallel_N=50,25
        --batch_size_tree=1,2
        --batch_size_rows=1,2
        --use_node3=0

Another example:

::

    python plot_op_tree_ensemble_optim.py
        --n_trees=100 --n_features=10 --batch_size=10000 --max_depth=8 -s SHORT        

.. GENERATED FROM PYTHON SOURCE LINES 54-106

.. code-block:: Python

    import logging
    import os
    import timeit
    from typing import Tuple
    import numpy
    import onnx
    from onnx import ModelProto
    from onnx.helper import make_graph, make_model
    from onnx.reference import ReferenceEvaluator
    from pandas import DataFrame
    from sklearn.datasets import make_regression
    from sklearn.ensemble import RandomForestRegressor
    from skl2onnx import to_onnx
    from onnxruntime import InferenceSession, SessionOptions
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    from onnx_extended.reference import CReferenceEvaluator
    from onnx_extended.ortops.optim.cpu import get_ort_ext_libs
    from onnx_extended.ortops.optim.optimize import (
        change_onnx_operator_domain,
        get_node_attribute,
        optimize_model,
    )
    from onnx_extended.tools.onnx_nodes import multiply_tree
    from onnx_extended.args import get_parsed_args
    from onnx_extended.ext_test_case import unit_test_going
    from onnx_extended.plotting.benchmark import hhistograms

    logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)

    script_args = get_parsed_args(
        "plot_op_tree_ensemble_optim",
        description=__doc__,
        scenarios={
            "SHORT": "short optimization (default)",
            "LONG": "test more options",
            "CUSTOM": "use values specified by the command line",
        },
        n_features=(2 if unit_test_going() else 5, "number of features to generate"),
        n_trees=(3 if unit_test_going() else 10, "number of trees to train"),
        max_depth=(2 if unit_test_going() else 5, "max_depth"),
        batch_size=(1000 if unit_test_going() else 10000, "batch size"),
        parallel_tree=("80,160,40", "values to try for parallel_tree"),
        parallel_tree_N=("256,128,64", "values to try for parallel_tree_N"),
        parallel_N=("100,50,25", "values to try for parallel_N"),
        batch_size_tree=("2,4,8", "values to try for batch_size_tree"),
        batch_size_rows=("2,4,8", "values to try for batch_size_rows"),
        use_node3=("0,1", "values to try for use_node3"),
        expose="",
        n_jobs=("-1", "number of jobs to train the RandomForestRegressor"),
    )









.. GENERATED FROM PYTHON SOURCE LINES 107-109

Training a model
++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 109-157

.. code-block:: Python



    def train_model(
        batch_size: int, n_features: int, n_trees: int, max_depth: int
    ) -> Tuple[str, numpy.ndarray, numpy.ndarray]:
        filename = f"plot_op_tree_ensemble_optim-f{n_features}-{n_trees}-d{max_depth}.onnx"
        if not os.path.exists(filename):
            X, y = make_regression(
                batch_size + max(batch_size, 2 ** (max_depth + 1)),
                n_features=n_features,
                n_targets=1,
            )
            print(f"Training to get {filename!r} with X.shape={X.shape}")
            X, y = X.astype(numpy.float32), y.astype(numpy.float32)
            # To be faster, we train only 1 tree.
            model = RandomForestRegressor(
                1, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)
            )
            model.fit(X[:-batch_size], y[:-batch_size])
            onx = to_onnx(model, X[:1])

            # And wd multiply the trees.
            node = multiply_tree(onx.graph.node[0], n_trees)
            onx = make_model(
                make_graph([node], onx.graph.name, onx.graph.input, onx.graph.output),
                domain=onx.domain,
                opset_imports=onx.opset_import,
            )

            with open(filename, "wb") as f:
                f.write(onx.SerializeToString())
        else:
            X, y = make_regression(batch_size, n_features=n_features, n_targets=1)
            X, y = X.astype(numpy.float32), y.astype(numpy.float32)
        Xb, yb = X[-batch_size:].copy(), y[-batch_size:].copy()
        return filename, Xb, yb


    batch_size = script_args.batch_size
    n_features = script_args.n_features
    n_trees = script_args.n_trees
    max_depth = script_args.max_depth

    print(f"batch_size={batch_size}")
    print(f"n_features={n_features}")
    print(f"n_trees={n_trees}")
    print(f"max_depth={max_depth}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    batch_size=10000
    n_features=5
    n_trees=10
    max_depth=5




.. GENERATED FROM PYTHON SOURCE LINES 158-159

training

.. GENERATED FROM PYTHON SOURCE LINES 159-165

.. code-block:: Python


    filename, Xb, yb = train_model(batch_size, n_features, n_trees, max_depth)

    print(f"Xb.shape={Xb.shape}")
    print(f"yb.shape={yb.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Xb.shape=(10000, 5)
    yb.shape=(10000,)




.. GENERATED FROM PYTHON SOURCE LINES 166-173

Rewrite the onnx file to use a different kernel
+++++++++++++++++++++++++++++++++++++++++++++++

The custom kernel is mapped to a custom operator with the same name
the attributes and domain = `"onnx_extented.ortops.optim.cpu"`.
We call a function to do that replacement.
First the current model.

.. GENERATED FROM PYTHON SOURCE LINES 173-178

.. code-block:: Python


    with open(filename, "rb") as f:
        onx = onnx.load(f)
    print(onnx_simple_text_plot(onx))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=19
    input: name='X' type=dtype('float32') shape=['', 5]
    TreeEnsembleRegressor(X, n_targets=1, nodes_falsenodeids=630:[32,17,10...62,0,0], nodes_featureids=630:[4,1,3...0,1,0], nodes_hitrates=630:[1.0,1.0...1.0,1.0], nodes_missing_value_tracks_true=630:[0,0,0...0,0,0], nodes_modes=630:[b'BRANCH_LEQ',b'BRANCH_LEQ'...b'LEAF',b'LEAF'], nodes_nodeids=630:[0,1,2...60,61,62], nodes_treeids=630:[0,0,0...9,9,9], nodes_truenodeids=630:[1,2,3...61,0,0], nodes_values=630:[-0.18342168629169464,-0.16073280572891235...0.6209087371826172,0.0], post_transform=b'NONE', target_ids=320:[0,0,0...0,0,0], target_nodeids=320:[5,6,8...59,61,62], target_treeids=320:[0,0,0...9,9,9], target_weights=320:[-317.9538879394531,-208.63888549804688...189.80604553222656,273.3984069824219]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 179-180

And then the modified model.

.. GENERATED FROM PYTHON SOURCE LINES 180-207

.. code-block:: Python



    def transform_model(model, **kwargs):
        onx = ModelProto()
        onx.ParseFromString(model.SerializeToString())
        att = get_node_attribute(onx.graph.node[0], "nodes_modes")
        modes = ",".join(map(lambda s: s.decode("ascii"), att.strings)).replace(
            "BRANCH_", ""
        )
        return change_onnx_operator_domain(
            onx,
            op_type="TreeEnsembleRegressor",
            op_domain="ai.onnx.ml",
            new_op_domain="onnx_extented.ortops.optim.cpu",
            nodes_modes=modes,
            **kwargs,
        )


    print("Tranform model to add a custom node.")
    onx_modified = transform_model(onx)
    print(f"Save into {filename + 'modified.onnx'!r}.")
    with open(filename + "modified.onnx", "wb") as f:
        f.write(onx_modified.SerializeToString())
    print("done.")
    print(onnx_simple_text_plot(onx_modified))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Tranform model to add a custom node.
    Save into 'plot_op_tree_ensemble_optim-f5-10-d5.onnxmodified.onnx'.
    done.
    opset: domain='ai.onnx.ml' version=1
    opset: domain='' version=19
    opset: domain='onnx_extented.ortops.optim.cpu' version=1
    input: name='X' type=dtype('float32') shape=['', 5]
    TreeEnsembleRegressor[onnx_extented.ortops.optim.cpu](X, nodes_modes=b'LEQ,LEQ,LEQ,LEQ,LEQ,LEAF,LEAF,LEQ,LEAF...LEAF,LEAF', n_targets=1, nodes_falsenodeids=630:[32,17,10...62,0,0], nodes_featureids=630:[4,1,3...0,1,0], nodes_hitrates=630:[1.0,1.0...1.0,1.0], nodes_missing_value_tracks_true=630:[0,0,0...0,0,0], nodes_nodeids=630:[0,1,2...60,61,62], nodes_treeids=630:[0,0,0...9,9,9], nodes_truenodeids=630:[1,2,3...61,0,0], nodes_values=630:[-0.18342168629169464,-0.16073280572891235...0.6209087371826172,0.0], post_transform=b'NONE', target_ids=320:[0,0,0...0,0,0], target_nodeids=320:[5,6,8...59,61,62], target_treeids=320:[0,0,0...9,9,9], target_weights=320:[-317.9538879394531,-208.63888549804688...189.80604553222656,273.3984069824219]) -> variable
    output: name='variable' type=dtype('float32') shape=['', 1]




.. GENERATED FROM PYTHON SOURCE LINES 208-210

Comparing onnxruntime and the custom kernel
+++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 210-231

.. code-block:: Python


    print(f"Loading {filename!r}")
    sess_ort = InferenceSession(filename, providers=["CPUExecutionProvider"])

    r = get_ort_ext_libs()
    print(f"Creating SessionOptions with {r!r}")
    opts = SessionOptions()
    if r is not None:
        opts.register_custom_ops_library(r[0])

    print(f"Loading modified {filename!r}")
    sess_cus = InferenceSession(
        onx_modified.SerializeToString(), opts, providers=["CPUExecutionProvider"]
    )

    print(f"Running once with shape {Xb.shape}.")
    base = sess_ort.run(None, {"X": Xb})[0]
    print(f"Running modified with shape {Xb.shape}.")
    got = sess_cus.run(None, {"X": Xb})[0]
    print("done.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading 'plot_op_tree_ensemble_optim-f5-10-d5.onnx'
    Creating SessionOptions with ['/home/xadupre/github/onnx-extended/onnx_extended/ortops/optim/cpu/libortops_optim_cpu.so']
    Loading modified 'plot_op_tree_ensemble_optim-f5-10-d5.onnx'
    Running once with shape (10000, 5).
    Running modified with shape (10000, 5).
    done.




.. GENERATED FROM PYTHON SOURCE LINES 232-233

Discrepancies?

.. GENERATED FROM PYTHON SOURCE LINES 233-238

.. code-block:: Python


    d = numpy.abs(base - got)
    ya = numpy.abs(base).mean()
    print(f"Discrepancies: max={d.max() / ya}, mean={d.mean() / ya} (A={ya})")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Discrepancies: max=3.6033654282618954e-07, mean=4.930417318860236e-08 (A=1355.070068359375)




.. GENERATED FROM PYTHON SOURCE LINES 239-243

Simple verification
+++++++++++++++++++

Baseline with onnxruntime.

.. GENERATED FROM PYTHON SOURCE LINES 243-246

.. code-block:: Python

    t1 = timeit.timeit(lambda: sess_ort.run(None, {"X": Xb}), number=50)
    print(f"baseline: {t1}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    baseline: 0.13015860000086832




.. GENERATED FROM PYTHON SOURCE LINES 247-248

The custom implementation.

.. GENERATED FROM PYTHON SOURCE LINES 248-251

.. code-block:: Python

    t2 = timeit.timeit(lambda: sess_cus.run(None, {"X": Xb}), number=50)
    print(f"new time: {t2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    new time: 0.0556018999996013




.. GENERATED FROM PYTHON SOURCE LINES 252-253

The same implementation but ran from the onnx python backend.

.. GENERATED FROM PYTHON SOURCE LINES 253-258

.. code-block:: Python

    ref = CReferenceEvaluator(filename)
    ref.run(None, {"X": Xb})
    t3 = timeit.timeit(lambda: ref.run(None, {"X": Xb}), number=50)
    print(f"CReferenceEvaluator: {t3}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    CReferenceEvaluator: 0.04286329999740701




.. GENERATED FROM PYTHON SOURCE LINES 259-260

The python implementation but from the onnx python backend.

.. GENERATED FROM PYTHON SOURCE LINES 260-268

.. code-block:: Python

    if n_trees < 50:
        # It is usully slow.
        ref = ReferenceEvaluator(filename)
        ref.run(None, {"X": Xb})
        t4 = timeit.timeit(lambda: ref.run(None, {"X": Xb}), number=5)
        print(f"ReferenceEvaluator: {t4} (only 5 times instead of 50)")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ReferenceEvaluator: 8.142533099999127 (only 5 times instead of 50)




.. GENERATED FROM PYTHON SOURCE LINES 269-278

Time for comparison
+++++++++++++++++++

The custom kernel supports the same attributes as *TreeEnsembleRegressor*
plus new ones to tune the parallelization. They can be seen in
`tree_ensemble.cc <https://github.com/sdpython/onnx-extended/
blob/main/onnx_extended/ortops/optim/cpu/tree_ensemble.cc#L102>`_.
Let's try out many possibilities.
The default values are the first ones.

.. GENERATED FROM PYTHON SOURCE LINES 278-326

.. code-block:: Python


    if unit_test_going():
        optim_params = dict(
            parallel_tree=[40],  # default is 80
            parallel_tree_N=[128],  # default is 128
            parallel_N=[50, 25],  # default is 50
            batch_size_tree=[1],  # default is 1
            batch_size_rows=[1],  # default is 1
            use_node3=[0],  # default is 0
        )
    elif script_args.scenario in (None, "SHORT"):
        optim_params = dict(
            parallel_tree=[80, 40],  # default is 80
            parallel_tree_N=[128, 64],  # default is 128
            parallel_N=[50, 25],  # default is 50
            batch_size_tree=[1],  # default is 1
            batch_size_rows=[1],  # default is 1
            use_node3=[0],  # default is 0
        )
    elif script_args.scenario == "LONG":
        optim_params = dict(
            parallel_tree=[80, 160, 40],
            parallel_tree_N=[256, 128, 64],
            parallel_N=[100, 50, 25],
            batch_size_tree=[1, 2, 4, 8],
            batch_size_rows=[1, 2, 4, 8],
            use_node3=[0, 1],
        )
    elif script_args.scenario == "CUSTOM":
        optim_params = dict(
            parallel_tree=list(int(i) for i in script_args.parallel_tree.split(",")),
            parallel_tree_N=list(int(i) for i in script_args.parallel_tree_N.split(",")),
            parallel_N=list(int(i) for i in script_args.parallel_N.split(",")),
            batch_size_tree=list(int(i) for i in script_args.batch_size_tree.split(",")),
            batch_size_rows=list(int(i) for i in script_args.batch_size_rows.split(",")),
            use_node3=list(int(i) for i in script_args.use_node3.split(",")),
        )
    else:
        raise ValueError(
            f"Unknown scenario {script_args.scenario!r}, use --help to get them."
        )

    cmds = []
    for att, value in optim_params.items():
        cmds.append(f"--{att}={','.join(map(str, value))}")
    print("Full list of optimization parameters:")
    print(" ".join(cmds))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Full list of optimization parameters:
    --parallel_tree=80,40 --parallel_tree_N=128,64 --parallel_N=50,25 --batch_size_tree=1 --batch_size_rows=1 --use_node3=0




.. GENERATED FROM PYTHON SOURCE LINES 327-328

Then the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 328-358

.. code-block:: Python



    def create_session(onx):
        opts = SessionOptions()
        r = get_ort_ext_libs()
        if r is None:
            raise RuntimeError("No custom implementation available.")
        opts.register_custom_ops_library(r[0])
        return InferenceSession(
            onx.SerializeToString(), opts, providers=["CPUExecutionProvider"]
        )


    res = optimize_model(
        onx,
        feeds={"X": Xb},
        transform=transform_model,
        session=create_session,
        baseline=lambda onx: InferenceSession(
            onx.SerializeToString(), providers=["CPUExecutionProvider"]
        ),
        params=optim_params,
        verbose=True,
        number=script_args.number,
        repeat=script_args.repeat,
        warmup=script_args.warmup,
        sleep=script_args.sleep,
        n_tries=script_args.tries,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:   0%|          | 0/16 [00:00<?, ?it/s]    i=1/16 TRY=0 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0:   6%|▋         | 1/16 [00:00<00:13,  1.14it/s]    i=2/16 TRY=0 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=1.53x:   6%|▋         | 1/16 [00:00<00:13,  1.14it/s]    i=2/16 TRY=0 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=1.53x:  12%|█▎        | 2/16 [00:01<00:08,  1.70it/s]    i=3/16 TRY=0 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=1.53x:  12%|█▎        | 2/16 [00:01<00:08,  1.70it/s]     i=3/16 TRY=0 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=1.53x:  19%|█▉        | 3/16 [00:01<00:05,  2.26it/s]    i=4/16 TRY=0 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.27x:  19%|█▉        | 3/16 [00:01<00:05,  2.26it/s]    i=4/16 TRY=0 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.27x:  25%|██▌       | 4/16 [00:01<00:04,  2.43it/s]    i=5/16 TRY=0 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.27x:  25%|██▌       | 4/16 [00:01<00:04,  2.43it/s]    i=5/16 TRY=0 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.27x:  31%|███▏      | 5/16 [00:02<00:04,  2.59it/s]    i=6/16 TRY=0 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.27x:  31%|███▏      | 5/16 [00:02<00:04,  2.59it/s]    i=6/16 TRY=0 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.27x:  38%|███▊      | 6/16 [00:02<00:03,  2.92it/s]    i=7/16 TRY=0 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.43x:  38%|███▊      | 6/16 [00:02<00:03,  2.92it/s]     i=7/16 TRY=0 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.43x:  44%|████▍     | 7/16 [00:02<00:02,  3.26it/s]    i=8/16 TRY=0 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  44%|████▍     | 7/16 [00:02<00:02,  3.26it/s]    i=8/16 TRY=0 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  50%|█████     | 8/16 [00:03<00:02,  2.85it/s]    i=9/16 TRY=1 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  50%|█████     | 8/16 [00:03<00:02,  2.85it/s]    i=9/16 TRY=1 //tree=80 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  56%|█████▋    | 9/16 [00:03<00:02,  3.06it/s]    i=10/16 TRY=1 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  56%|█████▋    | 9/16 [00:03<00:02,  3.06it/s]    i=10/16 TRY=1 //tree=80 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  62%|██████▎   | 10/16 [00:03<00:01,  3.34it/s]    i=11/16 TRY=1 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  62%|██████▎   | 10/16 [00:03<00:01,  3.34it/s]     i=11/16 TRY=1 //tree=80 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  69%|██████▉   | 11/16 [00:04<00:02,  2.40it/s]    i=12/16 TRY=1 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  69%|██████▉   | 11/16 [00:04<00:02,  2.40it/s]    i=12/16 TRY=1 //tree=80 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  75%|███████▌  | 12/16 [00:04<00:01,  2.65it/s]    i=13/16 TRY=1 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  75%|███████▌  | 12/16 [00:04<00:01,  2.65it/s]    i=13/16 TRY=1 //tree=40 //tree_N=128 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  81%|████████▏ | 13/16 [00:04<00:01,  2.95it/s]    i=14/16 TRY=1 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  81%|████████▏ | 13/16 [00:04<00:01,  2.95it/s]    i=14/16 TRY=1 //tree=40 //tree_N=128 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.85x:  88%|████████▊ | 14/16 [00:05<00:00,  3.25it/s]    i=15/16 TRY=1 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.98x:  88%|████████▊ | 14/16 [00:05<00:00,  3.25it/s]     i=15/16 TRY=1 //tree=40 //tree_N=64 //N=50 bs_tree=1 batch_size_rows=1 n3=0  ~=2.98x:  94%|█████████▍| 15/16 [00:05<00:00,  3.09it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.98x:  94%|█████████▍| 15/16 [00:05<00:00,  3.09it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.98x: 100%|██████████| 16/16 [00:05<00:00,  3.21it/s]    i=16/16 TRY=1 //tree=40 //tree_N=64 //N=25 bs_tree=1 batch_size_rows=1 n3=0  ~=2.98x: 100%|██████████| 16/16 [00:05<00:00,  2.76it/s]




.. GENERATED FROM PYTHON SOURCE LINES 359-360

And the results.

.. GENERATED FROM PYTHON SOURCE LINES 360-367

.. code-block:: Python


    df = DataFrame(res)
    df.to_csv("plot_op_tree_ensemble_optim.csv", index=False)
    df.to_excel("plot_op_tree_ensemble_optim.xlsx", index=False)
    print(df.columns)
    print(df.head(5))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Index(['average', 'deviation', 'min_exec', 'max_exec', 'repeat', 'number',
           'ttime', 'context_size', 'warmup_time', 'n_exp', 'n_exp_name',
           'short_name', 'TRY', 'name', 'parallel_tree', 'parallel_tree_N',
           'parallel_N', 'batch_size_tree', 'batch_size_rows', 'use_node3'],
          dtype='object')
        average  deviation  min_exec  ...  batch_size_tree  batch_size_rows  use_node3
    0  0.003378   0.002449  0.001742  ...              NaN              NaN        NaN
    1  0.002206   0.001942  0.000976  ...              1.0              1.0        0.0
    2  0.002311   0.002912  0.001122  ...              1.0              1.0        0.0
    3  0.001485   0.000397  0.001106  ...              1.0              1.0        0.0
    4  0.002387   0.001874  0.001040  ...              1.0              1.0        0.0

    [5 rows x 20 columns]




.. GENERATED FROM PYTHON SOURCE LINES 368-370

Sorting
+++++++

.. GENERATED FROM PYTHON SOURCE LINES 370-385

.. code-block:: Python


    small_df = df.drop(
        [
            "min_exec",
            "max_exec",
            "repeat",
            "number",
            "context_size",
            "n_exp_name",
        ],
        axis=1,
    ).sort_values("average")
    print(small_df.head(n=10))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

         average  deviation     ttime  ...  batch_size_tree  batch_size_rows use_node3
    14  0.001133   0.000134  0.011334  ...              1.0              1.0       0.0
    7   0.001186   0.000075  0.011859  ...              1.0              1.0       0.0
    10  0.001211   0.000264  0.012106  ...              1.0              1.0       0.0
    13  0.001318   0.000122  0.013176  ...              1.0              1.0       0.0
    6   0.001388   0.000315  0.013883  ...              1.0              1.0       0.0
    3   0.001485   0.000397  0.014854  ...              1.0              1.0       0.0
    9   0.001500   0.000316  0.015000  ...              1.0              1.0       0.0
    12  0.001629   0.000313  0.016289  ...              1.0              1.0       0.0
    16  0.001662   0.000429  0.016624  ...              1.0              1.0       0.0
    5   0.002140   0.001200  0.021403  ...              1.0              1.0       0.0

    [10 rows x 14 columns]




.. GENERATED FROM PYTHON SOURCE LINES 386-388

Worst
+++++

.. GENERATED FROM PYTHON SOURCE LINES 388-392

.. code-block:: Python


    print(small_df.tail(n=10))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

         average  deviation     ttime  ...  batch_size_tree  batch_size_rows use_node3
    16  0.001662   0.000429  0.016624  ...              1.0              1.0       0.0
    5   0.002140   0.001200  0.021403  ...              1.0              1.0       0.0
    17  0.002199   0.000651  0.021989  ...              NaN              NaN       NaN
    1   0.002206   0.001942  0.022063  ...              1.0              1.0       0.0
    15  0.002208   0.000876  0.022084  ...              1.0              1.0       0.0
    2   0.002311   0.002912  0.023106  ...              1.0              1.0       0.0
    4   0.002387   0.001874  0.023873  ...              1.0              1.0       0.0
    8   0.003288   0.002176  0.032879  ...              1.0              1.0       0.0
    0   0.003378   0.002449  0.033782  ...              NaN              NaN       NaN
    11  0.005516   0.000983  0.055164  ...              1.0              1.0       0.0

    [10 rows x 14 columns]




.. GENERATED FROM PYTHON SOURCE LINES 393-395

Plot
++++

.. GENERATED FROM PYTHON SOURCE LINES 395-401

.. code-block:: Python


    skeys = ",".join(optim_params.keys())
    title = f"TreeEnsemble tuning, n_tries={script_args.tries}\n{skeys}\nlower is better"
    ax = hhistograms(df, title=title, keys=("name",))
    fig = ax.get_figure()
    fig.savefig("plot_op_tree_ensemble_optim.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_op_tree_ensemble_optim_001.png
   :alt: TreeEnsemble tuning, n_tries=2 parallel_tree,parallel_tree_N,parallel_N,batch_size_tree,batch_size_rows,use_node3 lower is better
   :srcset: /auto_examples/images/sphx_glr_plot_op_tree_ensemble_optim_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 16.881 seconds)


.. _sphx_glr_download_auto_examples_plot_op_tree_ensemble_optim.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_op_tree_ensemble_optim.ipynb <plot_op_tree_ensemble_optim.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_op_tree_ensemble_optim.py <plot_op_tree_ensemble_optim.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
