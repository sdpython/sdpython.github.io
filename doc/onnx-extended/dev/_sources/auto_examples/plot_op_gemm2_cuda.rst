
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_op_gemm2_cuda.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_op_gemm2_cuda.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_op_gemm2_cuda.py:


.. _l-example-op-gemm2_cuda:

Gemm Exploration with CUDA
==========================

One big Gemm or two smaller gemm?

Cache Performance
+++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 12-81

.. code-block:: Python


    from onnx_extended.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_op_gemm2_cuda",
        description=__doc__,
        config=(
            "small",
            "small, short optimization (default), "
            "medium for medium sizes, "
            "large for big sizes",
        ),
        warmup=3,
        repeat=5,
        itype=(1, "1 or 10 for float or float16"),
        expose="config,itype,warmup,repeat",
    )

    itype = script_args.itype
    config = script_args.config
    print(f"config={config}")
    print(f"itype={itype}")

    if config == "small":
        sizes = (256, 512, 1024)
    elif config == "medium":
        sizes = (512, 1024, 2048)
    elif config == "large":
        sizes = (1024, 2048, 4096, 8192)
    else:
        try:
            sizes = list(map(int, config.split(",")))
        except (ValueError, TypeError) as e:
            raise AssertionError(f"Unexpected config value {config!r}.") from e

    import time
    import numpy as np
    import onnx.helper as oh
    from tqdm import tqdm
    from pandas import DataFrame
    from onnxruntime import InferenceSession, SessionOptions, get_available_providers
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    from onnx_extended.ortops.optim.cuda import get_ort_ext_libs


    def get_model1(itype):
        return oh.make_model(
            oh.make_graph(
                [
                    oh.make_node("Gemm", ["X", "Y"], ["XY"]),
                    oh.make_node("Gemm", ["X", "Z"], ["XZ"]),
                    oh.make_node("Concat", ["XY", "XZ"], ["XYZ"], axis=1),
                ],
                "nd",
                [
                    oh.make_tensor_value_info("X", itype, [None, None]),
                    oh.make_tensor_value_info("Y", itype, [None, None]),
                    oh.make_tensor_value_info("Z", itype, [None, None]),
                ],
                [oh.make_tensor_value_info("XYZ", itype, [None, None])],
            ),
            opset_imports=[oh.make_opsetid("", 18)],
            ir_version=9,
        )


    print(onnx_simple_text_plot(get_model1(itype)))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    config=small
    itype=1
    opset: domain='' version=18
    input: name='X' type=dtype('float32') shape=['', '']
    input: name='Y' type=dtype('float32') shape=['', '']
    input: name='Z' type=dtype('float32') shape=['', '']
    Gemm(X, Y) -> XY
    Gemm(X, Z) -> XZ
      Concat(XY, XZ, axis=1) -> XYZ
    output: name='XYZ' type=dtype('float32') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 82-83

And the other model

.. GENERATED FROM PYTHON SOURCE LINES 83-107

.. code-block:: Python



    def get_model2(itype):
        return oh.make_model(
            oh.make_graph(
                [
                    oh.make_node("Concat", ["Y", "Z"], ["YZ"], axis=1),
                    oh.make_node("Gemm", ["X", "YZ"], ["XYZ"]),
                ],
                "nd",
                [
                    oh.make_tensor_value_info("X", itype, [None, None]),
                    oh.make_tensor_value_info("Y", itype, [None, None]),
                    oh.make_tensor_value_info("Z", itype, [None, None]),
                ],
                [oh.make_tensor_value_info("XYZ", itype, [None, None])],
            ),
            opset_imports=[oh.make_opsetid("", 18)],
            ir_version=9,
        )


    print(onnx_simple_text_plot(get_model2(itype)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    input: name='X' type=dtype('float32') shape=['', '']
    input: name='Y' type=dtype('float32') shape=['', '']
    input: name='Z' type=dtype('float32') shape=['', '']
    Concat(Y, Z, axis=1) -> YZ
      Gemm(X, YZ) -> XYZ
    output: name='XYZ' type=dtype('float32') shape=['', '']




.. GENERATED FROM PYTHON SOURCE LINES 108-110

InferenceSession
++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 110-127

.. code-block:: Python


    has_cuda = "CUDAExecutionProvider" in get_available_providers()

    if has_cuda:

        dtype = np.float32 if itype == 1 else np.float16

        x = np.random.randn(16, 16).astype(dtype)
        y = np.random.randn(16, 16).astype(dtype)
        z = np.random.randn(16, 16).astype(dtype)
        feeds = dict(X=x, Y=y, Z=z)

        sess1 = InferenceSession(
            get_model1(itype).SerializeToString(), providers=["CUDAExecutionProvider"]
        )
        expected = sess1.run(None, feeds)[0]








.. GENERATED FROM PYTHON SOURCE LINES 128-129

The other model.

.. GENERATED FROM PYTHON SOURCE LINES 129-140

.. code-block:: Python


    if has_cuda:

        opts = SessionOptions()
        opts.register_custom_ops_library(get_ort_ext_libs()[0])

        sess2 = InferenceSession(
            get_model2(itype).SerializeToString(), opts, providers=["CUDAExecutionProvider"]
        )
        got = sess2.run(None, feeds)[0]








.. GENERATED FROM PYTHON SOURCE LINES 141-142

Discrepancies

.. GENERATED FROM PYTHON SOURCE LINES 142-149

.. code-block:: Python


    if has_cuda:

        diff = np.abs(got - expected).max()
        print(f"diff={diff}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    diff=0.0




.. GENERATED FROM PYTHON SOURCE LINES 150-154

Benchmark
+++++++++

some code to avoid measuring copying the data from host to device

.. GENERATED FROM PYTHON SOURCE LINES 154-182

.. code-block:: Python



    def move_inputs(sess, feeds):
        from onnxruntime.capi._pybind_state import (
            SessionIOBinding,
            OrtDevice as C_OrtDevice,
            OrtValue as C_OrtValue,
        )

        input_names = [i.name for i in sess.get_inputs()]

        ort_device = C_OrtDevice(C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0)

        feed_ort_value = [
            (name, C_OrtValue.ortvalue_from_numpy(feeds[name], ort_device))
            for name in input_names
        ]

        bind = SessionIOBinding(sess._sess)
        for name, value in feed_ort_value:
            bind.bind_input(
                name, ort_device, feeds[name].dtype, value.shape(), value.data_ptr()
            )
        for o in sess.get_outputs():
            bind.bind_output(o.name, ort_device)
        return bind, feed_ort_value









.. GENERATED FROM PYTHON SOURCE LINES 183-184

Benchmark function

.. GENERATED FROM PYTHON SOURCE LINES 184-225

.. code-block:: Python



    def benchmark(sess, sizes, label):

        data = []
        for size in tqdm(sizes):

            x = np.random.randn(size, size).astype(dtype)
            y = np.random.randn(size, size).astype(dtype)
            z = np.random.randn(size, size).astype(dtype)
            feeds = dict(X=x, Y=y, Z=z)
            bind, cuda_feeds = move_inputs(sess, feeds)

            begin = time.perf_counter()
            for _i in range(script_args.warmup):
                # sess.run(None, feeds)
                sess._sess.run_with_iobinding(bind, None)
            warmup = time.perf_counter() - begin

            times = []
            for _i in range(script_args.repeat):
                begin = time.perf_counter()
                # sess.run(None, feeds)
                sess._sess.run_with_iobinding(bind, None)
                times.append(time.perf_counter() - begin)

            npt = np.array(times)
            obs = dict(
                warmup=warmup,
                time=npt.mean(),
                std=npt.std(),
                min=npt.min(),
                max=npt.max(),
                repeat=script_args.repeat,
                size=size,
                label=label,
            )
            data.append(obs)
        return data









.. GENERATED FROM PYTHON SOURCE LINES 226-227

Not Fused.

.. GENERATED FROM PYTHON SOURCE LINES 227-234

.. code-block:: Python


    if has_cuda:

        print(f"sizes={sizes}")

        data_mul = benchmark(sess1, sizes, "Not Fused")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    sizes=(256, 512, 1024)
      0%|          | 0/3 [00:00<?, ?it/s]    100%|██████████| 3/3 [00:00<00:00, 11.46it/s]    100%|██████████| 3/3 [00:00<00:00, 11.44it/s]




.. GENERATED FROM PYTHON SOURCE LINES 235-236

Fused.

.. GENERATED FROM PYTHON SOURCE LINES 236-242

.. code-block:: Python


    if has_cuda:

        data_mulmul = benchmark(sess2, sizes, "Fused")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3 [00:00<?, ?it/s]    100%|██████████| 3/3 [00:00<00:00, 17.91it/s]    100%|██████████| 3/3 [00:00<00:00, 17.87it/s]




.. GENERATED FROM PYTHON SOURCE LINES 243-245

Data
++++

.. GENERATED FROM PYTHON SOURCE LINES 245-253

.. code-block:: Python


    if has_cuda:

        df = DataFrame(data_mul + data_mulmul)
        df.to_csv("plot_op_gemm2_cuda.csv", index=False)
        df.to_csv("plot_op_gemm2_cuda.xlsx", index=False)
        print(df.head())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

         warmup      time       std       min       max  repeat  size      label
    0  0.002362  0.000249  0.000031  0.000213  0.000285       5   256  Not Fused
    1  0.002367  0.000270  0.000027  0.000239  0.000309       5   512  Not Fused
    2  0.004879  0.000587  0.000014  0.000566  0.000603       5  1024  Not Fused
    3  0.004550  0.000168  0.000006  0.000163  0.000179       5   256      Fused
    4  0.006429  0.000304  0.000116  0.000243  0.000536       5   512      Fused




.. GENERATED FROM PYTHON SOURCE LINES 254-255

Pivot.

.. GENERATED FROM PYTHON SOURCE LINES 255-269

.. code-block:: Python


    if has_cuda:

        pivot = df.pivot(index="size", columns="label", values="time")
        pivot["ratio"] = pivot["Fused"] / pivot["Not Fused"]
        print(pivot)

        ax = pivot[["Not Fused", "Fused"]].plot(
            logx=True,
            logy=True,
            title=f"Fused/Unfused element wise multiplication on CUDA\nitype={itype}",
        )
        ax.get_figure().savefig("plot_op_gemm2_cuda.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_op_gemm2_cuda_001.png
   :alt: Fused/Unfused element wise multiplication on CUDA itype=1
   :srcset: /auto_examples/images/sphx_glr_plot_op_gemm2_cuda_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    label     Fused  Not Fused     ratio
    size                                
    256    0.000168   0.000249  0.675043
    512    0.000304   0.000270  1.128154
    1024   0.000537   0.000587  0.914766




.. GENERATED FROM PYTHON SOURCE LINES 270-271

It seems the fused operator is 33% faster.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.770 seconds)


.. _sphx_glr_download_auto_examples_plot_op_gemm2_cuda.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_op_gemm2_cuda.ipynb <plot_op_gemm2_cuda.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_op_gemm2_cuda.py <plot_op_gemm2_cuda.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_op_gemm2_cuda.zip <plot_op_gemm2_cuda.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
