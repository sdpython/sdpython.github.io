{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n# Measuring performance about Gemm with onnxruntime\n\nThe benchmark measures the performance of Gemm for different\ntypes and configuration. That includes a custom operator\nonly available on CUDA calling function :epkg:`cublasLtMatmul`.\nThis function offers many options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pprint\nimport platform\nfrom itertools import product\nimport numpy\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame, pivot_table\nfrom onnx import TensorProto\nfrom onnx.helper import (\n    make_model,\n    make_node,\n    make_graph,\n    make_tensor_value_info,\n    make_opsetid,\n)\nfrom onnx.checker import check_model\nfrom onnx.numpy_helper import from_array\nfrom onnx.reference import ReferenceEvaluator\nfrom onnxruntime import InferenceSession, SessionOptions, get_available_providers\nfrom onnxruntime.capi._pybind_state import (\n    OrtValue as C_OrtValue,\n    OrtDevice as C_OrtDevice,\n)\nfrom onnxruntime.capi.onnxruntime_pybind11_state import (\n    Fail,\n    NotImplemented,\n    InvalidGraph,\n    InvalidArgument,\n)\n\ntry:\n    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot\nexcept ImportError:\n    onnx_simple_text_plot = str\ntry:\n    from onnx_extended.reference import CReferenceEvaluator\nexcept ImportError:\n    CReferenceEvaluator = ReferenceEvaluator\nfrom onnx_extended.ext_test_case import unit_test_going, measure_time, get_parsed_args\n\ntry:\n    from onnx_extended.validation.cuda.cuda_example_py import get_device_prop\n    from onnx_extended.ortops.tutorial.cuda import get_ort_ext_libs\n\n    has_cuda = True\nexcept ImportError:\n\n    def get_device_prop():\n        return {\"name\": \"CPU\"}\n\n    def get_ort_ext_libs():\n        return None\n\n    has_cuda = False\n\ndefault_dims = (\n    \"32,32,32;64,64,64;128,128,128;256,256,256;\"\n    \"400,400,400;512,512,512;1024,1024,1024\"\n)\nif has_cuda:\n    prop = get_device_prop()\n    if prop.get(\"major\", 0) >= 7:\n        default_dims += \";2048,2048,2048;4096,4096,4096\"\n    if prop.get(\"major\", 0) >= 9:\n        default_dims += \";16384,16384,16384\"\n\n\nscript_args = get_parsed_args(\n    \"plot_bench_gemm_ort\",\n    description=__doc__,\n    dims=(\n        \"32,32,32;64,64,64\" if unit_test_going() else default_dims,\n        \"square matrix dimensions to try, comma separated values\",\n    ),\n    types=(\n        \"FLOAT\" if unit_test_going() else \"FLOAT8E4M3FN,FLOAT,FLOAT16,BFLOAT16\",\n        \"element type to teest\",\n    ),\n    number=2 if unit_test_going() else 4,\n    repeat=2 if unit_test_going() else 10,\n    warmup=2 if unit_test_going() else 5,\n    expose=\"repeat,number,warmup\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Device properties\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if has_cuda:\n    properties = get_device_prop()\n    pprint.pprint(properties)\nelse:\n    properties = {\"major\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model to benchmark\n\nIt includes one Gemm. The operator changes.\nIt can the regular Gemm, a custom Gemm from domain `com.microsoft`\nor a custom implementation from domain\n`onnx_extented.ortops.tutorial.cuda`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_model(\n    mat_type=TensorProto.FLOAT, provider=\"CUDAExecutionProvider\", domain=\"com.microsoft\"\n):\n    A = make_tensor_value_info(\"A\", mat_type, [None, None])\n    B = make_tensor_value_info(\"B\", mat_type, [None, None])\n    outputs = [make_tensor_value_info(\"C\", mat_type, [None, None])]\n    inits = []\n    if domain != \"\":\n        if provider != \"CUDAExecutionProvider\":\n            return None\n        f8 = False\n        if domain == \"com.microsoft\":\n            op_name = \"GemmFloat8\"\n            computeType = \"CUBLAS_COMPUTE_32F\"\n            node_output = [\"C\"]\n        elif mat_type == TensorProto.FLOAT:\n            op_name = \"CustomGemmFloat\"\n            computeType = \"CUBLAS_COMPUTE_32F_FAST_TF32\"\n            node_output = [\"C\"]\n        elif mat_type == TensorProto.FLOAT16:\n            op_name = \"CustomGemmFloat16\"\n            computeType = \"CUBLAS_COMPUTE_16F\"\n            node_output = [\"C\"]\n        elif mat_type in (TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2):\n            f8 = True\n            op_name = \"CustomGemmFloat8E4M3FN\"\n            computeType = \"CUBLAS_COMPUTE_32F\"\n            node_output = [\"C\"]\n            outputs = [\n                make_tensor_value_info(\"C\", TensorProto.FLOAT16, [None, None]),\n            ]\n            inits.append(from_array(numpy.array([1], dtype=numpy.float32), name=\"I\"))\n        else:\n            return None\n        node_kw = dict(\n            alpha=1.0,\n            transB=1,\n            domain=domain,\n            computeType=computeType,\n            fastAccumulationMode=1,\n            rowMajor=0 if op_name.startswith(\"CustomGemmFloat\") else 1,\n        )\n        node_kw[\"name\"] = (\n            f\"{mat_type}.{len(node_output)}.{len(outputs)}.\"\n            f\"{domain}..{node_kw['rowMajor']}..\"\n            f\"{node_kw['fastAccumulationMode']}..{node_kw['computeType']}..\"\n            f\"{f8}\"\n        )\n        node_inputs = [\"A\", \"B\"]\n        if f8:\n            node_inputs.append(\"\")\n            node_inputs.extend([\"I\"] * 3)\n        nodes = [make_node(op_name, node_inputs, node_output, **node_kw)]\n    else:\n        nodes = [\n            make_node(\"Gemm\", [\"A\", \"B\"], [\"C\"], transA=1, beta=0.0),\n        ]\n    graph = make_graph(nodes, \"a\", [A, B], outputs, inits)\n    if mat_type < 16:\n        # regular type\n        opset, ir = 18, 8\n    else:\n        opset, ir = 19, 9\n    onnx_model = make_model(\n        graph,\n        opset_imports=[\n            make_opsetid(\"\", opset),\n            make_opsetid(\"com.microsoft\", 1),\n            make_opsetid(\"onnx_extented.ortops.tutorial.cuda\", 1),\n        ],\n        ir_version=ir,\n    )\n    check_model(onnx_model)\n    return onnx_model\n\n\nprint(onnx_simple_text_plot(create_model()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model to cast into anytype.\nnumpy does not support float 8. onnxruntime is used\nto cast a float array into any type.\nIt must be called with tensor of type `OrtValue`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_cast(to, cuda=False):\n    A = make_tensor_value_info(\"A\", TensorProto.FLOAT, [None, None])\n    C = make_tensor_value_info(\"C\", to, [None, None])\n    if cuda:\n        nodes = [\n            make_node(\"Cast\", [\"A\"], [\"Cc\"], to=to),\n            make_node(\"MemcpyFromHost\", [\"Cc\"], [\"C\"]),\n        ]\n    else:\n        nodes = [make_node(\"Cast\", [\"A\"], [\"C\"], to=to)]\n    graph = make_graph(nodes, \"a\", [A], [C])\n    if to < 16:\n        # regular type\n        opset, ir = 18, 8\n    else:\n        opset, ir = 19, 9\n    onnx_model = make_model(\n        graph, opset_imports=[make_opsetid(\"\", opset)], ir_version=ir\n    )\n    if not cuda:\n        # OpType: MemcpyFromHost\n        check_model(onnx_model)\n    return onnx_model\n\n\nprint(onnx_simple_text_plot(create_cast(TensorProto.FLOAT16)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance\n\nThe benchmark will run the following configurations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "types = list(getattr(TensorProto, a) for a in script_args.types.split(\",\"))\nengine = [InferenceSession, CReferenceEvaluator]\nproviders = [\n    [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n    [\"CPUExecutionProvider\"],\n]\n# M, N, K\n# we use multiple of 8, otherwise, float8 does not work.\ndims = [list(int(i) for i in line.split(\",\")) for line in script_args.dims.split(\";\")]\ndomains = [\"onnx_extented.ortops.tutorial.cuda\", \"\", \"com.microsoft\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's cache the matrices involved.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def to_ort_value(m):\n    device = C_OrtDevice(C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)\n    ort_value = C_OrtValue.ortvalue_from_numpy(m, device)\n    return ort_value\n\n\ndef cached_inputs(dims, types):\n    matrices = {}\n    matrices_cuda = {}\n    pbar = tqdm(list(product(dims, types)))\n    for dim, tt in pbar:\n        m, n, k = dim\n        pbar.set_description(f\"t={tt} dim={dim}\")\n        for i, j in [(m, k), (k, n), (k, m)]:\n            if (tt, i, j) in matrices:\n                continue\n            # CPU\n            try:\n                sess = InferenceSession(\n                    create_cast(tt).SerializeToString(),\n                    providers=[\"CPUExecutionProvider\"],\n                )\n                cpu = True\n            except (InvalidGraph, InvalidArgument, NotImplemented):\n                # not support by this version of onnxruntime\n                cpu = False\n\n            if cpu:\n                vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)\n                ov = to_ort_value(vect)\n                ovtt = sess._sess.run_with_ort_values({\"A\": ov}, [\"C\"], None)[0]\n                matrices[tt, i, j] = ovtt\n            else:\n                continue\n\n            # CUDA\n            if \"CUDAExecutionProvider\" not in get_available_providers():\n                # No CUDA\n                continue\n            sess = InferenceSession(\n                create_cast(tt, cuda=True).SerializeToString(),\n                providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n            )\n            vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)\n            ov = to_ort_value(vect)\n            ovtt = sess._sess.run_with_ort_values({\"A\": ov}, [\"C\"], None)[0]\n            matrices_cuda[tt, i, j] = ovtt\n    return matrices, matrices_cuda\n\n\nmatrices, matrices_cuda = cached_inputs(dims, types)\nprint(f\"{len(matrices)} matrices were created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def rendering_obs(obs, dim, number, repeat, domain, provider, internal_time):\n    stype = {\n        TensorProto.FLOAT: \"f32\",\n        TensorProto.FLOAT16: \"f16\",\n        TensorProto.BFLOAT16: \"bf16\",\n        TensorProto.INT8: \"i8\",\n        TensorProto.INT16: \"i16\",\n        TensorProto.INT32: \"i32\",\n        TensorProto.UINT32: \"u32\",\n        TensorProto.FLOAT8E4M3FN: \"e4m3fn\",\n        TensorProto.FLOAT8E5M2: \"e5m2\",\n    }[tt]\n    obs.update(\n        dict(\n            engine={\"InferenceSession\": \"ort\", \"CReferenceEvaluator\": \"np\"}[\n                engine.__name__\n            ],\n            stype=stype,\n            type=f\"{stype}\",\n            M=dim[0],\n            N=dim[1],\n            K=dim[2],\n            cost=numpy.prod(dim) * 4,\n            cost_s=f\"{numpy.prod(dim) * 4}-{dim[0]}x{dim[1]}x{dim[2]}\",\n            repeat=repeat,\n            number=number,\n            domain={\n                \"\": \"ORT\",\n                \"com.microsoft\": \"COM\",\n                \"onnx_extented.ortops.tutorial.cuda\": \"EXT\",\n            }[domain],\n            provider={\n                \"CPUExecutionProvider\": \"cpu\",\n                \"CUDAExecutionProvider\": \"cuda\",\n            }[provider[0]],\n            platform=platform.processor(),\n            intime=internal_time,\n        )\n    )\n    return obs\n\n\nopts = SessionOptions()\nr = get_ort_ext_libs()\nif r is not None:\n    opts.register_custom_ops_library(r[0])\n\n\ndata = []\nerrors = []\npbar = tqdm(list(product(types, engine, providers, dims, domains)))\nfor tt, engine, provider, dim, domain in pbar:\n    if (\n        tt in {TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2}\n        and properties.get(\"major\", 0) < 9\n    ):\n        # f8 not available\n        if provider[0] == \"CPUExecutionProvider\":\n            continue\n        errors.append(\n            f\"f8 not available, major={properties.get('major', 0)}, \"\n            f\"tt={tt}, provider={provider!r}, domain={domain!r}.\"\n        )\n        continue\n    elif provider[0] == \"CPUExecutionProvider\" and max(dim) > 2000:\n        # too long\n        continue\n    if max(dim) <= 200:\n        repeat, number = script_args.repeat * 4, script_args.number * 4\n    elif max(dim) <= 256:\n        repeat, number = script_args.repeat * 2, script_args.number * 2\n    else:\n        repeat, number = script_args.repeat, script_args.number\n\n    onx = create_model(tt, provider=provider[0], domain=domain)\n    if onx is None:\n        if provider[0] == \"CPUExecutionProvider\":\n            continue\n        errors.append(\n            f\"No model for tt={tt}, provider={provider!r}, domain={domain!r}.\"\n        )\n        continue\n    with open(f\"plot_bench_gemm_ort_{tt}_{domain}.onnx\", \"wb\") as f:\n        f.write(onx.SerializeToString())\n    k1 = (tt, dim[2], dim[0])\n    k2 = (tt, dim[2], dim[1])\n    if k1 not in matrices:\n        errors.append(f\"Key k1={k1!r} not in matrices.\")\n        continue\n    if k2 not in matrices:\n        errors.append(f\"Key k2={k2!r} not in matrices.\")\n        continue\n\n    pbar.set_description(f\"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}\")\n\n    if engine == CReferenceEvaluator:\n        if (\n            domain != \"\"\n            or max(dim) > 256\n            or provider != [\"CPUExecutionProvider\"]\n            or tt not in [TensorProto.FLOAT, TensorProto.FLOAT16]\n        ):\n            # All impossible or slow cases.\n            continue\n        if tt == TensorProto.FLOAT16 and max(dim) > 50:\n            repeat, number = 2, 2\n\n        feeds = {\"A\": matrices[k1].numpy(), \"B\": matrices[k2].numpy()}\n        sess = engine(onx)\n        sess.run(None, feeds)\n        obs = measure_time(lambda: sess.run(None, feeds), repeat=repeat, number=number)\n\n    elif engine == InferenceSession:\n        if provider[0] not in get_available_providers():\n            errors.append(f\"provider={provider[0]} is missing\")\n            continue\n        try:\n            sess = engine(onx.SerializeToString(), opts, providers=provider)\n        except (NotImplemented, InvalidGraph, Fail) as e:\n            # not implemented\n            errors.append((tt, engine.__class__.__name__, provider, domain, e))\n            continue\n\n        the_feeds = (\n            {\"A\": matrices[k1], \"B\": matrices[k2]}\n            if provider == [\"CPUExecutionProvider\"]\n            else {\"A\": matrices_cuda[k1], \"B\": matrices_cuda[k2]}\n        )\n        out_names = [\"C\"]\n\n        # warmup\n        for i in range(script_args.warmup):\n            sess._sess.run_with_ort_values(the_feeds, out_names, None)[0]\n\n        # benchamrk\n        times = []\n\n        def fct_benchmarked():\n            got = sess._sess.run_with_ort_values(the_feeds, out_names, None)\n            if len(got) > 1:\n                times.append(got[1])\n\n        obs = measure_time(fct_benchmarked, repeat=repeat, number=number)\n        internal_time = None\n        if len(times) > 0:\n            np_times = [t.numpy() for t in times]\n            internal_time = (sum(np_times) / len(times))[0]\n\n    else:\n        errors.append(f\"unknown engine={engine}\")\n        continue\n\n    # improves the rendering\n    obs = rendering_obs(obs, dim, number, repeat, domain, provider, internal_time)\n    data.append(obs)\n    if unit_test_going() and len(data) >= 2:\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataFrame(data)\ndf.to_excel(\"plot_bench_gemm_ort.xlsx\")\ndf.to_csv(\"plot_bench_gemm_ort.csv\")\ndf.drop([\"min_exec\", \"max_exec\"], axis=1).to_csv(\"plot_bench_gemm_ort.csv\")\nprint(df.head().T)\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The errors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, e in enumerate(errors):\n    print(f\"{i+1}/{len(errors)}-{e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pivot_table(\n    df,\n    index=[\"cost\"],\n    columns=[\"provider\", \"type\", \"domain\", \"engine\"],\n    values=[\"average\", \"intime\"],\n)\npiv.reset_index(drop=False).to_excel(\"plot_bench_gemm_ort_summary.xlsx\")\npiv.reset_index(drop=False).to_csv(\"plot_bench_gemm_ort_summary.csv\")\n\n\nprint(\"summary\")\nprint(piv)\npiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pivs = pivot_table(\n    df,\n    index=[\"cost_s\"],\n    columns=[\"provider\", \"type\", \"domain\", \"engine\"],\n    values=[\"average\", \"intime\"],\n)\nprint(pivs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfi = df[\n    df.type.isin({\"f32\", \"f16\", \"bf16\", \"e4m3fn\", \"e5m2\"}) & df.engine.isin({\"ort\"})\n]\npivi = pivot_table(\n    dfi,\n    index=[\"cost\"],\n    columns=[\"type\", \"domain\", \"provider\", \"engine\"],\n    values=\"average\",\n)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\npiv.plot(ax=ax[0], title=\"Gemm performance\\nlower is better\", logx=True, logy=True)\nif pivi.shape[0] > 0:\n    pivi.plot(\n        ax=ax[1],\n        title=f\"Gemm performance ORT\\n{platform.processor()}\",\n        logx=True,\n        logy=True,\n    )\nfig.tight_layout()\nfig.savefig(\"plot_bench_gemm_ort.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}