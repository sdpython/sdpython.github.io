{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Compares implementations of Einsum\n\nThis example compares different equations for function :func:`numpy.einsum`.\nIt compares *numpy* implementation to a custom implementation,\n:epkg:`onnxruntime` implementation and :epkg:`opt-einsum` optimisation.\nIf available, :epkg:`tensorflow` and :epkg:`pytorch` are included as well.\nThe custom implementation does not do any transpose.\nIt uses parallelisation and SIMD optimization when the summation\nhappens on the last axis of both matrices. It only implements\nmatrix multiplication. We also measure the improvment made with\nfunction :func:`einsum <onnx_extended.tools.einsum.einsum_fct.einsum>`.\n\n## Available optimisation\n\nThe code shows which optimisation is used for the custom\nimplementation, *AVX* or *SSE* and the number of available processors,\nequal to the default number of used threads to parallelize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport numpy\nimport pandas\nimport matplotlib.pyplot as plt\nfrom onnx import TensorProto\nfrom onnx.helper import (\n    make_model,\n    make_graph,\n    make_node,\n    make_tensor_value_info,\n    make_opsetid,\n)\nfrom onnxruntime import InferenceSession\nfrom onnx_extended.ext_test_case import measure_time, unit_test_going\nfrom tqdm import tqdm\nfrom opt_einsum import contract\nfrom onnx_extended.tools.einsum.einsum_fct import _einsum\n\nlogging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\nlogging.getLogger(\"matplotlib.ticker\").setLevel(logging.ERROR)\nlogging.getLogger(\"PIL.PngImagePlugin\").setLevel(logging.ERROR)\nlogging.getLogger(\"onnx-extended\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Einsum: common code\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    from tensorflow import einsum as tf_einsum, convert_to_tensor\nexcept ImportError:\n    tf_einsum = None\ntry:\n    from torch import einsum as torch_einsum, from_numpy\nexcept ImportError:\n    torch_einsum = None\n\n\ndef build_ort_einsum(equation, op_version=18):  # opset=13, 14, ...\n    onx = make_model(\n        make_graph(\n            [make_node(\"Einsum\", [\"x\", \"y\"], [\"z\"], equation=equation)],\n            equation,\n            [\n                make_tensor_value_info(\"x\", TensorProto.FLOAT, None),\n                make_tensor_value_info(\"y\", TensorProto.FLOAT, None),\n            ],\n            [make_tensor_value_info(\"z\", TensorProto.FLOAT, None)],\n        ),\n        opset_imports=[make_opsetid(\"\", op_version)],\n        ir_version=9,\n    )\n    sess = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n    return lambda x, y: sess.run(None, {\"x\": x, \"y\": y})\n\n\ndef build_ort_decomposed(equation, op_version=18):  # opset=13, 14, ...\n    cache = _einsum(\n        equation,\n        numpy.float32,\n        opset=op_version,\n        optimize=True,\n        verbose=True,\n        runtime=\"python\",\n    )\n    if not hasattr(cache, \"onnx_\"):\n        cache.build()\n    sess = InferenceSession(\n        cache.onnx_.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n    )\n    return lambda x, y: sess.run(None, {\"X0\": x, \"X1\": y})\n\n\ndef loop_einsum_eq(fct, equation, xs, ys):\n    for x, y in zip(xs, ys):\n        fct(equation, x, y)\n\n\ndef loop_einsum_eq_th(fct, equation, xs, ys):\n    for x, y in zip(xs, ys):\n        fct(equation, x, y, nthread=-1)\n\n\ndef loop_einsum(fct, xs, ys):\n    for x, y in zip(xs, ys):\n        fct(x, y)\n\n\ndef timeit(stmt, ctx, dim, name):\n    obs = measure_time(stmt, div_by_number=True, context=ctx, repeat=5, number=1)\n    obs[\"dim\"] = dim\n    obs[\"fct\"] = name\n    return obs\n\n\ndef benchmark_equation(equation):\n    # equations\n    ort_einsum = build_ort_einsum(equation)\n    ort_einsum_decomposed = build_ort_decomposed(equation)\n    res = []\n    for dim in tqdm([8, 16, 32, 64, 100, 128, 200, 256]):  # , 500, 512]):\n        if unit_test_going() and dim > 64:\n            break\n        xs = [numpy.random.rand(2, dim, 12, 64).astype(numpy.float32) for _ in range(5)]\n        ys = [numpy.random.rand(2, dim, 12, 64).astype(numpy.float32) for _ in range(5)]\n\n        # numpy\n        ctx = dict(\n            equation=equation,\n            xs=xs,\n            ys=ys,\n            einsum=numpy.einsum,\n            loop_einsum=loop_einsum,\n            loop_einsum_eq=loop_einsum_eq,\n            loop_einsum_eq_th=loop_einsum_eq_th,\n        )\n        obs = timeit(\n            \"loop_einsum_eq(einsum, equation, xs, ys)\", ctx, dim, \"numpy.einsum\"\n        )\n        res.append(obs)\n\n        # opt-einsum\n        ctx[\"einsum\"] = contract\n        obs = timeit(\"loop_einsum_eq(einsum, equation, xs, ys)\", ctx, dim, \"opt-einsum\")\n        res.append(obs)\n\n        # onnxruntime\n        ctx[\"einsum\"] = ort_einsum\n        obs = timeit(\"loop_einsum(einsum, xs, ys)\", ctx, dim, \"ort-einsum\")\n        res.append(obs)\n\n        # onnxruntime decomposed\n        ctx[\"einsum\"] = ort_einsum_decomposed\n        obs = timeit(\"loop_einsum(einsum, xs, ys)\", ctx, dim, \"ort-dec\")\n        res.append(obs)\n\n        if tf_einsum is not None:\n            # tensorflow\n            ctx[\"einsum\"] = tf_einsum\n            ctx[\"xs\"] = [convert_to_tensor(x) for x in xs]\n            ctx[\"ys\"] = [convert_to_tensor(y) for y in ys]\n            obs = timeit(\n                \"loop_einsum_eq(einsum, equation, xs, ys)\", ctx, dim, \"tf-einsum\"\n            )\n            res.append(obs)\n\n        if torch_einsum is not None:\n            # torch\n            ctx[\"einsum\"] = torch_einsum\n            ctx[\"xs\"] = [from_numpy(x) for x in xs]\n            ctx[\"ys\"] = [from_numpy(y) for y in ys]\n            obs = timeit(\n                \"loop_einsum_eq(einsum, equation, xs, ys)\", ctx, dim, \"torch-einsum\"\n            )\n            res.append(obs)\n\n    # Dataframes\n    df = pandas.DataFrame(res)\n    piv = df.pivot(index=\"dim\", columns=\"fct\", values=\"average\")\n\n    rs = piv.copy()\n    for c in [\"ort-einsum\", \"ort-dec\", \"tf-einsum\", \"torch-einsum\", \"opt-einsum\"]:\n        if c not in rs.columns:\n            continue\n        rs[c] = rs[\"numpy.einsum\"] / rs[c]\n    rs[\"numpy.einsum\"] = 1.0\n\n    # Graphs.\n    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n    piv.plot(\n        logx=True,\n        logy=True,\n        ax=ax[0],\n        title=f\"Einsum benchmark\\n{equation} -- (2, N, 12, 64) lower better\",\n    )\n    ax[0].legend(prop={\"size\": 9})\n    rs.plot(\n        logx=True,\n        logy=True,\n        ax=ax[1],\n        title=\"Einsum Speedup, baseline=numpy\\n%s -- (2, N, 12, 64)\"\n        \" higher better\" % equation,\n    )\n    ax[1].plot([min(rs.index), max(rs.index)], [0.5, 0.5], \"g--\")\n    ax[1].plot([min(rs.index), max(rs.index)], [2.0, 2.0], \"g--\")\n    ax[1].legend(prop={\"size\": 9})\n\n    return df, rs, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First equation: bsnh,btnh->bnts\n\nThe decomposition of this equation without einsum function gives\nthe following.\n\n .. gdot::\n      :script:\n\n      from onnx_extended.tools.einsum import decompose_einsum_equation\n      dec = decompose_einsum_equation(\n          'bsnh,btnh->bnts', strategy='numpy', clean=True)\n      print(dec.to_dot())\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfs = []\nequation = \"bsnh,btnh->bnts\"\ndf, piv, ax = benchmark_equation(equation)\ndf.pivot(index=\"fct\", columns=\"dim\", values=\"average\")\ndfs.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second equation: bshn,bthn->bnts\n\nThe summation does not happen on the last axis but\non the previous one.\nIs it worth transposing before doing the summation...\nThe decomposition of this equation without einsum function gives\nthe following.\n\n .. gdot::\n      :script:\n\n      from onnx_extended.tools.einsum import decompose_einsum_equation\n      dec = decompose_einsum_equation(\n          'bshn,bthn->bnts', strategy='numpy', clean=True)\n      print(dec.to_dot())\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "equation = \"bshn,bthn->bnts\"\ndf, piv, ax = benchmark_equation(equation)\ndf.pivot(index=\"fct\", columns=\"dim\", values=\"average\")\ndfs.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Third equation: bhsn,bhtn->bnts\n\nThe summation does not happen on the last axis but\non the second one. It is worth transposing before multiplying.\nThe decomposition of this equation without einsum function gives\nthe following.\n\n .. gdot::\n      :script:\n\n      from onnx_extended.tools.einsum import decompose_einsum_equation\n      dec = decompose_einsum_equation(\n          'bhsn,bhtn->bnts', strategy='numpy', clean=True)\n      print(dec.to_dot())\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "equation = \"bhsn,bhtn->bnts\"\ndf, piv, ax = benchmark_equation(equation)\ndf.pivot(index=\"fct\", columns=\"dim\", values=\"average\")\ndfs.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\npytorch seems quite efficient on these examples.\nThe custom implementation was a way to investigate\nthe implementation of einsum and find some ways to optimize it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "merged = pandas.concat(dfs)\nname = \"einsum\"\nmerged.to_csv(f\"plot_{name}.csv\", index=False)\nmerged.to_excel(f\"plot_{name}.xlsx\", index=False)\nplt.savefig(f\"plot_{name}.png\")\n\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}