{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# Measuring performance of TfIdfVectorizer\n",
        "\n",
        "The banchmark measures the performance of a TfIdfVectizer along two\n",
        "parameters, the vocabulary size, the batch size whether. It measures\n",
        "the benefit of using sparse implementation through the computation\n",
        "time and the memory peak.\n",
        "\n",
        "## A simple model\n",
        "\n",
        "We start with a model including only one node TfIdfVectorizer.\n",
        "It only contains unigram. The model processes only sequences of 10\n",
        "integers. The sparsity of the results is then 10 divided by the size of\n",
        "vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import time\n",
        "import itertools\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas\n",
        "from onnx import ModelProto\n",
        "from onnx.helper import make_attribute\n",
        "from tqdm import tqdm\n",
        "from onnxruntime import InferenceSession, SessionOptions\n",
        "from onnx_extended.ext_test_case import measure_time, unit_test_going\n",
        "from onnx_extended.memory_peak import start_spying_on\n",
        "from onnx_extended.reference import CReferenceEvaluator\n",
        "from onnx_extended.ortops.optim.cpu import get_ort_ext_libs\n",
        "from onnx_extended.plotting.benchmark import vhistograms\n",
        "\n",
        "\n",
        "def make_onnx(n_words: int) -> ModelProto:\n",
        "    from skl2onnx.common.data_types import Int64TensorType, FloatTensorType\n",
        "    from skl2onnx.algebra.onnx_ops import OnnxTfIdfVectorizer\n",
        "\n",
        "    # from onnx_array_api.light_api import start\n",
        "    # onx = (\n",
        "    #     start(opset=19, opsets={\"ai.onnx.ml\": 3})\n",
        "    #     .vin(\"X\", elem_type=TensorProto.INT64)\n",
        "    #     .ai.onnx.TfIdfVectorizer(\n",
        "    #     ...\n",
        "    #     )\n",
        "    #     .rename(Y)\n",
        "    #     .vout(elem_type=TensorProto.FLOAT)\n",
        "    #     .to_onnx()\n",
        "    # )\n",
        "    onx = OnnxTfIdfVectorizer(\n",
        "        \"X\",\n",
        "        mode=\"TF\",\n",
        "        min_gram_length=1,\n",
        "        max_gram_length=1,\n",
        "        max_skip_count=0,\n",
        "        ngram_counts=[0],\n",
        "        ngram_indexes=np.arange(n_words).tolist(),\n",
        "        pool_int64s=np.arange(n_words).tolist(),\n",
        "        output_names=[\"Y\"],\n",
        "    ).to_onnx(inputs=[(\"X\", Int64TensorType())], outputs=[(\"Y\", FloatTensorType())])\n",
        "    #     .rename(Y)\n",
        "    #     .vout(elem_type=TensorProto.FLOAT)\n",
        "    #     .to_onnx()\n",
        "    # )\n",
        "    return onx\n",
        "\n",
        "\n",
        "onx = make_onnx(7)\n",
        "ref = CReferenceEvaluator(onx)\n",
        "got = ref.run(None, {\"X\": np.array([[0, 1], [2, 3]], dtype=np.int64)})\n",
        "print(got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works as expected. Let's now compare the execution\n",
        "with onnxruntime for different batch size and vocabulary size.\n",
        "\n",
        "## Benchmark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_sessions(\n",
        "    onx: ModelProto,\n",
        ") -> Tuple[InferenceSession, InferenceSession, InferenceSession]:\n",
        "    # first: onnxruntime\n",
        "    ref = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "    # second: custom kernel equivalent to the onnxruntime implementation\n",
        "    for node in onx.graph.node:\n",
        "        if node.op_type == \"TfIdfVectorizer\":\n",
        "            node.domain = \"onnx_extended.ortops.optim.cpu\"\n",
        "            # new_add = make_attribute(\"sparse\", 1)\n",
        "            # node.attribute.append(new_add)\n",
        "\n",
        "    d = onx.opset_import.add()\n",
        "    d.domain = \"onnx_extended.ortops.optim.cpu\"\n",
        "    d.version = 1\n",
        "\n",
        "    r = get_ort_ext_libs()\n",
        "    opts = SessionOptions()\n",
        "    opts.register_custom_ops_library(r[0])\n",
        "    cus = InferenceSession(\n",
        "        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n",
        "    )\n",
        "\n",
        "    # third: with sparse\n",
        "    for node in onx.graph.node:\n",
        "        if node.op_type == \"TfIdfVectorizer\":\n",
        "            new_add = make_attribute(\"sparse\", 1)\n",
        "            node.attribute.append(new_add)\n",
        "    cussp = InferenceSession(\n",
        "        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n",
        "    )\n",
        "\n",
        "    return ref, cus, cussp\n",
        "\n",
        "\n",
        "if unit_test_going():\n",
        "    vocabulary_sizes = [10, 20]\n",
        "    batch_sizes = [5, 10]\n",
        "else:\n",
        "    vocabulary_sizes = [100, 1000, 5000, 10000]\n",
        "    batch_sizes = [1, 10, 500, 1000, 2000]\n",
        "confs = list(itertools.product(vocabulary_sizes, batch_sizes))\n",
        "\n",
        "data = []\n",
        "for voc_size, batch_size in tqdm(confs):\n",
        "    onx = make_onnx(voc_size)\n",
        "    ref, cus, sparse = make_sessions(onx)\n",
        "    gc.collect()\n",
        "\n",
        "    feeds = dict(\n",
        "        X=(np.arange(batch_size * 10) % voc_size)\n",
        "        .reshape((batch_size, -1))\n",
        "        .astype(np.int64)\n",
        "    )\n",
        "\n",
        "    # sparse\n",
        "    p = start_spying_on(delay=0.0001)\n",
        "    sparse.run(None, feeds)\n",
        "    obs = measure_time(\n",
        "        lambda sparse=sparse, feeds=feeds: sparse.run(None, feeds), max_time=1\n",
        "    )\n",
        "    mem = p.stop()\n",
        "    obs[\"peak\"] = mem[\"cpu\"].max_peak - mem[\"cpu\"].begin\n",
        "    obs[\"name\"] = \"sparse\"\n",
        "    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n",
        "    data.append(obs)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    # reference\n",
        "    p = start_spying_on(delay=0.0001)\n",
        "    ref.run(None, feeds)\n",
        "    obs = measure_time(lambda ref=ref, feeds=feeds: ref.run(None, feeds), max_time=1)\n",
        "    mem = p.stop()\n",
        "    obs[\"peak\"] = mem[\"cpu\"].max_peak - mem[\"cpu\"].begin\n",
        "    obs[\"name\"] = \"ref\"\n",
        "    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n",
        "    data.append(obs)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    # custom\n",
        "    p = start_spying_on(delay=0.0001)\n",
        "    cus.run(None, feeds)\n",
        "    obs = measure_time(lambda cus=cus, feeds=feeds: cus.run(None, feeds), max_time=1)\n",
        "    mem = p.stop()\n",
        "    obs[\"peak\"] = mem[\"cpu\"].max_peak - mem[\"cpu\"].begin\n",
        "    obs[\"name\"] = \"custom\"\n",
        "    obs.update(dict(voc_size=voc_size, batch_size=batch_size))\n",
        "    data.append(obs)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    del sparse\n",
        "    del cus\n",
        "    del ref\n",
        "    del feeds\n",
        "\n",
        "df = pandas.DataFrame(data)\n",
        "df[\"time\"] = df[\"average\"]\n",
        "df.to_csv(\"plot_op_tfidfvectorizer_sparse.csv\", index=False)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pandas.pivot_table(\n",
        "    df, index=[\"voc_size\", \"name\"], columns=\"batch_size\", values=\"average\"\n",
        ")\n",
        "print(piv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory peak\n",
        "\n",
        "It is always difficult to estimate. A second process is started to measure\n",
        "the physical memory peak during the execution every ms. The figures\n",
        "is the difference between this peak and the memory when the measurement\n",
        "began.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pandas.pivot_table(\n",
        "    df, index=[\"voc_size\", \"name\"], columns=\"batch_size\", values=\"peak\"\n",
        ")\n",
        "print(piv / 2**20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = vhistograms(df)\n",
        "fig = ax[0, 0].get_figure()\n",
        "fig.savefig(\"plot_op_tfidfvectorizer_sparse.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Take away\n",
        "\n",
        "Sparse works better when the sparsity is big enough and the batch size as well.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
