{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TreeEnsemble optimization\n\nThe execution of a TreeEnsembleRegressor can lead to very different results\ndepending on how the computation is parallelized. By trees,\nby rows, by both, for only one row, for a short batch of rows, a longer one.\nThe implementation in :epkg:`onnxruntime` does not let the user changed\nthe predetermined settings but a custom kernel might. That's what this example\nis measuring.\n\nThe default set of optimized parameters is very short and is meant to be executed\nfast. Many more parameters can be tried.\n\n::\n\n    python plot_op_tree_ensemble_optim --scenario=LONG\n\nTo change the training parameters:\n\n::\n\n    python plot_op_tree_ensemble_optim.py\n        --n_trees=100\n        --max_depth=10\n        --n_features=50\n        --batch_size=100000\n    \nAnother example with a full list of parameters:\n\n    python plot_op_tree_ensemble_optim.py\n        --n_trees=100\n        --max_depth=10\n        --n_features=50\n        --batch_size=100000\n        --tries=3\n        --scenario=CUSTOM\n        --parallel_tree=80,40\n        --parallel_tree_N=128,64\n        --parallel_N=50,25\n        --batch_size_tree=1,2\n        --batch_size_rows=1,2\n        --use_node3=0\n\nAnother example:\n\n::\n\n    python plot_op_tree_ensemble_optim.py\n        --n_trees=100 --n_features=10 --batch_size=10000 --max_depth=8 -s SHORT        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport os\nimport timeit\nfrom typing import Tuple\nimport numpy\nimport onnx\nfrom onnx import ModelProto\nfrom onnx.helper import make_graph, make_model\nfrom onnx.reference import ReferenceEvaluator\nfrom pandas import DataFrame\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skl2onnx import to_onnx\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom onnx_array_api.plotting.text_plot import onnx_simple_text_plot\nfrom onnx_extended.reference import CReferenceEvaluator\nfrom onnx_extended.ortops.optim.cpu import get_ort_ext_libs\nfrom onnx_extended.ortops.optim.optimize import (\n    change_onnx_operator_domain,\n    get_node_attribute,\n    optimize_model,\n)\nfrom onnx_extended.tools.onnx_nodes import multiply_tree\nfrom onnx_extended.args import get_parsed_args\nfrom onnx_extended.ext_test_case import unit_test_going\nfrom onnx_extended.plotting.benchmark import hhistograms\n\nlogging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n\nscript_args = get_parsed_args(\n    \"plot_op_tree_ensemble_optim\",\n    description=__doc__,\n    scenarios={\n        \"SHORT\": \"short optimization (default)\",\n        \"LONG\": \"test more options\",\n        \"CUSTOM\": \"use values specified by the command line\",\n    },\n    n_features=(2 if unit_test_going() else 5, \"number of features to generate\"),\n    n_trees=(3 if unit_test_going() else 10, \"number of trees to train\"),\n    max_depth=(2 if unit_test_going() else 5, \"max_depth\"),\n    batch_size=(1000 if unit_test_going() else 10000, \"batch size\"),\n    parallel_tree=(\"80,160,40\", \"values to try for parallel_tree\"),\n    parallel_tree_N=(\"256,128,64\", \"values to try for parallel_tree_N\"),\n    parallel_N=(\"100,50,25\", \"values to try for parallel_N\"),\n    batch_size_tree=(\"2,4,8\", \"values to try for batch_size_tree\"),\n    batch_size_rows=(\"2,4,8\", \"values to try for batch_size_rows\"),\n    use_node3=(\"0,1\", \"values to try for use_node3\"),\n    expose=\"\",\n    n_jobs=(\"-1\", \"number of jobs to train the RandomForestRegressor\"),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(\n    batch_size: int, n_features: int, n_trees: int, max_depth: int\n) -> Tuple[str, numpy.ndarray, numpy.ndarray]:\n    filename = f\"plot_op_tree_ensemble_optim-f{n_features}-{n_trees}-d{max_depth}.onnx\"\n    if not os.path.exists(filename):\n        X, y = make_regression(\n            batch_size + max(batch_size, 2 ** (max_depth + 1)),\n            n_features=n_features,\n            n_targets=1,\n        )\n        print(f\"Training to get {filename!r} with X.shape={X.shape}\")\n        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n        # To be faster, we train only 1 tree.\n        model = RandomForestRegressor(\n            1, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)\n        )\n        model.fit(X[:-batch_size], y[:-batch_size])\n        onx = to_onnx(model, X[:1], target_opset={\"\": 18, \"ai.onnx.ml\": 3})\n\n        # And wd multiply the trees.\n        node = multiply_tree(onx.graph.node[0], n_trees)\n        onx = make_model(\n            make_graph([node], onx.graph.name, onx.graph.input, onx.graph.output),\n            domain=onx.domain,\n            opset_imports=onx.opset_import,\n            ir_version=onx.ir_version,\n        )\n\n        with open(filename, \"wb\") as f:\n            f.write(onx.SerializeToString())\n    else:\n        X, y = make_regression(batch_size, n_features=n_features, n_targets=1)\n        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n    Xb, yb = X[-batch_size:].copy(), y[-batch_size:].copy()\n    return filename, Xb, yb\n\n\nbatch_size = script_args.batch_size\nn_features = script_args.n_features\nn_trees = script_args.n_trees\nmax_depth = script_args.max_depth\n\nprint(f\"batch_size={batch_size}\")\nprint(f\"n_features={n_features}\")\nprint(f\"n_trees={n_trees}\")\nprint(f\"max_depth={max_depth}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename, Xb, yb = train_model(batch_size, n_features, n_trees, max_depth)\n\nprint(f\"Xb.shape={Xb.shape}\")\nprint(f\"yb.shape={yb.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite the onnx file to use a different kernel\n\nThe custom kernel is mapped to a custom operator with the same name\nthe attributes and domain = `\"onnx_extended.ortops.optim.cpu\"`.\nWe call a function to do that replacement.\nFirst the current model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(filename, \"rb\") as f:\n    onx = onnx.load(f)\nprint(onnx_simple_text_plot(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then the modified model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def transform_model(model, **kwargs):\n    onx = ModelProto()\n    onx.ParseFromString(model.SerializeToString())\n    att = get_node_attribute(onx.graph.node[0], \"nodes_modes\")\n    modes = \",\".join(map(lambda s: s.decode(\"ascii\"), att.strings)).replace(\n        \"BRANCH_\", \"\"\n    )\n    return change_onnx_operator_domain(\n        onx,\n        op_type=\"TreeEnsembleRegressor\",\n        op_domain=\"ai.onnx.ml\",\n        new_op_domain=\"onnx_extended.ortops.optim.cpu\",\n        nodes_modes=modes,\n        **kwargs,\n    )\n\n\nprint(\"Tranform model to add a custom node.\")\nonx_modified = transform_model(onx)\nprint(f\"Save into {filename + 'modified.onnx'!r}.\")\nwith open(filename + \"modified.onnx\", \"wb\") as f:\n    f.write(onx_modified.SerializeToString())\nprint(\"done.\")\nprint(onnx_simple_text_plot(onx_modified))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing onnxruntime and the custom kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Loading {filename!r}\")\nsess_ort = InferenceSession(filename, providers=[\"CPUExecutionProvider\"])\n\nr = get_ort_ext_libs()\nprint(f\"Creating SessionOptions with {r!r}\")\nopts = SessionOptions()\nif r is not None:\n    opts.register_custom_ops_library(r[0])\n\nprint(f\"Loading modified {filename!r}\")\nsess_cus = InferenceSession(\n    onx_modified.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n)\n\nprint(f\"Running once with shape {Xb.shape}.\")\nbase = sess_ort.run(None, {\"X\": Xb})[0]\nprint(f\"Running modified with shape {Xb.shape}.\")\ngot = sess_cus.run(None, {\"X\": Xb})[0]\nprint(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discrepancies?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "d = numpy.abs(base - got)\nya = numpy.abs(base).mean()\nprint(f\"Discrepancies: max={d.max() / ya}, mean={d.mean() / ya} (A={ya})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple verification\n\nBaseline with onnxruntime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t1 = timeit.timeit(lambda: sess_ort.run(None, {\"X\": Xb}), number=50)\nprint(f\"baseline: {t1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t2 = timeit.timeit(lambda: sess_cus.run(None, {\"X\": Xb}), number=50)\nprint(f\"new time: {t2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same implementation but ran from the onnx python backend.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref = CReferenceEvaluator(filename)\nref.run(None, {\"X\": Xb})\nt3 = timeit.timeit(lambda: ref.run(None, {\"X\": Xb}), number=50)\nprint(f\"CReferenceEvaluator: {t3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The python implementation but from the onnx python backend.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if n_trees < 50:\n    # It is usully slow.\n    ref = ReferenceEvaluator(filename)\n    ref.run(None, {\"X\": Xb})\n    t4 = timeit.timeit(lambda: ref.run(None, {\"X\": Xb}), number=5)\n    print(f\"ReferenceEvaluator: {t4} (only 5 times instead of 50)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time for comparison\n\nThe custom kernel supports the same attributes as *TreeEnsembleRegressor*\nplus new ones to tune the parallelization. They can be seen in\n[tree_ensemble.cc](https://github.com/sdpython/onnx-extended/\nblob/main/onnx_extended/ortops/optim/cpu/tree_ensemble.cc#L102).\nLet's try out many possibilities.\nThe default values are the first ones.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if unit_test_going():\n    optim_params = dict(\n        parallel_tree=[40],  # default is 80\n        parallel_tree_N=[128],  # default is 128\n        parallel_N=[50, 25],  # default is 50\n        batch_size_tree=[1],  # default is 1\n        batch_size_rows=[1],  # default is 1\n        use_node3=[0],  # default is 0\n    )\nelif script_args.scenario in (None, \"SHORT\"):\n    optim_params = dict(\n        parallel_tree=[80, 40],  # default is 80\n        parallel_tree_N=[128, 64],  # default is 128\n        parallel_N=[50, 25],  # default is 50\n        batch_size_tree=[1],  # default is 1\n        batch_size_rows=[1],  # default is 1\n        use_node3=[0],  # default is 0\n    )\nelif script_args.scenario == \"LONG\":\n    optim_params = dict(\n        parallel_tree=[80, 160, 40],\n        parallel_tree_N=[256, 128, 64],\n        parallel_N=[100, 50, 25],\n        batch_size_tree=[1, 2, 4, 8],\n        batch_size_rows=[1, 2, 4, 8],\n        use_node3=[0, 1],\n    )\nelif script_args.scenario == \"CUSTOM\":\n    optim_params = dict(\n        parallel_tree=list(int(i) for i in script_args.parallel_tree.split(\",\")),\n        parallel_tree_N=list(int(i) for i in script_args.parallel_tree_N.split(\",\")),\n        parallel_N=list(int(i) for i in script_args.parallel_N.split(\",\")),\n        batch_size_tree=list(int(i) for i in script_args.batch_size_tree.split(\",\")),\n        batch_size_rows=list(int(i) for i in script_args.batch_size_rows.split(\",\")),\n        use_node3=list(int(i) for i in script_args.use_node3.split(\",\")),\n    )\nelse:\n    raise ValueError(\n        f\"Unknown scenario {script_args.scenario!r}, use --help to get them.\"\n    )\n\ncmds = []\nfor att, value in optim_params.items():\n    cmds.append(f\"--{att}={','.join(map(str, value))}\")\nprint(\"Full list of optimization parameters:\")\nprint(\" \".join(cmds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_session(onx):\n    opts = SessionOptions()\n    r = get_ort_ext_libs()\n    if r is None:\n        raise RuntimeError(\"No custom implementation available.\")\n    opts.register_custom_ops_library(r[0])\n    return InferenceSession(\n        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n    )\n\n\nres = optimize_model(\n    onx,\n    feeds={\"X\": Xb},\n    transform=transform_model,\n    session=create_session,\n    baseline=lambda onx: InferenceSession(\n        onx.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n    ),\n    params=optim_params,\n    verbose=True,\n    number=script_args.number,\n    repeat=script_args.repeat,\n    warmup=script_args.warmup,\n    sleep=script_args.sleep,\n    n_tries=script_args.tries,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataFrame(res)\ndf.to_csv(\"plot_op_tree_ensemble_optim.csv\", index=False)\ndf.to_excel(\"plot_op_tree_ensemble_optim.xlsx\", index=False)\nprint(df.columns)\nprint(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sorting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "small_df = df.drop(\n    [\n        \"min_exec\",\n        \"max_exec\",\n        \"repeat\",\n        \"number\",\n        \"context_size\",\n        \"n_exp_name\",\n    ],\n    axis=1,\n).sort_values(\"average\")\nprint(small_df.head(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worst\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(small_df.tail(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skeys = \",\".join(optim_params.keys())\ntitle = f\"TreeEnsemble tuning, n_tries={script_args.tries}\\n{skeys}\\nlower is better\"\nax = hhistograms(df, title=title, keys=(\"name\",))\nfig = ax.get_figure()\nfig.savefig(\"plot_op_tree_ensemble_optim.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}