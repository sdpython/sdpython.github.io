{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# TreeEnsemble optimization\n",
        "\n",
        "The execution of a TreeEnsembleRegressor can lead to very different results\n",
        "depending on how the computation is parallelized. By trees,\n",
        "by rows, by both, for only one row, for a short batch of rows, a longer one.\n",
        "The implementation in :epkg:`onnxruntime` does not let the user changed\n",
        "the predetermined settings but a custom kernel might. That's what this example\n",
        "is measuring.\n",
        "\n",
        "The default set of optimized parameters is very short and is meant to be executed\n",
        "fast. Many more parameters can be tried.\n",
        "\n",
        "::\n",
        "\n",
        "    python plot_op_tree_ensemble_optim --scenario=LONG\n",
        "\n",
        "To change the training parameters:\n",
        "\n",
        "::\n",
        "\n",
        "    python plot_op_tree_ensemble_optim.py\n",
        "        --n_trees=100\n",
        "        --max_depth=10\n",
        "        --n_features=50\n",
        "        --batch_size=100000\n",
        "    \n",
        "Another example with a full list of parameters:\n",
        "\n",
        "    python plot_op_tree_ensemble_optim.py\n",
        "        --n_trees=100\n",
        "        --max_depth=10\n",
        "        --n_features=50\n",
        "        --batch_size=100000\n",
        "        --tries=3\n",
        "        --scenario=CUSTOM\n",
        "        --parallel_tree=80,40\n",
        "        --parallel_tree_N=128,64\n",
        "        --parallel_N=50,25\n",
        "        --batch_size_tree=1,2\n",
        "        --batch_size_rows=1,2\n",
        "        --use_node3=0\n",
        "\n",
        "Another example:\n",
        "\n",
        "::\n",
        "\n",
        "    python plot_op_tree_ensemble_optim.py\n",
        "        --n_trees=100 --n_features=10 --batch_size=10000 --max_depth=8 -s SHORT        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import timeit\n",
        "from typing import Tuple\n",
        "import numpy\n",
        "import onnx\n",
        "from onnx import ModelProto\n",
        "from onnx.helper import make_graph, make_model\n",
        "from onnx.reference import ReferenceEvaluator\n",
        "from pandas import DataFrame\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from skl2onnx import to_onnx\n",
        "from onnxruntime import InferenceSession, SessionOptions\n",
        "from onnx_array_api.plotting.text_plot import onnx_simple_text_plot\n",
        "from onnx_extended.reference import CReferenceEvaluator\n",
        "from onnx_extended.ortops.optim.cpu import get_ort_ext_libs\n",
        "from onnx_extended.ortops.optim.optimize import (\n",
        "    change_onnx_operator_domain,\n",
        "    get_node_attribute,\n",
        "    optimize_model,\n",
        ")\n",
        "from onnx_extended.tools.onnx_nodes import multiply_tree\n",
        "from onnx_extended.args import get_parsed_args\n",
        "from onnx_extended.ext_test_case import unit_test_going\n",
        "from onnx_extended.plotting.benchmark import hhistograms\n",
        "\n",
        "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
        "\n",
        "script_args = get_parsed_args(\n",
        "    \"plot_op_tree_ensemble_optim\",\n",
        "    description=__doc__,\n",
        "    scenarios={\n",
        "        \"SHORT\": \"short optimization (default)\",\n",
        "        \"LONG\": \"test more options\",\n",
        "        \"CUSTOM\": \"use values specified by the command line\",\n",
        "    },\n",
        "    n_features=(2 if unit_test_going() else 5, \"number of features to generate\"),\n",
        "    n_trees=(3 if unit_test_going() else 10, \"number of trees to train\"),\n",
        "    max_depth=(2 if unit_test_going() else 5, \"max_depth\"),\n",
        "    batch_size=(1000 if unit_test_going() else 10000, \"batch size\"),\n",
        "    parallel_tree=(\"80,160,40\", \"values to try for parallel_tree\"),\n",
        "    parallel_tree_N=(\"256,128,64\", \"values to try for parallel_tree_N\"),\n",
        "    parallel_N=(\"100,50,25\", \"values to try for parallel_N\"),\n",
        "    batch_size_tree=(\"2,4,8\", \"values to try for batch_size_tree\"),\n",
        "    batch_size_rows=(\"2,4,8\", \"values to try for batch_size_rows\"),\n",
        "    use_node3=(\"0,1\", \"values to try for use_node3\"),\n",
        "    expose=\"\",\n",
        "    n_jobs=(\"-1\", \"number of jobs to train the RandomForestRegressor\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    batch_size: int, n_features: int, n_trees: int, max_depth: int\n",
        ") -> Tuple[str, numpy.ndarray, numpy.ndarray]:\n",
        "    filename = f\"plot_op_tree_ensemble_optim-f{n_features}-{n_trees}-d{max_depth}.onnx\"\n",
        "    if not os.path.exists(filename):\n",
        "        X, y = make_regression(\n",
        "            batch_size + max(batch_size, 2 ** (max_depth + 1)),\n",
        "            n_features=n_features,\n",
        "            n_targets=1,\n",
        "        )\n",
        "        print(f\"Training to get {filename!r} with X.shape={X.shape}\")\n",
        "        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n",
        "        # To be faster, we train only 1 tree.\n",
        "        model = RandomForestRegressor(\n",
        "            1, max_depth=max_depth, verbose=2, n_jobs=int(script_args.n_jobs)\n",
        "        )\n",
        "        model.fit(X[:-batch_size], y[:-batch_size])\n",
        "        onx = to_onnx(model, X[:1], target_opset={\"\": 18, \"ai.onnx.ml\": 3})\n",
        "\n",
        "        # And wd multiply the trees.\n",
        "        node = multiply_tree(onx.graph.node[0], n_trees)\n",
        "        onx = make_model(\n",
        "            make_graph([node], onx.graph.name, onx.graph.input, onx.graph.output),\n",
        "            domain=onx.domain,\n",
        "            opset_imports=onx.opset_import,\n",
        "            ir_version=onx.ir_version,\n",
        "        )\n",
        "\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(onx.SerializeToString())\n",
        "    else:\n",
        "        X, y = make_regression(batch_size, n_features=n_features, n_targets=1)\n",
        "        X, y = X.astype(numpy.float32), y.astype(numpy.float32)\n",
        "    Xb, yb = X[-batch_size:].copy(), y[-batch_size:].copy()\n",
        "    return filename, Xb, yb\n",
        "\n",
        "\n",
        "batch_size = script_args.batch_size\n",
        "n_features = script_args.n_features\n",
        "n_trees = script_args.n_trees\n",
        "max_depth = script_args.max_depth\n",
        "\n",
        "print(f\"batch_size={batch_size}\")\n",
        "print(f\"n_features={n_features}\")\n",
        "print(f\"n_trees={n_trees}\")\n",
        "print(f\"max_depth={max_depth}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename, Xb, yb = train_model(batch_size, n_features, n_trees, max_depth)\n",
        "\n",
        "print(f\"Xb.shape={Xb.shape}\")\n",
        "print(f\"yb.shape={yb.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite the onnx file to use a different kernel\n",
        "\n",
        "The custom kernel is mapped to a custom operator with the same name\n",
        "the attributes and domain = `\"onnx_extented.ortops.optim.cpu\"`.\n",
        "We call a function to do that replacement.\n",
        "First the current model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(filename, \"rb\") as f:\n",
        "    onx = onnx.load(f)\n",
        "print(onnx_simple_text_plot(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then the modified model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def transform_model(model, **kwargs):\n",
        "    onx = ModelProto()\n",
        "    onx.ParseFromString(model.SerializeToString())\n",
        "    att = get_node_attribute(onx.graph.node[0], \"nodes_modes\")\n",
        "    modes = \",\".join(map(lambda s: s.decode(\"ascii\"), att.strings)).replace(\n",
        "        \"BRANCH_\", \"\"\n",
        "    )\n",
        "    return change_onnx_operator_domain(\n",
        "        onx,\n",
        "        op_type=\"TreeEnsembleRegressor\",\n",
        "        op_domain=\"ai.onnx.ml\",\n",
        "        new_op_domain=\"onnx_extended.ortops.optim.cpu\",\n",
        "        nodes_modes=modes,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Tranform model to add a custom node.\")\n",
        "onx_modified = transform_model(onx)\n",
        "print(f\"Save into {filename + 'modified.onnx'!r}.\")\n",
        "with open(filename + \"modified.onnx\", \"wb\") as f:\n",
        "    f.write(onx_modified.SerializeToString())\n",
        "print(\"done.\")\n",
        "print(onnx_simple_text_plot(onx_modified))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing onnxruntime and the custom kernel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Loading {filename!r}\")\n",
        "sess_ort = InferenceSession(filename, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "r = get_ort_ext_libs()\n",
        "print(f\"Creating SessionOptions with {r!r}\")\n",
        "opts = SessionOptions()\n",
        "if r is not None:\n",
        "    opts.register_custom_ops_library(r[0])\n",
        "\n",
        "print(f\"Loading modified {filename!r}\")\n",
        "sess_cus = InferenceSession(\n",
        "    onx_modified.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "\n",
        "print(f\"Running once with shape {Xb.shape}.\")\n",
        "base = sess_ort.run(None, {\"X\": Xb})[0]\n",
        "print(f\"Running modified with shape {Xb.shape}.\")\n",
        "got = sess_cus.run(None, {\"X\": Xb})[0]\n",
        "print(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discrepancies?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "d = numpy.abs(base - got)\n",
        "ya = numpy.abs(base).mean()\n",
        "print(f\"Discrepancies: max={d.max() / ya}, mean={d.mean() / ya} (A={ya})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple verification\n",
        "\n",
        "Baseline with onnxruntime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t1 = timeit.timeit(lambda: sess_ort.run(None, {\"X\": Xb}), number=50)\n",
        "print(f\"baseline: {t1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t2 = timeit.timeit(lambda: sess_cus.run(None, {\"X\": Xb}), number=50)\n",
        "print(f\"new time: {t2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same implementation but ran from the onnx python backend.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref = CReferenceEvaluator(filename)\n",
        "ref.run(None, {\"X\": Xb})\n",
        "t3 = timeit.timeit(lambda: ref.run(None, {\"X\": Xb}), number=50)\n",
        "print(f\"CReferenceEvaluator: {t3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The python implementation but from the onnx python backend.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if n_trees < 50:\n",
        "    # It is usully slow.\n",
        "    ref = ReferenceEvaluator(filename)\n",
        "    ref.run(None, {\"X\": Xb})\n",
        "    t4 = timeit.timeit(lambda: ref.run(None, {\"X\": Xb}), number=5)\n",
        "    print(f\"ReferenceEvaluator: {t4} (only 5 times instead of 50)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time for comparison\n",
        "\n",
        "The custom kernel supports the same attributes as *TreeEnsembleRegressor*\n",
        "plus new ones to tune the parallelization. They can be seen in\n",
        "[tree_ensemble.cc](https://github.com/sdpython/onnx-extended/\n",
        "blob/main/onnx_extended/ortops/optim/cpu/tree_ensemble.cc#L102).\n",
        "Let's try out many possibilities.\n",
        "The default values are the first ones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if unit_test_going():\n",
        "    optim_params = dict(\n",
        "        parallel_tree=[40],  # default is 80\n",
        "        parallel_tree_N=[128],  # default is 128\n",
        "        parallel_N=[50, 25],  # default is 50\n",
        "        batch_size_tree=[1],  # default is 1\n",
        "        batch_size_rows=[1],  # default is 1\n",
        "        use_node3=[0],  # default is 0\n",
        "    )\n",
        "elif script_args.scenario in (None, \"SHORT\"):\n",
        "    optim_params = dict(\n",
        "        parallel_tree=[80, 40],  # default is 80\n",
        "        parallel_tree_N=[128, 64],  # default is 128\n",
        "        parallel_N=[50, 25],  # default is 50\n",
        "        batch_size_tree=[1],  # default is 1\n",
        "        batch_size_rows=[1],  # default is 1\n",
        "        use_node3=[0],  # default is 0\n",
        "    )\n",
        "elif script_args.scenario == \"LONG\":\n",
        "    optim_params = dict(\n",
        "        parallel_tree=[80, 160, 40],\n",
        "        parallel_tree_N=[256, 128, 64],\n",
        "        parallel_N=[100, 50, 25],\n",
        "        batch_size_tree=[1, 2, 4, 8],\n",
        "        batch_size_rows=[1, 2, 4, 8],\n",
        "        use_node3=[0, 1],\n",
        "    )\n",
        "elif script_args.scenario == \"CUSTOM\":\n",
        "    optim_params = dict(\n",
        "        parallel_tree=list(int(i) for i in script_args.parallel_tree.split(\",\")),\n",
        "        parallel_tree_N=list(int(i) for i in script_args.parallel_tree_N.split(\",\")),\n",
        "        parallel_N=list(int(i) for i in script_args.parallel_N.split(\",\")),\n",
        "        batch_size_tree=list(int(i) for i in script_args.batch_size_tree.split(\",\")),\n",
        "        batch_size_rows=list(int(i) for i in script_args.batch_size_rows.split(\",\")),\n",
        "        use_node3=list(int(i) for i in script_args.use_node3.split(\",\")),\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Unknown scenario {script_args.scenario!r}, use --help to get them.\"\n",
        "    )\n",
        "\n",
        "cmds = []\n",
        "for att, value in optim_params.items():\n",
        "    cmds.append(f\"--{att}={','.join(map(str, value))}\")\n",
        "print(\"Full list of optimization parameters:\")\n",
        "print(\" \".join(cmds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_session(onx):\n",
        "    opts = SessionOptions()\n",
        "    r = get_ort_ext_libs()\n",
        "    if r is None:\n",
        "        raise RuntimeError(\"No custom implementation available.\")\n",
        "    opts.register_custom_ops_library(r[0])\n",
        "    return InferenceSession(\n",
        "        onx.SerializeToString(), opts, providers=[\"CPUExecutionProvider\"]\n",
        "    )\n",
        "\n",
        "\n",
        "res = optimize_model(\n",
        "    onx,\n",
        "    feeds={\"X\": Xb},\n",
        "    transform=transform_model,\n",
        "    session=create_session,\n",
        "    baseline=lambda onx: InferenceSession(\n",
        "        onx.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n",
        "    ),\n",
        "    params=optim_params,\n",
        "    verbose=True,\n",
        "    number=script_args.number,\n",
        "    repeat=script_args.repeat,\n",
        "    warmup=script_args.warmup,\n",
        "    sleep=script_args.sleep,\n",
        "    n_tries=script_args.tries,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataFrame(res)\n",
        "df.to_csv(\"plot_op_tree_ensemble_optim.csv\", index=False)\n",
        "df.to_excel(\"plot_op_tree_ensemble_optim.xlsx\", index=False)\n",
        "print(df.columns)\n",
        "print(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sorting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "small_df = df.drop(\n",
        "    [\n",
        "        \"min_exec\",\n",
        "        \"max_exec\",\n",
        "        \"repeat\",\n",
        "        \"number\",\n",
        "        \"context_size\",\n",
        "        \"n_exp_name\",\n",
        "    ],\n",
        "    axis=1,\n",
        ").sort_values(\"average\")\n",
        "print(small_df.head(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worst\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(small_df.tail(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skeys = \",\".join(optim_params.keys())\n",
        "title = f\"TreeEnsemble tuning, n_tries={script_args.tries}\\n{skeys}\\nlower is better\"\n",
        "ax = hhistograms(df, title=title, keys=(\"name\",))\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(\"plot_op_tree_ensemble_optim.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
