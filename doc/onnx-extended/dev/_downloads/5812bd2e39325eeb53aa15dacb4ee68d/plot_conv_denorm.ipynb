{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How float format has an impact on speed computation\n\nAn example with Conv. The floats followed the IEEE standard\n[Single-precision floating-point format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format).\nThe number is interprated in a different whether the exponent is null\nor not. When it is null, it is called a denormalized number\nor [subnormal number](https://en.wikipedia.org/wiki/Subnormal_number).\nLet's see their impact on the computation time through the operator Conv.\n\n## Create one model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import struct\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom tqdm import tqdm\nimport numpy\nfrom onnx import TensorProto\nfrom onnx.helper import (\n    make_model,\n    make_node,\n    make_graph,\n    make_tensor_value_info,\n    make_opsetid,\n)\nfrom onnx.checker import check_model\nfrom onnx.numpy_helper import to_array, from_array\nfrom onnxruntime import (\n    InferenceSession,\n    get_available_providers,\n    OrtValue,\n    SessionOptions,\n    GraphOptimizationLevel,\n)\nfrom onnx_array_api.plotting.text_plot import onnx_simple_text_plot\nfrom onnx_extended.ext_test_case import measure_time, unit_test_going\nfrom onnx_extended.reference import CReferenceEvaluator\n\ntry:\n    import torch\nexcept ImportError:\n    # no torch is available\n    print(\"torch is not available\")\n    torch = None\n\n\ndef _denorm(x):\n    i = int.from_bytes(struct.pack(\"<f\", numpy.float32(x)), \"little\")\n    i &= 0x807FFFFF\n    return numpy.uint32(i).view(numpy.float32)\n\n\ndenorm = numpy.vectorize(_denorm)\n\n\ndef create_model():\n    X = make_tensor_value_info(\"X\", TensorProto.FLOAT, [1, 256, 14, 14])\n    Y = make_tensor_value_info(\"Y\", TensorProto.FLOAT, [None, None, None, None])\n    B = from_array(numpy.zeros([256], dtype=numpy.float32), name=\"B\")\n    w = numpy.random.randn(256, 256, 3, 3).astype(numpy.float32)\n\n    # let's randomly denormalize some number\n    mask = (numpy.random.randint(2, size=w.shape) % 2).astype(numpy.float32)\n    d = denorm(w)\n    w = w * mask - (mask - 1) * d\n    W = from_array(w, name=\"W\")\n\n    node1 = make_node(\n        \"Conv\", [\"X\", \"W\", \"B\"], [\"Y\"], kernel_shape=[3, 3], pads=[1, 1, 1, 1]\n    )\n    graph = make_graph([node1], \"lr\", [X], [Y], [W, B])\n    onnx_model = make_model(graph, opset_imports=[make_opsetid(\"\", 18)], ir_version=8)\n    check_model(onnx_model)\n    return onnx_model\n\n\nonx = create_model()\nonnx_file = \"plot_conv_denorm.onnx\"\nwith open(onnx_file, \"wb\") as f:\n    f.write(onx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model looks like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(onnx_simple_text_plot(onx))\n\nonnx_model = onnx_file\ninput_shape = (1, 256, 14, 14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CReferenceEvaluator and InferenceSession\nLet's first compare the outputs are the same.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess_options = SessionOptions()\nsess_options.graph_optimization_level = GraphOptimizationLevel.ORT_DISABLE_ALL\n\n\nsess1 = CReferenceEvaluator(onnx_model)\nsess2 = InferenceSession(onnx_model, sess_options, providers=[\"CPUExecutionProvider\"])\n\nX = numpy.ones(input_shape, dtype=numpy.float32)\n\nexpected = sess1.run(None, {\"X\": X})[0]\ngot = sess2.run(None, {\"X\": X})[0]\ndiff = numpy.abs(expected - got).max()\nprint(f\"difference: {diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything works fine.\n\n## Time measurement\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feeds = {\"X\": X}\n\nt1 = measure_time(lambda: sess1.run(None, feeds), repeat=2, number=5)\nprint(f\"CReferenceEvaluator: {t1['average']}s\")\n\nt2 = measure_time(lambda: sess2.run(None, feeds), repeat=2, number=5)\nprint(f\"InferenceSession: {t2['average']}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting\n\nLet's modify the the weight of the model and multiply everything by a scalar.\nLet's choose an random input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "has_cuda = \"CUDAExecutionProvider\" in get_available_providers()\nX = numpy.random.random(X.shape).astype(X.dtype)\n\n\ndef modify(onx, scale):\n    t = to_array(onx.graph.initializer[0])\n    b = to_array(onx.graph.initializer[1]).copy()\n    t = (t * scale).astype(numpy.float32)\n    graph = make_graph(\n        onx.graph.node,\n        onx.graph.name,\n        onx.graph.input,\n        onx.graph.output,\n        [from_array(t, name=onx.graph.initializer[0].name), onx.graph.initializer[1]],\n    )\n    model = make_model(graph, opset_imports=onx.opset_import, ir_version=onx.ir_version)\n    return t, b, model\n\n\nscales = [2**p for p in range(0, 31, 2)]\ndata = []\nfeeds = {\"X\": X}\nexpected = sess2.run(None, feeds)[0]\nif torch is not None:\n    tx = torch.from_numpy(X)\n\nsess_options0 = SessionOptions()\nsess_options0.graph_optimization_level = GraphOptimizationLevel.ORT_DISABLE_ALL\nsess_options0.add_session_config_entry(\"session.set_denormal_as_zero\", \"1\")\n\nfor scale in tqdm(scales):\n    w, b, new_onx = modify(onx, scale)\n    n_denorm = (w == denorm(w)).astype(numpy.int32).sum() / w.size\n\n    # sess1 = CReferenceEvaluator(new_onx)\n    sess2 = InferenceSession(\n        new_onx.SerializeToString(), sess_options, providers=[\"CPUExecutionProvider\"]\n    )\n    sess3 = InferenceSession(\n        new_onx.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n    )\n    sess4 = InferenceSession(\n        new_onx.SerializeToString(), sess_options0, providers=[\"CPUExecutionProvider\"]\n    )\n\n    # sess1.run(None, feeds)\n    got = sess2.run(None, feeds)[0]\n    diff = numpy.abs(got / scale - expected).max()\n    sess3.run(None, feeds)\n    got0 = sess4.run(None, feeds)[0]\n    diff0 = numpy.abs(got0 / scale - expected).max()\n\n    # t1 = measure_time(lambda: sess1.run(None, feeds), repeat=2, number=5)\n    t2 = measure_time(lambda: sess2.run(None, feeds), repeat=2, number=5)\n    t3 = measure_time(lambda: sess3.run(None, feeds), repeat=2, number=5)\n    t4 = measure_time(lambda: sess4.run(None, feeds), repeat=2, number=5)\n    obs = dict(\n        scale=scale,\n        ort=t2[\"average\"],\n        diff=diff,\n        diff0=diff0,\n        ort0=t4[\"average\"],\n        n_denorm=n_denorm,\n    )\n    # obs[\"ref\"]=t1[\"average\"]\n    obs[\"ort-opt\"] = t3[\"average\"]\n\n    if torch is not None:\n        tw = torch.from_numpy(w)\n        tb = torch.from_numpy(b)\n        torch.nn.functional.conv2d(tx, tw, tb, padding=1)\n        t3 = measure_time(\n            lambda: torch.nn.functional.conv2d(tx, tw, tb, padding=1),\n            repeat=2,\n            number=5,\n        )\n        obs[\"torch\"] = t3[\"average\"]\n\n    if has_cuda:\n        sess2 = InferenceSession(\n            new_onx.SerializeToString(),\n            sess_options,\n            providers=[\"CUDAExecutionProvider\"],\n        )\n        sess3 = InferenceSession(\n            new_onx.SerializeToString(), providers=[\"CUDAExecutionProvider\"]\n        )\n        x_ortvalue = OrtValue.ortvalue_from_numpy(X, \"cuda\", 0)\n        cuda_feeds = {\"X\": x_ortvalue}\n        sess2.run_with_ort_values(None, cuda_feeds)\n        sess3.run_with_ort_values(None, cuda_feeds)\n        t2 = measure_time(lambda: sess2.run(None, cuda_feeds), repeat=2, number=5)\n        t3 = measure_time(lambda: sess3.run(None, cuda_feeds), repeat=2, number=5)\n        obs[\"ort-cuda\"] = t2[\"average\"]\n        obs[\"ort-cuda-opt\"] = t2[\"average\"]\n\n    data.append(obs)\n    if unit_test_going() and len(data) >= 2:\n        break\n\ndf = DataFrame(data)\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfp = df.drop([\"diff\", \"diff0\", \"n_denorm\"], axis=1).set_index(\"scale\")\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\ndfp.plot(ax=ax[0], logx=True, logy=True, title=\"Comparison of Conv processing time\")\ndf[[\"n_denorm\"]].plot(\n    ax=ax[1], logx=True, logy=True, title=\"Ratio of denormalized numbers\"\n)\n\nfig.savefig(\"plot_conv_denorm.png\")\n# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nDenormalized numbers should be avoided.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}