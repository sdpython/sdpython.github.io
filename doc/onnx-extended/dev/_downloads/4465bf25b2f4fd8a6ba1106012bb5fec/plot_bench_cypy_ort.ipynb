{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Measuring onnxruntime performance against a cython binding\n\nThe following code measures the performance of the python bindings\nagainst a :epkg:`cython` binding.\nThe time spent in it is not significant when the computation is huge\nbut it may be for small matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model,\n    make_node,\n    make_graph,\n    make_tensor_value_info,\n    make_opsetid,\n)\nfrom onnx.checker import check_model\nfrom onnxruntime import InferenceSession\nfrom onnx_extended.ortcy.wrap.ortinf import OrtSession\nfrom onnx_extended.args import get_parsed_args\nfrom onnx_extended.ext_test_case import measure_time, unit_test_going\n\n\nscript_args = get_parsed_args(\n    \"plot_bench_cypy_ort\",\n    description=__doc__,\n    dims=(\n        \"1,10\" if unit_test_going() else \"1,10,100,1000\",\n        \"square matrix dimensions to try, comma separated values\",\n    ),\n    expose=\"repeat,number\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A simple onnx model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A = numpy_helper.from_array(numpy.array([1], dtype=numpy.float32), name=\"A\")\nX = make_tensor_value_info(\"X\", TensorProto.FLOAT, [None, None])\nY = make_tensor_value_info(\"Y\", TensorProto.FLOAT, [None, None])\nnode1 = make_node(\"Add\", [\"X\", \"A\"], [\"Y\"])\ngraph = make_graph([node1], \"+1\", [X], [Y], [A])\nonnx_model = make_model(graph, opset_imports=[make_opsetid(\"\", 18)], ir_version=8)\ncheck_model(onnx_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two python bindings on CPU\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess_ort = InferenceSession(\n    onnx_model.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n)\nsess_ext = OrtSession(onnx_model.SerializeToString())\n\nx = numpy.random.randn(10, 10).astype(numpy.float32)\ny = x + 1\n\ny_ort = sess_ort.run(None, {\"X\": x})[0]\ny_ext = sess_ext.run([x])[0]\n\nd_ort = numpy.abs(y_ort - y).sum()\nd_ext = numpy.abs(y_ext - y).sum()\nprint(f\"Discrepancies: d_ort={d_ort}, d_ext={d_ext}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time measurement\n\n*run_1_1* is a specific implementation when there is only 1 input and output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t_ort = measure_time(lambda: sess_ort.run(None, {\"X\": x})[0], number=200, repeat=100)\nprint(f\"t_ort={t_ort}\")\n\nt_ext = measure_time(lambda: sess_ext.run([x])[0], number=200, repeat=100)\nprint(f\"t_ext={t_ext}\")\n\nt_ext2 = measure_time(lambda: sess_ext.run_1_1(x), number=200, repeat=100)\nprint(f\"t_ext2={t_ext2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dims = [int(i) for i in script_args.dims.split(\",\")]\n\ndata = []\nfor dim in tqdm(dims):\n    if dim < 1000:\n        number, repeat = script_args.number, script_args.repeat\n    else:\n        number, repeat = script_args.number * 5, script_args.repeat * 5\n    x = numpy.random.randn(dim, dim).astype(numpy.float32)\n    t_ort = measure_time(\n        lambda x=x: sess_ort.run(None, {\"X\": x})[0], number=number, repeat=50\n    )\n    t_ort[\"name\"] = \"ort\"\n    t_ort[\"dim\"] = dim\n    data.append(t_ort)\n\n    t_ext = measure_time(lambda x=x: sess_ext.run([x])[0], number=number, repeat=repeat)\n    t_ext[\"name\"] = \"ext\"\n    t_ext[\"dim\"] = dim\n    data.append(t_ext)\n\n    t_ext2 = measure_time(lambda x=x: sess_ext.run_1_1(x), number=number, repeat=repeat)\n    t_ext2[\"name\"] = \"ext_1_1\"\n    t_ext2[\"dim\"] = dim\n    data.append(t_ext2)\n\n    if unit_test_going() and dim >= 10:\n        break\n\n\ndf = DataFrame(data)\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = df.pivot(index=\"dim\", columns=\"name\", values=\"average\")\n\nfig, ax = plt.subplots(1, 1)\npiv.plot(ax=ax, title=\"Binding Comparison\", logy=True, logx=True)\nfig.tight_layout()\nfig.savefig(\"plot_bench_ort.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}