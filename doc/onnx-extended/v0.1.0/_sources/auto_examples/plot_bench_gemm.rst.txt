
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_bench_gemm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_bench_gemm.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_bench_gemm.py:


.. _l-example-bench-gemm:

Measuring performance about Gemm
================================

Differents types, differents backend, differents

Onnx Model
++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 12-75

.. code-block:: default

    import platform
    from itertools import product
    import numpy
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    from pandas import DataFrame, pivot_table
    from onnx import TensorProto
    from onnx.helper import (
        make_model,
        make_node,
        make_graph,
        make_tensor_value_info,
        make_opsetid,
    )
    from onnx.checker import check_model
    from onnx.numpy_helper import from_array
    from onnxruntime import InferenceSession, get_available_providers
    from onnxruntime.capi._pybind_state import (
        OrtValue as C_OrtValue,
        OrtDevice as C_OrtDevice,
    )
    from onnxruntime.capi.onnxruntime_pybind11_state import (
        NotImplemented,
        InvalidGraph,
        InvalidArgument,
    )
    from onnx_extended.reference import CReferenceEvaluator
    from onnx_extended.ext_test_case import unit_test_going, measure_time


    def create_model(mat_type=TensorProto.FLOAT):
        I1 = from_array(numpy.array([1], dtype=numpy.float32), name="I")
        A = make_tensor_value_info("A", mat_type, [None, None])
        B = make_tensor_value_info("B", mat_type, [None, None])
        C = make_tensor_value_info("C", mat_type, [None, None])
        nodes = [
            make_node("CastLike", ["I", "A"], ["Ic"]),
            make_node("Add", ["A", "Ic"], ["A1"]),
            make_node("Add", ["A1", "Ic"], ["A2"]),
            make_node("Add", ["A2", "Ic"], ["A3"]),
            make_node("MatMul", ["A", "B"], ["M0"]),
            make_node("MatMul", ["A1", "B"], ["M1"]),
            make_node("MatMul", ["A2", "B"], ["M2"]),
            make_node("MatMul", ["A3", "B"], ["M3"]),
            make_node("Add", ["M0", "M1"], ["M12"]),
            make_node("Add", ["M2", "M3"], ["M23"]),
            make_node("Add", ["M12", "M23"], ["C"]),
        ]
        graph = make_graph(nodes, "a", [A, B], [C], [I1])
        if mat_type < 16:
            # regular type
            opset, ir = 18, 8
        else:
            opset, ir = 19, 9
        onnx_model = make_model(
            graph, opset_imports=[make_opsetid("", opset)], ir_version=ir
        )
        check_model(onnx_model)
        return onnx_model


    create_model()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ir_version: 8
    graph {
      node {
        input: "I"
        input: "A"
        output: "Ic"
        op_type: "CastLike"
      }
      node {
        input: "A"
        input: "Ic"
        output: "A1"
        op_type: "Add"
      }
      node {
        input: "A1"
        input: "Ic"
        output: "A2"
        op_type: "Add"
      }
      node {
        input: "A2"
        input: "Ic"
        output: "A3"
        op_type: "Add"
      }
      node {
        input: "A"
        input: "B"
        output: "M0"
        op_type: "MatMul"
      }
      node {
        input: "A1"
        input: "B"
        output: "M1"
        op_type: "MatMul"
      }
      node {
        input: "A2"
        input: "B"
        output: "M2"
        op_type: "MatMul"
      }
      node {
        input: "A3"
        input: "B"
        output: "M3"
        op_type: "MatMul"
      }
      node {
        input: "M0"
        input: "M1"
        output: "M12"
        op_type: "Add"
      }
      node {
        input: "M2"
        input: "M3"
        output: "M23"
        op_type: "Add"
      }
      node {
        input: "M12"
        input: "M23"
        output: "C"
        op_type: "Add"
      }
      name: "a"
      initializer {
        dims: 1
        data_type: 1
        name: "I"
        raw_data: "\000\000\200?"
      }
      input {
        name: "A"
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
              }
              dim {
              }
            }
          }
        }
      }
      input {
        name: "B"
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
              }
              dim {
              }
            }
          }
        }
      }
      output {
        name: "C"
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
              }
              dim {
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: ""
      version: 18
    }




.. GENERATED FROM PYTHON SOURCE LINES 76-77

A model to cast

.. GENERATED FROM PYTHON SOURCE LINES 77-99

.. code-block:: default



    def create_cast(to):
        A = make_tensor_value_info("A", TensorProto.FLOAT, [None, None])
        C = make_tensor_value_info("C", to, [None, None])
        node1 = make_node("Cast", ["A"], ["C"], to=to)
        graph = make_graph([node1], "a", [A], [C])
        if to < 16:
            # regular type
            opset, ir = 18, 8
        else:
            opset, ir = 19, 9
        onnx_model = make_model(
            graph, opset_imports=[make_opsetid("", opset)], ir_version=ir
        )
        check_model(onnx_model)
        return onnx_model


    create_cast(TensorProto.FLOAT16)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ir_version: 8
    graph {
      node {
        input: "A"
        output: "C"
        op_type: "Cast"
        attribute {
          name: "to"
          i: 10
          type: INT
        }
      }
      name: "a"
      input {
        name: "A"
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
              }
              dim {
              }
            }
          }
        }
      }
      output {
        name: "C"
        type {
          tensor_type {
            elem_type: 10
            shape {
              dim {
              }
              dim {
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: ""
      version: 18
    }




.. GENERATED FROM PYTHON SOURCE LINES 100-104

Performance
+++++++++++

The benchmark will run the following configurations.

.. GENERATED FROM PYTHON SOURCE LINES 104-138

.. code-block:: default


    types = [
        TensorProto.FLOAT,
        TensorProto.UINT32,
        TensorProto.INT32,
        TensorProto.INT16,
        TensorProto.INT8,
        TensorProto.FLOAT16,
        TensorProto.BFLOAT16,
        TensorProto.FLOAT8E4M3FN,
        TensorProto.FLOAT8E5M2,
    ]
    engine = [CReferenceEvaluator, InferenceSession]
    providers = [
        ["CPUExecutionProvider"],
        ["CUDAExecutionProvider", "CPUExecutionProvider"],
    ]
    # M, N, K
    dims = [
        (10, 10, 10),
        (61, 62, 63),
        (64, 64, 64),
        (65, 66, 67),
        (100, 100, 100),
        (128, 128, 128),
        # (256, 256, 256),
        # (400, 400, 400),
        # (512, 512, 512),
    ]


    map_type = {TensorProto.FLOAT: numpy.float32, TensorProto.FLOAT16: numpy.float16}









.. GENERATED FROM PYTHON SOURCE LINES 139-140

Let's cache the matrices involved.

.. GENERATED FROM PYTHON SOURCE LINES 140-167

.. code-block:: default



    def to_ort_value(m):
        device = C_OrtDevice(C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)
        ort_value = C_OrtValue.ortvalue_from_numpy(m, device)
        return ort_value


    matrices = {}
    for m, n, k in dims:
        for tt in types:
            for i, j in [(m, k), (k, n)]:
                try:
                    sess = InferenceSession(
                        create_cast(tt).SerializeToString(),
                        providers=["CPUExecutionProvider"],
                    )
                except (InvalidGraph, InvalidArgument):
                    # not support by this version of onnxruntime
                    continue
                vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)
                ov = to_ort_value(vect)
                ovtt = sess._sess.run_with_ort_values({"A": ov}, ["C"], None)[0]
                matrices[tt, i, j] = ovtt

    print(f"{len(matrices)} matrices were created.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    72 matrices were created.




.. GENERATED FROM PYTHON SOURCE LINES 168-169

Let's run the benchmark

.. GENERATED FROM PYTHON SOURCE LINES 169-286

.. code-block:: default



    data = []
    errors = []
    pbar = tqdm(list(product(types, engine, providers, dims)))
    for tt, engine, provider, dim in pbar:
        if max(dim) <= 200:
            repeat, number = 50, 25
        elif max(dim) <= 256:
            repeat, number = 25, 10
        else:
            repeat, number = 10, 4

        onx = create_model(tt)
        with open(f"plot_bench_gemm_{tt}.onnx", "wb") as f:
            f.write(onx.SerializeToString())
        k1 = (tt, dim[0], dim[2])
        k2 = (tt, dim[2], dim[1])
        if k1 not in matrices:
            errors.append(f"Key k1={k1!r} not in matrices.")
            continue
        if k2 not in matrices:
            errors.append(f"Key k2={k2!r} not in matrices.")
            continue

        if engine == CReferenceEvaluator:
            if tt == TensorProto.FLOAT16 and max(dim) > 50:
                repeat, number = 2, 2
            if provider != ["CPUExecutionProvider"]:
                continue
            if tt not in [TensorProto.FLOAT, TensorProto.FLOAT16]:
                continue

            pbar.set_description(
                f"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}"
            )

            feeds = {"A": matrices[k1].numpy(), "B": matrices[k2].numpy()}
            sess = engine(onx)
            sess.run(None, feeds)
            obs = measure_time(lambda: sess.run(None, feeds), repeat=repeat, number=number)

        elif engine == InferenceSession:
            if provider[0] not in get_available_providers():
                continue
            pbar.set_description(
                f"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}"
            )
            feeds = {"A": matrices[k1], "B": matrices[k2]}
            try:
                sess = engine(onx.SerializeToString(), providers=provider)
            except (NotImplemented, InvalidGraph) as e:
                # not implemented
                errors.append(e)
                continue

            if provider == ["CPUExecutionProvider"]:
                the_feeds = feeds
            else:
                # moving values to CUDA
                device = C_OrtDevice(C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0)
                try:
                    the_feeds = {
                        k: C_OrtValue.ortvalue_from_numpy(v.numpy(), device)
                        for k, v in feeds.items()
                    }
                except RuntimeError as e:
                    errors.append(f"issue with cuda and type {tt} - {e}")
                    continue

            sess._sess.run_with_ort_values(the_feeds, ["C"], None)[0]
            obs = measure_time(
                lambda: sess._sess.run_with_ort_values(the_feeds, ["C"], None)[0],
                repeat=repeat,
                number=number,
            )

        else:
            continue

        obs.update(
            dict(
                engine={"InferenceSession": "ort", "CReferenceEvaluator": "np"}[
                    engine.__name__
                ],
                type={
                    TensorProto.FLOAT: "f32",
                    TensorProto.FLOAT16: "f16",
                    TensorProto.INT8: "i8",
                    TensorProto.INT16: "i16",
                    TensorProto.INT32: "i32",
                    TensorProto.UINT32: "u32",
                }[tt],
                M=dim[0],
                N=dim[1],
                K=dim[2],
                cost=numpy.prod(dim) * 4,
                cost_s=f"{numpy.prod(dim) * 4}-{dim[0]}x{dim[1]}x{dim[2]}",
                repeat=repeat,
                number=number,
                provider={"CPUExecutionProvider": "cpu", "CUDAExecutionProvider": "cuda"}[
                    provider[0]
                ],
                platform=platform.processor(),
            )
        )
        data.append(obs)
        if unit_test_going() and len(data) >= 2:
            break


    df = DataFrame(data)
    df.to_excel("plot_bench_gemm.xlsx")
    df.to_csv("plot_bench_gemm.csv")
    df.drop(["min_exec", "max_exec"], axis=1).to_csv("plot_bench_gemm_.csv")
    df





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/216 [00:00<?, ?it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(10, 10, 10):   0%|          | 0/216 [00:00<?, ?it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(10, 10, 10):   0%|          | 1/216 [00:00<00:44,  4.82it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(61, 62, 63):   0%|          | 1/216 [00:00<00:44,  4.82it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(61, 62, 63):   1%|          | 2/216 [00:00<00:39,  5.37it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(64, 64, 64):   1%|          | 2/216 [00:00<00:39,  5.37it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(64, 64, 64):   1%|1         | 3/216 [00:00<00:37,  5.61it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(65, 66, 67):   1%|1         | 3/216 [00:00<00:37,  5.61it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(65, 66, 67):   2%|1         | 4/216 [00:02<02:45,  1.28it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(100, 100, 100):   2%|1         | 4/216 [00:02<02:45,  1.28it/s]    t=1 e=CReferenceEvaluator p=CPUE dim=(100, 100, 100):   2%|2         | 5/216 [00:03<03:48,  1.09s/it]    t=1 e=CReferenceEvaluator p=CPUE dim=(128, 128, 128):   2%|2         | 5/216 [00:03<03:48,  1.09s/it]    t=1 e=CReferenceEvaluator p=CPUE dim=(128, 128, 128):   3%|2         | 6/216 [00:05<03:54,  1.12s/it]    t=1 e=InferenceSession p=CPUE dim=(10, 10, 10):   3%|2         | 6/216 [00:05<03:54,  1.12s/it]          t=1 e=InferenceSession p=CPUE dim=(10, 10, 10):   6%|6         | 13/216 [00:05<00:54,  3.74it/s]    t=1 e=InferenceSession p=CPUE dim=(61, 62, 63):   6%|6         | 13/216 [00:05<00:54,  3.74it/s]    t=1 e=InferenceSession p=CPUE dim=(64, 64, 64):   6%|6         | 13/216 [00:05<00:54,  3.74it/s]    t=1 e=InferenceSession p=CPUE dim=(65, 66, 67):   6%|6         | 13/216 [00:05<00:54,  3.74it/s]    t=1 e=InferenceSession p=CPUE dim=(65, 66, 67):   7%|7         | 16/216 [00:05<00:42,  4.73it/s]    t=1 e=InferenceSession p=CPUE dim=(100, 100, 100):   7%|7         | 16/216 [00:05<00:42,  4.73it/s]    t=1 e=InferenceSession p=CPUE dim=(128, 128, 128):   7%|7         | 16/216 [00:05<00:42,  4.73it/s]    t=1 e=InferenceSession p=CPUE dim=(128, 128, 128):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]    t=12 e=InferenceSession p=CPUE dim=(10, 10, 10):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]      t=12 e=InferenceSession p=CPUE dim=(61, 62, 63):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]    t=12 e=InferenceSession p=CPUE dim=(64, 64, 64):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]    t=12 e=InferenceSession p=CPUE dim=(65, 66, 67):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]    t=12 e=InferenceSession p=CPUE dim=(100, 100, 100):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]    t=12 e=InferenceSession p=CPUE dim=(128, 128, 128):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]    t=6 e=InferenceSession p=CPUE dim=(10, 10, 10):   8%|8         | 18/216 [00:05<00:42,  4.67it/s]        t=6 e=InferenceSession p=CPUE dim=(10, 10, 10):  28%|##8       | 61/216 [00:06<00:04, 35.02it/s]    t=6 e=InferenceSession p=CPUE dim=(61, 62, 63):  28%|##8       | 61/216 [00:06<00:04, 35.02it/s]    t=6 e=InferenceSession p=CPUE dim=(64, 64, 64):  28%|##8       | 61/216 [00:06<00:04, 35.02it/s]    t=6 e=InferenceSession p=CPUE dim=(65, 66, 67):  28%|##8       | 61/216 [00:06<00:04, 35.02it/s]    t=6 e=InferenceSession p=CPUE dim=(100, 100, 100):  28%|##8       | 61/216 [00:07<00:04, 35.02it/s]    t=6 e=InferenceSession p=CPUE dim=(128, 128, 128):  28%|##8       | 61/216 [00:09<00:04, 35.02it/s]    t=6 e=InferenceSession p=CPUE dim=(128, 128, 128):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=5 e=InferenceSession p=CPUE dim=(10, 10, 10):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]       t=5 e=InferenceSession p=CPUE dim=(61, 62, 63):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=5 e=InferenceSession p=CPUE dim=(64, 64, 64):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=5 e=InferenceSession p=CPUE dim=(65, 66, 67):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=5 e=InferenceSession p=CPUE dim=(100, 100, 100):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=5 e=InferenceSession p=CPUE dim=(128, 128, 128):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=3 e=InferenceSession p=CPUE dim=(10, 10, 10):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]       t=3 e=InferenceSession p=CPUE dim=(61, 62, 63):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=3 e=InferenceSession p=CPUE dim=(64, 64, 64):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=3 e=InferenceSession p=CPUE dim=(65, 66, 67):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=3 e=InferenceSession p=CPUE dim=(100, 100, 100):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=3 e=InferenceSession p=CPUE dim=(128, 128, 128):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=(10, 10, 10):  34%|###4      | 74/216 [00:13<00:24,  5.87it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=(10, 10, 10):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=(61, 62, 63):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=(64, 64, 64):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=(65, 66, 67):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=(100, 100, 100):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]    t=10 e=CReferenceEvaluator p=CPUE dim=(128, 128, 128):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]    t=10 e=InferenceSession p=CPUE dim=(10, 10, 10):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]          t=10 e=InferenceSession p=CPUE dim=(61, 62, 63):  56%|#####6    | 121/216 [00:13<00:06, 13.65it/s]    t=10 e=InferenceSession p=CPUE dim=(61, 62, 63):  62%|######2   | 134/216 [00:14<00:05, 14.24it/s]    t=10 e=InferenceSession p=CPUE dim=(64, 64, 64):  62%|######2   | 134/216 [00:14<00:05, 14.24it/s]    t=10 e=InferenceSession p=CPUE dim=(65, 66, 67):  62%|######2   | 134/216 [00:14<00:05, 14.24it/s]    t=10 e=InferenceSession p=CPUE dim=(100, 100, 100):  62%|######2   | 134/216 [00:14<00:05, 14.24it/s]    t=10 e=InferenceSession p=CPUE dim=(128, 128, 128):  62%|######2   | 134/216 [00:14<00:05, 14.24it/s]    t=10 e=InferenceSession p=CPUE dim=(128, 128, 128):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=16 e=InferenceSession p=CPUE dim=(10, 10, 10):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]       t=16 e=InferenceSession p=CPUE dim=(61, 62, 63):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=16 e=InferenceSession p=CPUE dim=(64, 64, 64):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=16 e=InferenceSession p=CPUE dim=(65, 66, 67):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=16 e=InferenceSession p=CPUE dim=(100, 100, 100):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=16 e=InferenceSession p=CPUE dim=(128, 128, 128):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=17 e=InferenceSession p=CPUE dim=(10, 10, 10):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]       t=17 e=InferenceSession p=CPUE dim=(61, 62, 63):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=17 e=InferenceSession p=CPUE dim=(64, 64, 64):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=17 e=InferenceSession p=CPUE dim=(65, 66, 67):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=17 e=InferenceSession p=CPUE dim=(100, 100, 100):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=17 e=InferenceSession p=CPUE dim=(128, 128, 128):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=19 e=InferenceSession p=CPUE dim=(10, 10, 10):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]       t=19 e=InferenceSession p=CPUE dim=(61, 62, 63):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=19 e=InferenceSession p=CPUE dim=(64, 64, 64):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=19 e=InferenceSession p=CPUE dim=(65, 66, 67):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=19 e=InferenceSession p=CPUE dim=(100, 100, 100):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=19 e=InferenceSession p=CPUE dim=(128, 128, 128):  67%|######6   | 144/216 [00:15<00:05, 13.07it/s]    t=19 e=InferenceSession p=CPUE dim=(128, 128, 128): 100%|##########| 216/216 [00:15<00:00, 14.17it/s]


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>average</th>
          <th>deviation</th>
          <th>min_exec</th>
          <th>max_exec</th>
          <th>repeat</th>
          <th>number</th>
          <th>ttime</th>
          <th>context_size</th>
          <th>engine</th>
          <th>type</th>
          <th>M</th>
          <th>N</th>
          <th>K</th>
          <th>cost</th>
          <th>cost_s</th>
          <th>provider</th>
          <th>platform</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.000098</td>
          <td>0.000017</td>
          <td>0.000086</td>
          <td>0.000159</td>
          <td>50</td>
          <td>25</td>
          <td>0.004923</td>
          <td>64</td>
          <td>np</td>
          <td>f32</td>
          <td>10</td>
          <td>10</td>
          <td>10</td>
          <td>4000</td>
          <td>4000-10x10x10</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.000135</td>
          <td>0.000010</td>
          <td>0.000129</td>
          <td>0.000181</td>
          <td>50</td>
          <td>25</td>
          <td>0.006764</td>
          <td>64</td>
          <td>np</td>
          <td>f32</td>
          <td>61</td>
          <td>62</td>
          <td>63</td>
          <td>953064</td>
          <td>953064-61x62x63</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.000133</td>
          <td>0.000010</td>
          <td>0.000127</td>
          <td>0.000193</td>
          <td>50</td>
          <td>25</td>
          <td>0.006672</td>
          <td>64</td>
          <td>np</td>
          <td>f32</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.001358</td>
          <td>0.004381</td>
          <td>0.000415</td>
          <td>0.031639</td>
          <td>50</td>
          <td>25</td>
          <td>0.067895</td>
          <td>64</td>
          <td>np</td>
          <td>f32</td>
          <td>65</td>
          <td>66</td>
          <td>67</td>
          <td>1149720</td>
          <td>1149720-65x66x67</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.001290</td>
          <td>0.000601</td>
          <td>0.000726</td>
          <td>0.003400</td>
          <td>50</td>
          <td>25</td>
          <td>0.064503</td>
          <td>64</td>
          <td>np</td>
          <td>f32</td>
          <td>100</td>
          <td>100</td>
          <td>100</td>
          <td>4000000</td>
          <td>4000000-100x100x100</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>5</th>
          <td>0.000941</td>
          <td>0.000348</td>
          <td>0.000689</td>
          <td>0.002080</td>
          <td>50</td>
          <td>25</td>
          <td>0.047035</td>
          <td>64</td>
          <td>np</td>
          <td>f32</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>6</th>
          <td>0.000070</td>
          <td>0.000126</td>
          <td>0.000029</td>
          <td>0.000710</td>
          <td>50</td>
          <td>25</td>
          <td>0.003508</td>
          <td>64</td>
          <td>ort</td>
          <td>f32</td>
          <td>10</td>
          <td>10</td>
          <td>10</td>
          <td>4000</td>
          <td>4000-10x10x10</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>7</th>
          <td>0.000071</td>
          <td>0.000014</td>
          <td>0.000046</td>
          <td>0.000091</td>
          <td>50</td>
          <td>25</td>
          <td>0.003528</td>
          <td>64</td>
          <td>ort</td>
          <td>f32</td>
          <td>61</td>
          <td>62</td>
          <td>63</td>
          <td>953064</td>
          <td>953064-61x62x63</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>8</th>
          <td>0.000065</td>
          <td>0.000004</td>
          <td>0.000061</td>
          <td>0.000076</td>
          <td>50</td>
          <td>25</td>
          <td>0.003248</td>
          <td>64</td>
          <td>ort</td>
          <td>f32</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>9</th>
          <td>0.000070</td>
          <td>0.000018</td>
          <td>0.000050</td>
          <td>0.000109</td>
          <td>50</td>
          <td>25</td>
          <td>0.003511</td>
          <td>64</td>
          <td>ort</td>
          <td>f32</td>
          <td>65</td>
          <td>66</td>
          <td>67</td>
          <td>1149720</td>
          <td>1149720-65x66x67</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>10</th>
          <td>0.000117</td>
          <td>0.000014</td>
          <td>0.000093</td>
          <td>0.000169</td>
          <td>50</td>
          <td>25</td>
          <td>0.005870</td>
          <td>64</td>
          <td>ort</td>
          <td>f32</td>
          <td>100</td>
          <td>100</td>
          <td>100</td>
          <td>4000000</td>
          <td>4000000-100x100x100</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>11</th>
          <td>0.000225</td>
          <td>0.000025</td>
          <td>0.000181</td>
          <td>0.000309</td>
          <td>50</td>
          <td>25</td>
          <td>0.011246</td>
          <td>64</td>
          <td>ort</td>
          <td>f32</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>12</th>
          <td>0.000030</td>
          <td>0.000013</td>
          <td>0.000020</td>
          <td>0.000054</td>
          <td>50</td>
          <td>25</td>
          <td>0.001505</td>
          <td>64</td>
          <td>ort</td>
          <td>i32</td>
          <td>10</td>
          <td>10</td>
          <td>10</td>
          <td>4000</td>
          <td>4000-10x10x10</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>13</th>
          <td>0.000338</td>
          <td>0.000051</td>
          <td>0.000288</td>
          <td>0.000547</td>
          <td>50</td>
          <td>25</td>
          <td>0.016887</td>
          <td>64</td>
          <td>ort</td>
          <td>i32</td>
          <td>61</td>
          <td>62</td>
          <td>63</td>
          <td>953064</td>
          <td>953064-61x62x63</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>14</th>
          <td>0.000344</td>
          <td>0.000039</td>
          <td>0.000315</td>
          <td>0.000544</td>
          <td>50</td>
          <td>25</td>
          <td>0.017199</td>
          <td>64</td>
          <td>ort</td>
          <td>i32</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>15</th>
          <td>0.000388</td>
          <td>0.000046</td>
          <td>0.000347</td>
          <td>0.000573</td>
          <td>50</td>
          <td>25</td>
          <td>0.019423</td>
          <td>64</td>
          <td>ort</td>
          <td>i32</td>
          <td>65</td>
          <td>66</td>
          <td>67</td>
          <td>1149720</td>
          <td>1149720-65x66x67</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>16</th>
          <td>0.001401</td>
          <td>0.000324</td>
          <td>0.001140</td>
          <td>0.002769</td>
          <td>50</td>
          <td>25</td>
          <td>0.070059</td>
          <td>64</td>
          <td>ort</td>
          <td>i32</td>
          <td>100</td>
          <td>100</td>
          <td>100</td>
          <td>4000000</td>
          <td>4000000-100x100x100</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>17</th>
          <td>0.003157</td>
          <td>0.000791</td>
          <td>0.002371</td>
          <td>0.005002</td>
          <td>50</td>
          <td>25</td>
          <td>0.157834</td>
          <td>64</td>
          <td>ort</td>
          <td>i32</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>18</th>
          <td>0.000152</td>
          <td>0.000030</td>
          <td>0.000126</td>
          <td>0.000307</td>
          <td>50</td>
          <td>25</td>
          <td>0.007576</td>
          <td>64</td>
          <td>np</td>
          <td>f16</td>
          <td>10</td>
          <td>10</td>
          <td>10</td>
          <td>4000</td>
          <td>4000-10x10x10</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>19</th>
          <td>0.007516</td>
          <td>0.000252</td>
          <td>0.007263</td>
          <td>0.007768</td>
          <td>2</td>
          <td>2</td>
          <td>0.015031</td>
          <td>64</td>
          <td>np</td>
          <td>f16</td>
          <td>61</td>
          <td>62</td>
          <td>63</td>
          <td>953064</td>
          <td>953064-61x62x63</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>20</th>
          <td>0.008402</td>
          <td>0.000310</td>
          <td>0.008092</td>
          <td>0.008712</td>
          <td>2</td>
          <td>2</td>
          <td>0.016804</td>
          <td>64</td>
          <td>np</td>
          <td>f16</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>21</th>
          <td>0.008053</td>
          <td>0.000081</td>
          <td>0.007972</td>
          <td>0.008134</td>
          <td>2</td>
          <td>2</td>
          <td>0.016106</td>
          <td>64</td>
          <td>np</td>
          <td>f16</td>
          <td>65</td>
          <td>66</td>
          <td>67</td>
          <td>1149720</td>
          <td>1149720-65x66x67</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>22</th>
          <td>0.029565</td>
          <td>0.000179</td>
          <td>0.029386</td>
          <td>0.029744</td>
          <td>2</td>
          <td>2</td>
          <td>0.059130</td>
          <td>64</td>
          <td>np</td>
          <td>f16</td>
          <td>100</td>
          <td>100</td>
          <td>100</td>
          <td>4000000</td>
          <td>4000000-100x100x100</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>23</th>
          <td>0.058163</td>
          <td>0.000116</td>
          <td>0.058046</td>
          <td>0.058279</td>
          <td>2</td>
          <td>2</td>
          <td>0.116326</td>
          <td>64</td>
          <td>np</td>
          <td>f16</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>24</th>
          <td>0.000023</td>
          <td>0.000008</td>
          <td>0.000017</td>
          <td>0.000043</td>
          <td>50</td>
          <td>25</td>
          <td>0.001164</td>
          <td>64</td>
          <td>ort</td>
          <td>f16</td>
          <td>10</td>
          <td>10</td>
          <td>10</td>
          <td>4000</td>
          <td>4000-10x10x10</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>25</th>
          <td>0.000105</td>
          <td>0.000015</td>
          <td>0.000069</td>
          <td>0.000137</td>
          <td>50</td>
          <td>25</td>
          <td>0.005261</td>
          <td>64</td>
          <td>ort</td>
          <td>f16</td>
          <td>61</td>
          <td>62</td>
          <td>63</td>
          <td>953064</td>
          <td>953064-61x62x63</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>26</th>
          <td>0.000113</td>
          <td>0.000027</td>
          <td>0.000077</td>
          <td>0.000217</td>
          <td>50</td>
          <td>25</td>
          <td>0.005674</td>
          <td>64</td>
          <td>ort</td>
          <td>f16</td>
          <td>64</td>
          <td>64</td>
          <td>64</td>
          <td>1048576</td>
          <td>1048576-64x64x64</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>27</th>
          <td>0.000110</td>
          <td>0.000024</td>
          <td>0.000082</td>
          <td>0.000168</td>
          <td>50</td>
          <td>25</td>
          <td>0.005515</td>
          <td>64</td>
          <td>ort</td>
          <td>f16</td>
          <td>65</td>
          <td>66</td>
          <td>67</td>
          <td>1149720</td>
          <td>1149720-65x66x67</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>28</th>
          <td>0.000229</td>
          <td>0.000028</td>
          <td>0.000168</td>
          <td>0.000298</td>
          <td>50</td>
          <td>25</td>
          <td>0.011461</td>
          <td>64</td>
          <td>ort</td>
          <td>f16</td>
          <td>100</td>
          <td>100</td>
          <td>100</td>
          <td>4000000</td>
          <td>4000000-100x100x100</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
        <tr>
          <th>29</th>
          <td>0.000334</td>
          <td>0.000050</td>
          <td>0.000237</td>
          <td>0.000469</td>
          <td>50</td>
          <td>25</td>
          <td>0.016705</td>
          <td>64</td>
          <td>ort</td>
          <td>f16</td>
          <td>128</td>
          <td>128</td>
          <td>128</td>
          <td>8388608</td>
          <td>8388608-128x128x128</td>
          <td>cpu</td>
          <td>x86_64</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 287-288

The errors.

.. GENERATED FROM PYTHON SOURCE LINES 288-292

.. code-block:: default


    for e in list(sorted(set(map(str, errors)))):
        print(e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Type Error: Type 'tensor(float8e4m3fn)' of input parameter (A) of operator (MatMul) in node () is invalid.
    [ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Type Error: Type 'tensor(float8e5m2)' of input parameter (A) of operator (MatMul) in node () is invalid.
    [ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Type Error: Type 'tensor(int16)' of input parameter (A) of operator (MatMul) in node () is invalid.
    [ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Type Error: Type 'tensor(int8)' of input parameter (A) of operator (MatMul) in node () is invalid.
    [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(14) node with name ''




.. GENERATED FROM PYTHON SOURCE LINES 293-295

Plots
+++++

.. GENERATED FROM PYTHON SOURCE LINES 295-304

.. code-block:: default


    piv = pivot_table(
        df, index=["cost"], columns=["engine", "type", "provider"], values="average"
    )
    piv.reset_index(drop=False).to_excel("plot_bench_gemm_summary.xlsx")
    piv.reset_index(drop=False).to_csv("plot_bench_gemm_summary.csv")
    print(piv)
    piv





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    engine          np                 ort                    
    type           f16       f32       f16       f32       i32
    provider       cpu       cpu       cpu       cpu       cpu
    cost                                                      
    4000      0.000152  0.000098  0.000023  0.000070  0.000030
    953064    0.007516  0.000135  0.000105  0.000071  0.000338
    1048576   0.008402  0.000133  0.000113  0.000065  0.000344
    1149720   0.008053  0.001358  0.000110  0.000070  0.000388
    4000000   0.029565  0.001290  0.000229  0.000117  0.001401
    8388608   0.058163  0.000941  0.000334  0.000225  0.003157


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead tr th {
            text-align: left;
        }

        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th>engine</th>
          <th colspan="2" halign="left">np</th>
          <th colspan="3" halign="left">ort</th>
        </tr>
        <tr>
          <th>type</th>
          <th>f16</th>
          <th>f32</th>
          <th>f16</th>
          <th>f32</th>
          <th>i32</th>
        </tr>
        <tr>
          <th>provider</th>
          <th>cpu</th>
          <th>cpu</th>
          <th>cpu</th>
          <th>cpu</th>
          <th>cpu</th>
        </tr>
        <tr>
          <th>cost</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>4000</th>
          <td>0.000152</td>
          <td>0.000098</td>
          <td>0.000023</td>
          <td>0.000070</td>
          <td>0.000030</td>
        </tr>
        <tr>
          <th>953064</th>
          <td>0.007516</td>
          <td>0.000135</td>
          <td>0.000105</td>
          <td>0.000071</td>
          <td>0.000338</td>
        </tr>
        <tr>
          <th>1048576</th>
          <td>0.008402</td>
          <td>0.000133</td>
          <td>0.000113</td>
          <td>0.000065</td>
          <td>0.000344</td>
        </tr>
        <tr>
          <th>1149720</th>
          <td>0.008053</td>
          <td>0.001358</td>
          <td>0.000110</td>
          <td>0.000070</td>
          <td>0.000388</td>
        </tr>
        <tr>
          <th>4000000</th>
          <td>0.029565</td>
          <td>0.001290</td>
          <td>0.000229</td>
          <td>0.000117</td>
          <td>0.001401</td>
        </tr>
        <tr>
          <th>8388608</th>
          <td>0.058163</td>
          <td>0.000941</td>
          <td>0.000334</td>
          <td>0.000225</td>
          <td>0.003157</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 305-306

With the dimensions.

.. GENERATED FROM PYTHON SOURCE LINES 306-311

.. code-block:: default

    pivs = pivot_table(
        df, index=["cost_s"], columns=["engine", "type", "provider"], values="average"
    )
    print(pivs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    engine                     np                 ort                    
    type                      f16       f32       f16       f32       i32
    provider                  cpu       cpu       cpu       cpu       cpu
    cost_s                                                               
    1048576-64x64x64     0.008402  0.000133  0.000113  0.000065  0.000344
    1149720-65x66x67     0.008053  0.001358  0.000110  0.000070  0.000388
    4000-10x10x10        0.000152  0.000098  0.000023  0.000070  0.000030
    4000000-100x100x100  0.029565  0.001290  0.000229  0.000117  0.001401
    8388608-128x128x128  0.058163  0.000941  0.000334  0.000225  0.003157
    953064-61x62x63      0.007516  0.000135  0.000105  0.000071  0.000338




.. GENERATED FROM PYTHON SOURCE LINES 312-313

plot

.. GENERATED FROM PYTHON SOURCE LINES 313-332

.. code-block:: default


    dfi = df[
        df.type.isin({"f32", "f16", "bf16", "f8e4m3", "f8e5m2"}) & df.engine.isin({"ort"})
    ]
    pivi = pivot_table(
        dfi, index=["cost"], columns=["engine", "type", "provider"], values="average"
    )

    fig, ax = plt.subplots(1, 2, figsize=(12, 6))
    piv.plot(ax=ax[0], title="Gemm performance\nlower is better", logx=True, logy=True)
    if pivi.shape[0] > 0:
        pivi.plot(
            ax=ax[1],
            title=f"Gemm performance ORT\n{platform.processor()}",
            logx=True,
            logy=True,
        )
    fig.tight_layout()
    fig.savefig("plot_bench_gemm.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_bench_gemm_001.png
   :alt: Gemm performance lower is better, Gemm performance ORT x86_64
   :srcset: /auto_examples/images/sphx_glr_plot_bench_gemm_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  16.194 seconds)


.. _sphx_glr_download_auto_examples_plot_bench_gemm.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bench_gemm.py <plot_bench_gemm.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bench_gemm.ipynb <plot_bench_gemm.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
