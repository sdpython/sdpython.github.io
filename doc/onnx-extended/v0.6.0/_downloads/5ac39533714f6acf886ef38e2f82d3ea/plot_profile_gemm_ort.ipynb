{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Profiles a simple onnx graph including a singleGemm\n\nThe benchmark profiles the execution of Gemm for different\ntypes and configuration. That includes a custom operator\nonly available on CUDA calling function :epkg:`cublasLtMatmul`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pprint\nfrom itertools import product\nimport numpy\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom pandas import pivot_table, concat\nfrom onnx import TensorProto\nfrom onnx.helper import (\n    make_model,\n    make_node,\n    make_graph,\n    make_tensor_value_info,\n    make_opsetid,\n)\nfrom onnx.checker import check_model\nfrom onnx.numpy_helper import from_array\nfrom onnx.reference import ReferenceEvaluator\nfrom onnxruntime import InferenceSession, SessionOptions, get_available_providers\nfrom onnxruntime.capi._pybind_state import (\n    OrtValue as C_OrtValue,\n    OrtDevice as C_OrtDevice,\n)\nfrom onnxruntime.capi.onnxruntime_pybind11_state import (\n    NotImplemented,\n    InvalidGraph,\n    InvalidArgument,\n)\n\ntry:\n    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot\n    from onnx_array_api.ort.ort_profile import ort_profile\nexcept ImportError:\n    onnx_simple_text_plot = str\n    ort_profile = None\ntry:\n    from onnx_extended.reference import CReferenceEvaluator\nexcept ImportError:\n    CReferenceEvaluator = ReferenceEvaluator\nfrom onnx_extended.args import get_parsed_args\nfrom onnx_extended.ext_test_case import unit_test_going\n\ntry:\n    from onnx_extended.validation.cuda.cuda_example_py import get_device_prop\n    from onnx_extended.ortops.tutorial.cuda import get_ort_ext_libs\nexcept ImportError:\n\n    def get_device_prop():\n        return {\"name\": \"CPU\"}\n\n    def get_ort_ext_libs():\n        return None\n\n\nproperties = get_device_prop()\n\nif unit_test_going():\n    default_dims = \"32,32,32;64,64,64\"\nelif properties.get(\"major\", 0) < 7:\n    default_dims = \"256,256,256;512,512,512\"\nelse:\n    default_dims = \"2048,2048,2048;4096,4096,4096\"\n\nscript_args = get_parsed_args(\n    \"plot_profile_gemm_ort\",\n    description=__doc__,\n    dims=(default_dims, \"dimensions to try for dims\"),\n    repeat_profile=(17, \"number of time to call ORT for profiling\"),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Device properties\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint.pprint(properties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model to benchmark\n\nIt includes one Gemm. The operator changes.\nIt can the regular Gemm, a custom Gemm from domain `com.microsoft`\nor a custom implementation from domain\n`onnx_extended.ortops.tutorial.cuda`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_model(\n    mat_type=TensorProto.FLOAT, provider=\"CUDAExecutionProvider\", domain=\"com.microsoft\"\n):\n    A = make_tensor_value_info(\"A\", mat_type, [None, None])\n    B = make_tensor_value_info(\"B\", mat_type, [None, None])\n    outputs = [make_tensor_value_info(\"C\", mat_type, [None, None])]\n    inits = []\n    if domain != \"\":\n        if provider != \"CUDAExecutionProvider\":\n            return None\n        f8 = False\n        if domain == \"com.microsoft\":\n            op_name = \"GemmFloat8\"\n            computeType = \"CUBLAS_COMPUTE_32F\"\n            node_output = [\"C\"]\n        elif mat_type == TensorProto.FLOAT:\n            op_name = \"CustomGemmFloat\"\n            computeType = \"CUBLAS_COMPUTE_32F_FAST_TF32\"\n            node_output = [\"C\"]\n        elif mat_type == TensorProto.FLOAT16:\n            op_name = \"CustomGemmFloat16\"\n            computeType = \"CUBLAS_COMPUTE_32F\"\n            node_output = [\"C\"]\n        elif mat_type in (TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2):\n            f8 = True\n            op_name = \"CustomGemmFloat8E4M3FN\"\n            computeType = \"CUBLAS_COMPUTE_32F\"\n            node_output = [\"C\"]\n            outputs = [\n                make_tensor_value_info(\"C\", TensorProto.FLOAT16, [None, None]),\n            ]\n            inits.append(from_array(numpy.array([1], dtype=numpy.float32), name=\"I\"))\n        else:\n            return None\n        node_kw = dict(\n            alpha=1.0,\n            transA=1,\n            domain=domain,\n            computeType=computeType,\n            fastAccumulationMode=1,\n            rowMajor=0 if op_name == \"CustomGemmFloat8E4M3FN\" else 1,\n        )\n        node_kw[\"name\"] = (\n            f\"{mat_type}.{len(node_output)}.{len(outputs)}.\"\n            f\"{domain}..{node_kw['rowMajor']}..\"\n            f\"{node_kw['fastAccumulationMode']}..{node_kw['computeType']}..\"\n            f\"{f8}\"\n        )\n        node_inputs = [\"A\", \"B\"]\n        if f8:\n            node_inputs.append(\"\")\n            node_inputs.extend([\"I\"] * 3)\n        nodes = [make_node(op_name, node_inputs, node_output, **node_kw)]\n    else:\n        nodes = [\n            make_node(\"Gemm\", [\"A\", \"B\"], [\"C\"], transA=1, beta=0.0),\n        ]\n    graph = make_graph(nodes, \"a\", [A, B], outputs, inits)\n    if mat_type < 16:\n        # regular type\n        opset, ir = 18, 8\n    else:\n        opset, ir = 19, 9\n    onnx_model = make_model(\n        graph,\n        opset_imports=[\n            make_opsetid(\"\", opset),\n            make_opsetid(\"com.microsoft\", 1),\n            make_opsetid(\"onnx_extended.ortops.tutorial.cuda\", 1),\n        ],\n        ir_version=ir,\n    )\n    check_model(onnx_model)\n    return onnx_model\n\n\nprint(onnx_simple_text_plot(create_model()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model to cast into anytype.\nnumpy does not support float 8. onnxruntime is used\nto cast a float array into any type.\nIt must be called with tensor of type `OrtValue`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_cast(to, cuda=False):\n    A = make_tensor_value_info(\"A\", TensorProto.FLOAT, [None, None])\n    C = make_tensor_value_info(\"C\", to, [None, None])\n    if cuda:\n        nodes = [\n            make_node(\"Cast\", [\"A\"], [\"Cc\"], to=to),\n            make_node(\"MemcpyFromHost\", [\"Cc\"], [\"C\"]),\n        ]\n    else:\n        nodes = [make_node(\"Cast\", [\"A\"], [\"C\"], to=to)]\n    graph = make_graph(nodes, \"a\", [A], [C])\n    if to < 16:\n        # regular type\n        opset, ir = 18, 8\n    else:\n        opset, ir = 19, 9\n    onnx_model = make_model(\n        graph, opset_imports=[make_opsetid(\"\", opset)], ir_version=ir\n    )\n    if not cuda:\n        # OpType: MemcpyFromHost\n        check_model(onnx_model)\n    return onnx_model\n\n\nprint(onnx_simple_text_plot(create_cast(TensorProto.FLOAT16)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling\n\nThe benchmark will run the following configurations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "types = [\n    TensorProto.FLOAT8E4M3FN,\n    TensorProto.FLOAT,\n    TensorProto.FLOAT16,\n    TensorProto.BFLOAT16,\n    # TensorProto.UINT32,\n    # TensorProto.INT32,\n    # TensorProto.INT16,\n    # TensorProto.INT8,\n]\nengine = [InferenceSession]\nproviders = [\n    [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n]\n# M, N, K\n# we use multiple of 8, otherwise, float8 does not work.\ndims = [tuple(int(i) for i in line.split(\",\")) for line in script_args.dims.split(\";\")]\ndomains = [\"onnx_extended.ortops.tutorial.cuda\", \"\", \"com.microsoft\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's cache the matrices involved.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def to_ort_value(m):\n    device = C_OrtDevice(C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)\n    ort_value = C_OrtValue.ortvalue_from_numpy(m, device)\n    return ort_value\n\n\ndef cached_inputs(dims, types):\n    matrices = {}\n    matrices_cuda = {}\n    for m, n, k in dims:\n        for tt in types:\n            for i, j in [(m, k), (k, n), (k, m)]:\n                if (tt, i, j) in matrices:\n                    continue\n                # CPU\n                try:\n                    sess = InferenceSession(\n                        create_cast(tt).SerializeToString(),\n                        providers=[\"CPUExecutionProvider\"],\n                    )\n                    cpu = True\n                except (InvalidGraph, InvalidArgument, NotImplemented):\n                    # not support by this version of onnxruntime\n                    cpu = False\n\n                if cpu:\n                    vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)\n                    ov = to_ort_value(vect)\n                    ovtt = sess._sess.run_with_ort_values({\"A\": ov}, [\"C\"], None)[0]\n                    matrices[tt, i, j] = ovtt\n                else:\n                    continue\n\n                # CUDA\n                if \"CUDAExecutionProvider\" not in get_available_providers():\n                    # No CUDA\n                    continue\n                sess = InferenceSession(\n                    create_cast(tt, cuda=True).SerializeToString(),\n                    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n                )\n                vect = (numpy.random.randn(i, j) * 10).astype(numpy.float32)\n                ov = to_ort_value(vect)\n                ovtt = sess._sess.run_with_ort_values({\"A\": ov}, [\"C\"], None)[0]\n                matrices_cuda[tt, i, j] = ovtt\n    return matrices, matrices_cuda\n\n\nmatrices, matrices_cuda = cached_inputs(dims, types)\nprint(f\"{len(matrices)} matrices were created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the profiles\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opts = SessionOptions()\nr = get_ort_ext_libs()\nif r is not None:\n    opts.register_custom_ops_library(r[0])\n\n\ndata = []\npbar = tqdm(list(product(types, engine, providers, dims, domains)))\nfor tt, engine, provider, dim, domain in pbar:\n    if \"CUDAExecutionProvider\" not in get_available_providers():\n        # No CUDA.\n        continue\n    if (\n        tt in {TensorProto.FLOAT8E4M3FN, TensorProto.FLOAT8E5M2}\n        and properties.get(\"major\", 0) < 9\n    ):\n        # f8 not available\n        continue\n\n    onx = create_model(tt, provider=provider[0], domain=domain)\n    if onx is None:\n        # Not available on this machine\n        continue\n    with open(f\"plot_bench_gemm_profile_{tt}_{domain}.onnx\", \"wb\") as f:\n        f.write(onx.SerializeToString())\n    k1 = (tt, dim[2], dim[0])\n    k2 = (tt, dim[2], dim[1])\n\n    pbar.set_description(f\"t={tt} e={engine.__name__} p={provider[0][:4]} dim={dim}\")\n\n    try:\n        sess = engine(onx.SerializeToString(), opts, providers=provider)\n    except Exception:\n        # Seomthing went wrong.\n        continue\n\n    the_feeds = {\"A\": matrices_cuda[k1], \"B\": matrices_cuda[k2]}\n    out_names = [\"C\"]\n\n    if ort_profile is None:\n        raise ImportError(\"Could not import ort_profile from onnx-array-api.\")\n    df = ort_profile(\n        onx,\n        the_feeds,\n        sess_options=opts,\n        repeat=script_args.repeat_profile,\n        as_df=True,\n        providers=provider,\n        first_it_out=True,\n        agg=True,\n    ).reset_index(drop=False)\n    columns = [\"xdim\", \"xdomain\", \"xdtype\", *df.columns]\n    df[\"xdim\"] = \"x\".join(map(str, dim))\n    df[\"xdomain\"] = {\n        \"onnx_extended.ortops.tutorial.cuda\": \"EXT\",\n        \"\": \"ORT\",\n        \"com.microsoft\": \"COM\",\n    }[domain]\n    df[\"args_op_name\"] = {\n        \"onnx_extended.ortops.tutorial.cuda\": \"CG\",\n        \"\": \"Gemm\",\n        \"com.microsoft\": \"G8\",\n    }[domain]\n    df[\"xdtype\"] = {1: \"f32\", 10: \"f16\", 16: \"bf16\", 17: \"e4m3fn\", 18: \"e5m2\"}[tt]\n    df = df[columns]\n    data.append(df)\n\n    if unit_test_going() and len(data) >= 2:\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data:\n    df = concat(data, axis=0)\n    df.to_excel(\"plot_profile_gemm_ort.xlsx\")\n    df.to_csv(\"plot_profile_gemm_ort.csv\")\n    print(df.head().T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data:\n    piv = pivot_table(\n        df[df[\"it==0\"] == 0],\n        index=[\"xdim\", \"cat\", \"event_name\"],\n        columns=[\"xdtype\", \"xdomain\", \"args_op_name\"],\n        values=[\"dur\"],\n    )\n    piv.reset_index(drop=False).to_excel(\"plot_profile_gemm_ort_summary.xlsx\")\n    piv.reset_index(drop=False).to_csv(\"plot_profile_gemm_ort_summary.csv\")\n\n    print()\n    print(\"summary\")\n    print(piv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data:\n    print()\n    print(\"compact\")\n\n    pivi = pivot_table(\n        df[(df[\"it==0\"] == 0) & (df[\"event_name\"] == \"kernel_time\")],\n        index=[\"xdim\"],\n        columns=[\"xdtype\", \"xdomain\", \"args_op_name\"],\n        values=\"dur\",\n    )\n    print(pivi)\n\n    print()\n    print(\"not operator\")\n\n    pivinot = pivot_table(\n        df[df[\"cat\"] != \"Node\"],\n        index=[\"xdim\", \"event_name\"],\n        columns=[\"xdtype\", \"xdomain\"],\n        values=\"dur\",\n    )\n    print(pivinot)\n\n\nif data:\n    fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n    pivi.T.plot(\n        ax=ax[0, 0],\n        title=\"kernel time\",\n        kind=\"barh\",\n        logx=True,\n    )\n    pivinot.T.plot(\n        ax=ax[1, 0],\n        title=\"Global times\",\n        kind=\"barh\",\n        logx=True,\n    )\n\n    for i, name in enumerate([\"kernel_time\"]):\n        pivi = pivot_table(\n            df[(df[\"it==0\"] == 0) & (df[\"event_name\"] == name)],\n            index=[\"xdim\"],\n            columns=[\"xdtype\", \"xdomain\", \"args_op_name\"],\n            values=\"dur\",\n        )\n        if pivi.shape[0]:\n            pivi.T.plot(\n                ax=ax[i, 1],\n                title=f\"{name}\",\n                kind=\"barh\",\n                logx=True,\n            )\n\n    fig.tight_layout()\n    fig.savefig(\"plot_bench_gemm_ort.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}