{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Optimizing ScatterND operator on CUDA\n\nHow to parallelize something like the following?\n\n## ScatterND\n\nThis configuration happens in a :epkg:`Llama` model.\n\n::\n\n    gradient = ScatterND(zeros, indices, updates)\n\nWhere the shapes are:\n\n* zeros: 32000x4096\n* indices: 2x1024x1\n* updates: 2x1024x4096\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnx_extended.args import get_parsed_args\n\nscript_args = get_parsed_args(\n    \"plot_op_scatternd_cuda\",\n    description=__doc__,\n    config=(\n        \"small\",\n        \"small, short optimization (default), \"\n        \"medium for medium sizes, \"\n        \"large for big sizes\",\n        \"llama for a specific case on llama\",\n    ),\n    warmup=3,\n    repeat=5,\n    itype=(1, \"1 or 10 for float or float16\"),\n    expose=\"config,itype,warmup,repeat\",\n)\n\nimport time\nimport numpy as np\nfrom numpy.testing import assert_almost_equal\nfrom pandas import DataFrame\nfrom tqdm import tqdm\nimport onnx.helper as oh\nfrom onnx import TensorProto\nfrom onnx.reference import ReferenceEvaluator\nfrom onnx.reference.op_run import OpRun\nfrom onnx_array_api.plotting.text_plot import onnx_simple_text_plot\n\nitype = script_args.itype\ndtype = np.float32 if itype == TensorProto.FLOAT else np.float16\nconfig = script_args.config\nprint(f\"config={config}\")\nprint(f\"itype={itype}, dtype={dtype}\")\n\nif config == \"small\":\n    sizes = (256, 512, 1024)\nelif config == \"medium\":\n    sizes = (512, 1024, 2048)\nelif config == \"large\":\n    sizes = (1024, 2048, 4096, 8192)\nelif config == \"llama\":\n    sizes = (16000, 32000)\nelse:\n    try:\n        sizes = list(map(int, config.split(\",\")))\n    except (ValueError, TypeError) as e:\n        raise AssertionError(f\"Unexpected config value {config!r}.\") from e\n\n\ndef get_model(d3=True, optimize=False, shape_input=False, itype=TensorProto.FLOAT):\n    indices_shape = [\"i\", \"j\", 1] if d3 else [\"m\", 1]\n    updates_shape = [\"i\", \"j\", \"b\"] if d3 else [\"m\", \"b\"]\n    kwargs = dict(reduction=\"add\")\n    if shape_input:\n        kwargs[\"domain\"] = \"onnx_extended.ortops.optim.cuda\"\n    if optimize:\n        kwargs[\"strategy\"] = \"optimize\"\n\n    model = oh.make_model(\n        oh.make_graph(\n            [\n                oh.make_node(\n                    \"ScatterNDOfShape\" if shape_input else \"ScatterND\",\n                    [\"shape\" if shape_input else \"X\", \"indices\", \"updates\"],\n                    [\"Y\"],\n                    **kwargs,\n                )\n            ],\n            \"g\",\n            [\n                (\n                    oh.make_tensor_value_info(\"shape\", TensorProto.INT64, [\"s\"])\n                    if shape_input\n                    else oh.make_tensor_value_info(\"X\", itype, [\"a\", \"b\"])\n                ),\n                oh.make_tensor_value_info(\"indices\", TensorProto.INT64, indices_shape),\n                oh.make_tensor_value_info(\"updates\", itype, updates_shape),\n            ],\n            [oh.make_tensor_value_info(\"Y\", itype, [\"a\", \"b\"])],\n        ),\n        opset_imports=[\n            oh.make_opsetid(\"\", 18),\n            oh.make_opsetid(\"onnx_extended.ortops.optim.cuda\", 1),\n        ],\n        ir_version=9,\n    )\n    return model\n\n\nmodel = get_model()\nprint(onnx_simple_text_plot(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see the evaluation by the ReferenceEvaluator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _scatter_nd_impl(data, indices, updates, reduction=None, verbose=False):  # type: ignore\n    output = np.copy(data)\n    for i in np.ndindex(indices.shape[:-1]):\n        if verbose:\n            print(f\"updates for i={i}, indices={indices[i]}, updates={updates[i]}\")\n        if reduction == \"add\":\n            output[tuple(indices[i])] += updates[i]\n        elif reduction == \"mul\":\n            output[tuple(indices[i])] *= updates[i]\n        elif reduction == \"max\":\n            output[tuple(indices[i])] = np.maximum(output[indices[i]], updates[i])\n        elif reduction == \"min\":\n            output[tuple(indices[i])] = np.minimum(output[indices[i]], updates[i])\n        else:\n            output[tuple(indices[i])] = updates[i]\n    return output\n\n\nclass ScatterND(OpRun):\n    def _run(self, data, indices, updates, reduction=None, optimize=None):  # type: ignore\n        y = _scatter_nd_impl(data, indices, updates, reduction=reduction, verbose=True)\n        return (y,)\n\n\nclass ScatterNDOfShape(OpRun):\n    op_domain = \"onnx_extended.ortops.optim.cuda\"\n\n    def _run(self, shape, indices, updates, reduction=None, optimize=None):  # type: ignore\n        data = np.zeros(tuple(shape.tolist()), dtype=updates.dtype)\n        y = _scatter_nd_impl(data, indices, updates, reduction=reduction)\n        return (y,)\n\n\nshape = (5, 7)\nX = np.zeros(shape, dtype=dtype)\nindices = np.zeros((2, 10, 1)).astype(np.int64)\nindices[:, ::2, 0] = 3\nupdates = np.ones((2, 10, 7)).astype(dtype)\nfeeds = {\"X\": X, \"indices\": indices, \"updates\": updates}\n\n\nref = ReferenceEvaluator(model, new_ops=[ScatterND])\ngot = ref.run(None, feeds)[0]\nprint(got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generalize, let's change the shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = get_model(d3=False, itype=itype)\nprint(onnx_simple_text_plot(model))\n\n\nnew_indices = indices.reshape((-1, 1))\nnew_updates = updates.reshape((-1, updates.shape[-1]))\nfeeds = {\"X\": X, \"indices\": indices, \"updates\": updates}\n\nref = ReferenceEvaluator(model, new_ops=[ScatterND])\ngot = ref.run(None, feeds)[0]\nprint(got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First scenario\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = get_model(d3=False, shape_input=True, itype=itype)\nprint(onnx_simple_text_plot(model))\n\n\nfeeds = {\n    \"shape\": np.array(X.shape, dtype=np.int64),\n    \"indices\": indices.reshape((-1, 1)),\n    \"updates\": updates.reshape((-1, updates.shape[-1])),\n}\n\nref = ReferenceEvaluator(model, new_ops=[ScatterNDOfShape])\nexpected = ref.run(None, feeds)[0]\nprint(expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With onnxruntime\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_session(model):\n    import onnxruntime\n    from onnx_extended.ortops.optim.cuda import get_ort_ext_libs\n\n    if \"CUDAExecutionProvider\" not in onnxruntime.get_available_providers():\n        return None\n\n    opts = onnxruntime.SessionOptions()\n    opts.register_custom_ops_library(get_ort_ext_libs()[0])\n    sess = onnxruntime.InferenceSession(\n        model.SerializeToString(),\n        opts,\n        providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n    )\n    return sess\n\n\nsess1 = get_session(model)\nif sess1 is not None:\n    for k, v in feeds.items():\n        print(k, v.dtype, v.shape)\n    got = sess1.run(None, feeds)[0]\n    print(got)\n    assert_almost_equal(expected, got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same model but using an optimization to compute it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = get_model(d3=False, shape_input=True, optimize=True, itype=itype)\nprint(onnx_simple_text_plot(model))\n\nsess2 = get_session(model)\nif sess2 is not None:\n    got = sess2.run(None, feeds)[0]\n    print(got)\n    assert_almost_equal(expected, got)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def move_inputs(sess, feeds):\n    from onnxruntime.capi._pybind_state import (\n        SessionIOBinding,\n        OrtDevice as C_OrtDevice,\n        OrtValue as C_OrtValue,\n    )\n\n    input_names = [i.name for i in sess.get_inputs()]\n\n    ort_device = C_OrtDevice(C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0)\n\n    feed_ort_value = [\n        (name, C_OrtValue.ortvalue_from_numpy(feeds[name], ort_device))\n        for name in input_names\n    ]\n\n    bind = SessionIOBinding(sess._sess)\n    for name, value in feed_ort_value:\n        bind.bind_input(\n            name, ort_device, feeds[name].dtype, value.shape(), value.data_ptr()\n        )\n    for o in sess.get_outputs():\n        bind.bind_output(o.name, ort_device)\n    return bind, feed_ort_value\n\n\ndef benchmark(\n    sess, sizes, config, label, itype, times_col: int = 1, times_indices: int = 1\n):\n\n    data = []\n    for size in tqdm(sizes):\n\n        if config == \"llama\":\n            # zeros: 32000x4096\n            # indices: 2x1024x1\n            # updates: 2x1024x4096\n            nrow, ncol = size, 4096\n            nind = 1024\n        else:\n            nrow, ncol = size, int(size * times_col)\n            nind = int(size * times_indices)\n\n        shape = np.array([nrow, ncol], dtype=np.int64)\n        indices = np.array(\n            [np.random.randint(0, nrow - 1) for _ in range(nind)], dtype=np.int64\n        ).reshape((-1, 1))\n        updates = np.random.randn(nind, ncol).astype(\n            np.float32 if itype == TensorProto.FLOAT else np.float16\n        )\n        feeds = dict(shape=shape, indices=indices, updates=updates)\n        bind, cuda_feeds = move_inputs(sess, feeds)\n\n        begin = time.perf_counter()\n        for _i in range(script_args.warmup):\n            # sess.run(None, feeds)\n            sess._sess.run_with_iobinding(bind, None)\n        warmup = time.perf_counter() - begin\n\n        times = []\n        for _i in range(script_args.repeat):\n            begin = time.perf_counter()\n            # sess.run(None, feeds)\n            sess._sess.run_with_iobinding(bind, None)\n            times.append(time.perf_counter() - begin)\n\n        npt = np.array(times)\n        obs = dict(\n            warmup=warmup,\n            time=npt.mean(),\n            std=npt.std(),\n            min=npt.min(),\n            max=npt.max(),\n            repeat=script_args.repeat,\n            size=size,\n            label=label,\n        )\n        data.append(obs)\n    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not Fused.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if sess1 is not None:\n\n    print(f\"sizes={sizes}\")\n\n    data_nd1 = benchmark(\n        sess1, sizes, script_args.config, \"Atomic/Not Fused\", itype=itype\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fused.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if sess2 is not None:\n\n    data_nd2 = benchmark(\n        sess2, sizes, script_args.config, \"No Atomic/Fused\", itype=itype\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if sess2 is not None:\n\n    df = DataFrame(data_nd1 + data_nd2)\n    df.to_csv(\"plot_op_scatternd_cuda.csv\", index=False)\n    df.to_csv(\"plot_op_scatternd_cuda.xlsx\", index=False)\n    print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pivot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if sess2 is not None:\n\n    pivot = df.pivot(index=\"size\", columns=\"label\", values=\"time\")\n    pivot[\"ratio\"] = pivot[\"Atomic/Not Fused\"] / pivot[\"No Atomic/Fused\"]\n    print(\"Speed up compare to the onnx standaed.\")\n    print(pivot)\n\n    ax = pivot[[\"Atomic/Not Fused\", \"No Atomic/Fused\"]].plot(\n        logx=True,\n        logy=True,\n        title=f\"Atomic/No-Atomic implementation for ScatterND on CUDA\\nitype={itype}\",\n    )\n    ax.get_figure().savefig(\"plot_op_scatternd_cuda.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best choice depends on the input sizes,\nFor big matrices, the use of atomic is slowing down\nthe computation.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}