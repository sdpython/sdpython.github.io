
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_op_conv_denorm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_op_conv_denorm.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_op_conv_denorm.py:


How float format has an impact on speed computation
===================================================

An example with Conv. The floats followed the IEEE standard
`Single-precision floating-point format
<https://en.wikipedia.org/wiki/Single-precision_floating-point_format>`_.
The number is interprated in a different whether the exponent is null
or not. When it is null, it is called a denormalized number
or `subnormal number <https://en.wikipedia.org/wiki/Subnormal_number>`_.
Let's see their impact on the computation time through the operator Conv.

Create one model
++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 16-88

.. code-block:: Python


    import struct
    import matplotlib.pyplot as plt
    from pandas import DataFrame
    from tqdm import tqdm
    import numpy
    from onnx import TensorProto
    from onnx.helper import (
        make_model,
        make_node,
        make_graph,
        make_tensor_value_info,
        make_opsetid,
    )
    from onnx.checker import check_model
    from onnx.numpy_helper import to_array, from_array
    from onnxruntime import (
        InferenceSession,
        get_available_providers,
        OrtValue,
        SessionOptions,
        GraphOptimizationLevel,
    )
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    from onnx_extended.ext_test_case import measure_time, unit_test_going
    from onnx_extended.reference import CReferenceEvaluator

    try:
        import torch
    except ImportError:
        # no torch is available
        print("torch is not available")
        torch = None

    DIM = 64 if unit_test_going() else 256


    def _denorm(x):
        i = int.from_bytes(struct.pack("<f", numpy.float32(x)), "little")
        i &= 0x807FFFFF
        return numpy.uint32(i).view(numpy.float32)


    denorm = numpy.vectorize(_denorm)


    def create_model():
        X = make_tensor_value_info("X", TensorProto.FLOAT, [1, DIM, 14, 14])
        Y = make_tensor_value_info("Y", TensorProto.FLOAT, [None, None, None, None])
        B = from_array(numpy.zeros([DIM], dtype=numpy.float32), name="B")
        w = numpy.random.randn(DIM, DIM, 3, 3).astype(numpy.float32)

        # let's randomly denormalize some number
        mask = (numpy.random.randint(2, size=w.shape) % 2).astype(numpy.float32)
        d = denorm(w)
        w = w * mask - (mask - 1) * d
        W = from_array(w, name="W")

        node1 = make_node(
            "Conv", ["X", "W", "B"], ["Y"], kernel_shape=[3, 3], pads=[1, 1, 1, 1]
        )
        graph = make_graph([node1], "lr", [X], [Y], [W, B])
        onnx_model = make_model(graph, opset_imports=[make_opsetid("", 18)], ir_version=8)
        check_model(onnx_model)
        return onnx_model


    onx = create_model()
    onnx_file = "plot_op_conv_denorm.onnx"
    with open(onnx_file, "wb") as f:
        f.write(onx.SerializeToString())








.. GENERATED FROM PYTHON SOURCE LINES 89-90

The model looks like:

.. GENERATED FROM PYTHON SOURCE LINES 90-96

.. code-block:: Python


    print(onnx_simple_text_plot(onx))

    onnx_model = onnx_file
    input_shape = (1, DIM, 14, 14)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    input: name='X' type=dtype('float32') shape=[1, 256, 14, 14]
    init: name='W' type=float32 shape=(256, 256, 3, 3)
    init: name='B' type=float32 shape=(256,)
    Conv(X, W, B, kernel_shape=[3,3], pads=[1,1,1,1]) -> Y
    output: name='Y' type=dtype('float32') shape=['', '', '', '']




.. GENERATED FROM PYTHON SOURCE LINES 97-100

CReferenceEvaluator and InferenceSession
++++++++++++++++++++++++++++++++++++++++
Let's first compare the outputs are the same.

.. GENERATED FROM PYTHON SOURCE LINES 100-115

.. code-block:: Python


    sess_options = SessionOptions()
    sess_options.graph_optimization_level = GraphOptimizationLevel.ORT_DISABLE_ALL


    sess1 = CReferenceEvaluator(onnx_model)
    sess2 = InferenceSession(onnx_model, sess_options, providers=["CPUExecutionProvider"])

    X = numpy.ones(input_shape, dtype=numpy.float32)

    expected = sess1.run(None, {"X": X})[0]
    got = sess2.run(None, {"X": X})[0]
    diff = numpy.abs(expected - got).max()
    print(f"difference: {diff}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    difference: 3.814697265625e-05




.. GENERATED FROM PYTHON SOURCE LINES 116-120

Everything works fine.

Time measurement
++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 120-129

.. code-block:: Python


    feeds = {"X": X}

    t1 = measure_time(lambda: sess1.run(None, feeds), repeat=2, number=5)
    print(f"CReferenceEvaluator: {t1['average']}s")

    t2 = measure_time(lambda: sess2.run(None, feeds), repeat=2, number=5)
    print(f"InferenceSession: {t2['average']}s")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    CReferenceEvaluator: 0.0729901491999044s
    InferenceSession: 0.04597134340001503s




.. GENERATED FROM PYTHON SOURCE LINES 130-135

Plotting
++++++++

Let's modify the the weight of the model and multiply everything by a scalar.
Let's choose an random input.

.. GENERATED FROM PYTHON SOURCE LINES 135-248

.. code-block:: Python

    has_cuda = "CUDAExecutionProvider" in get_available_providers()
    X = numpy.random.random(X.shape).astype(X.dtype)


    def modify(onx, scale):
        t = to_array(onx.graph.initializer[0])
        b = to_array(onx.graph.initializer[1]).copy()
        t = (t * scale).astype(numpy.float32)
        graph = make_graph(
            onx.graph.node,
            onx.graph.name,
            onx.graph.input,
            onx.graph.output,
            [from_array(t, name=onx.graph.initializer[0].name), onx.graph.initializer[1]],
        )
        model = make_model(graph, opset_imports=onx.opset_import, ir_version=onx.ir_version)
        return t, b, model


    scales = [2**p for p in range(0, 31, 2)]
    data = []
    feeds = {"X": X}
    expected = sess2.run(None, feeds)[0]
    if torch is not None:
        tx = torch.from_numpy(X)

    sess_options0 = SessionOptions()
    sess_options0.graph_optimization_level = GraphOptimizationLevel.ORT_DISABLE_ALL
    sess_options0.add_session_config_entry("session.set_denormal_as_zero", "1")

    for scale in tqdm(scales):
        w, b, new_onx = modify(onx, scale)
        n_denorm = (w == denorm(w)).astype(numpy.int32).sum() / w.size

        # sess1 = CReferenceEvaluator(new_onx)
        sess2 = InferenceSession(
            new_onx.SerializeToString(), sess_options, providers=["CPUExecutionProvider"]
        )
        sess3 = InferenceSession(
            new_onx.SerializeToString(), providers=["CPUExecutionProvider"]
        )
        sess4 = InferenceSession(
            new_onx.SerializeToString(), sess_options0, providers=["CPUExecutionProvider"]
        )

        # sess1.run(None, feeds)
        got = sess2.run(None, feeds)[0]
        diff = numpy.abs(got / scale - expected).max()
        sess3.run(None, feeds)
        got0 = sess4.run(None, feeds)[0]
        diff0 = numpy.abs(got0 / scale - expected).max()

        # t1 = measure_time(lambda: sess1.run(None, feeds), repeat=2, number=5)
        t2 = measure_time(lambda sess2=sess3: sess2.run(None, feeds), repeat=2, number=5)
        t3 = measure_time(lambda sess3=sess3: sess3.run(None, feeds), repeat=2, number=5)
        t4 = measure_time(lambda sess4=sess4: sess4.run(None, feeds), repeat=2, number=5)
        obs = dict(
            scale=scale,
            ort=t2["average"],
            diff=diff,
            diff0=diff0,
            ort0=t4["average"],
            n_denorm=n_denorm,
        )
        # obs["ref"]=t1["average"]
        obs["ort-opt"] = t3["average"]

        if torch is not None:
            tw = torch.from_numpy(w)
            tb = torch.from_numpy(b)
            torch.nn.functional.conv2d(tx, tw, tb, padding=1)
            t3 = measure_time(
                lambda tx=tx, tw=tw, tb=tb: torch.nn.functional.conv2d(
                    tx, tw, tb, padding=1
                ),
                repeat=2,
                number=5,
            )
            obs["torch"] = t3["average"]

        if has_cuda:
            sess2 = InferenceSession(
                new_onx.SerializeToString(),
                sess_options,
                providers=["CUDAExecutionProvider"],
            )
            sess3 = InferenceSession(
                new_onx.SerializeToString(), providers=["CUDAExecutionProvider"]
            )
            x_ortvalue = OrtValue.ortvalue_from_numpy(X, "cuda", 0)
            cuda_feeds = {"X": x_ortvalue}
            sess2.run_with_ort_values(None, cuda_feeds)
            sess3.run_with_ort_values(None, cuda_feeds)
            t2 = measure_time(
                lambda sess2=sess2, cuda_feeds=cuda_feeds: sess2.run(None, cuda_feeds),
                repeat=2,
                number=5,
            )
            t3 = measure_time(
                lambda sess3=sess3, cuda_feeds=cuda_feeds: sess3.run(None, cuda_feeds),
                repeat=2,
                number=5,
            )
            obs["ort-cuda"] = t2["average"]
            obs["ort-cuda-opt"] = t2["average"]

        data.append(obs)
        if unit_test_going() and len(data) >= 2:
            break

    df = DataFrame(data)
    df





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/16 [00:00<?, ?it/s]      6%|▋         | 1/16 [00:08<02:12,  8.83s/it]     12%|█▎        | 2/16 [00:13<01:27,  6.23s/it]     19%|█▉        | 3/16 [00:16<01:02,  4.79s/it]     25%|██▌       | 4/16 [00:18<00:45,  3.80s/it]     31%|███▏      | 5/16 [00:20<00:35,  3.20s/it]     38%|███▊      | 6/16 [00:22<00:27,  2.73s/it]     44%|████▍     | 7/16 [00:24<00:21,  2.41s/it]     50%|█████     | 8/16 [00:26<00:17,  2.20s/it]     56%|█████▋    | 9/16 [00:27<00:14,  2.09s/it]     62%|██████▎   | 10/16 [00:29<00:12,  2.01s/it]     69%|██████▉   | 11/16 [00:31<00:09,  1.91s/it]     75%|███████▌  | 12/16 [00:33<00:07,  1.85s/it]     81%|████████▏ | 13/16 [00:34<00:05,  1.81s/it]     88%|████████▊ | 14/16 [00:36<00:03,  1.78s/it]     94%|█████████▍| 15/16 [00:38<00:01,  1.77s/it]    100%|██████████| 16/16 [00:40<00:00,  1.77s/it]    100%|██████████| 16/16 [00:40<00:00,  2.50s/it]


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>scale</th>
          <th>ort</th>
          <th>diff</th>
          <th>diff0</th>
          <th>ort0</th>
          <th>n_denorm</th>
          <th>ort-opt</th>
          <th>torch</th>
          <th>ort-cuda</th>
          <th>ort-cuda-opt</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>1</td>
          <td>0.086995</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.041269</td>
          <td>0.500019</td>
          <td>0.085703</td>
          <td>0.084737</td>
          <td>0.001447</td>
          <td>0.001447</td>
        </tr>
        <tr>
          <th>1</th>
          <td>4</td>
          <td>0.065801</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.016703</td>
          <td>0.160639</td>
          <td>0.065438</td>
          <td>0.064813</td>
          <td>0.001808</td>
          <td>0.001808</td>
        </tr>
        <tr>
          <th>2</th>
          <td>16</td>
          <td>0.030800</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.009150</td>
          <td>0.043499</td>
          <td>0.029419</td>
          <td>0.026619</td>
          <td>0.001755</td>
          <td>0.001755</td>
        </tr>
        <tr>
          <th>3</th>
          <td>64</td>
          <td>0.023773</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.005605</td>
          <td>0.011112</td>
          <td>0.009083</td>
          <td>0.010378</td>
          <td>0.001453</td>
          <td>0.001453</td>
        </tr>
        <tr>
          <th>4</th>
          <td>256</td>
          <td>0.021523</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.003629</td>
          <td>0.002814</td>
          <td>0.002830</td>
          <td>0.009758</td>
          <td>0.001367</td>
          <td>0.001367</td>
        </tr>
        <tr>
          <th>5</th>
          <td>1024</td>
          <td>0.012530</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.003996</td>
          <td>0.000676</td>
          <td>0.007555</td>
          <td>0.004122</td>
          <td>0.001021</td>
          <td>0.001021</td>
        </tr>
        <tr>
          <th>6</th>
          <td>4096</td>
          <td>0.010709</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.007516</td>
          <td>0.000159</td>
          <td>0.004486</td>
          <td>0.002035</td>
          <td>0.001118</td>
          <td>0.001118</td>
        </tr>
        <tr>
          <th>7</th>
          <td>16384</td>
          <td>0.007154</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.005920</td>
          <td>0.000034</td>
          <td>0.003413</td>
          <td>0.003627</td>
          <td>0.001657</td>
          <td>0.001657</td>
        </tr>
        <tr>
          <th>8</th>
          <td>65536</td>
          <td>0.009805</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.005209</td>
          <td>0.000008</td>
          <td>0.006098</td>
          <td>0.001472</td>
          <td>0.001327</td>
          <td>0.001327</td>
        </tr>
        <tr>
          <th>9</th>
          <td>262144</td>
          <td>0.009469</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.006873</td>
          <td>0.000002</td>
          <td>0.004559</td>
          <td>0.001849</td>
          <td>0.001225</td>
          <td>0.001225</td>
        </tr>
        <tr>
          <th>10</th>
          <td>1048576</td>
          <td>0.004918</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.004065</td>
          <td>0.000002</td>
          <td>0.008477</td>
          <td>0.001522</td>
          <td>0.001183</td>
          <td>0.001183</td>
        </tr>
        <tr>
          <th>11</th>
          <td>4194304</td>
          <td>0.012985</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.007906</td>
          <td>0.000000</td>
          <td>0.002688</td>
          <td>0.001735</td>
          <td>0.001047</td>
          <td>0.001047</td>
        </tr>
        <tr>
          <th>12</th>
          <td>16777216</td>
          <td>0.009105</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.004443</td>
          <td>0.000000</td>
          <td>0.005823</td>
          <td>0.001994</td>
          <td>0.001174</td>
          <td>0.001174</td>
        </tr>
        <tr>
          <th>13</th>
          <td>67108864</td>
          <td>0.008370</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.005543</td>
          <td>0.000000</td>
          <td>0.005946</td>
          <td>0.004476</td>
          <td>0.001061</td>
          <td>0.001061</td>
        </tr>
        <tr>
          <th>14</th>
          <td>268435456</td>
          <td>0.004552</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.005116</td>
          <td>0.000000</td>
          <td>0.004081</td>
          <td>0.004501</td>
          <td>0.001127</td>
          <td>0.001127</td>
        </tr>
        <tr>
          <th>15</th>
          <td>1073741824</td>
          <td>0.009256</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.006974</td>
          <td>0.000000</td>
          <td>0.005713</td>
          <td>0.001476</td>
          <td>0.001551</td>
          <td>0.001551</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 249-250

Finally.

.. GENERATED FROM PYTHON SOURCE LINES 250-263

.. code-block:: Python


    dfp = df.drop(["diff", "diff0", "n_denorm"], axis=1).set_index("scale")
    fig, ax = plt.subplots(1, 2, figsize=(10, 4))
    dfp.plot(ax=ax[0], logx=True, logy=True, title="Comparison of Conv processing time")
    df[["n_denorm"]].plot(
        ax=ax[1], logx=True, logy=True, title="Ratio of denormalized numbers"
    )

    fig.tight_layout()
    fig.savefig("plot_op_conv_denorm.png")
    # plt.show()





.. image-sg:: /auto_examples/images/sphx_glr_plot_op_conv_denorm_001.png
   :alt: Comparison of Conv processing time, Ratio of denormalized numbers
   :srcset: /auto_examples/images/sphx_glr_plot_op_conv_denorm_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 264-268

Conclusion
++++++++++

Denormalized numbers should be avoided.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 43.956 seconds)


.. _sphx_glr_download_auto_examples_plot_op_conv_denorm.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_op_conv_denorm.ipynb <plot_op_conv_denorm.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_op_conv_denorm.py <plot_op_conv_denorm.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_op_conv_denorm.zip <plot_op_conv_denorm.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
