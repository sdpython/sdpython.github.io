{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export a LLAMA model into ONNX\n\nThis script does not export a full llama model but a shorter one\nto be able to fast iterate on improvments.\nSee [LlamaConfig](https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig).\nThe model is then converted into ONNX.\nIt can be seen with :epkg:`Netron` which can be also used through a VS Code Extension.\n\n## The model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport random\n\n\ndef ids_tensor(shape, vocab_size, rng=None, name=None):\n    #  Creates a random int32 tensor of the shape within the vocab size\n    import torch\n\n    if rng is None:\n        rng = random.Random()\n\n    total_dims = 1\n    for dim in shape:\n        total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n        values.append(rng.randint(0, vocab_size - 1))\n\n    return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()\n\n\ndef get_llama_model(\n    input_dims=[(2, 1024)],  # noqa: B006\n    hidden_size=1024,  # 4096,\n    num_hidden_layers=1,\n    vocab_size=32000,\n    intermediate_size=11008,\n    max_position_embeddings=2048,\n    num_attention_heads=4,  # 32,\n    _attn_implementation=\"eager\",\n    with_mask: bool = True,\n):\n    import torch\n    from transformers import LlamaConfig\n    from transformers.models.llama.modeling_llama import LlamaModel\n\n    config = LlamaConfig(\n        num_hidden_layers=num_hidden_layers,\n        vocab_size=vocab_size,\n        hidden_size=hidden_size,\n        intermediate_size=intermediate_size,\n        max_position_embeddings=max_position_embeddings,\n        num_attention_heads=num_attention_heads,\n    )\n    if _attn_implementation:\n        config._attn_implementation = _attn_implementation\n\n    class LlamaModelWrapper(torch.nn.Module):\n        def __init__(self, config):\n            super().__init__()\n            self.model = LlamaModel(config)\n\n        def forward(self, input_ids, attention_mask):\n            model_output = self.model(input_ids, attention_mask=attention_mask)\n            return model_output.to_tuple()\n\n    def generate_example_inputs(batch: int, seq: int, vocab_size: int):\n        input_ids = ids_tensor([batch, seq], vocab_size)\n        input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))\n        assert input_mask.dtype == torch.float32\n        return input_ids, input_mask\n\n    example_args_collection = []\n    for b, s in input_dims:\n        example_args_collection.append(generate_example_inputs(b, s, vocab_size))\n\n    return LlamaModelWrapper(config), example_args_collection\n\n\nprint(\"creation of the model.\")\nmodel, example_args_collection = get_llama_model()\nprint(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The conversion to ONNX\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def export(model, args, filename, dynamic_shapes):\n    from experimental_experiment.torch_interpreter import to_onnx, ExportOptions\n    from onnx_diagnostic.torch_export_patches import bypass_export_some_errors\n\n    with bypass_export_some_errors(patch_transformers=True):\n        to_onnx(\n            model,\n            args,\n            filename=filename,\n            target_opset=18,\n            dynamic_shapes=dynamic_shapes,\n            export_options=ExportOptions(strict=False),\n        )\n\n\nfilename = \"dump_llama.onnx\"\nprint(f\"conversion to ONNX in file {filename!r}\")\nexport(\n    model,\n    example_args_collection[0],\n    filename,\n    dynamic_shapes=({0: \"batch\", 1: \"seq_length\"}, {0: \"batch\", 1: \"seq_length\"}),\n)\nprint(\"done.\")\nprint(f\"model size {os.stat(filename).st_size / 2**20} Mb.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gives the following in :epkg:`Netron`:\n\n<img src=\"file://../images/llama.png\">\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}