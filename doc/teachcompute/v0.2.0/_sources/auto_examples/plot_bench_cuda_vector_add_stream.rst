
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_bench_cuda_vector_add_stream.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_bench_cuda_vector_add_stream.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_bench_cuda_vector_add_stream.py:


.. _l-example-cuda-vector-addition-stream:

Measuring CUDA performance with a vector addition with streams
==============================================================

Measure the time between two additions, with or without streams.
The script can be profiled with :epkg:`Nsight`.

::

    nsys profile python _doc/examples/plot_bench_cuda_vector_add_stream.py

Vector Add
++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 17-93

.. code-block:: Python


    from tqdm import tqdm
    import numpy
    import matplotlib.pyplot as plt
    from pandas import DataFrame
    from teachcompute.ext_test_case import measure_time, unit_test_going
    import torch

    has_cuda = torch.cuda.is_available()

    try:
        from teachcompute.validation.cuda.cuda_example_py import (
            vector_add,
            vector_add_stream,
        )
    except ImportError:
        has_cuda = False


    def cuda_vector_add(values):
        torch.cuda.nvtx.range_push(f"CUDA dim={values.size}")
        res = vector_add(values, values, 0, repeat=10)
        torch.cuda.nvtx.range_pop()
        return res


    def cuda_vector_add_stream(values):
        torch.cuda.nvtx.range_push(f"CUDA stream dim={values.size}")
        res = vector_add_stream(values, values, 0, repeat=10)
        torch.cuda.nvtx.range_pop()
        return res


    obs = []
    dims = [2**10, 2**15, 2**20]
    if unit_test_going():
        dims = [10, 20, 30]
    for dim in tqdm(dims):
        values = numpy.ones((dim,), dtype=numpy.float32).ravel()

        if has_cuda:
            diff = numpy.abs(vector_add(values, values, 0) - (values + values)).max()
            res = measure_time(lambda: cuda_vector_add(values), max_time=0.5)

            obs.append(
                dict(
                    dim=dim,
                    size=values.size,
                    time=res["average"],
                    fct="CUDA",
                    time_per_element=res["average"] / dim,
                    diff=diff,
                )
            )

            diff = numpy.abs(vector_add_stream(values, values, 0) - (values + values)).max()
            res = measure_time(lambda: cuda_vector_add_stream(values), max_time=0.5)

            obs.append(
                dict(
                    dim=dim,
                    size=values.size,
                    time=res["average"],
                    fct="CUDA-stream",
                    time_per_element=res["average"] / dim,
                    diff=diff,
                )
            )


    if has_cuda:
        df = DataFrame(obs)
        piv = df.pivot(index="dim", columns="fct", values="time_per_element")
        print(piv)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/3 [00:00<?, ?it/s]     33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]     67%|██████▋   | 2/3 [00:02<00:01,  1.18s/it]    100%|██████████| 3/3 [00:03<00:00,  1.22s/it]    100%|██████████| 3/3 [00:03<00:00,  1.22s/it]
    fct              CUDA   CUDA-stream
    dim                                
    1024     2.634389e-07  5.046857e-07
    32768    1.130820e-08  1.892522e-08
    1048576  3.546513e-09  3.950572e-09




.. GENERATED FROM PYTHON SOURCE LINES 94-96

Plots
+++++

.. GENERATED FROM PYTHON SOURCE LINES 96-108

.. code-block:: Python


    if has_cuda:
        piv_diff = df.pivot(index="dim", columns="fct", values="diff")
        piv_time = df.pivot(index="dim", columns="fct", values="time")

        fig, ax = plt.subplots(1, 3, figsize=(12, 6))
        piv.plot(ax=ax[0], logx=True, title="Comparison between two summation")
        piv_diff.plot(ax=ax[1], logx=True, logy=True, title="Summation errors")
        piv_time.plot(ax=ax[2], logx=True, logy=True, title="Total time")
        fig.tight_layout()
        fig.savefig("plot_bench_cuda_vector_add_stream.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_bench_cuda_vector_add_stream_001.png
   :alt: Comparison between two summation, Summation errors, Total time
   :srcset: /auto_examples/images/sphx_glr_plot_bench_cuda_vector_add_stream_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/mamba/lib/python3.11/site-packages/pandas/plotting/_matplotlib/core.py:822: UserWarning: Data has no positive values, and therefore cannot be log-scaled.
      labels = axis.get_majorticklabels() + axis.get_minorticklabels()




.. GENERATED FROM PYTHON SOURCE LINES 109-112

In practice, one stream is usually enough.
CUDA parallelizes everything and takes all the computing power.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 4.814 seconds)


.. _sphx_glr_download_auto_examples_plot_bench_cuda_vector_add_stream.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bench_cuda_vector_add_stream.ipynb <plot_bench_cuda_vector_add_stream.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bench_cuda_vector_add_stream.py <plot_bench_cuda_vector_add_stream.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
