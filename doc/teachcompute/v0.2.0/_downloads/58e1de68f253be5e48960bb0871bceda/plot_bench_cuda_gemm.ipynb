{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Comparing GEMM implementation\n\nIt is not exactly GEMM but MatMul with transpose attributes.\n\n::\n\n    nsys profile python _doc/examples/plot_bench_cuda_gemm.py\n\n## Vector Add\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nimport numpy\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom teachcompute.ext_test_case import measure_time, unit_test_going\nimport torch\n\nhas_cuda = torch.cuda.is_available()\n\ntry:\n    from teachcompute.validation.cuda.cuda_gemm import (\n        matmul_v1_cuda,\n        matmul_v2_cuda,\n        matmul_v3_cuda,\n    )\nexcept ImportError:\n    has_cuda = False\n\n\ndef torch_matmul(m1, m2, r, trans_a, trans_b):\n    torch.cuda.nvtx.range_push(\n        f\"torch_matmul, tA={1 if trans_a else 0}, tB={1 if trans_b else 0}\"\n    )\n    if trans_a:\n        if trans_b:\n            r += m1.T @ m2.T\n        else:\n            r += m1.T @ m2\n    elif trans_b:\n        r += m1 @ m2.T\n    else:\n        r += m1 @ m2\n    torch.cuda.nvtx.range_pop()\n\n\ndef matmul_v1(t1, t2, r, trans_a, trans_b):\n    torch.cuda.nvtx.range_push(\n        f\"matmul_v1, tA={1 if trans_a else 0}, tB={1 if trans_b else 0}\"\n    )\n    matmul_v1_cuda(\n        *t1.shape, t1.data_ptr(), *t2.shape, t2.data_ptr(), r.data_ptr(), True, True\n    )\n    torch.cuda.nvtx.range_pop()\n\n\ndef matmul_v2(t1, t2, r, trans_a, trans_b):\n    torch.cuda.nvtx.range_push(\n        f\"matmul_v2, tA={1 if trans_a else 0}, tB={1 if trans_b else 0}\"\n    )\n    matmul_v2_cuda(\n        *t1.shape, t1.data_ptr(), *t2.shape, t2.data_ptr(), r.data_ptr(), True, True\n    )\n    torch.cuda.nvtx.range_pop()\n\n\ndef matmul_v3(t1, t2, r, trans_a, trans_b):\n    torch.cuda.nvtx.range_push(\n        f\"matmul_v3, tA={1 if trans_a else 0}, tB={1 if trans_b else 0}\"\n    )\n    matmul_v3_cuda(\n        *t1.shape, t1.data_ptr(), *t2.shape, t2.data_ptr(), r.data_ptr(), True, True\n    )\n    torch.cuda.nvtx.range_pop()\n\n\nfcts = [torch_matmul, matmul_v1, matmul_v2, matmul_v3]\n\nobs = []\ndims = [2**9, 2**10]  # , 2**11]\nif unit_test_going():\n    dims = [16, 32, 64]\nfor trans_a, trans_b, dim, fct in tqdm(\n    list(itertools.product([False, True], [False, True], dims, fcts))\n):\n    repeat, number = (10, 10) if dim <= 2**10 else (5, 5)\n    values = numpy.ones((dim, dim), dtype=numpy.float32) / (dim * repeat * number)\n    t1 = torch.Tensor(values).to(\"cuda:0\")\n    t2 = torch.Tensor(values).to(\"cuda:0\")\n    r = torch.zeros(t1.shape).to(\"cuda:0\")\n\n    if has_cuda:\n\n        # warmup\n        for _ in range(3):\n            fct(t1, t2, r, trans_a=trans_a, trans_b=trans_b)\n        r = torch.zeros(t1.shape).to(\"cuda:0\")\n        res = measure_time(\n            lambda fct=fct, t1=t1, t2=t2, r=r, trans_a=trans_a, trans_b=trans_b: fct(\n                t1, t2, r, trans_a=trans_a, trans_b=trans_b\n            ),\n            repeat=repeat,\n            number=number,\n            div_by_number=True,\n        )\n\n        res.update(\n            dict(\n                dim=dim,\n                shape=tuple(values.shape),\n                fct=fct.__name__,\n                tA=trans_a,\n                tB=trans_b,\n                tt=f\"tt{1 if trans_a else 0}{1 if trans_b else 0}\",\n            )\n        )\n        obs.append(res)\n\n\nif has_cuda:\n    df = DataFrame(obs)\n    df.to_csv(\"plot_bench_cuda_gemm.csv\", index=False)\n    df.to_excel(\"plot_bench_cuda_gemm.xlsx\", index=False)\n    print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if has_cuda:\n    fig, ax = plt.subplots(2, 2, figsize=(12, 6))\n    for tt in [\"tt00\", \"tt01\", \"tt10\", \"tt11\"]:\n        piv_time = df[df.tt == tt].pivot(index=\"dim\", columns=\"fct\", values=\"average\")\n        a = ax[int(tt[2]), int(tt[3])]\n        piv_time.plot(ax=a, logx=True, title=f\"tA,tB={tt}\")\n        cb = piv_time[\"torch_matmul\"].astype(float).copy()\n        for c in piv_time.columns:\n            piv_time[c] = cb / piv_time[c].astype(float)\n        print(f\"speed up for tt={tt}\")\n        print(piv_time)\n        print()\n    fig.suptitle(\"greater is better\")\n    fig.tight_layout()\n    fig.savefig(\"plot_bench_cuda_gemm.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}