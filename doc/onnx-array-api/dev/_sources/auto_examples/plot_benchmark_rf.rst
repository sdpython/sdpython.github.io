
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_benchmark_rf.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_benchmark_rf.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_benchmark_rf.py:


.. _l-example-benchmark-tree-implementation:

Benchmark of TreeEnsemble implementation
========================================

The following example compares the inference time between
:epkg:`onnxruntime` and :class:`sklearn.ensemble.RandomForestRegressor`,
fow different number of estimators, max depth, and parallelization.
It does it for a fixed number of rows and features.

import and registration of necessary converters
++++++++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 15-65

.. code-block:: Python


    import pickle
    import os
    import time
    from itertools import product

    import matplotlib.pyplot as plt
    import numpy
    import pandas
    from lightgbm import LGBMRegressor
    from onnxmltools.convert.lightgbm.operator_converters.LightGbm import convert_lightgbm
    from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost
    from onnxruntime import InferenceSession, SessionOptions
    from psutil import cpu_count
    from sphinx_runpython.runpython import run_cmd
    from skl2onnx import to_onnx, update_registered_converter
    from skl2onnx.common.shape_calculator import calculate_linear_regressor_output_shapes
    from sklearn import set_config
    from sklearn.ensemble import RandomForestRegressor
    from tqdm import tqdm
    from xgboost import XGBRegressor


    def skl2onnx_convert_lightgbm(scope, operator, container):
        options = scope.get_options(operator.raw_operator)
        if "split" in options:
            operator.split = options["split"]
        else:
            operator.split = None
        convert_lightgbm(scope, operator, container)


    update_registered_converter(
        LGBMRegressor,
        "LightGbmLGBMRegressor",
        calculate_linear_regressor_output_shapes,
        skl2onnx_convert_lightgbm,
        options={"split": None},
    )
    update_registered_converter(
        XGBRegressor,
        "XGBoostXGBRegressor",
        calculate_linear_regressor_output_shapes,
        convert_xgboost,
    )

    # The following instruction reduces the time spent by scikit-learn
    # to validate the data.
    set_config(assume_finite=True)








.. GENERATED FROM PYTHON SOURCE LINES 66-68

Machine details
+++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 68-72

.. code-block:: Python



    print(f"Number of cores: {cpu_count()}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Number of cores: 8




.. GENERATED FROM PYTHON SOURCE LINES 73-75

But this information is not usually enough.
Let's extract the cache information.

.. GENERATED FROM PYTHON SOURCE LINES 75-82

.. code-block:: Python


    try:
        out, err = run_cmd("lscpu")
        print(out)
    except Exception as e:
        print(f"lscpu not available: {e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    <Popen: returncode: None args: ['lscpu']>




.. GENERATED FROM PYTHON SOURCE LINES 83-84

Or with the following command.

.. GENERATED FROM PYTHON SOURCE LINES 84-87

.. code-block:: Python

    out, err = run_cmd("cat /proc/cpuinfo")
    print(out)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    <Popen: returncode: None args: ['cat', '/proc/cpuinfo']>




.. GENERATED FROM PYTHON SOURCE LINES 88-90

Fonction to measure inference time
++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 90-121

.. code-block:: Python



    def measure_inference(fct, X, repeat, max_time=5, quantile=1):
        """
        Run *repeat* times the same function on data *X*.

        :param fct: fonction to run
        :param X: data
        :param repeat: number of times to run
        :param max_time: maximum time to use to measure the inference
        :return: number of runs, sum of the time, average, median
        """
        times = []
        for n in range(repeat):
            perf = time.perf_counter()
            fct(X)
            delta = time.perf_counter() - perf
            times.append(delta)
            if len(times) < 3:
                continue
            if max_time is not None and sum(times) >= max_time:
                break
        times.sort()
        quantile = 0 if (len(times) - quantile * 2) < 3 else quantile
        if quantile == 0:
            tt = times
        else:
            tt = times[quantile:-quantile]
        return (len(times), sum(times), sum(tt) / len(tt), times[len(times) // 2])









.. GENERATED FROM PYTHON SOURCE LINES 122-128

Benchmark
+++++++++

The following script benchmarks the inference for the same
model for a random forest and onnxruntime after it was converted
into ONNX and for the following configurations.

.. GENERATED FROM PYTHON SOURCE LINES 128-150

.. code-block:: Python


    small = cpu_count() < 12
    if small:
        N = 1000
        n_features = 10
        n_jobs = [1, cpu_count() // 2, cpu_count()]
        n_ests = [10, 20, 30]
        depth = [4, 6, 8, 10]
        Regressor = RandomForestRegressor
    else:
        N = 100000
        n_features = 50
        n_jobs = [cpu_count(), cpu_count() // 2, 1]
        n_ests = [100, 200, 400]
        depth = [6, 8, 10, 12, 14]
        Regressor = RandomForestRegressor

    legend = f"parallel-nf-{n_features}-"

    # avoid duplicates on machine with 1 or 2 cores.
    n_jobs = list(sorted(set(n_jobs), reverse=True))








.. GENERATED FROM PYTHON SOURCE LINES 151-152

Benchmark parameters

.. GENERATED FROM PYTHON SOURCE LINES 152-156

.. code-block:: Python

    repeat = 7  # repeat n times the same inference
    quantile = 1  # exclude extreme times
    max_time = 5  # maximum number of seconds to spend on one configuration








.. GENERATED FROM PYTHON SOURCE LINES 157-158

Data

.. GENERATED FROM PYTHON SOURCE LINES 158-248

.. code-block:: Python



    X = numpy.random.randn(N, n_features).astype(numpy.float32)
    noise = (numpy.random.randn(X.shape[0]) / (n_features // 5)).astype(numpy.float32)
    y = X.mean(axis=1) + noise
    n_train = min(N, N // 3)


    data = []
    couples = list(product(n_jobs, depth, n_ests))
    bar = tqdm(couples)
    cache_dir = "_cache"
    if not os.path.exists(cache_dir):
        os.mkdir(cache_dir)

    for n_j, max_depth, n_estimators in bar:
        if n_j == 1 and n_estimators > n_ests[0]:
            # skipping
            continue

        # parallelization
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.pkl"
        )
        if os.path.exists(cache_name):
            with open(cache_name, "rb") as f:
                rf = pickle.load(f)
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} train rf")
            if n_j == 1 and issubclass(Regressor, RandomForestRegressor):
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=-1)
                rf.fit(X[:n_train], y[:n_train])
                rf.n_jobs = 1
            else:
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_j)
                rf.fit(X[:n_train], y[:n_train])
            with open(cache_name, "wb") as f:
                pickle.dump(rf, f)

        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} ISession")
        so = SessionOptions()
        so.intra_op_num_threads = n_j
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.onnx"
        )
        if os.path.exists(cache_name):
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} cvt onnx")
            onx = to_onnx(rf, X[:1])
            with open(cache_name, "wb") as f:
                f.write(onx.SerializeToString())
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        onx_size = os.stat(cache_name).st_size

        # run once to avoid counting the first run
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predict1")
        rf.predict(X)
        sess.run(None, {"X": X})

        # fixed data
        obs = dict(
            n_jobs=n_j,
            max_depth=max_depth,
            n_estimators=n_estimators,
            repeat=repeat,
            max_time=max_time,
            name=rf.__class__.__name__,
            n_rows=X.shape[0],
            n_features=X.shape[1],
            onnx_size=onx_size,
        )

        # baseline
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictB")
        r, t, mean, med = measure_inference(rf.predict, X, repeat=repeat, max_time=max_time)
        o1 = obs.copy()
        o1.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="base"))
        data.append(o1)

        # onnxruntime
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictO")
        r, t, mean, med = measure_inference(
            lambda x: sess.run(None, {"X": x}), X, repeat=repeat, max_time=max_time
        )
        o2 = obs.copy()
        o2.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="ort_"))
        data.append(o2)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 train rf:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 ISession:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 cvt onnx:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predict1:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictB:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictO:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=20 D=4 train rf:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=20 D=4 ISession:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=20 D=4 cvt onnx:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=20 D=4 predict1:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=20 D=4 predictB:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=20 D=4 predictO:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=20 D=4 predictO:   6%|▌         | 2/36 [00:00<00:05,  5.74it/s]    J=8 E=30 D=4 train rf:   6%|▌         | 2/36 [00:00<00:05,  5.74it/s]    J=8 E=30 D=4 ISession:   6%|▌         | 2/36 [00:00<00:05,  5.74it/s]    J=8 E=30 D=4 cvt onnx:   6%|▌         | 2/36 [00:00<00:05,  5.74it/s]    J=8 E=30 D=4 predict1:   6%|▌         | 2/36 [00:00<00:05,  5.74it/s]    J=8 E=30 D=4 predictB:   6%|▌         | 2/36 [00:00<00:05,  5.74it/s]    J=8 E=30 D=4 predictO:   6%|▌         | 2/36 [00:00<00:05,  5.74it/s]    J=8 E=30 D=4 predictO:   8%|▊         | 3/36 [00:00<00:06,  5.20it/s]    J=8 E=10 D=6 train rf:   8%|▊         | 3/36 [00:00<00:06,  5.20it/s]    J=8 E=10 D=6 ISession:   8%|▊         | 3/36 [00:00<00:06,  5.20it/s]    J=8 E=10 D=6 cvt onnx:   8%|▊         | 3/36 [00:00<00:06,  5.20it/s]    J=8 E=10 D=6 predict1:   8%|▊         | 3/36 [00:00<00:06,  5.20it/s]    J=8 E=10 D=6 predictB:   8%|▊         | 3/36 [00:00<00:06,  5.20it/s]    J=8 E=10 D=6 predictO:   8%|▊         | 3/36 [00:00<00:06,  5.20it/s]    J=8 E=10 D=6 predictO:  11%|█         | 4/36 [00:00<00:05,  5.80it/s]    J=8 E=20 D=6 train rf:  11%|█         | 4/36 [00:00<00:05,  5.80it/s]    J=8 E=20 D=6 ISession:  11%|█         | 4/36 [00:00<00:05,  5.80it/s]    J=8 E=20 D=6 cvt onnx:  11%|█         | 4/36 [00:00<00:05,  5.80it/s]    J=8 E=20 D=6 predict1:  11%|█         | 4/36 [00:00<00:05,  5.80it/s]    J=8 E=20 D=6 predictB:  11%|█         | 4/36 [00:00<00:05,  5.80it/s]    J=8 E=20 D=6 predictO:  11%|█         | 4/36 [00:00<00:05,  5.80it/s]    J=8 E=20 D=6 predictO:  14%|█▍        | 5/36 [00:00<00:05,  5.50it/s]    J=8 E=30 D=6 train rf:  14%|█▍        | 5/36 [00:00<00:05,  5.50it/s]    J=8 E=30 D=6 ISession:  14%|█▍        | 5/36 [00:00<00:05,  5.50it/s]    J=8 E=30 D=6 cvt onnx:  14%|█▍        | 5/36 [00:00<00:05,  5.50it/s]    J=8 E=30 D=6 predict1:  14%|█▍        | 5/36 [00:01<00:05,  5.50it/s]    J=8 E=30 D=6 predictB:  14%|█▍        | 5/36 [00:01<00:05,  5.50it/s]    J=8 E=30 D=6 predictO:  14%|█▍        | 5/36 [00:01<00:05,  5.50it/s]    J=8 E=30 D=6 predictO:  17%|█▋        | 6/36 [00:01<00:06,  4.34it/s]    J=8 E=10 D=8 train rf:  17%|█▋        | 6/36 [00:01<00:06,  4.34it/s]    J=8 E=10 D=8 ISession:  17%|█▋        | 6/36 [00:01<00:06,  4.34it/s]    J=8 E=10 D=8 cvt onnx:  17%|█▋        | 6/36 [00:01<00:06,  4.34it/s]    J=8 E=10 D=8 predict1:  17%|█▋        | 6/36 [00:01<00:06,  4.34it/s]    J=8 E=10 D=8 predictB:  17%|█▋        | 6/36 [00:01<00:06,  4.34it/s]    J=8 E=10 D=8 predictO:  17%|█▋        | 6/36 [00:01<00:06,  4.34it/s]    J=8 E=10 D=8 predictO:  19%|█▉        | 7/36 [00:01<00:06,  4.46it/s]    J=8 E=20 D=8 train rf:  19%|█▉        | 7/36 [00:01<00:06,  4.46it/s]    J=8 E=20 D=8 ISession:  19%|█▉        | 7/36 [00:01<00:06,  4.46it/s]    J=8 E=20 D=8 cvt onnx:  19%|█▉        | 7/36 [00:01<00:06,  4.46it/s]    J=8 E=20 D=8 predict1:  19%|█▉        | 7/36 [00:01<00:06,  4.46it/s]    J=8 E=20 D=8 predictB:  19%|█▉        | 7/36 [00:01<00:06,  4.46it/s]    J=8 E=20 D=8 predictO:  19%|█▉        | 7/36 [00:01<00:06,  4.46it/s]    J=8 E=20 D=8 predictO:  22%|██▏       | 8/36 [00:01<00:06,  4.42it/s]    J=8 E=30 D=8 train rf:  22%|██▏       | 8/36 [00:01<00:06,  4.42it/s]    J=8 E=30 D=8 ISession:  22%|██▏       | 8/36 [00:01<00:06,  4.42it/s]    J=8 E=30 D=8 cvt onnx:  22%|██▏       | 8/36 [00:01<00:06,  4.42it/s]    J=8 E=30 D=8 predict1:  22%|██▏       | 8/36 [00:01<00:06,  4.42it/s]    J=8 E=30 D=8 predictB:  22%|██▏       | 8/36 [00:01<00:06,  4.42it/s]    J=8 E=30 D=8 predictO:  22%|██▏       | 8/36 [00:02<00:06,  4.42it/s]    J=8 E=30 D=8 predictO:  25%|██▌       | 9/36 [00:02<00:07,  3.84it/s]    J=8 E=10 D=10 train rf:  25%|██▌       | 9/36 [00:02<00:07,  3.84it/s]    J=8 E=10 D=10 ISession:  25%|██▌       | 9/36 [00:02<00:07,  3.84it/s]    J=8 E=10 D=10 cvt onnx:  25%|██▌       | 9/36 [00:02<00:07,  3.84it/s]    J=8 E=10 D=10 predict1:  25%|██▌       | 9/36 [00:02<00:07,  3.84it/s]    J=8 E=10 D=10 predictB:  25%|██▌       | 9/36 [00:02<00:07,  3.84it/s]    J=8 E=10 D=10 predictO:  25%|██▌       | 9/36 [00:02<00:07,  3.84it/s]    J=8 E=10 D=10 predictO:  28%|██▊       | 10/36 [00:02<00:05,  4.56it/s]    J=8 E=20 D=10 train rf:  28%|██▊       | 10/36 [00:02<00:05,  4.56it/s]    J=8 E=20 D=10 ISession:  28%|██▊       | 10/36 [00:02<00:05,  4.56it/s]    J=8 E=20 D=10 cvt onnx:  28%|██▊       | 10/36 [00:02<00:05,  4.56it/s]    J=8 E=20 D=10 predict1:  28%|██▊       | 10/36 [00:02<00:05,  4.56it/s]    J=8 E=20 D=10 predictB:  28%|██▊       | 10/36 [00:02<00:05,  4.56it/s]    J=8 E=20 D=10 predictO:  28%|██▊       | 10/36 [00:02<00:05,  4.56it/s]    J=8 E=20 D=10 predictO:  31%|███       | 11/36 [00:02<00:05,  4.81it/s]    J=8 E=30 D=10 train rf:  31%|███       | 11/36 [00:02<00:05,  4.81it/s]    J=8 E=30 D=10 ISession:  31%|███       | 11/36 [00:02<00:05,  4.81it/s]    J=8 E=30 D=10 cvt onnx:  31%|███       | 11/36 [00:02<00:05,  4.81it/s]    J=8 E=30 D=10 predict1:  31%|███       | 11/36 [00:02<00:05,  4.81it/s]    J=8 E=30 D=10 predictB:  31%|███       | 11/36 [00:02<00:05,  4.81it/s]    J=8 E=30 D=10 predictO:  31%|███       | 11/36 [00:02<00:05,  4.81it/s]    J=8 E=30 D=10 predictO:  33%|███▎      | 12/36 [00:02<00:05,  4.01it/s]    J=4 E=10 D=4 train rf:  33%|███▎      | 12/36 [00:02<00:05,  4.01it/s]     J=4 E=10 D=4 ISession:  33%|███▎      | 12/36 [00:02<00:05,  4.01it/s]    J=4 E=10 D=4 cvt onnx:  33%|███▎      | 12/36 [00:02<00:05,  4.01it/s]    J=4 E=10 D=4 predict1:  33%|███▎      | 12/36 [00:02<00:05,  4.01it/s]    J=4 E=10 D=4 predictB:  33%|███▎      | 12/36 [00:02<00:05,  4.01it/s]    J=4 E=10 D=4 predictO:  33%|███▎      | 12/36 [00:02<00:05,  4.01it/s]    J=4 E=10 D=4 predictO:  36%|███▌      | 13/36 [00:02<00:04,  4.74it/s]    J=4 E=20 D=4 train rf:  36%|███▌      | 13/36 [00:02<00:04,  4.74it/s]    J=4 E=20 D=4 ISession:  36%|███▌      | 13/36 [00:02<00:04,  4.74it/s]    J=4 E=20 D=4 cvt onnx:  36%|███▌      | 13/36 [00:02<00:04,  4.74it/s]    J=4 E=20 D=4 predict1:  36%|███▌      | 13/36 [00:02<00:04,  4.74it/s]    J=4 E=20 D=4 predictB:  36%|███▌      | 13/36 [00:02<00:04,  4.74it/s]    J=4 E=20 D=4 predictO:  36%|███▌      | 13/36 [00:02<00:04,  4.74it/s]    J=4 E=20 D=4 predictO:  39%|███▉      | 14/36 [00:02<00:04,  5.08it/s]    J=4 E=30 D=4 train rf:  39%|███▉      | 14/36 [00:02<00:04,  5.08it/s]    J=4 E=30 D=4 ISession:  39%|███▉      | 14/36 [00:03<00:04,  5.08it/s]    J=4 E=30 D=4 cvt onnx:  39%|███▉      | 14/36 [00:03<00:04,  5.08it/s]    J=4 E=30 D=4 predict1:  39%|███▉      | 14/36 [00:03<00:04,  5.08it/s]    J=4 E=30 D=4 predictB:  39%|███▉      | 14/36 [00:03<00:04,  5.08it/s]    J=4 E=30 D=4 predictO:  39%|███▉      | 14/36 [00:03<00:04,  5.08it/s]    J=4 E=30 D=4 predictO:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=10 D=6 train rf:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=10 D=6 ISession:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=10 D=6 cvt onnx:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=10 D=6 predict1:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=10 D=6 predictB:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=10 D=6 predictO:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=20 D=6 train rf:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=20 D=6 ISession:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=20 D=6 cvt onnx:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=20 D=6 predict1:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=20 D=6 predictB:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=20 D=6 predictO:  42%|████▏     | 15/36 [00:03<00:04,  4.96it/s]    J=4 E=20 D=6 predictO:  47%|████▋     | 17/36 [00:03<00:03,  5.88it/s]    J=4 E=30 D=6 train rf:  47%|████▋     | 17/36 [00:03<00:03,  5.88it/s]    J=4 E=30 D=6 ISession:  47%|████▋     | 17/36 [00:03<00:03,  5.88it/s]    J=4 E=30 D=6 cvt onnx:  47%|████▋     | 17/36 [00:03<00:03,  5.88it/s]    J=4 E=30 D=6 predict1:  47%|████▋     | 17/36 [00:03<00:03,  5.88it/s]    J=4 E=30 D=6 predictB:  47%|████▋     | 17/36 [00:03<00:03,  5.88it/s]    J=4 E=30 D=6 predictO:  47%|████▋     | 17/36 [00:03<00:03,  5.88it/s]    J=4 E=30 D=6 predictO:  50%|█████     | 18/36 [00:03<00:03,  5.48it/s]    J=4 E=10 D=8 train rf:  50%|█████     | 18/36 [00:03<00:03,  5.48it/s]    J=4 E=10 D=8 ISession:  50%|█████     | 18/36 [00:03<00:03,  5.48it/s]    J=4 E=10 D=8 cvt onnx:  50%|█████     | 18/36 [00:03<00:03,  5.48it/s]    J=4 E=10 D=8 predict1:  50%|█████     | 18/36 [00:03<00:03,  5.48it/s]    J=4 E=10 D=8 predictB:  50%|█████     | 18/36 [00:03<00:03,  5.48it/s]    J=4 E=10 D=8 predictO:  50%|█████     | 18/36 [00:03<00:03,  5.48it/s]    J=4 E=10 D=8 predictO:  53%|█████▎    | 19/36 [00:03<00:02,  6.15it/s]    J=4 E=20 D=8 train rf:  53%|█████▎    | 19/36 [00:03<00:02,  6.15it/s]    J=4 E=20 D=8 ISession:  53%|█████▎    | 19/36 [00:03<00:02,  6.15it/s]    J=4 E=20 D=8 cvt onnx:  53%|█████▎    | 19/36 [00:03<00:02,  6.15it/s]    J=4 E=20 D=8 predict1:  53%|█████▎    | 19/36 [00:03<00:02,  6.15it/s]    J=4 E=20 D=8 predictB:  53%|█████▎    | 19/36 [00:03<00:02,  6.15it/s]    J=4 E=20 D=8 predictO:  53%|█████▎    | 19/36 [00:04<00:02,  6.15it/s]    J=4 E=20 D=8 predictO:  56%|█████▌    | 20/36 [00:04<00:04,  3.95it/s]    J=4 E=30 D=8 train rf:  56%|█████▌    | 20/36 [00:04<00:04,  3.95it/s]    J=4 E=30 D=8 ISession:  56%|█████▌    | 20/36 [00:04<00:04,  3.95it/s]    J=4 E=30 D=8 cvt onnx:  56%|█████▌    | 20/36 [00:04<00:04,  3.95it/s]    J=4 E=30 D=8 predict1:  56%|█████▌    | 20/36 [00:04<00:04,  3.95it/s]    J=4 E=30 D=8 predictB:  56%|█████▌    | 20/36 [00:04<00:04,  3.95it/s]    J=4 E=30 D=8 predictO:  56%|█████▌    | 20/36 [00:04<00:04,  3.95it/s]    J=4 E=30 D=8 predictO:  58%|█████▊    | 21/36 [00:04<00:04,  3.64it/s]    J=4 E=10 D=10 train rf:  58%|█████▊    | 21/36 [00:04<00:04,  3.64it/s]    J=4 E=10 D=10 ISession:  58%|█████▊    | 21/36 [00:04<00:04,  3.64it/s]    J=4 E=10 D=10 cvt onnx:  58%|█████▊    | 21/36 [00:04<00:04,  3.64it/s]    J=4 E=10 D=10 predict1:  58%|█████▊    | 21/36 [00:04<00:04,  3.64it/s]    J=4 E=10 D=10 predictB:  58%|█████▊    | 21/36 [00:04<00:04,  3.64it/s]    J=4 E=10 D=10 predictO:  58%|█████▊    | 21/36 [00:04<00:04,  3.64it/s]    J=4 E=10 D=10 predictO:  61%|██████    | 22/36 [00:04<00:03,  4.24it/s]    J=4 E=20 D=10 train rf:  61%|██████    | 22/36 [00:04<00:03,  4.24it/s]    J=4 E=20 D=10 ISession:  61%|██████    | 22/36 [00:04<00:03,  4.24it/s]    J=4 E=20 D=10 cvt onnx:  61%|██████    | 22/36 [00:04<00:03,  4.24it/s]    J=4 E=20 D=10 predict1:  61%|██████    | 22/36 [00:04<00:03,  4.24it/s]    J=4 E=20 D=10 predictB:  61%|██████    | 22/36 [00:04<00:03,  4.24it/s]    J=4 E=20 D=10 predictO:  61%|██████    | 22/36 [00:04<00:03,  4.24it/s]    J=4 E=20 D=10 predictO:  64%|██████▍   | 23/36 [00:04<00:02,  4.38it/s]    J=4 E=30 D=10 train rf:  64%|██████▍   | 23/36 [00:04<00:02,  4.38it/s]    J=4 E=30 D=10 ISession:  64%|██████▍   | 23/36 [00:05<00:02,  4.38it/s]    J=4 E=30 D=10 cvt onnx:  64%|██████▍   | 23/36 [00:05<00:02,  4.38it/s]    J=4 E=30 D=10 predict1:  64%|██████▍   | 23/36 [00:05<00:02,  4.38it/s]    J=4 E=30 D=10 predictB:  64%|██████▍   | 23/36 [00:05<00:02,  4.38it/s]    J=4 E=30 D=10 predictO:  64%|██████▍   | 23/36 [00:05<00:02,  4.38it/s]    J=4 E=30 D=10 predictO:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=4 train rf:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]     J=1 E=10 D=4 ISession:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=4 cvt onnx:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=4 predict1:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=4 predictB:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=4 predictO:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=6 train rf:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=6 ISession:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=6 cvt onnx:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=6 predict1:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=6 predictB:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=6 predictO:  67%|██████▋   | 24/36 [00:05<00:03,  3.78it/s]    J=1 E=10 D=6 predictO:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=8 train rf:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=8 ISession:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=8 cvt onnx:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=8 predict1:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=8 predictB:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=8 predictO:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=10 train rf:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=10 ISession:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=10 cvt onnx:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=10 predict1:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=10 predictB:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=10 predictO:  78%|███████▊  | 28/36 [00:05<00:00,  8.63it/s]    J=1 E=10 D=10 predictO:  94%|█████████▍| 34/36 [00:05<00:00, 16.19it/s]    J=1 E=10 D=10 predictO: 100%|██████████| 36/36 [00:05<00:00,  6.52it/s]




.. GENERATED FROM PYTHON SOURCE LINES 249-251

Saving data
+++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 251-260

.. code-block:: Python


    name = os.path.join(cache_dir, "plot_beanchmark_rf")
    print(f"Saving data into {name!r}")

    df = pandas.DataFrame(data)
    df2 = df.copy()
    df2["legend"] = legend
    df2.to_csv(f"{name}-{legend}.csv", index=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Saving data into '_cache/plot_beanchmark_rf'




.. GENERATED FROM PYTHON SOURCE LINES 261-262

Printing the data

.. GENERATED FROM PYTHON SOURCE LINES 262-264

.. code-block:: Python

    df






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>n_jobs</th>
          <th>max_depth</th>
          <th>n_estimators</th>
          <th>repeat</th>
          <th>max_time</th>
          <th>name</th>
          <th>n_rows</th>
          <th>n_features</th>
          <th>onnx_size</th>
          <th>avg</th>
          <th>med</th>
          <th>n_runs</th>
          <th>ttime</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11016</td>
          <td>0.004663</td>
          <td>0.003874</td>
          <td>7</td>
          <td>0.035049</td>
        </tr>
        <tr>
          <th>1</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11016</td>
          <td>0.000109</td>
          <td>0.000104</td>
          <td>7</td>
          <td>0.000961</td>
        </tr>
        <tr>
          <th>2</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>21920</td>
          <td>0.022436</td>
          <td>0.020714</td>
          <td>7</td>
          <td>0.162124</td>
        </tr>
        <tr>
          <th>3</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>21920</td>
          <td>0.000348</td>
          <td>0.000273</td>
          <td>7</td>
          <td>0.016084</td>
        </tr>
        <tr>
          <th>4</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>31581</td>
          <td>0.013858</td>
          <td>0.015409</td>
          <td>7</td>
          <td>0.094792</td>
        </tr>
        <tr>
          <th>5</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>31581</td>
          <td>0.000231</td>
          <td>0.000221</td>
          <td>7</td>
          <td>0.001835</td>
        </tr>
        <tr>
          <th>6</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>32334</td>
          <td>0.007907</td>
          <td>0.007529</td>
          <td>7</td>
          <td>0.057407</td>
        </tr>
        <tr>
          <th>7</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>32334</td>
          <td>0.000124</td>
          <td>0.000121</td>
          <td>7</td>
          <td>0.001088</td>
        </tr>
        <tr>
          <th>8</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>63384</td>
          <td>0.012955</td>
          <td>0.013004</td>
          <td>7</td>
          <td>0.096642</td>
        </tr>
        <tr>
          <th>9</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>63384</td>
          <td>0.000286</td>
          <td>0.000291</td>
          <td>7</td>
          <td>0.002325</td>
        </tr>
        <tr>
          <th>10</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>93267</td>
          <td>0.027431</td>
          <td>0.028835</td>
          <td>7</td>
          <td>0.200870</td>
        </tr>
        <tr>
          <th>11</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>93267</td>
          <td>0.000504</td>
          <td>0.000457</td>
          <td>7</td>
          <td>0.004844</td>
        </tr>
        <tr>
          <th>12</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>70250</td>
          <td>0.011049</td>
          <td>0.011233</td>
          <td>7</td>
          <td>0.082753</td>
        </tr>
        <tr>
          <th>13</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>70250</td>
          <td>0.000192</td>
          <td>0.000191</td>
          <td>7</td>
          <td>0.001593</td>
        </tr>
        <tr>
          <th>14</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>136595</td>
          <td>0.012846</td>
          <td>0.013259</td>
          <td>7</td>
          <td>0.088674</td>
        </tr>
        <tr>
          <th>15</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>136595</td>
          <td>0.000355</td>
          <td>0.000352</td>
          <td>7</td>
          <td>0.005967</td>
        </tr>
        <tr>
          <th>16</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>200526</td>
          <td>0.016845</td>
          <td>0.016797</td>
          <td>7</td>
          <td>0.116471</td>
        </tr>
        <tr>
          <th>17</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>200526</td>
          <td>0.000407</td>
          <td>0.000421</td>
          <td>7</td>
          <td>0.002960</td>
        </tr>
        <tr>
          <th>18</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>110896</td>
          <td>0.005824</td>
          <td>0.004942</td>
          <td>7</td>
          <td>0.041422</td>
        </tr>
        <tr>
          <th>19</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>110896</td>
          <td>0.000198</td>
          <td>0.000204</td>
          <td>7</td>
          <td>0.015227</td>
        </tr>
        <tr>
          <th>20</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>205168</td>
          <td>0.008968</td>
          <td>0.009674</td>
          <td>7</td>
          <td>0.062466</td>
        </tr>
        <tr>
          <th>21</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>205168</td>
          <td>0.000336</td>
          <td>0.000333</td>
          <td>7</td>
          <td>0.002585</td>
        </tr>
        <tr>
          <th>22</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>337667</td>
          <td>0.016275</td>
          <td>0.015951</td>
          <td>7</td>
          <td>0.114295</td>
        </tr>
        <tr>
          <th>23</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>337667</td>
          <td>0.001137</td>
          <td>0.001014</td>
          <td>7</td>
          <td>0.021866</td>
        </tr>
        <tr>
          <th>24</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>10651</td>
          <td>0.008808</td>
          <td>0.008799</td>
          <td>7</td>
          <td>0.061873</td>
        </tr>
        <tr>
          <th>25</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>10651</td>
          <td>0.000143</td>
          <td>0.000139</td>
          <td>7</td>
          <td>0.001274</td>
        </tr>
        <tr>
          <th>26</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>22066</td>
          <td>0.012346</td>
          <td>0.012440</td>
          <td>7</td>
          <td>0.086840</td>
        </tr>
        <tr>
          <th>27</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>22066</td>
          <td>0.000280</td>
          <td>0.000242</td>
          <td>7</td>
          <td>0.002151</td>
        </tr>
        <tr>
          <th>28</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>32019</td>
          <td>0.016049</td>
          <td>0.016524</td>
          <td>7</td>
          <td>0.112539</td>
        </tr>
        <tr>
          <th>29</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>32019</td>
          <td>0.000322</td>
          <td>0.000319</td>
          <td>7</td>
          <td>0.002475</td>
        </tr>
        <tr>
          <th>30</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>29122</td>
          <td>0.007346</td>
          <td>0.007374</td>
          <td>7</td>
          <td>0.051454</td>
        </tr>
        <tr>
          <th>31</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>29122</td>
          <td>0.000199</td>
          <td>0.000187</td>
          <td>7</td>
          <td>0.001529</td>
        </tr>
        <tr>
          <th>32</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>60902</td>
          <td>0.012161</td>
          <td>0.012156</td>
          <td>7</td>
          <td>0.086634</td>
        </tr>
        <tr>
          <th>33</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>60902</td>
          <td>0.000320</td>
          <td>0.000321</td>
          <td>7</td>
          <td>0.002459</td>
        </tr>
        <tr>
          <th>34</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>90785</td>
          <td>0.014893</td>
          <td>0.015159</td>
          <td>7</td>
          <td>0.104060</td>
        </tr>
        <tr>
          <th>35</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>90785</td>
          <td>0.000444</td>
          <td>0.000437</td>
          <td>7</td>
          <td>0.003262</td>
        </tr>
        <tr>
          <th>36</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>63147</td>
          <td>0.006897</td>
          <td>0.006924</td>
          <td>7</td>
          <td>0.048358</td>
        </tr>
        <tr>
          <th>37</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>63147</td>
          <td>0.000237</td>
          <td>0.000220</td>
          <td>7</td>
          <td>0.001840</td>
        </tr>
        <tr>
          <th>38</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>129927</td>
          <td>0.042975</td>
          <td>0.042962</td>
          <td>7</td>
          <td>0.297704</td>
        </tr>
        <tr>
          <th>39</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>129927</td>
          <td>0.000601</td>
          <td>0.000562</td>
          <td>7</td>
          <td>0.004383</td>
        </tr>
        <tr>
          <th>40</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>199897</td>
          <td>0.020627</td>
          <td>0.020247</td>
          <td>7</td>
          <td>0.145145</td>
        </tr>
        <tr>
          <th>41</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>199897</td>
          <td>0.000688</td>
          <td>0.000654</td>
          <td>7</td>
          <td>0.004908</td>
        </tr>
        <tr>
          <th>42</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>115188</td>
          <td>0.007659</td>
          <td>0.007324</td>
          <td>7</td>
          <td>0.054239</td>
        </tr>
        <tr>
          <th>43</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>115188</td>
          <td>0.000177</td>
          <td>0.000175</td>
          <td>7</td>
          <td>0.001430</td>
        </tr>
        <tr>
          <th>44</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>206338</td>
          <td>0.011118</td>
          <td>0.009635</td>
          <td>7</td>
          <td>0.080058</td>
        </tr>
        <tr>
          <th>45</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>206338</td>
          <td>0.000376</td>
          <td>0.000372</td>
          <td>7</td>
          <td>0.002772</td>
        </tr>
        <tr>
          <th>46</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>321591</td>
          <td>0.013835</td>
          <td>0.013905</td>
          <td>7</td>
          <td>0.099340</td>
        </tr>
        <tr>
          <th>47</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>321591</td>
          <td>0.000491</td>
          <td>0.000488</td>
          <td>7</td>
          <td>0.003665</td>
        </tr>
        <tr>
          <th>48</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11527</td>
          <td>0.001336</td>
          <td>0.001321</td>
          <td>7</td>
          <td>0.009368</td>
        </tr>
        <tr>
          <th>49</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11527</td>
          <td>0.000272</td>
          <td>0.000271</td>
          <td>7</td>
          <td>0.001935</td>
        </tr>
        <tr>
          <th>50</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>35181</td>
          <td>0.001307</td>
          <td>0.001291</td>
          <td>7</td>
          <td>0.009515</td>
        </tr>
        <tr>
          <th>51</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>35181</td>
          <td>0.000331</td>
          <td>0.000331</td>
          <td>7</td>
          <td>0.002349</td>
        </tr>
        <tr>
          <th>52</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>69863</td>
          <td>0.001439</td>
          <td>0.001430</td>
          <td>7</td>
          <td>0.010137</td>
        </tr>
        <tr>
          <th>53</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>69863</td>
          <td>0.000450</td>
          <td>0.000448</td>
          <td>7</td>
          <td>0.003211</td>
        </tr>
        <tr>
          <th>54</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>110504</td>
          <td>0.001469</td>
          <td>0.001454</td>
          <td>7</td>
          <td>0.010462</td>
        </tr>
        <tr>
          <th>55</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>110504</td>
          <td>0.000504</td>
          <td>0.000490</td>
          <td>7</td>
          <td>0.003574</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 265-267

Plot
++++

.. GENERATED FROM PYTHON SOURCE LINES 267-309

.. code-block:: Python


    n_rows = len(n_jobs)
    n_cols = len(n_ests)


    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))
    fig.suptitle(f"{rf.__class__.__name__}\nX.shape={X.shape}")

    for n_j, n_estimators in tqdm(product(n_jobs, n_ests)):
        i = n_jobs.index(n_j)
        j = n_ests.index(n_estimators)
        ax = axes[i, j]

        subdf = df[(df.n_estimators == n_estimators) & (df.n_jobs == n_j)]
        if subdf.shape[0] == 0:
            continue
        piv = subdf.pivot(index="max_depth", columns="name", values=["avg", "med"])
        piv.plot(ax=ax, title=f"jobs={n_j}, trees={n_estimators}")
        ax.set_ylabel(f"n_jobs={n_j}", fontsize="small")
        ax.set_xlabel("max_depth", fontsize="small")

        # ratio
        ax2 = ax.twinx()
        piv1 = subdf.pivot(index="max_depth", columns="name", values="avg")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "b--", label="speedup avg")

        piv1 = subdf.pivot(index="max_depth", columns="name", values="med")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "y--", label="speedup med")
        ax2.legend(fontsize="x-small")

        # 1
        ax2.plot(piv1.index, [1 for _ in piv1.index], "k--", label="no speedup")

    for i in range(axes.shape[0]):
        for j in range(axes.shape[1]):
            axes[i, j].legend(fontsize="small")

    fig.tight_layout()
    fig.savefig(f"{name}-{legend}.png")
    # plt.show()



.. image-sg:: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :alt: RandomForestRegressor X.shape=(1000, 10), jobs=8, trees=10, jobs=8, trees=20, jobs=8, trees=30, jobs=4, trees=10, jobs=4, trees=20, jobs=4, trees=30, jobs=1, trees=10
   :srcset: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0it [00:00, ?it/s]    4it [00:00, 32.09it/s]    9it [00:00, 40.54it/s]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 8.198 seconds)


.. _sphx_glr_download_auto_examples_plot_benchmark_rf.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_benchmark_rf.ipynb <plot_benchmark_rf.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_benchmark_rf.py <plot_benchmark_rf.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
