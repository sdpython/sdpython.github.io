
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_benchmark_rf.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_benchmark_rf.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_benchmark_rf.py:


.. _l-example-benchmark-tree-implementation:

Benchmark of TreeEnsemble implementation
========================================

The following example compares the inference time between
:epkg:`onnxruntime` and :class:`sklearn.ensemble.RandomForestRegressor`,
fow different number of estimators, max depth, and parallelization.
It does it for a fixed number of rows and features.

import and registration of necessary converters
++++++++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 15-64

.. code-block:: default

    import pickle
    import os
    import time
    from itertools import product

    import matplotlib.pyplot as plt
    import numpy
    import pandas
    from lightgbm import LGBMRegressor
    from onnxmltools.convert.lightgbm.operator_converters.LightGbm import convert_lightgbm
    from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost
    from onnxruntime import InferenceSession, SessionOptions
    from psutil import cpu_count
    from pyquickhelper.loghelper import run_cmd
    from skl2onnx import to_onnx, update_registered_converter
    from skl2onnx.common.shape_calculator import calculate_linear_regressor_output_shapes
    from sklearn import set_config
    from sklearn.ensemble import RandomForestRegressor
    from tqdm import tqdm
    from xgboost import XGBRegressor


    def skl2onnx_convert_lightgbm(scope, operator, container):
        options = scope.get_options(operator.raw_operator)
        if "split" in options:
            operator.split = options["split"]
        else:
            operator.split = None
        convert_lightgbm(scope, operator, container)


    update_registered_converter(
        LGBMRegressor,
        "LightGbmLGBMRegressor",
        calculate_linear_regressor_output_shapes,
        skl2onnx_convert_lightgbm,
        options={"split": None},
    )
    update_registered_converter(
        XGBRegressor,
        "XGBoostXGBRegressor",
        calculate_linear_regressor_output_shapes,
        convert_xgboost,
    )

    # The following instruction reduces the time spent by scikit-learn
    # to validate the data.
    set_config(assume_finite=True)








.. GENERATED FROM PYTHON SOURCE LINES 65-67

Machine details
+++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 67-71

.. code-block:: default



    print(f"Number of cores: {cpu_count()}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Number of cores: 8




.. GENERATED FROM PYTHON SOURCE LINES 72-74

But this information is not usually enough.
Let's extract the cache information.

.. GENERATED FROM PYTHON SOURCE LINES 74-81

.. code-block:: default


    try:
        out, err = run_cmd("lscpu")
        print(out)
    except Exception as e:
        print(f"lscpu not available: {e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    <Popen: returncode: None args: ['lscpu']>




.. GENERATED FROM PYTHON SOURCE LINES 82-83

Or with the following command.

.. GENERATED FROM PYTHON SOURCE LINES 83-86

.. code-block:: default

    out, err = run_cmd("cat /proc/cpuinfo")
    print(out)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    <Popen: returncode: None args: ['cat', '/proc/cpuinfo']>




.. GENERATED FROM PYTHON SOURCE LINES 87-89

Fonction to measure inference time
++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 89-120

.. code-block:: default



    def measure_inference(fct, X, repeat, max_time=5, quantile=1):
        """
        Run *repeat* times the same function on data *X*.

        :param fct: fonction to run
        :param X: data
        :param repeat: number of times to run
        :param max_time: maximum time to use to measure the inference
        :return: number of runs, sum of the time, average, median
        """
        times = []
        for n in range(repeat):
            perf = time.perf_counter()
            fct(X)
            delta = time.perf_counter() - perf
            times.append(delta)
            if len(times) < 3:
                continue
            if max_time is not None and sum(times) >= max_time:
                break
        times.sort()
        quantile = 0 if (len(times) - quantile * 2) < 3 else quantile
        if quantile == 0:
            tt = times
        else:
            tt = times[quantile:-quantile]
        return (len(times), sum(times), sum(tt) / len(tt), times[len(times) // 2])









.. GENERATED FROM PYTHON SOURCE LINES 121-127

Benchmark
+++++++++

The following script benchmarks the inference for the same
model for a random forest and onnxruntime after it was converted
into ONNX and for the following configurations.

.. GENERATED FROM PYTHON SOURCE LINES 127-149

.. code-block:: default


    small = cpu_count() < 12
    if small:
        N = 1000
        n_features = 10
        n_jobs = [1, cpu_count() // 2, cpu_count()]
        n_ests = [10, 20, 30]
        depth = [4, 6, 8, 10]
        Regressor = RandomForestRegressor
    else:
        N = 100000
        n_features = 50
        n_jobs = [cpu_count(), cpu_count() // 2, 1]
        n_ests = [100, 200, 400]
        depth = [6, 8, 10, 12, 14]
        Regressor = RandomForestRegressor

    legend = f"parallel-nf-{n_features}-"

    # avoid duplicates on machine with 1 or 2 cores.
    n_jobs = list(sorted(set(n_jobs), reverse=True))








.. GENERATED FROM PYTHON SOURCE LINES 150-151

Benchmark parameters

.. GENERATED FROM PYTHON SOURCE LINES 151-155

.. code-block:: default

    repeat = 7  # repeat n times the same inference
    quantile = 1  # exclude extreme times
    max_time = 5  # maximum number of seconds to spend on one configuration








.. GENERATED FROM PYTHON SOURCE LINES 156-157

Data

.. GENERATED FROM PYTHON SOURCE LINES 157-247

.. code-block:: default



    X = numpy.random.randn(N, n_features).astype(numpy.float32)
    noise = (numpy.random.randn(X.shape[0]) / (n_features // 5)).astype(numpy.float32)
    y = X.mean(axis=1) + noise
    n_train = min(N, N // 3)


    data = []
    couples = list(product(n_jobs, depth, n_ests))
    bar = tqdm(couples)
    cache_dir = "_cache"
    if not os.path.exists(cache_dir):
        os.mkdir(cache_dir)

    for n_j, max_depth, n_estimators in bar:
        if n_j == 1 and n_estimators > n_ests[0]:
            # skipping
            continue

        # parallelization
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.pkl"
        )
        if os.path.exists(cache_name):
            with open(cache_name, "rb") as f:
                rf = pickle.load(f)
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} train rf")
            if n_j == 1 and issubclass(Regressor, RandomForestRegressor):
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=-1)
                rf.fit(X[:n_train], y[:n_train])
                rf.n_jobs = 1
            else:
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_j)
                rf.fit(X[:n_train], y[:n_train])
            with open(cache_name, "wb") as f:
                pickle.dump(rf, f)

        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} ISession")
        so = SessionOptions()
        so.intra_op_num_threads = n_j
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.onnx"
        )
        if os.path.exists(cache_name):
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} cvt onnx")
            onx = to_onnx(rf, X[:1])
            with open(cache_name, "wb") as f:
                f.write(onx.SerializeToString())
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        onx_size = os.stat(cache_name).st_size

        # run once to avoid counting the first run
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predict1")
        rf.predict(X)
        sess.run(None, {"X": X})

        # fixed data
        obs = dict(
            n_jobs=n_j,
            max_depth=max_depth,
            n_estimators=n_estimators,
            repeat=repeat,
            max_time=max_time,
            name=rf.__class__.__name__,
            n_rows=X.shape[0],
            n_features=X.shape[1],
            onnx_size=onx_size,
        )

        # baseline
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictB")
        r, t, mean, med = measure_inference(rf.predict, X, repeat=repeat, max_time=max_time)
        o1 = obs.copy()
        o1.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="base"))
        data.append(o1)

        # onnxruntime
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictO")
        r, t, mean, med = measure_inference(
            lambda x: sess.run(None, {"X": x}), X, repeat=repeat, max_time=max_time
        )
        o2 = obs.copy()
        o2.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="ort_"))
        data.append(o2)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 ISession:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predict1:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictB:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictO:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictO:   3%|2         | 1/36 [00:00<00:10,  3.21it/s]    J=8 E=20 D=4 ISession:   3%|2         | 1/36 [00:00<00:10,  3.21it/s]    J=8 E=20 D=4 predict1:   3%|2         | 1/36 [00:00<00:10,  3.21it/s]    J=8 E=20 D=4 predictB:   3%|2         | 1/36 [00:00<00:10,  3.21it/s]    J=8 E=20 D=4 predictO:   3%|2         | 1/36 [00:00<00:10,  3.21it/s]    J=8 E=20 D=4 predictO:   6%|5         | 2/36 [00:00<00:11,  3.04it/s]    J=8 E=30 D=4 ISession:   6%|5         | 2/36 [00:00<00:11,  3.04it/s]    J=8 E=30 D=4 predict1:   6%|5         | 2/36 [00:00<00:11,  3.04it/s]    J=8 E=30 D=4 predictB:   6%|5         | 2/36 [00:00<00:11,  3.04it/s]    J=8 E=30 D=4 predictO:   6%|5         | 2/36 [00:01<00:11,  3.04it/s]    J=8 E=30 D=4 predictO:   8%|8         | 3/36 [00:01<00:12,  2.55it/s]    J=8 E=10 D=6 ISession:   8%|8         | 3/36 [00:01<00:12,  2.55it/s]    J=8 E=10 D=6 predict1:   8%|8         | 3/36 [00:01<00:12,  2.55it/s]    J=8 E=10 D=6 predictB:   8%|8         | 3/36 [00:01<00:12,  2.55it/s]    J=8 E=10 D=6 predictO:   8%|8         | 3/36 [00:01<00:12,  2.55it/s]    J=8 E=10 D=6 predictO:  11%|#1        | 4/36 [00:01<00:09,  3.24it/s]    J=8 E=20 D=6 ISession:  11%|#1        | 4/36 [00:01<00:09,  3.24it/s]    J=8 E=20 D=6 predict1:  11%|#1        | 4/36 [00:01<00:09,  3.24it/s]    J=8 E=20 D=6 predictB:  11%|#1        | 4/36 [00:01<00:09,  3.24it/s]    J=8 E=20 D=6 predictO:  11%|#1        | 4/36 [00:01<00:09,  3.24it/s]    J=8 E=20 D=6 predictO:  14%|#3        | 5/36 [00:01<00:09,  3.13it/s]    J=8 E=30 D=6 ISession:  14%|#3        | 5/36 [00:01<00:09,  3.13it/s]    J=8 E=30 D=6 predict1:  14%|#3        | 5/36 [00:01<00:09,  3.13it/s]    J=8 E=30 D=6 predictB:  14%|#3        | 5/36 [00:01<00:09,  3.13it/s]    J=8 E=30 D=6 predictO:  14%|#3        | 5/36 [00:02<00:09,  3.13it/s]    J=8 E=30 D=6 predictO:  17%|#6        | 6/36 [00:02<00:11,  2.65it/s]    J=8 E=10 D=8 ISession:  17%|#6        | 6/36 [00:02<00:11,  2.65it/s]    J=8 E=10 D=8 predict1:  17%|#6        | 6/36 [00:02<00:11,  2.65it/s]    J=8 E=10 D=8 predictB:  17%|#6        | 6/36 [00:02<00:11,  2.65it/s]    J=8 E=10 D=8 predictO:  17%|#6        | 6/36 [00:02<00:11,  2.65it/s]    J=8 E=10 D=8 predictO:  19%|#9        | 7/36 [00:02<00:08,  3.30it/s]    J=8 E=20 D=8 ISession:  19%|#9        | 7/36 [00:02<00:08,  3.30it/s]    J=8 E=20 D=8 predict1:  19%|#9        | 7/36 [00:02<00:08,  3.30it/s]    J=8 E=20 D=8 predictB:  19%|#9        | 7/36 [00:02<00:08,  3.30it/s]    J=8 E=20 D=8 predictO:  19%|#9        | 7/36 [00:02<00:08,  3.30it/s]    J=8 E=20 D=8 predictO:  22%|##2       | 8/36 [00:02<00:08,  3.38it/s]    J=8 E=30 D=8 ISession:  22%|##2       | 8/36 [00:02<00:08,  3.38it/s]    J=8 E=30 D=8 predict1:  22%|##2       | 8/36 [00:02<00:08,  3.38it/s]    J=8 E=30 D=8 predictB:  22%|##2       | 8/36 [00:02<00:08,  3.38it/s]    J=8 E=30 D=8 predictO:  22%|##2       | 8/36 [00:03<00:08,  3.38it/s]    J=8 E=30 D=8 predictO:  25%|##5       | 9/36 [00:03<00:09,  2.71it/s]    J=8 E=10 D=10 ISession:  25%|##5       | 9/36 [00:03<00:09,  2.71it/s]    J=8 E=10 D=10 predict1:  25%|##5       | 9/36 [00:03<00:09,  2.71it/s]    J=8 E=10 D=10 predictB:  25%|##5       | 9/36 [00:03<00:09,  2.71it/s]    J=8 E=10 D=10 predictO:  25%|##5       | 9/36 [00:03<00:09,  2.71it/s]    J=8 E=10 D=10 predictO:  28%|##7       | 10/36 [00:03<00:07,  3.26it/s]    J=8 E=20 D=10 ISession:  28%|##7       | 10/36 [00:03<00:07,  3.26it/s]    J=8 E=20 D=10 predict1:  28%|##7       | 10/36 [00:03<00:07,  3.26it/s]    J=8 E=20 D=10 predictB:  28%|##7       | 10/36 [00:03<00:07,  3.26it/s]    J=8 E=20 D=10 predictO:  28%|##7       | 10/36 [00:03<00:07,  3.26it/s]    J=8 E=20 D=10 predictO:  31%|###       | 11/36 [00:03<00:07,  3.41it/s]    J=8 E=30 D=10 ISession:  31%|###       | 11/36 [00:03<00:07,  3.41it/s]    J=8 E=30 D=10 predict1:  31%|###       | 11/36 [00:03<00:07,  3.41it/s]    J=8 E=30 D=10 predictB:  31%|###       | 11/36 [00:03<00:07,  3.41it/s]    J=8 E=30 D=10 predictO:  31%|###       | 11/36 [00:03<00:07,  3.41it/s]    J=8 E=30 D=10 predictO:  33%|###3      | 12/36 [00:03<00:07,  3.23it/s]    J=4 E=10 D=4 ISession:  33%|###3      | 12/36 [00:03<00:07,  3.23it/s]     J=4 E=10 D=4 predict1:  33%|###3      | 12/36 [00:03<00:07,  3.23it/s]    J=4 E=10 D=4 predictB:  33%|###3      | 12/36 [00:03<00:07,  3.23it/s]    J=4 E=10 D=4 predictO:  33%|###3      | 12/36 [00:03<00:07,  3.23it/s]    J=4 E=10 D=4 predictO:  36%|###6      | 13/36 [00:03<00:05,  4.03it/s]    J=4 E=20 D=4 ISession:  36%|###6      | 13/36 [00:03<00:05,  4.03it/s]    J=4 E=20 D=4 predict1:  36%|###6      | 13/36 [00:03<00:05,  4.03it/s]    J=4 E=20 D=4 predictB:  36%|###6      | 13/36 [00:03<00:05,  4.03it/s]    J=4 E=20 D=4 predictO:  36%|###6      | 13/36 [00:04<00:05,  4.03it/s]    J=4 E=20 D=4 predictO:  39%|###8      | 14/36 [00:04<00:05,  3.94it/s]    J=4 E=30 D=4 ISession:  39%|###8      | 14/36 [00:04<00:05,  3.94it/s]    J=4 E=30 D=4 predict1:  39%|###8      | 14/36 [00:04<00:05,  3.94it/s]    J=4 E=30 D=4 predictB:  39%|###8      | 14/36 [00:04<00:05,  3.94it/s]    J=4 E=30 D=4 predictO:  39%|###8      | 14/36 [00:04<00:05,  3.94it/s]    J=4 E=30 D=4 predictO:  42%|####1     | 15/36 [00:04<00:07,  2.82it/s]    J=4 E=10 D=6 ISession:  42%|####1     | 15/36 [00:04<00:07,  2.82it/s]    J=4 E=10 D=6 predict1:  42%|####1     | 15/36 [00:04<00:07,  2.82it/s]    J=4 E=10 D=6 predictB:  42%|####1     | 15/36 [00:04<00:07,  2.82it/s]    J=4 E=10 D=6 predictO:  42%|####1     | 15/36 [00:04<00:07,  2.82it/s]    J=4 E=10 D=6 predictO:  44%|####4     | 16/36 [00:04<00:05,  3.56it/s]    J=4 E=20 D=6 ISession:  44%|####4     | 16/36 [00:04<00:05,  3.56it/s]    J=4 E=20 D=6 predict1:  44%|####4     | 16/36 [00:04<00:05,  3.56it/s]    J=4 E=20 D=6 predictB:  44%|####4     | 16/36 [00:04<00:05,  3.56it/s]    J=4 E=20 D=6 predictO:  44%|####4     | 16/36 [00:05<00:05,  3.56it/s]    J=4 E=20 D=6 predictO:  47%|####7     | 17/36 [00:05<00:04,  3.84it/s]    J=4 E=30 D=6 ISession:  47%|####7     | 17/36 [00:05<00:04,  3.84it/s]    J=4 E=30 D=6 predict1:  47%|####7     | 17/36 [00:05<00:04,  3.84it/s]    J=4 E=30 D=6 predictB:  47%|####7     | 17/36 [00:05<00:04,  3.84it/s]    J=4 E=30 D=6 predictO:  47%|####7     | 17/36 [00:05<00:04,  3.84it/s]    J=4 E=30 D=6 predictO:  50%|#####     | 18/36 [00:05<00:05,  3.59it/s]    J=4 E=10 D=8 ISession:  50%|#####     | 18/36 [00:05<00:05,  3.59it/s]    J=4 E=10 D=8 predict1:  50%|#####     | 18/36 [00:05<00:05,  3.59it/s]    J=4 E=10 D=8 predictB:  50%|#####     | 18/36 [00:05<00:05,  3.59it/s]    J=4 E=10 D=8 predictO:  50%|#####     | 18/36 [00:05<00:05,  3.59it/s]    J=4 E=10 D=8 predictO:  53%|#####2    | 19/36 [00:05<00:04,  4.12it/s]    J=4 E=20 D=8 ISession:  53%|#####2    | 19/36 [00:05<00:04,  4.12it/s]    J=4 E=20 D=8 predict1:  53%|#####2    | 19/36 [00:05<00:04,  4.12it/s]    J=4 E=20 D=8 predictB:  53%|#####2    | 19/36 [00:05<00:04,  4.12it/s]    J=4 E=20 D=8 predictO:  53%|#####2    | 19/36 [00:05<00:04,  4.12it/s]    J=4 E=20 D=8 predictO:  56%|#####5    | 20/36 [00:05<00:03,  4.04it/s]    J=4 E=30 D=8 ISession:  56%|#####5    | 20/36 [00:05<00:03,  4.04it/s]    J=4 E=30 D=8 predict1:  56%|#####5    | 20/36 [00:05<00:03,  4.04it/s]    J=4 E=30 D=8 predictB:  56%|#####5    | 20/36 [00:05<00:03,  4.04it/s]    J=4 E=30 D=8 predictO:  56%|#####5    | 20/36 [00:06<00:03,  4.04it/s]    J=4 E=30 D=8 predictO:  58%|#####8    | 21/36 [00:06<00:05,  2.91it/s]    J=4 E=10 D=10 ISession:  58%|#####8    | 21/36 [00:06<00:05,  2.91it/s]    J=4 E=10 D=10 predict1:  58%|#####8    | 21/36 [00:06<00:05,  2.91it/s]    J=4 E=10 D=10 predictB:  58%|#####8    | 21/36 [00:06<00:05,  2.91it/s]    J=4 E=10 D=10 predictO:  58%|#####8    | 21/36 [00:06<00:05,  2.91it/s]    J=4 E=10 D=10 predictO:  61%|######1   | 22/36 [00:06<00:03,  3.64it/s]    J=4 E=20 D=10 ISession:  61%|######1   | 22/36 [00:06<00:03,  3.64it/s]    J=4 E=20 D=10 predict1:  61%|######1   | 22/36 [00:06<00:03,  3.64it/s]    J=4 E=20 D=10 predictB:  61%|######1   | 22/36 [00:06<00:03,  3.64it/s]    J=4 E=20 D=10 predictO:  61%|######1   | 22/36 [00:06<00:03,  3.64it/s]    J=4 E=20 D=10 predictO:  64%|######3   | 23/36 [00:06<00:03,  3.78it/s]    J=4 E=30 D=10 ISession:  64%|######3   | 23/36 [00:06<00:03,  3.78it/s]    J=4 E=30 D=10 predict1:  64%|######3   | 23/36 [00:06<00:03,  3.78it/s]    J=4 E=30 D=10 predictB:  64%|######3   | 23/36 [00:06<00:03,  3.78it/s]    J=4 E=30 D=10 predictO:  64%|######3   | 23/36 [00:07<00:03,  3.78it/s]    J=4 E=30 D=10 predictO:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=4 ISession:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]     J=1 E=10 D=4 predict1:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=4 predictB:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=4 predictO:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=6 ISession:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=6 predict1:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=6 predictB:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=6 predictO:  67%|######6   | 24/36 [00:07<00:04,  2.49it/s]    J=1 E=10 D=6 predictO:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=8 ISession:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=8 predict1:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=8 predictB:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=8 predictO:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=10 ISession:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=10 predict1:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=10 predictB:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=10 predictO:  78%|#######7  | 28/36 [00:07<00:01,  5.91it/s]    J=1 E=10 D=10 predictO:  94%|#########4| 34/36 [00:07<00:00, 12.11it/s]    J=1 E=10 D=10 predictO: 100%|##########| 36/36 [00:07<00:00,  4.63it/s]




.. GENERATED FROM PYTHON SOURCE LINES 248-250

Saving data
+++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 250-259

.. code-block:: default


    name = os.path.join(cache_dir, "plot_beanchmark_rf")
    print(f"Saving data into {name!r}")

    df = pandas.DataFrame(data)
    df2 = df.copy()
    df2["legend"] = legend
    df2.to_csv(f"{name}-{legend}.csv", index=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Saving data into '_cache/plot_beanchmark_rf'




.. GENERATED FROM PYTHON SOURCE LINES 260-261

Printing the data

.. GENERATED FROM PYTHON SOURCE LINES 261-263

.. code-block:: default

    df






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>n_jobs</th>
          <th>max_depth</th>
          <th>n_estimators</th>
          <th>repeat</th>
          <th>max_time</th>
          <th>name</th>
          <th>n_rows</th>
          <th>n_features</th>
          <th>onnx_size</th>
          <th>avg</th>
          <th>med</th>
          <th>n_runs</th>
          <th>ttime</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11089</td>
          <td>0.018438</td>
          <td>0.019370</td>
          <td>7</td>
          <td>0.136712</td>
        </tr>
        <tr>
          <th>1</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11089</td>
          <td>0.000251</td>
          <td>0.000250</td>
          <td>7</td>
          <td>0.031749</td>
        </tr>
        <tr>
          <th>2</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>21920</td>
          <td>0.036034</td>
          <td>0.035185</td>
          <td>7</td>
          <td>0.260255</td>
        </tr>
        <tr>
          <th>3</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>21920</td>
          <td>0.000802</td>
          <td>0.000737</td>
          <td>7</td>
          <td>0.006384</td>
        </tr>
        <tr>
          <th>4</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>32822</td>
          <td>0.050198</td>
          <td>0.051844</td>
          <td>7</td>
          <td>0.356246</td>
        </tr>
        <tr>
          <th>5</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>32822</td>
          <td>0.004465</td>
          <td>0.003182</td>
          <td>7</td>
          <td>0.039865</td>
        </tr>
        <tr>
          <th>6</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>34816</td>
          <td>0.011063</td>
          <td>0.011212</td>
          <td>7</td>
          <td>0.076032</td>
        </tr>
        <tr>
          <th>7</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>34816</td>
          <td>0.000263</td>
          <td>0.000250</td>
          <td>7</td>
          <td>0.002191</td>
        </tr>
        <tr>
          <th>8</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>68349</td>
          <td>0.033899</td>
          <td>0.026755</td>
          <td>7</td>
          <td>0.252523</td>
        </tr>
        <tr>
          <th>9</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>68349</td>
          <td>0.002387</td>
          <td>0.001693</td>
          <td>7</td>
          <td>0.019931</td>
        </tr>
        <tr>
          <th>10</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>102465</td>
          <td>0.050146</td>
          <td>0.053385</td>
          <td>7</td>
          <td>0.363663</td>
        </tr>
        <tr>
          <th>11</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>102465</td>
          <td>0.000463</td>
          <td>0.000464</td>
          <td>7</td>
          <td>0.003635</td>
        </tr>
        <tr>
          <th>12</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>72981</td>
          <td>0.016923</td>
          <td>0.017388</td>
          <td>7</td>
          <td>0.119821</td>
        </tr>
        <tr>
          <th>13</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>72981</td>
          <td>0.000318</td>
          <td>0.000310</td>
          <td>7</td>
          <td>0.002693</td>
        </tr>
        <tr>
          <th>14</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>143302</td>
          <td>0.022924</td>
          <td>0.022552</td>
          <td>7</td>
          <td>0.186691</td>
        </tr>
        <tr>
          <th>15</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>143302</td>
          <td>0.000907</td>
          <td>0.000902</td>
          <td>7</td>
          <td>0.007435</td>
        </tr>
        <tr>
          <th>16</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>214018</td>
          <td>0.052673</td>
          <td>0.054076</td>
          <td>7</td>
          <td>0.379242</td>
        </tr>
        <tr>
          <th>17</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>214018</td>
          <td>0.003152</td>
          <td>0.003008</td>
          <td>7</td>
          <td>0.022111</td>
        </tr>
        <tr>
          <th>18</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>114952</td>
          <td>0.010943</td>
          <td>0.011776</td>
          <td>7</td>
          <td>0.074539</td>
        </tr>
        <tr>
          <th>19</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>114952</td>
          <td>0.000285</td>
          <td>0.000290</td>
          <td>7</td>
          <td>0.002274</td>
        </tr>
        <tr>
          <th>20</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>226847</td>
          <td>0.021003</td>
          <td>0.017283</td>
          <td>7</td>
          <td>0.172294</td>
        </tr>
        <tr>
          <th>21</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>226847</td>
          <td>0.003082</td>
          <td>0.002391</td>
          <td>7</td>
          <td>0.027456</td>
        </tr>
        <tr>
          <th>22</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>335254</td>
          <td>0.027012</td>
          <td>0.027543</td>
          <td>7</td>
          <td>0.186998</td>
        </tr>
        <tr>
          <th>23</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>335254</td>
          <td>0.000532</td>
          <td>0.000522</td>
          <td>7</td>
          <td>0.004027</td>
        </tr>
        <tr>
          <th>24</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11673</td>
          <td>0.011657</td>
          <td>0.011900</td>
          <td>7</td>
          <td>0.080858</td>
        </tr>
        <tr>
          <th>25</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11673</td>
          <td>0.000161</td>
          <td>0.000160</td>
          <td>7</td>
          <td>0.001452</td>
        </tr>
        <tr>
          <th>26</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>22212</td>
          <td>0.031776</td>
          <td>0.029280</td>
          <td>7</td>
          <td>0.234549</td>
        </tr>
        <tr>
          <th>27</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>22212</td>
          <td>0.000914</td>
          <td>0.000867</td>
          <td>7</td>
          <td>0.006867</td>
        </tr>
        <tr>
          <th>28</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>32749</td>
          <td>0.067641</td>
          <td>0.069188</td>
          <td>7</td>
          <td>0.454741</td>
        </tr>
        <tr>
          <th>29</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>32749</td>
          <td>0.001368</td>
          <td>0.001177</td>
          <td>7</td>
          <td>0.010390</td>
        </tr>
        <tr>
          <th>30</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>30290</td>
          <td>0.010774</td>
          <td>0.010910</td>
          <td>7</td>
          <td>0.076307</td>
        </tr>
        <tr>
          <th>31</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>30290</td>
          <td>0.000276</td>
          <td>0.000266</td>
          <td>7</td>
          <td>0.002202</td>
        </tr>
        <tr>
          <th>32</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>67619</td>
          <td>0.024731</td>
          <td>0.025411</td>
          <td>7</td>
          <td>0.174263</td>
        </tr>
        <tr>
          <th>33</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>67619</td>
          <td>0.000677</td>
          <td>0.000668</td>
          <td>7</td>
          <td>0.005010</td>
        </tr>
        <tr>
          <th>34</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>101516</td>
          <td>0.037157</td>
          <td>0.037504</td>
          <td>7</td>
          <td>0.260565</td>
        </tr>
        <tr>
          <th>35</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>101516</td>
          <td>0.001012</td>
          <td>0.000993</td>
          <td>7</td>
          <td>0.007345</td>
        </tr>
        <tr>
          <th>36</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>71504</td>
          <td>0.017118</td>
          <td>0.016857</td>
          <td>7</td>
          <td>0.120895</td>
        </tr>
        <tr>
          <th>37</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>71504</td>
          <td>0.000348</td>
          <td>0.000326</td>
          <td>7</td>
          <td>0.002850</td>
        </tr>
        <tr>
          <th>38</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>144705</td>
          <td>0.027505</td>
          <td>0.027140</td>
          <td>7</td>
          <td>0.194985</td>
        </tr>
        <tr>
          <th>39</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>144705</td>
          <td>0.001045</td>
          <td>0.000995</td>
          <td>7</td>
          <td>0.008861</td>
        </tr>
        <tr>
          <th>40</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>219945</td>
          <td>0.060631</td>
          <td>0.070707</td>
          <td>7</td>
          <td>0.432751</td>
        </tr>
        <tr>
          <th>41</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>219945</td>
          <td>0.006048</td>
          <td>0.006149</td>
          <td>7</td>
          <td>0.044378</td>
        </tr>
        <tr>
          <th>42</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>107465</td>
          <td>0.010329</td>
          <td>0.010345</td>
          <td>7</td>
          <td>0.073477</td>
        </tr>
        <tr>
          <th>43</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>107465</td>
          <td>0.000385</td>
          <td>0.000383</td>
          <td>7</td>
          <td>0.002908</td>
        </tr>
        <tr>
          <th>44</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>221779</td>
          <td>0.024395</td>
          <td>0.019348</td>
          <td>7</td>
          <td>0.191750</td>
        </tr>
        <tr>
          <th>45</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>221779</td>
          <td>0.001693</td>
          <td>0.001670</td>
          <td>7</td>
          <td>0.012515</td>
        </tr>
        <tr>
          <th>46</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>331281</td>
          <td>0.074872</td>
          <td>0.075112</td>
          <td>7</td>
          <td>0.524743</td>
        </tr>
        <tr>
          <th>47</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>331281</td>
          <td>0.002620</td>
          <td>0.002624</td>
          <td>7</td>
          <td>0.018602</td>
        </tr>
        <tr>
          <th>48</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11600</td>
          <td>0.005666</td>
          <td>0.005590</td>
          <td>7</td>
          <td>0.042866</td>
        </tr>
        <tr>
          <th>49</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11600</td>
          <td>0.000891</td>
          <td>0.000879</td>
          <td>7</td>
          <td>0.006338</td>
        </tr>
        <tr>
          <th>50</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>34159</td>
          <td>0.005124</td>
          <td>0.005393</td>
          <td>7</td>
          <td>0.035771</td>
        </tr>
        <tr>
          <th>51</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>34159</td>
          <td>0.000688</td>
          <td>0.000698</td>
          <td>7</td>
          <td>0.004806</td>
        </tr>
        <tr>
          <th>52</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>70489</td>
          <td>0.002758</td>
          <td>0.002858</td>
          <td>7</td>
          <td>0.019880</td>
        </tr>
        <tr>
          <th>53</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>70489</td>
          <td>0.001117</td>
          <td>0.001103</td>
          <td>7</td>
          <td>0.007843</td>
        </tr>
        <tr>
          <th>54</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>110428</td>
          <td>0.005389</td>
          <td>0.005389</td>
          <td>7</td>
          <td>0.037912</td>
        </tr>
        <tr>
          <th>55</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>110428</td>
          <td>0.001929</td>
          <td>0.001972</td>
          <td>7</td>
          <td>0.013552</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 264-266

Plot
++++

.. GENERATED FROM PYTHON SOURCE LINES 266-308

.. code-block:: default


    n_rows = len(n_jobs)
    n_cols = len(n_ests)


    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))
    fig.suptitle(f"{rf.__class__.__name__}\nX.shape={X.shape}")

    for n_j, n_estimators in tqdm(product(n_jobs, n_ests)):
        i = n_jobs.index(n_j)
        j = n_ests.index(n_estimators)
        ax = axes[i, j]

        subdf = df[(df.n_estimators == n_estimators) & (df.n_jobs == n_j)]
        if subdf.shape[0] == 0:
            continue
        piv = subdf.pivot(index="max_depth", columns="name", values=["avg", "med"])
        piv.plot(ax=ax, title=f"jobs={n_j}, trees={n_estimators}")
        ax.set_ylabel(f"n_jobs={n_j}", fontsize="small")
        ax.set_xlabel("max_depth", fontsize="small")

        # ratio
        ax2 = ax.twinx()
        piv1 = subdf.pivot(index="max_depth", columns="name", values="avg")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "b--", label="speedup avg")

        piv1 = subdf.pivot(index="max_depth", columns="name", values="med")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "y--", label="speedup med")
        ax2.legend(fontsize="x-small")

        # 1
        ax2.plot(piv1.index, [1 for _ in piv1.index], "k--", label="no speedup")

    for i in range(axes.shape[0]):
        for j in range(axes.shape[1]):
            axes[i, j].legend(fontsize="small")

    fig.tight_layout()
    fig.savefig(f"{name}-{legend}.png")
    # plt.show()



.. image-sg:: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :alt: RandomForestRegressor X.shape=(1000, 10), jobs=8, trees=10, jobs=8, trees=20, jobs=8, trees=30, jobs=4, trees=10, jobs=4, trees=20, jobs=4, trees=30, jobs=1, trees=10
   :srcset: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0it [00:00, ?it/s]    1it [00:00,  4.90it/s]    2it [00:00,  3.39it/s]    3it [00:00,  4.65it/s]    4it [00:00,  5.54it/s]    6it [00:00,  7.82it/s]    9it [00:01,  8.61it/s]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  18.814 seconds)


.. _sphx_glr_download_auto_examples_plot_benchmark_rf.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_benchmark_rf.py <plot_benchmark_rf.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_benchmark_rf.ipynb <plot_benchmark_rf.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
