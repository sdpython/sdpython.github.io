
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_benchmark_rf.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_benchmark_rf.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_benchmark_rf.py:


.. _l-example-benchmark-tree-implementation:

Benchmark of TreeEnsemble implementation
========================================

The following example compares the inference time between
:epkg:`onnxruntime` and :class:`sklearn.ensemble.RandomForestRegressor`,
fow different number of estimators, max depth, and parallelization.
It does it for a fixed number of rows and features.

import and registration of necessary converters
++++++++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 15-64

.. code-block:: Python

    import pickle
    import os
    import time
    from itertools import product

    import matplotlib.pyplot as plt
    import numpy
    import pandas
    from lightgbm import LGBMRegressor
    from onnxmltools.convert.lightgbm.operator_converters.LightGbm import convert_lightgbm
    from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost
    from onnxruntime import InferenceSession, SessionOptions
    from psutil import cpu_count
    from sphinx_runpython.runpython import run_cmd
    from skl2onnx import to_onnx, update_registered_converter
    from skl2onnx.common.shape_calculator import calculate_linear_regressor_output_shapes
    from sklearn import set_config
    from sklearn.ensemble import RandomForestRegressor
    from tqdm import tqdm
    from xgboost import XGBRegressor


    def skl2onnx_convert_lightgbm(scope, operator, container):
        options = scope.get_options(operator.raw_operator)
        if "split" in options:
            operator.split = options["split"]
        else:
            operator.split = None
        convert_lightgbm(scope, operator, container)


    update_registered_converter(
        LGBMRegressor,
        "LightGbmLGBMRegressor",
        calculate_linear_regressor_output_shapes,
        skl2onnx_convert_lightgbm,
        options={"split": None},
    )
    update_registered_converter(
        XGBRegressor,
        "XGBoostXGBRegressor",
        calculate_linear_regressor_output_shapes,
        convert_xgboost,
    )

    # The following instruction reduces the time spent by scikit-learn
    # to validate the data.
    set_config(assume_finite=True)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [2023-12-28 19:25:43,247] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)




.. GENERATED FROM PYTHON SOURCE LINES 65-67

Machine details
+++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 67-71

.. code-block:: Python



    print(f"Number of cores: {cpu_count()}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Number of cores: 8




.. GENERATED FROM PYTHON SOURCE LINES 72-74

But this information is not usually enough.
Let's extract the cache information.

.. GENERATED FROM PYTHON SOURCE LINES 74-81

.. code-block:: Python


    try:
        out, err = run_cmd("lscpu")
        print(out)
    except Exception as e:
        print(f"lscpu not available: {e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    <Popen: returncode: None args: ['lscpu']>




.. GENERATED FROM PYTHON SOURCE LINES 82-83

Or with the following command.

.. GENERATED FROM PYTHON SOURCE LINES 83-86

.. code-block:: Python

    out, err = run_cmd("cat /proc/cpuinfo")
    print(out)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    <Popen: returncode: None args: ['cat', '/proc/cpuinfo']>




.. GENERATED FROM PYTHON SOURCE LINES 87-89

Fonction to measure inference time
++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 89-120

.. code-block:: Python



    def measure_inference(fct, X, repeat, max_time=5, quantile=1):
        """
        Run *repeat* times the same function on data *X*.

        :param fct: fonction to run
        :param X: data
        :param repeat: number of times to run
        :param max_time: maximum time to use to measure the inference
        :return: number of runs, sum of the time, average, median
        """
        times = []
        for n in range(repeat):
            perf = time.perf_counter()
            fct(X)
            delta = time.perf_counter() - perf
            times.append(delta)
            if len(times) < 3:
                continue
            if max_time is not None and sum(times) >= max_time:
                break
        times.sort()
        quantile = 0 if (len(times) - quantile * 2) < 3 else quantile
        if quantile == 0:
            tt = times
        else:
            tt = times[quantile:-quantile]
        return (len(times), sum(times), sum(tt) / len(tt), times[len(times) // 2])









.. GENERATED FROM PYTHON SOURCE LINES 121-127

Benchmark
+++++++++

The following script benchmarks the inference for the same
model for a random forest and onnxruntime after it was converted
into ONNX and for the following configurations.

.. GENERATED FROM PYTHON SOURCE LINES 127-149

.. code-block:: Python


    small = cpu_count() < 12
    if small:
        N = 1000
        n_features = 10
        n_jobs = [1, cpu_count() // 2, cpu_count()]
        n_ests = [10, 20, 30]
        depth = [4, 6, 8, 10]
        Regressor = RandomForestRegressor
    else:
        N = 100000
        n_features = 50
        n_jobs = [cpu_count(), cpu_count() // 2, 1]
        n_ests = [100, 200, 400]
        depth = [6, 8, 10, 12, 14]
        Regressor = RandomForestRegressor

    legend = f"parallel-nf-{n_features}-"

    # avoid duplicates on machine with 1 or 2 cores.
    n_jobs = list(sorted(set(n_jobs), reverse=True))








.. GENERATED FROM PYTHON SOURCE LINES 150-151

Benchmark parameters

.. GENERATED FROM PYTHON SOURCE LINES 151-155

.. code-block:: Python

    repeat = 7  # repeat n times the same inference
    quantile = 1  # exclude extreme times
    max_time = 5  # maximum number of seconds to spend on one configuration








.. GENERATED FROM PYTHON SOURCE LINES 156-157

Data

.. GENERATED FROM PYTHON SOURCE LINES 157-247

.. code-block:: Python



    X = numpy.random.randn(N, n_features).astype(numpy.float32)
    noise = (numpy.random.randn(X.shape[0]) / (n_features // 5)).astype(numpy.float32)
    y = X.mean(axis=1) + noise
    n_train = min(N, N // 3)


    data = []
    couples = list(product(n_jobs, depth, n_ests))
    bar = tqdm(couples)
    cache_dir = "_cache"
    if not os.path.exists(cache_dir):
        os.mkdir(cache_dir)

    for n_j, max_depth, n_estimators in bar:
        if n_j == 1 and n_estimators > n_ests[0]:
            # skipping
            continue

        # parallelization
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.pkl"
        )
        if os.path.exists(cache_name):
            with open(cache_name, "rb") as f:
                rf = pickle.load(f)
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} train rf")
            if n_j == 1 and issubclass(Regressor, RandomForestRegressor):
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=-1)
                rf.fit(X[:n_train], y[:n_train])
                rf.n_jobs = 1
            else:
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_j)
                rf.fit(X[:n_train], y[:n_train])
            with open(cache_name, "wb") as f:
                pickle.dump(rf, f)

        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} ISession")
        so = SessionOptions()
        so.intra_op_num_threads = n_j
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.onnx"
        )
        if os.path.exists(cache_name):
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} cvt onnx")
            onx = to_onnx(rf, X[:1])
            with open(cache_name, "wb") as f:
                f.write(onx.SerializeToString())
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        onx_size = os.stat(cache_name).st_size

        # run once to avoid counting the first run
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predict1")
        rf.predict(X)
        sess.run(None, {"X": X})

        # fixed data
        obs = dict(
            n_jobs=n_j,
            max_depth=max_depth,
            n_estimators=n_estimators,
            repeat=repeat,
            max_time=max_time,
            name=rf.__class__.__name__,
            n_rows=X.shape[0],
            n_features=X.shape[1],
            onnx_size=onx_size,
        )

        # baseline
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictB")
        r, t, mean, med = measure_inference(rf.predict, X, repeat=repeat, max_time=max_time)
        o1 = obs.copy()
        o1.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="base"))
        data.append(o1)

        # onnxruntime
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictO")
        r, t, mean, med = measure_inference(
            lambda x: sess.run(None, {"X": x}), X, repeat=repeat, max_time=max_time
        )
        o2 = obs.copy()
        o2.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="ort_"))
        data.append(o2)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 ISession:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predict1:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictB:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictO:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictO:   3%|▎         | 1/36 [00:00<00:04,  7.71it/s]    J=8 E=20 D=4 ISession:   3%|▎         | 1/36 [00:00<00:04,  7.71it/s]    J=8 E=20 D=4 predict1:   3%|▎         | 1/36 [00:00<00:04,  7.71it/s]    J=8 E=20 D=4 predictB:   3%|▎         | 1/36 [00:00<00:04,  7.71it/s]    J=8 E=20 D=4 predictO:   3%|▎         | 1/36 [00:00<00:04,  7.71it/s]    J=8 E=20 D=4 predictO:   6%|▌         | 2/36 [00:00<00:05,  6.56it/s]    J=8 E=30 D=4 ISession:   6%|▌         | 2/36 [00:00<00:05,  6.56it/s]    J=8 E=30 D=4 predict1:   6%|▌         | 2/36 [00:00<00:05,  6.56it/s]    J=8 E=30 D=4 predictB:   6%|▌         | 2/36 [00:00<00:05,  6.56it/s]    J=8 E=30 D=4 predictO:   6%|▌         | 2/36 [00:00<00:05,  6.56it/s]    J=8 E=30 D=4 predictO:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=10 D=6 ISession:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=10 D=6 predict1:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=10 D=6 predictB:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=10 D=6 predictO:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=20 D=6 ISession:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=20 D=6 predict1:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=20 D=6 predictB:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=20 D=6 predictO:   8%|▊         | 3/36 [00:00<00:06,  5.24it/s]    J=8 E=20 D=6 predictO:  14%|█▍        | 5/36 [00:00<00:04,  6.58it/s]    J=8 E=30 D=6 ISession:  14%|█▍        | 5/36 [00:00<00:04,  6.58it/s]    J=8 E=30 D=6 predict1:  14%|█▍        | 5/36 [00:00<00:04,  6.58it/s]    J=8 E=30 D=6 predictB:  14%|█▍        | 5/36 [00:00<00:04,  6.58it/s]    J=8 E=30 D=6 predictO:  14%|█▍        | 5/36 [00:00<00:04,  6.58it/s]    J=8 E=30 D=6 predictO:  17%|█▋        | 6/36 [00:01<00:05,  5.81it/s]    J=8 E=10 D=8 ISession:  17%|█▋        | 6/36 [00:01<00:05,  5.81it/s]    J=8 E=10 D=8 predict1:  17%|█▋        | 6/36 [00:01<00:05,  5.81it/s]    J=8 E=10 D=8 predictB:  17%|█▋        | 6/36 [00:01<00:05,  5.81it/s]    J=8 E=10 D=8 predictO:  17%|█▋        | 6/36 [00:01<00:05,  5.81it/s]    J=8 E=10 D=8 predictO:  19%|█▉        | 7/36 [00:01<00:04,  5.86it/s]    J=8 E=20 D=8 ISession:  19%|█▉        | 7/36 [00:01<00:04,  5.86it/s]    J=8 E=20 D=8 predict1:  19%|█▉        | 7/36 [00:01<00:04,  5.86it/s]    J=8 E=20 D=8 predictB:  19%|█▉        | 7/36 [00:01<00:04,  5.86it/s]    J=8 E=20 D=8 predictO:  19%|█▉        | 7/36 [00:01<00:04,  5.86it/s]    J=8 E=20 D=8 predictO:  22%|██▏       | 8/36 [00:01<00:05,  5.39it/s]    J=8 E=30 D=8 ISession:  22%|██▏       | 8/36 [00:01<00:05,  5.39it/s]    J=8 E=30 D=8 predict1:  22%|██▏       | 8/36 [00:01<00:05,  5.39it/s]    J=8 E=30 D=8 predictB:  22%|██▏       | 8/36 [00:01<00:05,  5.39it/s]    J=8 E=30 D=8 predictO:  22%|██▏       | 8/36 [00:01<00:05,  5.39it/s]    J=8 E=30 D=8 predictO:  25%|██▌       | 9/36 [00:01<00:05,  5.25it/s]    J=8 E=10 D=10 ISession:  25%|██▌       | 9/36 [00:01<00:05,  5.25it/s]    J=8 E=10 D=10 predict1:  25%|██▌       | 9/36 [00:01<00:05,  5.25it/s]    J=8 E=10 D=10 predictB:  25%|██▌       | 9/36 [00:01<00:05,  5.25it/s]    J=8 E=10 D=10 predictO:  25%|██▌       | 9/36 [00:01<00:05,  5.25it/s]    J=8 E=10 D=10 predictO:  28%|██▊       | 10/36 [00:01<00:04,  6.05it/s]    J=8 E=20 D=10 ISession:  28%|██▊       | 10/36 [00:01<00:04,  6.05it/s]    J=8 E=20 D=10 predict1:  28%|██▊       | 10/36 [00:01<00:04,  6.05it/s]    J=8 E=20 D=10 predictB:  28%|██▊       | 10/36 [00:01<00:04,  6.05it/s]    J=8 E=20 D=10 predictO:  28%|██▊       | 10/36 [00:01<00:04,  6.05it/s]    J=8 E=20 D=10 predictO:  31%|███       | 11/36 [00:01<00:04,  5.94it/s]    J=8 E=30 D=10 ISession:  31%|███       | 11/36 [00:01<00:04,  5.94it/s]    J=8 E=30 D=10 predict1:  31%|███       | 11/36 [00:01<00:04,  5.94it/s]    J=8 E=30 D=10 predictB:  31%|███       | 11/36 [00:01<00:04,  5.94it/s]    J=8 E=30 D=10 predictO:  31%|███       | 11/36 [00:02<00:04,  5.94it/s]    J=8 E=30 D=10 predictO:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=10 D=4 ISession:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]     J=4 E=10 D=4 predict1:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=10 D=4 predictB:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=10 D=4 predictO:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=20 D=4 ISession:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=20 D=4 predict1:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=20 D=4 predictB:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=20 D=4 predictO:  33%|███▎      | 12/36 [00:02<00:04,  5.15it/s]    J=4 E=20 D=4 predictO:  39%|███▉      | 14/36 [00:02<00:03,  6.45it/s]    J=4 E=30 D=4 ISession:  39%|███▉      | 14/36 [00:02<00:03,  6.45it/s]    J=4 E=30 D=4 predict1:  39%|███▉      | 14/36 [00:02<00:03,  6.45it/s]    J=4 E=30 D=4 predictB:  39%|███▉      | 14/36 [00:02<00:03,  6.45it/s]    J=4 E=30 D=4 predictO:  39%|███▉      | 14/36 [00:02<00:03,  6.45it/s]    J=4 E=30 D=4 predictO:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=10 D=6 ISession:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=10 D=6 predict1:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=10 D=6 predictB:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=10 D=6 predictO:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=20 D=6 ISession:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=20 D=6 predict1:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=20 D=6 predictB:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=20 D=6 predictO:  42%|████▏     | 15/36 [00:02<00:03,  6.22it/s]    J=4 E=20 D=6 predictO:  47%|████▋     | 17/36 [00:02<00:02,  7.24it/s]    J=4 E=30 D=6 ISession:  47%|████▋     | 17/36 [00:02<00:02,  7.24it/s]    J=4 E=30 D=6 predict1:  47%|████▋     | 17/36 [00:02<00:02,  7.24it/s]    J=4 E=30 D=6 predictB:  47%|████▋     | 17/36 [00:02<00:02,  7.24it/s]    J=4 E=30 D=6 predictO:  47%|████▋     | 17/36 [00:02<00:02,  7.24it/s]    J=4 E=30 D=6 predictO:  50%|█████     | 18/36 [00:02<00:02,  6.81it/s]    J=4 E=10 D=8 ISession:  50%|█████     | 18/36 [00:02<00:02,  6.81it/s]    J=4 E=10 D=8 predict1:  50%|█████     | 18/36 [00:02<00:02,  6.81it/s]    J=4 E=10 D=8 predictB:  50%|█████     | 18/36 [00:02<00:02,  6.81it/s]    J=4 E=10 D=8 predictO:  50%|█████     | 18/36 [00:02<00:02,  6.81it/s]    J=4 E=20 D=8 train rf:  50%|█████     | 18/36 [00:02<00:02,  6.81it/s]    J=4 E=20 D=8 ISession:  50%|█████     | 18/36 [00:11<00:02,  6.81it/s]    J=4 E=20 D=8 cvt onnx:  50%|█████     | 18/36 [00:11<00:02,  6.81it/s]    J=4 E=20 D=8 predict1:  50%|█████     | 18/36 [00:11<00:02,  6.81it/s]    J=4 E=20 D=8 predictB:  50%|█████     | 18/36 [00:11<00:02,  6.81it/s]    J=4 E=20 D=8 predictO:  50%|█████     | 18/36 [00:11<00:02,  6.81it/s]    J=4 E=20 D=8 predictO:  56%|█████▌    | 20/36 [00:11<00:29,  1.87s/it]    J=4 E=30 D=8 train rf:  56%|█████▌    | 20/36 [00:11<00:29,  1.87s/it]    J=4 E=30 D=8 ISession:  56%|█████▌    | 20/36 [00:25<00:29,  1.87s/it]    J=4 E=30 D=8 cvt onnx:  56%|█████▌    | 20/36 [00:25<00:29,  1.87s/it]    J=4 E=30 D=8 predict1:  56%|█████▌    | 20/36 [00:25<00:29,  1.87s/it]    J=4 E=30 D=8 predictB:  56%|█████▌    | 20/36 [00:25<00:29,  1.87s/it]    J=4 E=30 D=8 predictO:  56%|█████▌    | 20/36 [00:25<00:29,  1.87s/it]    J=4 E=30 D=8 predictO:  58%|█████▊    | 21/36 [00:25<01:07,  4.53s/it]    J=4 E=10 D=10 train rf:  58%|█████▊    | 21/36 [00:25<01:07,  4.53s/it]    J=4 E=10 D=10 ISession:  58%|█████▊    | 21/36 [00:30<01:07,  4.53s/it]    J=4 E=10 D=10 cvt onnx:  58%|█████▊    | 21/36 [00:30<01:07,  4.53s/it]    J=4 E=10 D=10 predict1:  58%|█████▊    | 21/36 [00:30<01:07,  4.53s/it]    J=4 E=10 D=10 predictB:  58%|█████▊    | 21/36 [00:30<01:07,  4.53s/it]    J=4 E=10 D=10 predictO:  58%|█████▊    | 21/36 [00:30<01:07,  4.53s/it]    J=4 E=10 D=10 predictO:  61%|██████    | 22/36 [00:30<01:04,  4.60s/it]    J=4 E=20 D=10 train rf:  61%|██████    | 22/36 [00:30<01:04,  4.60s/it]    J=4 E=20 D=10 ISession:  61%|██████    | 22/36 [00:42<01:04,  4.60s/it]    J=4 E=20 D=10 cvt onnx:  61%|██████    | 22/36 [00:42<01:04,  4.60s/it]    J=4 E=20 D=10 predict1:  61%|██████    | 22/36 [00:42<01:04,  4.60s/it]    J=4 E=20 D=10 predictB:  61%|██████    | 22/36 [00:42<01:04,  4.60s/it]    J=4 E=20 D=10 predictO:  61%|██████    | 22/36 [00:42<01:04,  4.60s/it]    J=4 E=20 D=10 predictO:  64%|██████▍   | 23/36 [00:42<01:24,  6.47s/it]    J=4 E=30 D=10 train rf:  64%|██████▍   | 23/36 [00:42<01:24,  6.47s/it]    J=4 E=30 D=10 ISession:  64%|██████▍   | 23/36 [00:59<01:24,  6.47s/it]    J=4 E=30 D=10 cvt onnx:  64%|██████▍   | 23/36 [00:59<01:24,  6.47s/it]    J=4 E=30 D=10 predict1:  64%|██████▍   | 23/36 [00:59<01:24,  6.47s/it]    J=4 E=30 D=10 predictB:  64%|██████▍   | 23/36 [00:59<01:24,  6.47s/it]    J=4 E=30 D=10 predictO:  64%|██████▍   | 23/36 [00:59<01:24,  6.47s/it]    J=4 E=30 D=10 predictO:  67%|██████▋   | 24/36 [00:59<01:51,  9.26s/it]    J=1 E=10 D=4 train rf:  67%|██████▋   | 24/36 [00:59<01:51,  9.26s/it]     J=1 E=10 D=4 ISession:  67%|██████▋   | 24/36 [01:11<01:51,  9.26s/it]    J=1 E=10 D=4 cvt onnx:  67%|██████▋   | 24/36 [01:11<01:51,  9.26s/it]    J=1 E=10 D=4 predict1:  67%|██████▋   | 24/36 [01:11<01:51,  9.26s/it]    J=1 E=10 D=4 predictB:  67%|██████▋   | 24/36 [01:11<01:51,  9.26s/it]    J=1 E=10 D=4 predictO:  67%|██████▋   | 24/36 [01:11<01:51,  9.26s/it]    J=1 E=10 D=4 predictO:  69%|██████▉   | 25/36 [01:11<01:50, 10.02s/it]    J=1 E=10 D=6 train rf:  69%|██████▉   | 25/36 [01:11<01:50, 10.02s/it]    J=1 E=10 D=6 ISession:  69%|██████▉   | 25/36 [01:29<01:50, 10.02s/it]    J=1 E=10 D=6 cvt onnx:  69%|██████▉   | 25/36 [01:29<01:50, 10.02s/it]    J=1 E=10 D=6 predict1:  69%|██████▉   | 25/36 [01:29<01:50, 10.02s/it]    J=1 E=10 D=6 predictB:  69%|██████▉   | 25/36 [01:29<01:50, 10.02s/it]    J=1 E=10 D=6 predictO:  69%|██████▉   | 25/36 [01:29<01:50, 10.02s/it]    J=1 E=10 D=6 predictO:  78%|███████▊  | 28/36 [01:29<01:03,  7.89s/it]    J=1 E=10 D=8 train rf:  78%|███████▊  | 28/36 [01:29<01:03,  7.89s/it]    J=1 E=10 D=8 ISession:  78%|███████▊  | 28/36 [01:52<01:03,  7.89s/it]    J=1 E=10 D=8 cvt onnx:  78%|███████▊  | 28/36 [01:52<01:03,  7.89s/it]    J=1 E=10 D=8 predict1:  78%|███████▊  | 28/36 [01:52<01:03,  7.89s/it]    J=1 E=10 D=8 predictB:  78%|███████▊  | 28/36 [01:52<01:03,  7.89s/it]    J=1 E=10 D=8 predictO:  78%|███████▊  | 28/36 [01:52<01:03,  7.89s/it]    J=1 E=10 D=8 predictO:  86%|████████▌ | 31/36 [01:52<00:38,  7.77s/it]    J=1 E=10 D=10 train rf:  86%|████████▌ | 31/36 [01:52<00:38,  7.77s/it]    J=1 E=10 D=10 ISession:  86%|████████▌ | 31/36 [02:20<00:38,  7.77s/it]    J=1 E=10 D=10 cvt onnx:  86%|████████▌ | 31/36 [02:20<00:38,  7.77s/it]    J=1 E=10 D=10 predict1:  86%|████████▌ | 31/36 [02:20<00:38,  7.77s/it]    J=1 E=10 D=10 predictB:  86%|████████▌ | 31/36 [02:20<00:38,  7.77s/it]    J=1 E=10 D=10 predictO:  86%|████████▌ | 31/36 [02:20<00:38,  7.77s/it]    J=1 E=10 D=10 predictO:  94%|█████████▍| 34/36 [02:20<00:16,  8.32s/it]    J=1 E=10 D=10 predictO: 100%|██████████| 36/36 [02:20<00:00,  3.90s/it]




.. GENERATED FROM PYTHON SOURCE LINES 248-250

Saving data
+++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 250-259

.. code-block:: Python


    name = os.path.join(cache_dir, "plot_beanchmark_rf")
    print(f"Saving data into {name!r}")

    df = pandas.DataFrame(data)
    df2 = df.copy()
    df2["legend"] = legend
    df2.to_csv(f"{name}-{legend}.csv", index=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Saving data into '_cache/plot_beanchmark_rf'




.. GENERATED FROM PYTHON SOURCE LINES 260-261

Printing the data

.. GENERATED FROM PYTHON SOURCE LINES 261-263

.. code-block:: Python

    df






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>n_jobs</th>
          <th>max_depth</th>
          <th>n_estimators</th>
          <th>repeat</th>
          <th>max_time</th>
          <th>name</th>
          <th>n_rows</th>
          <th>n_features</th>
          <th>onnx_size</th>
          <th>avg</th>
          <th>med</th>
          <th>n_runs</th>
          <th>ttime</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11454</td>
          <td>0.010142</td>
          <td>0.010367</td>
          <td>7</td>
          <td>0.091385</td>
        </tr>
        <tr>
          <th>1</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11454</td>
          <td>0.000192</td>
          <td>0.000158</td>
          <td>7</td>
          <td>0.001523</td>
        </tr>
        <tr>
          <th>2</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>21993</td>
          <td>0.014738</td>
          <td>0.014907</td>
          <td>7</td>
          <td>0.123639</td>
        </tr>
        <tr>
          <th>3</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>21993</td>
          <td>0.000244</td>
          <td>0.000243</td>
          <td>7</td>
          <td>0.002033</td>
        </tr>
        <tr>
          <th>4</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>33406</td>
          <td>0.021579</td>
          <td>0.021488</td>
          <td>7</td>
          <td>0.171277</td>
        </tr>
        <tr>
          <th>5</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>33406</td>
          <td>0.000381</td>
          <td>0.000371</td>
          <td>7</td>
          <td>0.002897</td>
        </tr>
        <tr>
          <th>6</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>34816</td>
          <td>0.008140</td>
          <td>0.006880</td>
          <td>7</td>
          <td>0.057479</td>
        </tr>
        <tr>
          <th>7</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>34816</td>
          <td>0.000225</td>
          <td>0.000205</td>
          <td>7</td>
          <td>0.001829</td>
        </tr>
        <tr>
          <th>8</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>65501</td>
          <td>0.017179</td>
          <td>0.017019</td>
          <td>7</td>
          <td>0.127163</td>
        </tr>
        <tr>
          <th>9</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>65501</td>
          <td>0.000318</td>
          <td>0.000305</td>
          <td>7</td>
          <td>0.014826</td>
        </tr>
        <tr>
          <th>10</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>102100</td>
          <td>0.021702</td>
          <td>0.022043</td>
          <td>7</td>
          <td>0.147325</td>
        </tr>
        <tr>
          <th>11</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>102100</td>
          <td>0.000567</td>
          <td>0.000557</td>
          <td>7</td>
          <td>0.018216</td>
        </tr>
        <tr>
          <th>12</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>73993</td>
          <td>0.012422</td>
          <td>0.012309</td>
          <td>7</td>
          <td>0.109499</td>
        </tr>
        <tr>
          <th>13</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>73993</td>
          <td>0.000275</td>
          <td>0.000258</td>
          <td>7</td>
          <td>0.023612</td>
        </tr>
        <tr>
          <th>14</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>138467</td>
          <td>0.017408</td>
          <td>0.016963</td>
          <td>7</td>
          <td>0.141777</td>
        </tr>
        <tr>
          <th>15</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>138467</td>
          <td>0.000504</td>
          <td>0.000478</td>
          <td>7</td>
          <td>0.019634</td>
        </tr>
        <tr>
          <th>16</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>206329</td>
          <td>0.023257</td>
          <td>0.023168</td>
          <td>7</td>
          <td>0.163119</td>
        </tr>
        <tr>
          <th>17</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>206329</td>
          <td>0.000585</td>
          <td>0.000581</td>
          <td>7</td>
          <td>0.004334</td>
        </tr>
        <tr>
          <th>18</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>108945</td>
          <td>0.008682</td>
          <td>0.009945</td>
          <td>7</td>
          <td>0.068417</td>
        </tr>
        <tr>
          <th>19</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>108945</td>
          <td>0.000310</td>
          <td>0.000278</td>
          <td>7</td>
          <td>0.016257</td>
        </tr>
        <tr>
          <th>20</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>223495</td>
          <td>0.014528</td>
          <td>0.015711</td>
          <td>7</td>
          <td>0.100923</td>
        </tr>
        <tr>
          <th>21</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>223495</td>
          <td>0.000472</td>
          <td>0.000470</td>
          <td>7</td>
          <td>0.003697</td>
        </tr>
        <tr>
          <th>22</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>327137</td>
          <td>0.023836</td>
          <td>0.024102</td>
          <td>7</td>
          <td>0.187845</td>
        </tr>
        <tr>
          <th>23</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>327137</td>
          <td>0.000596</td>
          <td>0.000596</td>
          <td>7</td>
          <td>0.004332</td>
        </tr>
        <tr>
          <th>24</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11235</td>
          <td>0.010172</td>
          <td>0.010238</td>
          <td>7</td>
          <td>0.070432</td>
        </tr>
        <tr>
          <th>25</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11235</td>
          <td>0.000148</td>
          <td>0.000143</td>
          <td>7</td>
          <td>0.001268</td>
        </tr>
        <tr>
          <th>26</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>22358</td>
          <td>0.015201</td>
          <td>0.015411</td>
          <td>7</td>
          <td>0.107710</td>
        </tr>
        <tr>
          <th>27</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>22358</td>
          <td>0.000279</td>
          <td>0.000288</td>
          <td>7</td>
          <td>0.002194</td>
        </tr>
        <tr>
          <th>28</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>32968</td>
          <td>0.020757</td>
          <td>0.020448</td>
          <td>7</td>
          <td>0.146659</td>
        </tr>
        <tr>
          <th>29</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>32968</td>
          <td>0.000328</td>
          <td>0.000319</td>
          <td>7</td>
          <td>0.002577</td>
        </tr>
        <tr>
          <th>30</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>33283</td>
          <td>0.008933</td>
          <td>0.009139</td>
          <td>7</td>
          <td>0.062343</td>
        </tr>
        <tr>
          <th>31</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>33283</td>
          <td>0.000190</td>
          <td>0.000183</td>
          <td>7</td>
          <td>0.001517</td>
        </tr>
        <tr>
          <th>32</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>65355</td>
          <td>0.015167</td>
          <td>0.014986</td>
          <td>7</td>
          <td>0.107656</td>
        </tr>
        <tr>
          <th>33</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>65355</td>
          <td>0.000407</td>
          <td>0.000397</td>
          <td>7</td>
          <td>0.003063</td>
        </tr>
        <tr>
          <th>34</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>100202</td>
          <td>0.020402</td>
          <td>0.019833</td>
          <td>7</td>
          <td>0.142072</td>
        </tr>
        <tr>
          <th>35</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>100202</td>
          <td>0.000518</td>
          <td>0.000510</td>
          <td>7</td>
          <td>0.003922</td>
        </tr>
        <tr>
          <th>36</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>73448</td>
          <td>0.009503</td>
          <td>0.009579</td>
          <td>7</td>
          <td>0.066942</td>
        </tr>
        <tr>
          <th>37</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>73448</td>
          <td>0.000237</td>
          <td>0.000221</td>
          <td>7</td>
          <td>0.001899</td>
        </tr>
        <tr>
          <th>38</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>121164</td>
          <td>0.014151</td>
          <td>0.014154</td>
          <td>7</td>
          <td>0.099080</td>
        </tr>
        <tr>
          <th>39</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>121164</td>
          <td>0.000468</td>
          <td>0.000460</td>
          <td>7</td>
          <td>0.003488</td>
        </tr>
        <tr>
          <th>40</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>162802</td>
          <td>0.018489</td>
          <td>0.018229</td>
          <td>7</td>
          <td>0.130982</td>
        </tr>
        <tr>
          <th>41</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>162802</td>
          <td>0.000618</td>
          <td>0.000620</td>
          <td>7</td>
          <td>0.004555</td>
        </tr>
        <tr>
          <th>42</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>85700</td>
          <td>0.009032</td>
          <td>0.008620</td>
          <td>7</td>
          <td>0.063412</td>
        </tr>
        <tr>
          <th>43</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>85700</td>
          <td>0.000252</td>
          <td>0.000248</td>
          <td>7</td>
          <td>0.001956</td>
        </tr>
        <tr>
          <th>44</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>193936</td>
          <td>0.015249</td>
          <td>0.014768</td>
          <td>7</td>
          <td>0.108218</td>
        </tr>
        <tr>
          <th>45</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>193936</td>
          <td>0.000499</td>
          <td>0.000497</td>
          <td>7</td>
          <td>0.003711</td>
        </tr>
        <tr>
          <th>46</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>275579</td>
          <td>0.018889</td>
          <td>0.019297</td>
          <td>7</td>
          <td>0.132190</td>
        </tr>
        <tr>
          <th>47</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>275579</td>
          <td>0.000793</td>
          <td>0.000803</td>
          <td>7</td>
          <td>0.005702</td>
        </tr>
        <tr>
          <th>48</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>10505</td>
          <td>0.001634</td>
          <td>0.001638</td>
          <td>7</td>
          <td>0.011477</td>
        </tr>
        <tr>
          <th>49</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>10505</td>
          <td>0.000281</td>
          <td>0.000280</td>
          <td>7</td>
          <td>0.002058</td>
        </tr>
        <tr>
          <th>50</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>28684</td>
          <td>0.001857</td>
          <td>0.001847</td>
          <td>7</td>
          <td>0.013000</td>
        </tr>
        <tr>
          <th>51</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>28684</td>
          <td>0.000440</td>
          <td>0.000430</td>
          <td>7</td>
          <td>0.003136</td>
        </tr>
        <tr>
          <th>52</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>58388</td>
          <td>0.002127</td>
          <td>0.002113</td>
          <td>7</td>
          <td>0.014960</td>
        </tr>
        <tr>
          <th>53</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>58388</td>
          <td>0.000612</td>
          <td>0.000613</td>
          <td>7</td>
          <td>0.004403</td>
        </tr>
        <tr>
          <th>54</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>90535</td>
          <td>0.002190</td>
          <td>0.002203</td>
          <td>7</td>
          <td>0.015484</td>
        </tr>
        <tr>
          <th>55</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>90535</td>
          <td>0.000719</td>
          <td>0.000715</td>
          <td>7</td>
          <td>0.005091</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 264-266

Plot
++++

.. GENERATED FROM PYTHON SOURCE LINES 266-308

.. code-block:: Python


    n_rows = len(n_jobs)
    n_cols = len(n_ests)


    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))
    fig.suptitle(f"{rf.__class__.__name__}\nX.shape={X.shape}")

    for n_j, n_estimators in tqdm(product(n_jobs, n_ests)):
        i = n_jobs.index(n_j)
        j = n_ests.index(n_estimators)
        ax = axes[i, j]

        subdf = df[(df.n_estimators == n_estimators) & (df.n_jobs == n_j)]
        if subdf.shape[0] == 0:
            continue
        piv = subdf.pivot(index="max_depth", columns="name", values=["avg", "med"])
        piv.plot(ax=ax, title=f"jobs={n_j}, trees={n_estimators}")
        ax.set_ylabel(f"n_jobs={n_j}", fontsize="small")
        ax.set_xlabel("max_depth", fontsize="small")

        # ratio
        ax2 = ax.twinx()
        piv1 = subdf.pivot(index="max_depth", columns="name", values="avg")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "b--", label="speedup avg")

        piv1 = subdf.pivot(index="max_depth", columns="name", values="med")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "y--", label="speedup med")
        ax2.legend(fontsize="x-small")

        # 1
        ax2.plot(piv1.index, [1 for _ in piv1.index], "k--", label="no speedup")

    for i in range(axes.shape[0]):
        for j in range(axes.shape[1]):
            axes[i, j].legend(fontsize="small")

    fig.tight_layout()
    fig.savefig(f"{name}-{legend}.png")
    # plt.show()



.. image-sg:: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :alt: RandomForestRegressor X.shape=(1000, 10), jobs=8, trees=10, jobs=8, trees=20, jobs=8, trees=30, jobs=4, trees=10, jobs=4, trees=20, jobs=4, trees=30, jobs=1, trees=10
   :srcset: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0it [00:00, ?it/s]    2it [00:00, 14.34it/s]    4it [00:00, 14.67it/s]    7it [00:00, 17.25it/s]    9it [00:00, 21.17it/s]
    2023-12-28 19:28:07,432 matplotlib.legend [WARNING] - No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
    2023-12-28 19:28:07,433 matplotlib.legend [WARNING] - No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 34.581 seconds)


.. _sphx_glr_download_auto_examples_plot_benchmark_rf.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_benchmark_rf.ipynb <plot_benchmark_rf.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_benchmark_rf.py <plot_benchmark_rf.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
