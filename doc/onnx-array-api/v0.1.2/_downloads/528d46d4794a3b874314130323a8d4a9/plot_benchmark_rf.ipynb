{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark of TreeEnsemble implementation\n\nThe following example compares the inference time between\n:epkg:`onnxruntime` and :class:`sklearn.ensemble.RandomForestRegressor`,\nfow different number of estimators, max depth, and parallelization.\nIt does it for a fixed number of rows and features.\n\n## import and registration of necessary converters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\nimport os\nimport time\nfrom itertools import product\n\nimport matplotlib.pyplot as plt\nimport numpy\nimport pandas\nfrom lightgbm import LGBMRegressor\nfrom onnxmltools.convert.lightgbm.operator_converters.LightGbm import convert_lightgbm\nfrom onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost\nfrom onnxruntime import InferenceSession, SessionOptions\nfrom psutil import cpu_count\nfrom sphinx_runpython.runpython import run_cmd\nfrom skl2onnx import to_onnx, update_registered_converter\nfrom skl2onnx.common.shape_calculator import calculate_linear_regressor_output_shapes\nfrom sklearn import set_config\nfrom sklearn.ensemble import RandomForestRegressor\nfrom tqdm import tqdm\nfrom xgboost import XGBRegressor\n\n\ndef skl2onnx_convert_lightgbm(scope, operator, container):\n    options = scope.get_options(operator.raw_operator)\n    if \"split\" in options:\n        operator.split = options[\"split\"]\n    else:\n        operator.split = None\n    convert_lightgbm(scope, operator, container)\n\n\nupdate_registered_converter(\n    LGBMRegressor,\n    \"LightGbmLGBMRegressor\",\n    calculate_linear_regressor_output_shapes,\n    skl2onnx_convert_lightgbm,\n    options={\"split\": None},\n)\nupdate_registered_converter(\n    XGBRegressor,\n    \"XGBoostXGBRegressor\",\n    calculate_linear_regressor_output_shapes,\n    convert_xgboost,\n)\n\n# The following instruction reduces the time spent by scikit-learn\n# to validate the data.\nset_config(assume_finite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine details\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Number of cores: {cpu_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But this information is not usually enough.\nLet's extract the cache information.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    out, err = run_cmd(\"lscpu\")\n    print(out)\nexcept Exception as e:\n    print(f\"lscpu not available: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or with the following command.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out, err = run_cmd(\"cat /proc/cpuinfo\")\nprint(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fonction to measure inference time\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def measure_inference(fct, X, repeat, max_time=5, quantile=1):\n    \"\"\"\n    Run *repeat* times the same function on data *X*.\n\n    :param fct: fonction to run\n    :param X: data\n    :param repeat: number of times to run\n    :param max_time: maximum time to use to measure the inference\n    :return: number of runs, sum of the time, average, median\n    \"\"\"\n    times = []\n    for n in range(repeat):\n        perf = time.perf_counter()\n        fct(X)\n        delta = time.perf_counter() - perf\n        times.append(delta)\n        if len(times) < 3:\n            continue\n        if max_time is not None and sum(times) >= max_time:\n            break\n    times.sort()\n    quantile = 0 if (len(times) - quantile * 2) < 3 else quantile\n    if quantile == 0:\n        tt = times\n    else:\n        tt = times[quantile:-quantile]\n    return (len(times), sum(times), sum(tt) / len(tt), times[len(times) // 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark\n\nThe following script benchmarks the inference for the same\nmodel for a random forest and onnxruntime after it was converted\ninto ONNX and for the following configurations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "small = cpu_count() < 12\nif small:\n    N = 1000\n    n_features = 10\n    n_jobs = [1, cpu_count() // 2, cpu_count()]\n    n_ests = [10, 20, 30]\n    depth = [4, 6, 8, 10]\n    Regressor = RandomForestRegressor\nelse:\n    N = 100000\n    n_features = 50\n    n_jobs = [cpu_count(), cpu_count() // 2, 1]\n    n_ests = [100, 200, 400]\n    depth = [6, 8, 10, 12, 14]\n    Regressor = RandomForestRegressor\n\nlegend = f\"parallel-nf-{n_features}-\"\n\n# avoid duplicates on machine with 1 or 2 cores.\nn_jobs = list(sorted(set(n_jobs), reverse=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark parameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "repeat = 7  # repeat n times the same inference\nquantile = 1  # exclude extreme times\nmax_time = 5  # maximum number of seconds to spend on one configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = numpy.random.randn(N, n_features).astype(numpy.float32)\nnoise = (numpy.random.randn(X.shape[0]) / (n_features // 5)).astype(numpy.float32)\ny = X.mean(axis=1) + noise\nn_train = min(N, N // 3)\n\n\ndata = []\ncouples = list(product(n_jobs, depth, n_ests))\nbar = tqdm(couples)\ncache_dir = \"_cache\"\nif not os.path.exists(cache_dir):\n    os.mkdir(cache_dir)\n\nfor n_j, max_depth, n_estimators in bar:\n    if n_j == 1 and n_estimators > n_ests[0]:\n        # skipping\n        continue\n\n    # parallelization\n    cache_name = os.path.join(\n        cache_dir, f\"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.pkl\"\n    )\n    if os.path.exists(cache_name):\n        with open(cache_name, \"rb\") as f:\n            rf = pickle.load(f)\n    else:\n        bar.set_description(f\"J={n_j} E={n_estimators} D={max_depth} train rf\")\n        if n_j == 1 and issubclass(Regressor, RandomForestRegressor):\n            rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=-1)\n            rf.fit(X[:n_train], y[:n_train])\n            rf.n_jobs = 1\n        else:\n            rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_j)\n            rf.fit(X[:n_train], y[:n_train])\n        with open(cache_name, \"wb\") as f:\n            pickle.dump(rf, f)\n\n    bar.set_description(f\"J={n_j} E={n_estimators} D={max_depth} ISession\")\n    so = SessionOptions()\n    so.intra_op_num_threads = n_j\n    cache_name = os.path.join(\n        cache_dir, f\"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.onnx\"\n    )\n    if os.path.exists(cache_name):\n        sess = InferenceSession(cache_name, so, providers=[\"CPUExecutionProvider\"])\n    else:\n        bar.set_description(f\"J={n_j} E={n_estimators} D={max_depth} cvt onnx\")\n        onx = to_onnx(rf, X[:1])\n        with open(cache_name, \"wb\") as f:\n            f.write(onx.SerializeToString())\n        sess = InferenceSession(cache_name, so, providers=[\"CPUExecutionProvider\"])\n    onx_size = os.stat(cache_name).st_size\n\n    # run once to avoid counting the first run\n    bar.set_description(f\"J={n_j} E={n_estimators} D={max_depth} predict1\")\n    rf.predict(X)\n    sess.run(None, {\"X\": X})\n\n    # fixed data\n    obs = dict(\n        n_jobs=n_j,\n        max_depth=max_depth,\n        n_estimators=n_estimators,\n        repeat=repeat,\n        max_time=max_time,\n        name=rf.__class__.__name__,\n        n_rows=X.shape[0],\n        n_features=X.shape[1],\n        onnx_size=onx_size,\n    )\n\n    # baseline\n    bar.set_description(f\"J={n_j} E={n_estimators} D={max_depth} predictB\")\n    r, t, mean, med = measure_inference(rf.predict, X, repeat=repeat, max_time=max_time)\n    o1 = obs.copy()\n    o1.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name=\"base\"))\n    data.append(o1)\n\n    # onnxruntime\n    bar.set_description(f\"J={n_j} E={n_estimators} D={max_depth} predictO\")\n    r, t, mean, med = measure_inference(\n        lambda x: sess.run(None, {\"X\": x}), X, repeat=repeat, max_time=max_time\n    )\n    o2 = obs.copy()\n    o2.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name=\"ort_\"))\n    data.append(o2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = os.path.join(cache_dir, \"plot_beanchmark_rf\")\nprint(f\"Saving data into {name!r}\")\n\ndf = pandas.DataFrame(data)\ndf2 = df.copy()\ndf2[\"legend\"] = legend\ndf2.to_csv(f\"{name}-{legend}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Printing the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_rows = len(n_jobs)\nn_cols = len(n_ests)\n\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\nfig.suptitle(f\"{rf.__class__.__name__}\\nX.shape={X.shape}\")\n\nfor n_j, n_estimators in tqdm(product(n_jobs, n_ests)):\n    i = n_jobs.index(n_j)\n    j = n_ests.index(n_estimators)\n    ax = axes[i, j]\n\n    subdf = df[(df.n_estimators == n_estimators) & (df.n_jobs == n_j)]\n    if subdf.shape[0] == 0:\n        continue\n    piv = subdf.pivot(index=\"max_depth\", columns=\"name\", values=[\"avg\", \"med\"])\n    piv.plot(ax=ax, title=f\"jobs={n_j}, trees={n_estimators}\")\n    ax.set_ylabel(f\"n_jobs={n_j}\", fontsize=\"small\")\n    ax.set_xlabel(\"max_depth\", fontsize=\"small\")\n\n    # ratio\n    ax2 = ax.twinx()\n    piv1 = subdf.pivot(index=\"max_depth\", columns=\"name\", values=\"avg\")\n    piv1[\"speedup\"] = piv1.base / piv1.ort_\n    ax2.plot(piv1.index, piv1.speedup, \"b--\", label=\"speedup avg\")\n\n    piv1 = subdf.pivot(index=\"max_depth\", columns=\"name\", values=\"med\")\n    piv1[\"speedup\"] = piv1.base / piv1.ort_\n    ax2.plot(piv1.index, piv1.speedup, \"y--\", label=\"speedup med\")\n    ax2.legend(fontsize=\"x-small\")\n\n    # 1\n    ax2.plot(piv1.index, [1 for _ in piv1.index], \"k--\", label=\"no speedup\")\n\nfor i in range(axes.shape[0]):\n    for j in range(axes.shape[1]):\n        axes[i, j].legend(fontsize=\"small\")\n\nfig.tight_layout()\nfig.savefig(f\"{name}-{legend}.png\")\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}