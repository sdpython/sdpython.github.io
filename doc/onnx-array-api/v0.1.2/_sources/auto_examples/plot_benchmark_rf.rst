
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_benchmark_rf.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_benchmark_rf.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_benchmark_rf.py:


.. _l-example-benchmark-tree-implementation:

Benchmark of TreeEnsemble implementation
========================================

The following example compares the inference time between
:epkg:`onnxruntime` and :class:`sklearn.ensemble.RandomForestRegressor`,
fow different number of estimators, max depth, and parallelization.
It does it for a fixed number of rows and features.

import and registration of necessary converters
++++++++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 15-64

.. code-block:: default

    import pickle
    import os
    import time
    from itertools import product

    import matplotlib.pyplot as plt
    import numpy
    import pandas
    from lightgbm import LGBMRegressor
    from onnxmltools.convert.lightgbm.operator_converters.LightGbm import convert_lightgbm
    from onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost
    from onnxruntime import InferenceSession, SessionOptions
    from psutil import cpu_count
    from sphinx_runpython.runpython import run_cmd
    from skl2onnx import to_onnx, update_registered_converter
    from skl2onnx.common.shape_calculator import calculate_linear_regressor_output_shapes
    from sklearn import set_config
    from sklearn.ensemble import RandomForestRegressor
    from tqdm import tqdm
    from xgboost import XGBRegressor


    def skl2onnx_convert_lightgbm(scope, operator, container):
        options = scope.get_options(operator.raw_operator)
        if "split" in options:
            operator.split = options["split"]
        else:
            operator.split = None
        convert_lightgbm(scope, operator, container)


    update_registered_converter(
        LGBMRegressor,
        "LightGbmLGBMRegressor",
        calculate_linear_regressor_output_shapes,
        skl2onnx_convert_lightgbm,
        options={"split": None},
    )
    update_registered_converter(
        XGBRegressor,
        "XGBoostXGBRegressor",
        calculate_linear_regressor_output_shapes,
        convert_xgboost,
    )

    # The following instruction reduces the time spent by scikit-learn
    # to validate the data.
    set_config(assume_finite=True)








.. GENERATED FROM PYTHON SOURCE LINES 65-67

Machine details
+++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 67-71

.. code-block:: default



    print(f"Number of cores: {cpu_count()}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Number of cores: 8




.. GENERATED FROM PYTHON SOURCE LINES 72-74

But this information is not usually enough.
Let's extract the cache information.

.. GENERATED FROM PYTHON SOURCE LINES 74-81

.. code-block:: default


    try:
        out, err = run_cmd("lscpu")
        print(out)
    except Exception as e:
        print(f"lscpu not available: {e}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ['lscpu']
    <Popen: returncode: None args: ['lscpu']>




.. GENERATED FROM PYTHON SOURCE LINES 82-83

Or with the following command.

.. GENERATED FROM PYTHON SOURCE LINES 83-86

.. code-block:: default

    out, err = run_cmd("cat /proc/cpuinfo")
    print(out)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ['cat', '/proc/cpuinfo']
    <Popen: returncode: None args: ['cat', '/proc/cpuinfo']>




.. GENERATED FROM PYTHON SOURCE LINES 87-89

Fonction to measure inference time
++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 89-120

.. code-block:: default



    def measure_inference(fct, X, repeat, max_time=5, quantile=1):
        """
        Run *repeat* times the same function on data *X*.

        :param fct: fonction to run
        :param X: data
        :param repeat: number of times to run
        :param max_time: maximum time to use to measure the inference
        :return: number of runs, sum of the time, average, median
        """
        times = []
        for n in range(repeat):
            perf = time.perf_counter()
            fct(X)
            delta = time.perf_counter() - perf
            times.append(delta)
            if len(times) < 3:
                continue
            if max_time is not None and sum(times) >= max_time:
                break
        times.sort()
        quantile = 0 if (len(times) - quantile * 2) < 3 else quantile
        if quantile == 0:
            tt = times
        else:
            tt = times[quantile:-quantile]
        return (len(times), sum(times), sum(tt) / len(tt), times[len(times) // 2])









.. GENERATED FROM PYTHON SOURCE LINES 121-127

Benchmark
+++++++++

The following script benchmarks the inference for the same
model for a random forest and onnxruntime after it was converted
into ONNX and for the following configurations.

.. GENERATED FROM PYTHON SOURCE LINES 127-149

.. code-block:: default


    small = cpu_count() < 12
    if small:
        N = 1000
        n_features = 10
        n_jobs = [1, cpu_count() // 2, cpu_count()]
        n_ests = [10, 20, 30]
        depth = [4, 6, 8, 10]
        Regressor = RandomForestRegressor
    else:
        N = 100000
        n_features = 50
        n_jobs = [cpu_count(), cpu_count() // 2, 1]
        n_ests = [100, 200, 400]
        depth = [6, 8, 10, 12, 14]
        Regressor = RandomForestRegressor

    legend = f"parallel-nf-{n_features}-"

    # avoid duplicates on machine with 1 or 2 cores.
    n_jobs = list(sorted(set(n_jobs), reverse=True))








.. GENERATED FROM PYTHON SOURCE LINES 150-151

Benchmark parameters

.. GENERATED FROM PYTHON SOURCE LINES 151-155

.. code-block:: default

    repeat = 7  # repeat n times the same inference
    quantile = 1  # exclude extreme times
    max_time = 5  # maximum number of seconds to spend on one configuration








.. GENERATED FROM PYTHON SOURCE LINES 156-157

Data

.. GENERATED FROM PYTHON SOURCE LINES 157-247

.. code-block:: default



    X = numpy.random.randn(N, n_features).astype(numpy.float32)
    noise = (numpy.random.randn(X.shape[0]) / (n_features // 5)).astype(numpy.float32)
    y = X.mean(axis=1) + noise
    n_train = min(N, N // 3)


    data = []
    couples = list(product(n_jobs, depth, n_ests))
    bar = tqdm(couples)
    cache_dir = "_cache"
    if not os.path.exists(cache_dir):
        os.mkdir(cache_dir)

    for n_j, max_depth, n_estimators in bar:
        if n_j == 1 and n_estimators > n_ests[0]:
            # skipping
            continue

        # parallelization
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.pkl"
        )
        if os.path.exists(cache_name):
            with open(cache_name, "rb") as f:
                rf = pickle.load(f)
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} train rf")
            if n_j == 1 and issubclass(Regressor, RandomForestRegressor):
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=-1)
                rf.fit(X[:n_train], y[:n_train])
                rf.n_jobs = 1
            else:
                rf = Regressor(max_depth=max_depth, n_estimators=n_estimators, n_jobs=n_j)
                rf.fit(X[:n_train], y[:n_train])
            with open(cache_name, "wb") as f:
                pickle.dump(rf, f)

        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} ISession")
        so = SessionOptions()
        so.intra_op_num_threads = n_j
        cache_name = os.path.join(
            cache_dir, f"nf-{X.shape[1]}-rf-J-{n_j}-E-{n_estimators}-D-{max_depth}.onnx"
        )
        if os.path.exists(cache_name):
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        else:
            bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} cvt onnx")
            onx = to_onnx(rf, X[:1])
            with open(cache_name, "wb") as f:
                f.write(onx.SerializeToString())
            sess = InferenceSession(cache_name, so, providers=["CPUExecutionProvider"])
        onx_size = os.stat(cache_name).st_size

        # run once to avoid counting the first run
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predict1")
        rf.predict(X)
        sess.run(None, {"X": X})

        # fixed data
        obs = dict(
            n_jobs=n_j,
            max_depth=max_depth,
            n_estimators=n_estimators,
            repeat=repeat,
            max_time=max_time,
            name=rf.__class__.__name__,
            n_rows=X.shape[0],
            n_features=X.shape[1],
            onnx_size=onx_size,
        )

        # baseline
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictB")
        r, t, mean, med = measure_inference(rf.predict, X, repeat=repeat, max_time=max_time)
        o1 = obs.copy()
        o1.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="base"))
        data.append(o1)

        # onnxruntime
        bar.set_description(f"J={n_j} E={n_estimators} D={max_depth} predictO")
        r, t, mean, med = measure_inference(
            lambda x: sess.run(None, {"X": x}), X, repeat=repeat, max_time=max_time
        )
        o2 = obs.copy()
        o2.update(dict(avg=mean, med=med, n_runs=r, ttime=t, name="ort_"))
        data.append(o2)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 ISession:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predict1:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictB:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictO:   0%|          | 0/36 [00:00<?, ?it/s]    J=8 E=10 D=4 predictO:   3%|2         | 1/36 [00:00<00:04,  8.48it/s]    J=8 E=20 D=4 ISession:   3%|2         | 1/36 [00:00<00:04,  8.48it/s]    J=8 E=20 D=4 predict1:   3%|2         | 1/36 [00:00<00:04,  8.48it/s]    J=8 E=20 D=4 predictB:   3%|2         | 1/36 [00:00<00:04,  8.48it/s]    J=8 E=20 D=4 predictO:   3%|2         | 1/36 [00:00<00:04,  8.48it/s]    J=8 E=20 D=4 predictO:   6%|5         | 2/36 [00:00<00:04,  7.28it/s]    J=8 E=30 D=4 ISession:   6%|5         | 2/36 [00:00<00:04,  7.28it/s]    J=8 E=30 D=4 predict1:   6%|5         | 2/36 [00:00<00:04,  7.28it/s]    J=8 E=30 D=4 predictB:   6%|5         | 2/36 [00:00<00:04,  7.28it/s]    J=8 E=30 D=4 predictO:   6%|5         | 2/36 [00:00<00:04,  7.28it/s]    J=8 E=30 D=4 predictO:   8%|8         | 3/36 [00:00<00:05,  6.03it/s]    J=8 E=10 D=6 ISession:   8%|8         | 3/36 [00:00<00:05,  6.03it/s]    J=8 E=10 D=6 predict1:   8%|8         | 3/36 [00:00<00:05,  6.03it/s]    J=8 E=10 D=6 predictB:   8%|8         | 3/36 [00:00<00:05,  6.03it/s]    J=8 E=10 D=6 predictO:   8%|8         | 3/36 [00:00<00:05,  6.03it/s]    J=8 E=10 D=6 predictO:  11%|#1        | 4/36 [00:00<00:04,  7.04it/s]    J=8 E=20 D=6 ISession:  11%|#1        | 4/36 [00:00<00:04,  7.04it/s]    J=8 E=20 D=6 predict1:  11%|#1        | 4/36 [00:00<00:04,  7.04it/s]    J=8 E=20 D=6 predictB:  11%|#1        | 4/36 [00:00<00:04,  7.04it/s]    J=8 E=20 D=6 predictO:  11%|#1        | 4/36 [00:00<00:04,  7.04it/s]    J=8 E=20 D=6 predictO:  14%|#3        | 5/36 [00:00<00:04,  7.07it/s]    J=8 E=30 D=6 ISession:  14%|#3        | 5/36 [00:00<00:04,  7.07it/s]    J=8 E=30 D=6 predict1:  14%|#3        | 5/36 [00:00<00:04,  7.07it/s]    J=8 E=30 D=6 predictB:  14%|#3        | 5/36 [00:00<00:04,  7.07it/s]    J=8 E=30 D=6 predictO:  14%|#3        | 5/36 [00:00<00:04,  7.07it/s]    J=8 E=30 D=6 predictO:  17%|#6        | 6/36 [00:00<00:04,  6.53it/s]    J=8 E=10 D=8 ISession:  17%|#6        | 6/36 [00:00<00:04,  6.53it/s]    J=8 E=10 D=8 predict1:  17%|#6        | 6/36 [00:00<00:04,  6.53it/s]    J=8 E=10 D=8 predictB:  17%|#6        | 6/36 [00:00<00:04,  6.53it/s]    J=8 E=10 D=8 predictO:  17%|#6        | 6/36 [00:01<00:04,  6.53it/s]    J=8 E=10 D=8 predictO:  19%|#9        | 7/36 [00:01<00:04,  7.16it/s]    J=8 E=20 D=8 ISession:  19%|#9        | 7/36 [00:01<00:04,  7.16it/s]    J=8 E=20 D=8 predict1:  19%|#9        | 7/36 [00:01<00:04,  7.16it/s]    J=8 E=20 D=8 predictB:  19%|#9        | 7/36 [00:01<00:04,  7.16it/s]    J=8 E=20 D=8 predictO:  19%|#9        | 7/36 [00:01<00:04,  7.16it/s]    J=8 E=20 D=8 predictO:  22%|##2       | 8/36 [00:01<00:04,  6.98it/s]    J=8 E=30 D=8 ISession:  22%|##2       | 8/36 [00:01<00:04,  6.98it/s]    J=8 E=30 D=8 predict1:  22%|##2       | 8/36 [00:01<00:04,  6.98it/s]    J=8 E=30 D=8 predictB:  22%|##2       | 8/36 [00:01<00:04,  6.98it/s]    J=8 E=30 D=8 predictO:  22%|##2       | 8/36 [00:01<00:04,  6.98it/s]    J=8 E=30 D=8 predictO:  25%|##5       | 9/36 [00:01<00:04,  6.09it/s]    J=8 E=10 D=10 ISession:  25%|##5       | 9/36 [00:01<00:04,  6.09it/s]    J=8 E=10 D=10 predict1:  25%|##5       | 9/36 [00:01<00:04,  6.09it/s]    J=8 E=10 D=10 predictB:  25%|##5       | 9/36 [00:01<00:04,  6.09it/s]    J=8 E=10 D=10 predictO:  25%|##5       | 9/36 [00:01<00:04,  6.09it/s]    J=8 E=10 D=10 predictO:  28%|##7       | 10/36 [00:01<00:03,  6.58it/s]    J=8 E=20 D=10 ISession:  28%|##7       | 10/36 [00:01<00:03,  6.58it/s]    J=8 E=20 D=10 predict1:  28%|##7       | 10/36 [00:01<00:03,  6.58it/s]    J=8 E=20 D=10 predictB:  28%|##7       | 10/36 [00:01<00:03,  6.58it/s]    J=8 E=20 D=10 predictO:  28%|##7       | 10/36 [00:01<00:03,  6.58it/s]    J=8 E=20 D=10 predictO:  31%|###       | 11/36 [00:01<00:03,  6.55it/s]    J=8 E=30 D=10 ISession:  31%|###       | 11/36 [00:01<00:03,  6.55it/s]    J=8 E=30 D=10 predict1:  31%|###       | 11/36 [00:01<00:03,  6.55it/s]    J=8 E=30 D=10 predictB:  31%|###       | 11/36 [00:01<00:03,  6.55it/s]    J=8 E=30 D=10 predictO:  31%|###       | 11/36 [00:01<00:03,  6.55it/s]    J=8 E=30 D=10 predictO:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]    J=4 E=10 D=4 ISession:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]     J=4 E=10 D=4 predict1:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]    J=4 E=10 D=4 predictB:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]    J=4 E=10 D=4 predictO:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]    J=4 E=20 D=4 ISession:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]    J=4 E=20 D=4 predict1:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]    J=4 E=20 D=4 predictB:  33%|###3      | 12/36 [00:01<00:04,  5.84it/s]    J=4 E=20 D=4 predictO:  33%|###3      | 12/36 [00:02<00:04,  5.84it/s]    J=4 E=20 D=4 predictO:  39%|###8      | 14/36 [00:02<00:03,  7.28it/s]    J=4 E=30 D=4 ISession:  39%|###8      | 14/36 [00:02<00:03,  7.28it/s]    J=4 E=30 D=4 predict1:  39%|###8      | 14/36 [00:02<00:03,  7.28it/s]    J=4 E=30 D=4 predictB:  39%|###8      | 14/36 [00:02<00:03,  7.28it/s]    J=4 E=30 D=4 predictO:  39%|###8      | 14/36 [00:02<00:03,  7.28it/s]    J=4 E=30 D=4 predictO:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=10 D=6 ISession:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=10 D=6 predict1:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=10 D=6 predictB:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=10 D=6 predictO:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=20 D=6 ISession:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=20 D=6 predict1:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=20 D=6 predictB:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=20 D=6 predictO:  42%|####1     | 15/36 [00:02<00:02,  7.14it/s]    J=4 E=20 D=6 predictO:  47%|####7     | 17/36 [00:02<00:02,  8.33it/s]    J=4 E=30 D=6 ISession:  47%|####7     | 17/36 [00:02<00:02,  8.33it/s]    J=4 E=30 D=6 predict1:  47%|####7     | 17/36 [00:02<00:02,  8.33it/s]    J=4 E=30 D=6 predictB:  47%|####7     | 17/36 [00:02<00:02,  8.33it/s]    J=4 E=30 D=6 predictO:  47%|####7     | 17/36 [00:02<00:02,  8.33it/s]    J=4 E=30 D=6 predictO:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=10 D=8 ISession:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=10 D=8 predict1:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=10 D=8 predictB:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=10 D=8 predictO:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=20 D=8 ISession:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=20 D=8 predict1:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=20 D=8 predictB:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=20 D=8 predictO:  50%|#####     | 18/36 [00:02<00:02,  7.79it/s]    J=4 E=20 D=8 predictO:  56%|#####5    | 20/36 [00:02<00:01,  8.63it/s]    J=4 E=30 D=8 ISession:  56%|#####5    | 20/36 [00:02<00:01,  8.63it/s]    J=4 E=30 D=8 predict1:  56%|#####5    | 20/36 [00:02<00:01,  8.63it/s]    J=4 E=30 D=8 predictB:  56%|#####5    | 20/36 [00:02<00:01,  8.63it/s]    J=4 E=30 D=8 predictO:  56%|#####5    | 20/36 [00:02<00:01,  8.63it/s]    J=4 E=30 D=8 predictO:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=10 D=10 ISession:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=10 D=10 predict1:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=10 D=10 predictB:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=10 D=10 predictO:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=20 D=10 ISession:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=20 D=10 predict1:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=20 D=10 predictB:  58%|#####8    | 21/36 [00:02<00:01,  8.29it/s]    J=4 E=20 D=10 predictO:  58%|#####8    | 21/36 [00:03<00:01,  8.29it/s]    J=4 E=20 D=10 predictO:  64%|######3   | 23/36 [00:03<00:01,  8.75it/s]    J=4 E=30 D=10 ISession:  64%|######3   | 23/36 [00:03<00:01,  8.75it/s]    J=4 E=30 D=10 predict1:  64%|######3   | 23/36 [00:03<00:01,  8.75it/s]    J=4 E=30 D=10 predictB:  64%|######3   | 23/36 [00:03<00:01,  8.75it/s]    J=4 E=30 D=10 predictO:  64%|######3   | 23/36 [00:03<00:01,  8.75it/s]    J=4 E=30 D=10 predictO:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=4 ISession:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]     J=1 E=10 D=4 predict1:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=4 predictB:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=4 predictO:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=6 ISession:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=6 predict1:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=6 predictB:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=6 predictO:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=8 ISession:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=8 predict1:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=8 predictB:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=8 predictO:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=10 ISession:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=10 predict1:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=10 predictB:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=10 predictO:  67%|######6   | 24/36 [00:03<00:01,  7.32it/s]    J=1 E=10 D=10 predictO:  94%|#########4| 34/36 [00:03<00:00, 23.42it/s]    J=1 E=10 D=10 predictO: 100%|##########| 36/36 [00:03<00:00, 10.52it/s]




.. GENERATED FROM PYTHON SOURCE LINES 248-250

Saving data
+++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 250-259

.. code-block:: default


    name = os.path.join(cache_dir, "plot_beanchmark_rf")
    print(f"Saving data into {name!r}")

    df = pandas.DataFrame(data)
    df2 = df.copy()
    df2["legend"] = legend
    df2.to_csv(f"{name}-{legend}.csv", index=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Saving data into '_cache/plot_beanchmark_rf'




.. GENERATED FROM PYTHON SOURCE LINES 260-261

Printing the data

.. GENERATED FROM PYTHON SOURCE LINES 261-263

.. code-block:: default

    df






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>n_jobs</th>
          <th>max_depth</th>
          <th>n_estimators</th>
          <th>repeat</th>
          <th>max_time</th>
          <th>name</th>
          <th>n_rows</th>
          <th>n_features</th>
          <th>onnx_size</th>
          <th>avg</th>
          <th>med</th>
          <th>n_runs</th>
          <th>ttime</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11089</td>
          <td>0.013271</td>
          <td>0.013449</td>
          <td>7</td>
          <td>0.090108</td>
        </tr>
        <tr>
          <th>1</th>
          <td>8</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11089</td>
          <td>0.000161</td>
          <td>0.000148</td>
          <td>7</td>
          <td>0.001510</td>
        </tr>
        <tr>
          <th>2</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>21920</td>
          <td>0.012371</td>
          <td>0.012129</td>
          <td>7</td>
          <td>0.109870</td>
        </tr>
        <tr>
          <th>3</th>
          <td>8</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>21920</td>
          <td>0.000163</td>
          <td>0.000162</td>
          <td>7</td>
          <td>0.001428</td>
        </tr>
        <tr>
          <th>4</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>32822</td>
          <td>0.019804</td>
          <td>0.019161</td>
          <td>7</td>
          <td>0.141788</td>
        </tr>
        <tr>
          <th>5</th>
          <td>8</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>32822</td>
          <td>0.000877</td>
          <td>0.000590</td>
          <td>7</td>
          <td>0.013612</td>
        </tr>
        <tr>
          <th>6</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>34816</td>
          <td>0.008833</td>
          <td>0.008909</td>
          <td>7</td>
          <td>0.064498</td>
        </tr>
        <tr>
          <th>7</th>
          <td>8</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>34816</td>
          <td>0.000176</td>
          <td>0.000172</td>
          <td>7</td>
          <td>0.009211</td>
        </tr>
        <tr>
          <th>8</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>68349</td>
          <td>0.012703</td>
          <td>0.012669</td>
          <td>7</td>
          <td>0.090856</td>
        </tr>
        <tr>
          <th>9</th>
          <td>8</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>68349</td>
          <td>0.000267</td>
          <td>0.000248</td>
          <td>7</td>
          <td>0.008172</td>
        </tr>
        <tr>
          <th>10</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>102465</td>
          <td>0.016200</td>
          <td>0.016205</td>
          <td>7</td>
          <td>0.112334</td>
        </tr>
        <tr>
          <th>11</th>
          <td>8</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>102465</td>
          <td>0.000350</td>
          <td>0.000334</td>
          <td>7</td>
          <td>0.002725</td>
        </tr>
        <tr>
          <th>12</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>72981</td>
          <td>0.008978</td>
          <td>0.008909</td>
          <td>7</td>
          <td>0.064398</td>
        </tr>
        <tr>
          <th>13</th>
          <td>8</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>72981</td>
          <td>0.000190</td>
          <td>0.000184</td>
          <td>7</td>
          <td>0.001647</td>
        </tr>
        <tr>
          <th>14</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>143302</td>
          <td>0.014107</td>
          <td>0.014066</td>
          <td>7</td>
          <td>0.103939</td>
        </tr>
        <tr>
          <th>15</th>
          <td>8</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>143302</td>
          <td>0.000301</td>
          <td>0.000294</td>
          <td>7</td>
          <td>0.002321</td>
        </tr>
        <tr>
          <th>16</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>214018</td>
          <td>0.017487</td>
          <td>0.017749</td>
          <td>7</td>
          <td>0.123962</td>
        </tr>
        <tr>
          <th>17</th>
          <td>8</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>214018</td>
          <td>0.000425</td>
          <td>0.000421</td>
          <td>7</td>
          <td>0.003134</td>
        </tr>
        <tr>
          <th>18</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>114952</td>
          <td>0.009929</td>
          <td>0.009477</td>
          <td>7</td>
          <td>0.074473</td>
        </tr>
        <tr>
          <th>19</th>
          <td>8</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>114952</td>
          <td>0.000218</td>
          <td>0.000221</td>
          <td>7</td>
          <td>0.001768</td>
        </tr>
        <tr>
          <th>20</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>226847</td>
          <td>0.013402</td>
          <td>0.013138</td>
          <td>7</td>
          <td>0.092491</td>
        </tr>
        <tr>
          <th>21</th>
          <td>8</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>226847</td>
          <td>0.000377</td>
          <td>0.000368</td>
          <td>7</td>
          <td>0.002925</td>
        </tr>
        <tr>
          <th>22</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>335254</td>
          <td>0.016574</td>
          <td>0.017075</td>
          <td>7</td>
          <td>0.116400</td>
        </tr>
        <tr>
          <th>23</th>
          <td>8</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>335254</td>
          <td>0.000426</td>
          <td>0.000389</td>
          <td>7</td>
          <td>0.022789</td>
        </tr>
        <tr>
          <th>24</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11673</td>
          <td>0.008325</td>
          <td>0.008347</td>
          <td>7</td>
          <td>0.058635</td>
        </tr>
        <tr>
          <th>25</th>
          <td>4</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11673</td>
          <td>0.000169</td>
          <td>0.000161</td>
          <td>7</td>
          <td>0.001344</td>
        </tr>
        <tr>
          <th>26</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>22212</td>
          <td>0.013365</td>
          <td>0.013688</td>
          <td>7</td>
          <td>0.093536</td>
        </tr>
        <tr>
          <th>27</th>
          <td>4</td>
          <td>4</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>22212</td>
          <td>0.000228</td>
          <td>0.000219</td>
          <td>7</td>
          <td>0.001773</td>
        </tr>
        <tr>
          <th>28</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>32749</td>
          <td>0.016798</td>
          <td>0.016740</td>
          <td>7</td>
          <td>0.117199</td>
        </tr>
        <tr>
          <th>29</th>
          <td>4</td>
          <td>4</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>32749</td>
          <td>0.000335</td>
          <td>0.000328</td>
          <td>7</td>
          <td>0.002524</td>
        </tr>
        <tr>
          <th>30</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>30290</td>
          <td>0.007367</td>
          <td>0.007323</td>
          <td>7</td>
          <td>0.052338</td>
        </tr>
        <tr>
          <th>31</th>
          <td>4</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>30290</td>
          <td>0.000188</td>
          <td>0.000179</td>
          <td>7</td>
          <td>0.001521</td>
        </tr>
        <tr>
          <th>32</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>67619</td>
          <td>0.012100</td>
          <td>0.012238</td>
          <td>7</td>
          <td>0.084379</td>
        </tr>
        <tr>
          <th>33</th>
          <td>4</td>
          <td>6</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>67619</td>
          <td>0.000298</td>
          <td>0.000294</td>
          <td>7</td>
          <td>0.002286</td>
        </tr>
        <tr>
          <th>34</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>101516</td>
          <td>0.017729</td>
          <td>0.016861</td>
          <td>7</td>
          <td>0.125102</td>
        </tr>
        <tr>
          <th>35</th>
          <td>4</td>
          <td>6</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>101516</td>
          <td>0.000431</td>
          <td>0.000431</td>
          <td>7</td>
          <td>0.003188</td>
        </tr>
        <tr>
          <th>36</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>71504</td>
          <td>0.008022</td>
          <td>0.007967</td>
          <td>7</td>
          <td>0.056057</td>
        </tr>
        <tr>
          <th>37</th>
          <td>4</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>71504</td>
          <td>0.000186</td>
          <td>0.000195</td>
          <td>7</td>
          <td>0.001497</td>
        </tr>
        <tr>
          <th>38</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>144705</td>
          <td>0.011905</td>
          <td>0.011980</td>
          <td>7</td>
          <td>0.082227</td>
        </tr>
        <tr>
          <th>39</th>
          <td>4</td>
          <td>8</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>144705</td>
          <td>0.000403</td>
          <td>0.000405</td>
          <td>7</td>
          <td>0.002959</td>
        </tr>
        <tr>
          <th>40</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>219945</td>
          <td>0.014526</td>
          <td>0.014843</td>
          <td>7</td>
          <td>0.101083</td>
        </tr>
        <tr>
          <th>41</th>
          <td>4</td>
          <td>8</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>219945</td>
          <td>0.000421</td>
          <td>0.000416</td>
          <td>7</td>
          <td>0.003187</td>
        </tr>
        <tr>
          <th>42</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>107465</td>
          <td>0.007807</td>
          <td>0.008008</td>
          <td>7</td>
          <td>0.054330</td>
        </tr>
        <tr>
          <th>43</th>
          <td>4</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>107465</td>
          <td>0.000289</td>
          <td>0.000283</td>
          <td>7</td>
          <td>0.002224</td>
        </tr>
        <tr>
          <th>44</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>221779</td>
          <td>0.012670</td>
          <td>0.012907</td>
          <td>7</td>
          <td>0.088462</td>
        </tr>
        <tr>
          <th>45</th>
          <td>4</td>
          <td>10</td>
          <td>20</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>221779</td>
          <td>0.000445</td>
          <td>0.000435</td>
          <td>7</td>
          <td>0.003255</td>
        </tr>
        <tr>
          <th>46</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>331281</td>
          <td>0.016064</td>
          <td>0.015825</td>
          <td>7</td>
          <td>0.118449</td>
        </tr>
        <tr>
          <th>47</th>
          <td>4</td>
          <td>10</td>
          <td>30</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>331281</td>
          <td>0.001272</td>
          <td>0.000856</td>
          <td>7</td>
          <td>0.022613</td>
        </tr>
        <tr>
          <th>48</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>11600</td>
          <td>0.001937</td>
          <td>0.002212</td>
          <td>7</td>
          <td>0.013395</td>
        </tr>
        <tr>
          <th>49</th>
          <td>1</td>
          <td>4</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>11600</td>
          <td>0.000523</td>
          <td>0.000533</td>
          <td>7</td>
          <td>0.004870</td>
        </tr>
        <tr>
          <th>50</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>34159</td>
          <td>0.001861</td>
          <td>0.001879</td>
          <td>7</td>
          <td>0.013240</td>
        </tr>
        <tr>
          <th>51</th>
          <td>1</td>
          <td>6</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>34159</td>
          <td>0.000431</td>
          <td>0.000419</td>
          <td>7</td>
          <td>0.003082</td>
        </tr>
        <tr>
          <th>52</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>70489</td>
          <td>0.001608</td>
          <td>0.001600</td>
          <td>7</td>
          <td>0.011164</td>
        </tr>
        <tr>
          <th>53</th>
          <td>1</td>
          <td>8</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>70489</td>
          <td>0.000464</td>
          <td>0.000464</td>
          <td>7</td>
          <td>0.003275</td>
        </tr>
        <tr>
          <th>54</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>base</td>
          <td>1000</td>
          <td>10</td>
          <td>110428</td>
          <td>0.001576</td>
          <td>0.001558</td>
          <td>7</td>
          <td>0.011071</td>
        </tr>
        <tr>
          <th>55</th>
          <td>1</td>
          <td>10</td>
          <td>10</td>
          <td>7</td>
          <td>5</td>
          <td>ort_</td>
          <td>1000</td>
          <td>10</td>
          <td>110428</td>
          <td>0.000532</td>
          <td>0.000524</td>
          <td>7</td>
          <td>0.003770</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 264-266

Plot
++++

.. GENERATED FROM PYTHON SOURCE LINES 266-308

.. code-block:: default


    n_rows = len(n_jobs)
    n_cols = len(n_ests)


    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))
    fig.suptitle(f"{rf.__class__.__name__}\nX.shape={X.shape}")

    for n_j, n_estimators in tqdm(product(n_jobs, n_ests)):
        i = n_jobs.index(n_j)
        j = n_ests.index(n_estimators)
        ax = axes[i, j]

        subdf = df[(df.n_estimators == n_estimators) & (df.n_jobs == n_j)]
        if subdf.shape[0] == 0:
            continue
        piv = subdf.pivot(index="max_depth", columns="name", values=["avg", "med"])
        piv.plot(ax=ax, title=f"jobs={n_j}, trees={n_estimators}")
        ax.set_ylabel(f"n_jobs={n_j}", fontsize="small")
        ax.set_xlabel("max_depth", fontsize="small")

        # ratio
        ax2 = ax.twinx()
        piv1 = subdf.pivot(index="max_depth", columns="name", values="avg")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "b--", label="speedup avg")

        piv1 = subdf.pivot(index="max_depth", columns="name", values="med")
        piv1["speedup"] = piv1.base / piv1.ort_
        ax2.plot(piv1.index, piv1.speedup, "y--", label="speedup med")
        ax2.legend(fontsize="x-small")

        # 1
        ax2.plot(piv1.index, [1 for _ in piv1.index], "k--", label="no speedup")

    for i in range(axes.shape[0]):
        for j in range(axes.shape[1]):
            axes[i, j].legend(fontsize="small")

    fig.tight_layout()
    fig.savefig(f"{name}-{legend}.png")
    # plt.show()



.. image-sg:: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :alt: RandomForestRegressor X.shape=(1000, 10), jobs=8, trees=10, jobs=8, trees=20, jobs=8, trees=30, jobs=4, trees=10, jobs=4, trees=20, jobs=4, trees=30, jobs=1, trees=10
   :srcset: /auto_examples/images/sphx_glr_plot_benchmark_rf_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0it [00:00, ?it/s]    2it [00:00, 16.56it/s]    5it [00:00, 18.71it/s]    7it [00:00, 19.10it/s]    9it [00:00, 24.01it/s]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  7.878 seconds)


.. _sphx_glr_download_auto_examples_plot_benchmark_rf.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_benchmark_rf.py <plot_benchmark_rf.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_benchmark_rf.ipynb <plot_benchmark_rf.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
