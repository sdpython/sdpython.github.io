{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# LogisticRegression and Clustering\n\nA logistic regression implements a convex partition of the features\nspaces. A clustering algorithm applied before the trainer modifies the\nfeature space in way the partition is not necessarily convex in the\ninitial features. Let's see how.\n\n## A dummy datasets and not convex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nimport pandas\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlinsights.mlmodel import ClassifierAfterKMeans\n\nXs = []\nYs = []\nn = 20\nfor i in range(5):\n    for j in range(4):\n        x1 = numpy.random.rand(n) + i * 1.1\n        x2 = numpy.random.rand(n) + j * 1.1\n        Xs.append(numpy.vstack([x1, x2]).T)\n        cl = numpy.random.randint(0, 4)\n        Ys.extend([cl for i in range(n)])\nX = numpy.vstack(Xs)\nY = numpy.array(Ys)\nX.shape, Y.shape, set(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\nfor i in set(Y):\n    ax.plot(\n        X[i == Y, 0], X[i == Y, 1], \"o\", label=\"cl%d\" % i, color=plt.cm.tab20.colors[i]\n    )\nax.legend()\nax.set_title(\"Classification not convex\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One function to plot classification in 2D\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def draw_border(\n    clr,\n    X,\n    y,\n    fct=None,\n    incx=1,\n    incy=1,\n    figsize=None,\n    border=True,\n    clusters=None,\n    ax=None,\n):\n    # see https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n    # https://matplotlib.org/examples/color/colormaps_reference.html\n\n    h = 0.02  # step size in the mesh\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - incx, X[:, 0].max() + incx\n    y_min, y_max = X[:, 1].min() - incy, X[:, 1].max() + incy\n    xx, yy = numpy.meshgrid(\n        numpy.arange(x_min, x_max, h), numpy.arange(y_min, y_max, h)\n    )\n    if fct is None:\n        Z = clr.predict(numpy.c_[xx.ravel(), yy.ravel()])\n    else:\n        Z = fct(clr, numpy.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    cmap = plt.cm.tab20\n    Z = Z.reshape(xx.shape)\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize or (4, 3))\n    ax.pcolormesh(xx, yy, Z, cmap=cmap)\n\n    # Plot also the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", cmap=cmap)\n    ax.set_xlabel(\"Sepal length\")\n    ax.set_ylabel(\"Sepal width\")\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n\n    # Plot clusters\n    if clusters is not None:\n        mat = []\n        ym = []\n        for k, v in clusters.items():\n            mat.append(v.cluster_centers_)\n            ym.extend(k for i in range(v.cluster_centers_.shape[0]))\n        cx = numpy.vstack(mat)\n        ym = numpy.array(ym)\n        ax.scatter(cx[:, 0], cx[:, 1], c=ym, edgecolors=\"y\", cmap=cmap, s=300)\n    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clr = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\nclr.fit(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = draw_border(clr, X, Y, incx=1, incy=1, figsize=(6, 4), border=False)\nax.set_title(\"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not quite close!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression and k-means\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clk = ClassifierAfterKMeans(e_solver=\"lbfgs\", e_multi_class=\"multinomial\")\nclk.fit(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The centers of the first k-means:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clk.clus_[0].cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = draw_border(\n    clk, X, Y, incx=1, incy=1, figsize=(6, 4), border=False, clusters=clk.clus_\n)\nax.set_title(\"Logistic Regression and K-Means - 2 clusters per class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The big cricles are the centers of the k-means fitted for each class. It\nlook better!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dt = []\nfor cl in range(1, 6):\n    clk = ClassifierAfterKMeans(\n        c_n_clusters=cl, e_solver=\"lbfgs\", e_multi_class=\"multinomial\", e_max_iter=700\n    )\n    clk.fit(X, Y)\n    sc = clk.score(X, Y)\n    dt.append(dict(score=sc, nb_clusters=cl))\n\n\npandas.DataFrame(dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = draw_border(\n    clk, X, Y, incx=1, incy=1, figsize=(6, 4), border=False, clusters=clk.clus_\n)\nax.set_title(\"Logistic Regression and K-Means - 8 clusters per class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The random forest works without any clustering as expected.\n\n\nrf = RandomForestClassifier(n_estimators=20)\nrf.fit(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = draw_border(rf, X, Y, incx=1, incy=1, figsize=(6, 4), border=False)\nax.set_title(\"Random Forest\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}