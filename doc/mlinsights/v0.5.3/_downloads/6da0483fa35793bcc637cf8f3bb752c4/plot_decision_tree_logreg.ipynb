{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Decision Tree and Logistic Regression\n\nThe notebook demonstrates the model *DecisionTreeLogisticRegression*\nwhich replaces the decision based on one variable by a logistic\nregression.\n\n## Iris dataset and logistic regression\n\nThe following code shows the border defined by two machine learning\nmodels on the [Iris\ndataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom tqdm import tqdm\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom mlinsights.mlmodel import DecisionTreeLogisticRegression\nfrom mlinsights.mltree import predict_leaves\n\n\ndef plot_classifier_decision_zone(clf, X, y, title=None, ax=None):\n    if ax is None:\n        ax = plt.gca()\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    dhx = (x_max - x_min) / 100\n    dhy = (y_max - y_min) / 100\n    xx, yy = numpy.meshgrid(\n        numpy.arange(x_min, x_max, dhx), numpy.arange(y_min, y_max, dhy)\n    )\n\n    Z = clf.predict(numpy.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    ax.contourf(xx, yy, Z, alpha=0.5)\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\", lw=0.5)\n    if title is not None:\n        ax.set_title(title)\n\n\niris = load_iris()\nX = iris.data[:, [0, 2]]\ny = iris.target\ny = y % 2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\nlr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(criterion=\"entropy\")\ndt.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\nplot_classifier_decision_zone(lr, X_test, y_test, ax=ax[0], title=\"LogisticRegression\")\nplot_classifier_decision_zone(\n    dt, X_test, y_test, ax=ax[1], title=\"DecisionTreeClassifier\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The logistic regression is not very stable on this sort of problem. No\nlinear separator can work on this dataset. Let's dig into it.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DecisionTreeLogisticRegression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtlr = DecisionTreeLogisticRegression(\n    estimator=LogisticRegression(solver=\"liblinear\"),\n    min_samples_leaf=10,\n    min_samples_split=10,\n    max_depth=1,\n    fit_improve_algo=\"none\",\n)\ndtlr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtlr2 = DecisionTreeLogisticRegression(\n    estimator=LogisticRegression(solver=\"liblinear\"),\n    min_samples_leaf=4,\n    min_samples_split=4,\n    max_depth=10,\n    fit_improve_algo=\"intercept_sort_always\",\n)\ndtlr2.fit(X_train, y_train)\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 8))\nplot_classifier_decision_zone(\n    dtlr,\n    X_train,\n    y_train,\n    ax=ax[0, 0],\n    title=\"DecisionTreeLogisticRegression\\ndepth=%d - train\" % dtlr.tree_depth_,\n)\nplot_classifier_decision_zone(\n    dtlr2,\n    X_train,\n    y_train,\n    ax=ax[0, 1],\n    title=\"DecisionTreeLogisticRegression\\ndepth=%d - train\" % dtlr2.tree_depth_,\n)\nplot_classifier_decision_zone(\n    dtlr,\n    X_test,\n    y_test,\n    ax=ax[1, 0],\n    title=\"DecisionTreeLogisticRegression\\ndepth=%d - test\" % dtlr.tree_depth_,\n)\nplot_classifier_decision_zone(\n    dtlr2,\n    X_test,\n    y_test,\n    ax=ax[1, 1],\n    title=\"DecisionTreeLogisticRegression\\ndepth=%d - test\" % dtlr2.tree_depth_,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rows = []\nfor model in [lr, dt, dtlr, dtlr2]:\n    val = (\" - depth=%d\" % model.tree_depth_) if hasattr(model, \"tree_depth_\") else \"\"\n    obs = dict(\n        name=\"%s%s\" % (model.__class__.__name__, val), score=model.score(X_test, y_test)\n    )\n    rows.append(obs)\n\nDataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A first example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def random_set_simple(n):\n    X = numpy.random.rand(n, 2)\n    y = ((X[:, 0] ** 2 + X[:, 1] ** 2) <= 1).astype(numpy.int32).ravel()\n    return X, y\n\n\nX, y = random_set_simple(2000)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)\ndt8 = DecisionTreeClassifier(max_depth=10)\ndt8.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\nplot_classifier_decision_zone(\n    dt,\n    X_test,\n    y_test,\n    ax=ax[0],\n    title=\"DecisionTree - max_depth=%d\\nacc=%1.2f\"\n    % (dt.max_depth, dt.score(X_test, y_test)),\n)\nplot_classifier_decision_zone(\n    dt8,\n    X_test,\n    y_test,\n    ax=ax[1],\n    title=\"DecisionTree - max_depth=%d\\nacc=%1.2f\"\n    % (dt8.max_depth, dt8.score(X_test, y_test)),\n)\nax[0].set_xlim([0, 1])\nax[1].set_xlim([0, 1])\nax[0].set_ylim([0, 1])\n\ndtlr = DecisionTreeLogisticRegression(\n    max_depth=3, fit_improve_algo=\"intercept_sort_always\", verbose=1\n)\ndtlr.fit(X_train, y_train)\ndtlr8 = DecisionTreeLogisticRegression(\n    max_depth=10, min_samples_split=4, fit_improve_algo=\"intercept_sort_always\"\n)\ndtlr8.fit(X_train, y_train)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\nplot_classifier_decision_zone(\n    dtlr,\n    X_test,\n    y_test,\n    ax=ax[0],\n    title=\"DecisionTreeLogReg - depth=%d\\nacc=%1.2f\"\n    % (dtlr.tree_depth_, dtlr.score(X_test, y_test)),\n)\nplot_classifier_decision_zone(\n    dtlr8,\n    X_test,\n    y_test,\n    ax=ax[1],\n    title=\"DecisionTreeLogReg - depth=%d\\nacc=%1.2f\"\n    % (dtlr8.tree_depth_, dtlr8.score(X_test, y_test)),\n)\nax[0].set_xlim([0, 1])\nax[1].set_xlim([0, 1])\nax[0].set_ylim([0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def draw_border(\n    clr,\n    X,\n    y,\n    fct=None,\n    incx=0.1,\n    incy=0.1,\n    figsize=None,\n    border=True,\n    ax=None,\n    s=10.0,\n    linewidths=0.1,\n):\n    h = 0.02\n    x_min, x_max = X[:, 0].min() - incx, X[:, 0].max() + incx\n    y_min, y_max = X[:, 1].min() - incy, X[:, 1].max() + incy\n    xx, yy = numpy.meshgrid(\n        numpy.arange(x_min, x_max, h), numpy.arange(y_min, y_max, h)\n    )\n    if fct is None:\n        Z = clr.predict(numpy.c_[xx.ravel(), yy.ravel()])\n    else:\n        Z = fct(clr, numpy.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    cmap = plt.cm.tab20\n    Z = Z.reshape(xx.shape)\n    if ax is None:\n        _fig, ax = plt.subplots(1, 1, figsize=figsize or (4, 3))\n    ax.pcolormesh(xx, yy, Z, cmap=cmap)\n\n    # Plot also the training points\n    ax.scatter(\n        X[:, 0], X[:, 1], c=y, edgecolors=\"k\", cmap=cmap, s=s, linewidths=linewidths\n    )\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    return ax\n\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 4))\ndraw_border(dt, X_test, y_test, border=False, ax=ax[0])\nax[0].set_title(\"Iris\")\ndraw_border(dt, X, y, border=False, ax=ax[1], fct=lambda m, x: predict_leaves(m, x))\nax[1].set_title(\"DecisionTree\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(6, 4, figsize=(12, 16))\nfor i, depth in tqdm(enumerate((1, 2, 3, 4, 5, 6))):\n    dtl = DecisionTreeLogisticRegression(\n        max_depth=depth, fit_improve_algo=\"intercept_sort_always\", min_samples_leaf=2\n    )\n    dtl.fit(X_train, y_train)\n    draw_border(dtl, X_test, y_test, border=False, ax=ax[i, 0], s=4.0)\n    draw_border(\n        dtl,\n        X,\n        y,\n        border=False,\n        ax=ax[i, 1],\n        fct=lambda m, x: predict_leaves(m, x),\n        s=4.0,\n    )\n    ax[i, 0].set_title(\n        \"Depth=%d nodes=%d score=%1.2f\"\n        % (dtl.tree_depth_, dtl.n_nodes_, dtl.score(X_test, y_test))\n    )\n    ax[i, 1].set_title(\"DTLR Leaves zones\")\n\n    dtl = DecisionTreeClassifier(max_depth=depth)\n    dtl.fit(X_train, y_train)\n    draw_border(dtl, X_test, y_test, border=False, ax=ax[i, 2], s=4.0)\n    draw_border(\n        dtl,\n        X,\n        y,\n        border=False,\n        ax=ax[i, 3],\n        fct=lambda m, x: predict_leaves(m, x),\n        s=4.0,\n    )\n    ax[i, 2].set_title(\n        \"Depth=%d nodes=%d score=%1.2f\"\n        % (dtl.max_depth, dtl.tree_.node_count, dtl.score(X_test, y_test))\n    )\n    ax[i, 3].set_title(\"DT Leaves zones\")\n\n    for k in range(ax.shape[1]):\n        ax[i, k].get_xaxis().set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Another example designed to fail\n\nDesigned to be difficult with a regular decision tree.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def random_set(n):\n    X = numpy.random.rand(n, 2)\n    y = (\n        (cdist(X, numpy.array([[0.5, 0.5]]), metric=\"minkowski\", p=1) <= 0.5)\n        .astype(numpy.int32)\n        .ravel()\n    )\n    return X, y\n\n\nX, y = random_set(2000)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)\ndt8 = DecisionTreeClassifier(max_depth=10)\ndt8.fit(X_train, y_train)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\nplot_classifier_decision_zone(\n    dt,\n    X_test,\n    y_test,\n    ax=ax[0],\n    title=\"DecisionTree - max_depth=%d\\nacc=%1.2f\"\n    % (dt.max_depth, dt.score(X_test, y_test)),\n)\nplot_classifier_decision_zone(\n    dt8,\n    X_test,\n    y_test,\n    ax=ax[1],\n    title=\"DecisionTree - max_depth=%d\\nacc=%1.2f\"\n    % (dt8.max_depth, dt8.score(X_test, y_test)),\n)\nax[0].set_xlim([0, 1])\nax[1].set_xlim([0, 1])\nax[0].set_ylim([0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The example is a square rotated by 45 degrees. Every sample in the\nsquare is a positive sample, every sample outside is a negative one. The\ntree approximates the border with horizontal and vertical lines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtlr = DecisionTreeLogisticRegression(\n    max_depth=3, fit_improve_algo=\"intercept_sort_always\", verbose=1\n)\ndtlr.fit(X_train, y_train)\ndtlr8 = DecisionTreeLogisticRegression(\n    max_depth=10, min_samples_split=4, fit_improve_algo=\"intercept_sort_always\"\n)\ndtlr8.fit(X_train, y_train)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\nplot_classifier_decision_zone(\n    dtlr,\n    X_test,\n    y_test,\n    ax=ax[0],\n    title=\"DecisionTreeLogReg - depth=%d\\nacc=%1.2f\"\n    % (dtlr.tree_depth_, dtlr.score(X_test, y_test)),\n)\nplot_classifier_decision_zone(\n    dtlr8,\n    X_test,\n    y_test,\n    ax=ax[1],\n    title=\"DecisionTreeLogReg - depth=%d\\nacc=%1.2f\"\n    % (dtlr8.tree_depth_, dtlr8.score(X_test, y_test)),\n)\nax[0].set_xlim([0, 1])\nax[1].set_xlim([0, 1])\nax[0].set_ylim([0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leave zones\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We use method *decision_path* to understand which leaf is responsible\n# for which zone.\n\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 4))\ndraw_border(dtlr, X_test, y_test, border=False, ax=ax[0])\nax[0].set_title(\"Iris\")\ndraw_border(dtlr, X, y, border=False, ax=ax[1], fct=lambda m, x: predict_leaves(m, x))\nax[1].set_title(\"DecisionTreeLogisticRegression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(6, 4, figsize=(12, 16))\nfor i, depth in tqdm(enumerate((1, 2, 3, 4, 5, 6))):\n    dtl = DecisionTreeLogisticRegression(\n        max_depth=depth, fit_improve_algo=\"intercept_sort_always\", min_samples_leaf=2\n    )\n    dtl.fit(X_train, y_train)\n    draw_border(dtl, X_test, y_test, border=False, ax=ax[i, 0], s=4.0)\n    draw_border(\n        dtl,\n        X,\n        y,\n        border=False,\n        ax=ax[i, 1],\n        fct=lambda m, x: predict_leaves(m, x),\n        s=4.0,\n    )\n    ax[i, 0].set_title(\n        \"Depth=%d nodes=%d score=%1.2f\"\n        % (dtl.tree_depth_, dtl.n_nodes_, dtl.score(X_test, y_test))\n    )\n    ax[i, 1].set_title(\"DTLR Leaves zones\")\n\n    dtl = DecisionTreeClassifier(max_depth=depth)\n    dtl.fit(X_train, y_train)\n    draw_border(dtl, X_test, y_test, border=False, ax=ax[i, 2], s=4.0)\n    draw_border(\n        dtl,\n        X,\n        y,\n        border=False,\n        ax=ax[i, 3],\n        fct=lambda m, x: predict_leaves(m, x),\n        s=4.0,\n    )\n    ax[i, 2].set_title(\n        \"Depth=%d nodes=%d score=%1.2f\"\n        % (dtl.max_depth, dtl.tree_.node_count, dtl.score(X_test, y_test))\n    )\n    ax[i, 3].set_title(\"DT Leaves zones\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}