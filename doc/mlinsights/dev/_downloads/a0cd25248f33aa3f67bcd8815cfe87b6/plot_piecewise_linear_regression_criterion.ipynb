{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Custom DecisionTreeRegressor adapted to a linear regression\n \nA :class:`sklearn.tree.DecisionTreeRegressor`\ncan be trained with a couple of possible criterions but it is possible \nto implement a custom one (see [hellinger_distance_criterion](https://github.com/EvgeniDubov/hellinger-distance-criterion/blob/master/hellinger_distance_criterion.pyx)).\nSee also tutorial\n[Cython example of exposing C-computed arrays in Python without data copies](http://gael-varoquaux.info/programming/cython-example-of-exposing-c-computed-arrays-in-python-without-data-copies.html)\nwhich describes a way to implement fast :epkg:`Cython` extensions.\n\n## Piecewise data\n\nLet's build a toy problem based on two linear models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy\nimport numpy.random as npr\nfrom mlinsights.ext_test_case import measure_time\nfrom mlinsights.mlmodel.piecewise_tree_regression import PiecewiseTreeRegressor\nfrom mlinsights.mlmodel.piecewise_tree_regression_criterion import (\n    SimpleRegressorCriterion,\n)\nfrom mlinsights.mlmodel.piecewise_tree_regression_criterion_fast import (\n    SimpleRegressorCriterionFast,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\nX = npr.normal(size=(1000, 4))\nalpha = [4, -2]\nt = (X[:, 0] + X[:, 3] * 0.5) > 0\nswitch = numpy.zeros(X.shape[0])\nswitch[t] = 1\ny = alpha[0] * X[:, 0] * t + alpha[1] * X[:, 0] * (1 - t) + X[:, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\nax.plot(X[:, 0], y, \".\")\nax.set_title(\"Piecewise examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DecisionTreeRegressor\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[:, :1], y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeRegressor(min_samples_leaf=100)\nmodel.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = model.predict(X_test)\npred[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\nax.plot(X_test[:, 0], y_test, \".\", label=\"data\")\nax.plot(X_test[:, 0], pred, \".\", label=\"predictions\")\nax.set_title(\"DecisionTreeRegressor\")\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DecisionTreeRegressor with custom implementation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model2 = DecisionTreeRegressor(\n    min_samples_leaf=100, criterion=SimpleRegressorCriterion(1, X_train.shape[0])\n)\nmodel2.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = model2.predict(X_test)\npred[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\nax.plot(X_test[:, 0], y_test, \".\", label=\"data\")\nax.plot(X_test[:, 0], pred, \".\", label=\"predictions\")\nax.set_title(\"DecisionTreeRegressor\\nwith custom criterion\")\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computation time\n\nThe custom criterion is not really efficient but it was meant that way.\nThe code can be found in [piecewise_tree_regression_criterion](https://github.com/sdpython/mlinsights/blob/main/src/mlinsights/mlmodel/piecewise_tree_regression_criterion.pyx).\nBascially, it is slow because each time the algorithm optimizing the\ntree needs the class Criterion to evaluate the impurity reduction for a split,\nthe computation happens on the whole data under the node being split.\nThe implementation in [_criterion.pyx](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_criterion.pyx)\ndoes it once.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measure_time(\"model.fit(X_train, y_train)\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measure_time(\"model2.fit(X_train, y_train)\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A loop is involved every time the criterion of the node is involved\nwhich raises a the computation cost of lot. The method ``_mse``\nis called each time the algorithm training the decision tree needs\nto evaluate a cut, one cut involves elements betwee, position\n``[start, end[``.\n\n::\n\n   cdef void _mean(self, SIZE_t start, SIZE_t end, DOUBLE_t *mean,\n                   DOUBLE_t *weight) nogil:\n       if start == end:\n           mean[0] = 0.\n           return\n       cdef DOUBLE_t m = 0.\n       cdef DOUBLE_t w = 0.\n       cdef int k\n       for k in range(start, end):\n           m += self.sample_wy[k]\n           w += self.sample_w[k]\n       weight[0] = w\n       mean[0] = 0. if w == 0. else m / w\n\n   cdef double _mse(self, SIZE_t start, SIZE_t end, DOUBLE_t mean,\n                    DOUBLE_t weight) nogil:\n       if start == end:\n           return 0.\n       cdef DOUBLE_t squ = 0.\n       cdef int k\n       for k in range(start, end):\n           squ += (self.y[self.sample_i[k], 0] - mean) ** 2 * self.sample_w[k]\n       return 0. if weight == 0. else squ / weight\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Better implementation\n\nI rewrote my first implementation to be closer to what\n:epkg:`scikit-learn` is doing. The criterion is computed once\nfor all possible cut and then retrieved on demand.\nThe code is below, arrays ``sample_wy_left`` is the cumulated sum\nof $weight * Y$ starting from the left side\n(lower *Y*). The loop disappeared.\n\n::\n\n   cdef void _mean(self, SIZE_t start, SIZE_t end, DOUBLE_t *mean,\n                   DOUBLE_t *weight) nogil:\n       if start == end:\n           mean[0] = 0.\n           return\n       cdef DOUBLE_t m = self.sample_wy_left[end-1] -\n                         (self.sample_wy_left[start-1] if start > 0 else 0)\n       cdef DOUBLE_t w = self.sample_w_left[end-1] -\n                         (self.sample_w_left[start-1] if start > 0 else 0)\n       weight[0] = w\n       mean[0] = 0. if w == 0. else m / w\n\n   cdef double _mse(self, SIZE_t start, SIZE_t end, DOUBLE_t mean,\n                    DOUBLE_t weight) nogil:\n       if start == end:\n           return 0.\n       cdef DOUBLE_t squ = self.sample_wy2_left[end-1] -\n                           (self.sample_wy2_left[start-1] if start > 0 else 0)\n       # This formula only holds if mean is computed on the same interval.\n       # Otherwise, it is squ / weight - true_mean ** 2 + (mean - true_mean) ** 2.\n       return 0. if weight == 0. else squ / weight - mean ** 2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model3 = DecisionTreeRegressor(\n    min_samples_leaf=100, criterion=SimpleRegressorCriterionFast(1, X_train.shape[0])\n)\nmodel3.fit(X_train, y_train)\npred = model3.predict(X_test)\npred[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\nax.plot(X_test[:, 0], y_test, \".\", label=\"data\")\nax.plot(X_test[:, 0], pred, \".\", label=\"predictions\")\nax.set_title(\"DecisionTreeRegressor\\nwith fast custom criterion\")\nax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measure_time(\"model3.fit(X_train, y_train)\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Much better even though this implementation is currently 3, 4 times\nslower than scikit-learn's. Let's check with a datasets three times\nbigger to see if it is a fix cost or a cost.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train3 = numpy.vstack([X_train, X_train, X_train])\ny_train3 = numpy.hstack([y_train, y_train, y_train])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_train3.shape, y_train3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measure_time(\"model.fit(X_train3, y_train3)\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The criterion needs to be reinstanciated since it depends on the features\n*X*. The computation does not but the design does. This was introduced to\ncompare the current output with a decision tree optimizing for\na piecewise linear regression and not a stepwise regression.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    model3.fit(X_train3, y_train3)\nexcept Exception as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model3 = DecisionTreeRegressor(\n    min_samples_leaf=100, criterion=SimpleRegressorCriterionFast(1, X_train3.shape[0])\n)\nmeasure_time(\"model3.fit(X_train3, y_train3)\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Still almost 2 times slower but of the same order of magnitude.\nWe could go further and investigate why or continue and introduce a\ncriterion which optimizes a piecewise linear regression instead of a\nstepwise regression.\n\n## Criterion adapted for a linear regression\n\nThe previous examples are all about decision trees which approximates a\nfunction by a stepwise function. On every interval $[r_1, r_2]$,\nthe model optimizes\n$\\sum_i (y_i - C)^2 \\mathbb{1}_{ r_1 \\leqslant x_i \\leqslant r_2}$\nand finds the best constant (= the average)\napproxmating the function on this interval.\nWe would to like to approximate the function by a regression line and not a\nconstant anymore. It means minimizing\n$\\sum_i (y_i - X_i \\beta)^2 \\mathbb{1}_{ r_1 \\leqslant x_i \\leqslant r_2}$.\nDoing this require to change the criterion used to split the space of feature\ninto buckets and the prediction function of the decision tree which now\nneeds to return a dot product.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fixed = False\nif fixed:\n    # It does not work yet.\n    piece = PiecewiseTreeRegressor(criterion=\"mselin\", min_samples_leaf=100)\n    piece.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if fixed:\n    pred = piece.predict(X_test)\n    pred[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if fixed:\n    fig, ax = plt.subplots(1, 1)\n    ax.plot(X_test[:, 0], y_test, \".\", label=\"data\")\n    ax.plot(X_test[:, 0], pred, \".\", label=\"predictions\")\n    ax.set_title(\"DecisionTreeRegressor\\nwith criterion adapted to linear regression\")\n    ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The coefficients for the linear regressions are kept into the following attribute:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if fixed:\n    piece.betas_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mapped to the following leaves:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if fixed:\n    piece.leaves_index_, piece.leaves_mapping_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can get the leave each observation falls into:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if fixed:\n    piece.predict_leaves(X_test)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training is quite slow as it is training many\nlinear regressions each time a split is evaluated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if fixed:\n    measure_time(\"piece.fit(X_train, y_train)\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if fixed:\n    measure_time(\"piece.fit(X_train3, y_train3)\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works but it is slow, slower than the slow implementation\nof the standard criterion for decision tree regression.\n\n## Next\n\nPR [Model trees (M5P and co)](https://github.com/scikit-learn/scikit-learn/issues/13106)\nand issue [Model trees (M5P)](https://github.com/scikit-learn/scikit-learn/pull/13732)\npropose an implementation a piecewise regression with any kind of regression model.\nIt is based on [Building Model Trees](https://github.com/ankonzoid/LearningX/tree/master/advanced_ML/model_tree).\nIt fits many models to find the best splits and should be slower than this\nimplementation in the case of a decision tree regressor\nassociated with linear regressions.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}