
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_piecewise_linear_regression_criterion.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_piecewise_linear_regression_criterion.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_piecewise_linear_regression_criterion.py:


Custom DecisionTreeRegressor adapted to a linear regression
===========================================================
 
A :class:`sklearn.tree.DecisionTreeRegressor`
can be trained with a couple of possible criterions but it is possible 
to implement a custom one (see `hellinger_distance_criterion
<https://github.com/EvgeniDubov/hellinger-distance-criterion/blob/master/hellinger_distance_criterion.pyx>`_).
See also tutorial
`Cython example of exposing C-computed arrays in Python without data copies
<http://gael-varoquaux.info/programming/cython-example-of-exposing-c-computed-arrays-in-python-without-data-copies.html>`_
which describes a way to implement fast :epkg:`Cython` extensions.

Piecewise data
++++++++++++++

Let's build a toy problem based on two linear models.

.. GENERATED FROM PYTHON SOURCE LINES 19-43

.. code-block:: default



    import matplotlib.pyplot as plt
    import numpy
    import numpy.random as npr
    from mlinsights.ext_test_case import measure_time
    from mlinsights.mlmodel.piecewise_tree_regression import PiecewiseTreeRegressor
    from mlinsights.mlmodel.piecewise_tree_regression_criterion import (
        SimpleRegressorCriterion,
    )
    from mlinsights.mlmodel.piecewise_tree_regression_criterion_fast import (
        SimpleRegressorCriterionFast,
    )
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeRegressor

    X = npr.normal(size=(1000, 4))
    alpha = [4, -2]
    t = (X[:, 0] + X[:, 3] * 0.5) > 0
    switch = numpy.zeros(X.shape[0])
    switch[t] = 1
    y = alpha[0] * X[:, 0] * t + alpha[1] * X[:, 0] * (1 - t) + X[:, 2]









.. GENERATED FROM PYTHON SOURCE LINES 45-52

.. code-block:: default



    fig, ax = plt.subplots(1, 1)
    ax.plot(X[:, 0], y, ".")
    ax.set_title("Piecewise examples")





.. image-sg:: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_001.png
   :alt: Piecewise examples
   :srcset: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Text(0.5, 1.0, 'Piecewise examples')



.. GENERATED FROM PYTHON SOURCE LINES 53-55

DecisionTreeRegressor
+++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 55-60

.. code-block:: default



    X_train, X_test, y_train, y_test = train_test_split(X[:, :1], y)









.. GENERATED FROM PYTHON SOURCE LINES 62-68

.. code-block:: default



    model = DecisionTreeRegressor(min_samples_leaf=100)
    model.fit(X_train, y_train)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(min_samples_leaf=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(min_samples_leaf=100)</pre></div></div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 70-76

.. code-block:: default



    pred = model.predict(X_test)
    pred[:5]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([0.0938218 , 5.603082  , 2.09844053, 0.8745272 , 1.15558524])



.. GENERATED FROM PYTHON SOURCE LINES 78-87

.. code-block:: default



    fig, ax = plt.subplots(1, 1)
    ax.plot(X_test[:, 0], y_test, ".", label="data")
    ax.plot(X_test[:, 0], pred, ".", label="predictions")
    ax.set_title("DecisionTreeRegressor")
    ax.legend()





.. image-sg:: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_002.png
   :alt: DecisionTreeRegressor
   :srcset: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7efcb04ee140>



.. GENERATED FROM PYTHON SOURCE LINES 88-90

DecisionTreeRegressor with custom implementation
================================================

.. GENERATED FROM PYTHON SOURCE LINES 94-102

.. code-block:: default



    model2 = DecisionTreeRegressor(
        min_samples_leaf=100, criterion=SimpleRegressorCriterion(1, X_train.shape[0])
    )
    model2.fit(X_train, y_train)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(criterion=&lt;mlinsights.mlmodel.piecewise_tree_regression_criterion.SimpleRegressorCriterion object at 0x5648f2a90fa0&gt;,
                          min_samples_leaf=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(criterion=&lt;mlinsights.mlmodel.piecewise_tree_regression_criterion.SimpleRegressorCriterion object at 0x5648f2a90fa0&gt;,
                          min_samples_leaf=100)</pre></div></div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 104-110

.. code-block:: default



    pred = model2.predict(X_test)
    pred[:5]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([0.07928335, 5.37381102, 1.77155748, 0.55809551, 1.15558524])



.. GENERATED FROM PYTHON SOURCE LINES 112-121

.. code-block:: default



    fig, ax = plt.subplots(1, 1)
    ax.plot(X_test[:, 0], y_test, ".", label="data")
    ax.plot(X_test[:, 0], pred, ".", label="predictions")
    ax.set_title("DecisionTreeRegressor\nwith custom criterion")
    ax.legend()





.. image-sg:: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_003.png
   :alt: DecisionTreeRegressor with custom criterion
   :srcset: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7efcb0598c10>



.. GENERATED FROM PYTHON SOURCE LINES 122-134

Computation time
++++++++++++++++

The custom criterion is not really efficient but it was meant that way.
The code can be found in `piecewise_tree_regression_criterion
<https://github.com/sdpython/mlinsights/blob/main/src/mlinsights/mlmodel/piecewise_tree_regression_criterion.pyx>`_.
Bascially, it is slow because each time the algorithm optimizing the
tree needs the class Criterion to evaluate the impurity reduction for a split,
the computation happens on the whole data under the node being split.
The implementation in `_criterion.pyx
<https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_criterion.pyx>`_
does it once.

.. GENERATED FROM PYTHON SOURCE LINES 134-139

.. code-block:: default



    measure_time("model.fit(X_train, y_train)", globals())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'average': 0.0019556138000007195, 'deviation': 0.0009423924172708237, 'min_exec': 0.0008730760000003102, 'max_exec': 0.0039576280000005685, 'repeat': 10, 'number': 50, 'ttime': 0.019556138000007196, 'context_size': 1176, 'warmup_time': 0.0014233000000558604}



.. GENERATED FROM PYTHON SOURCE LINES 141-146

.. code-block:: default



    measure_time("model2.fit(X_train, y_train)", globals())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'average': 0.012786253599999553, 'deviation': 0.0030817916498254655, 'min_exec': 0.008734299999998711, 'max_exec': 0.01898910200000046, 'repeat': 10, 'number': 50, 'ttime': 0.12786253599999553, 'context_size': 1176, 'warmup_time': 0.014135100000089551}



.. GENERATED FROM PYTHON SOURCE LINES 147-178

A loop is involved every time the criterion of the node is involved
which raises a the computation cost of lot. The method ``_mse``
is called each time the algorithm training the decision tree needs
to evaluate a cut, one cut involves elements betwee, position
``[start, end[``.

::

   cdef void _mean(self, SIZE_t start, SIZE_t end, DOUBLE_t *mean,
                   DOUBLE_t *weight) nogil:
       if start == end:
           mean[0] = 0.
           return
       cdef DOUBLE_t m = 0.
       cdef DOUBLE_t w = 0.
       cdef int k
       for k in range(start, end):
           m += self.sample_wy[k]
           w += self.sample_w[k]
       weight[0] = w
       mean[0] = 0. if w == 0. else m / w

   cdef double _mse(self, SIZE_t start, SIZE_t end, DOUBLE_t mean,
                    DOUBLE_t weight) nogil:
       if start == end:
           return 0.
       cdef DOUBLE_t squ = 0.
       cdef int k
       for k in range(start, end):
           squ += (self.y[self.sample_i[k], 0] - mean) ** 2 * self.sample_w[k]
       return 0. if weight == 0. else squ / weight

.. GENERATED FROM PYTHON SOURCE LINES 180-213

Better implementation
+++++++++++++++++++++

I rewrote my first implementation to be closer to what
:epkg:`scikit-learn` is doing. The criterion is computed once
for all possible cut and then retrieved on demand.
The code is below, arrays ``sample_wy_left`` is the cumulated sum
of :math:`weight * Y` starting from the left side
(lower *Y*). The loop disappeared.

::

   cdef void _mean(self, SIZE_t start, SIZE_t end, DOUBLE_t *mean,
                   DOUBLE_t *weight) nogil:
       if start == end:
           mean[0] = 0.
           return
       cdef DOUBLE_t m = self.sample_wy_left[end-1] -
                         (self.sample_wy_left[start-1] if start > 0 else 0)
       cdef DOUBLE_t w = self.sample_w_left[end-1] -
                         (self.sample_w_left[start-1] if start > 0 else 0)
       weight[0] = w
       mean[0] = 0. if w == 0. else m / w

   cdef double _mse(self, SIZE_t start, SIZE_t end, DOUBLE_t mean,
                    DOUBLE_t weight) nogil:
       if start == end:
           return 0.
       cdef DOUBLE_t squ = self.sample_wy2_left[end-1] -
                           (self.sample_wy2_left[start-1] if start > 0 else 0)
       # This formula only holds if mean is computed on the same interval.
       # Otherwise, it is squ / weight - true_mean ** 2 + (mean - true_mean) ** 2.
       return 0. if weight == 0. else squ / weight - mean ** 2

.. GENERATED FROM PYTHON SOURCE LINES 216-226

.. code-block:: default



    model3 = DecisionTreeRegressor(
        min_samples_leaf=100, criterion=SimpleRegressorCriterionFast(1, X_train.shape[0])
    )
    model3.fit(X_train, y_train)
    pred = model3.predict(X_test)
    pred[:5]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([0.07928335, 5.37381102, 1.77155748, 0.55809551, 1.15558524])



.. GENERATED FROM PYTHON SOURCE LINES 228-237

.. code-block:: default



    fig, ax = plt.subplots(1, 1)
    ax.plot(X_test[:, 0], y_test, ".", label="data")
    ax.plot(X_test[:, 0], pred, ".", label="predictions")
    ax.set_title("DecisionTreeRegressor\nwith fast custom criterion")
    ax.legend()





.. image-sg:: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_004.png
   :alt: DecisionTreeRegressor with fast custom criterion
   :srcset: /auto_examples/images/sphx_glr_plot_piecewise_linear_regression_criterion_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7efcb03ffcd0>



.. GENERATED FROM PYTHON SOURCE LINES 239-244

.. code-block:: default



    measure_time("model3.fit(X_train, y_train)", globals())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'average': 0.002707601000000523, 'deviation': 0.0005112412251705095, 'min_exec': 0.0017382620000012138, 'max_exec': 0.003291436000001795, 'repeat': 10, 'number': 50, 'ttime': 0.02707601000000523, 'context_size': 1176, 'warmup_time': 0.0023509000000103697}



.. GENERATED FROM PYTHON SOURCE LINES 245-248

Much better even though this implementation is currently 3, 4 times
slower than scikit-learn's. Let's check with a datasets three times
bigger to see if it is a fix cost or a cost.

.. GENERATED FROM PYTHON SOURCE LINES 248-254

.. code-block:: default



    X_train3 = numpy.vstack([X_train, X_train, X_train])
    y_train3 = numpy.hstack([y_train, y_train, y_train])









.. GENERATED FROM PYTHON SOURCE LINES 256-261

.. code-block:: default



    X_train.shape, X_train3.shape, y_train3.shape






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ((750, 1), (2250, 1), (2250,))



.. GENERATED FROM PYTHON SOURCE LINES 263-267

.. code-block:: default



    measure_time("model.fit(X_train3, y_train3)", globals())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'average': 0.0020086470000003375, 'deviation': 0.0004815767809362375, 'min_exec': 0.001630857999998625, 'max_exec': 0.0029482740000003104, 'repeat': 10, 'number': 50, 'ttime': 0.020086470000003374, 'context_size': 1176, 'warmup_time': 0.0033651999999619875}



.. GENERATED FROM PYTHON SOURCE LINES 268-272

The criterion needs to be reinstanciated since it depends on the features
*X*. The computation does not but the design does. This was introduced to
compare the current output with a decision tree optimizing for
a piecewise linear regression and not a stepwise regression.

.. GENERATED FROM PYTHON SOURCE LINES 272-280

.. code-block:: default



    try:
        model3.fit(X_train3, y_train3)
    except Exception as e:
        print(e)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    n_samples=750 -- y.shape=[2250, 1, 0, 0, 0, 0, 0, 0]




.. GENERATED FROM PYTHON SOURCE LINES 282-290

.. code-block:: default



    model3 = DecisionTreeRegressor(
        min_samples_leaf=100, criterion=SimpleRegressorCriterionFast(1, X_train3.shape[0])
    )
    measure_time("model3.fit(X_train3, y_train3)", globals())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'average': 0.007336418399999728, 'deviation': 0.0017915009585068127, 'min_exec': 0.004666116000000784, 'max_exec': 0.010466213999998218, 'repeat': 10, 'number': 50, 'ttime': 0.07336418399999728, 'context_size': 1176, 'warmup_time': 0.004179399999998168}



.. GENERATED FROM PYTHON SOURCE LINES 291-311

Still almost 2 times slower but of the same order of magnitude.
We could go further and investigate why or continue and introduce a
criterion which optimizes a piecewise linear regression instead of a
stepwise regression.

Criterion adapted for a linear regression
+++++++++++++++++++++++++++++++++++++++++

The previous examples are all about decision trees which approximates a
function by a stepwise function. On every interval :math:`[r_1, r_2]`,
the model optimizes
:math:`\sum_i (y_i - C)^2 \mathbb{1}_{ r_1 \leqslant x_i \leqslant r_2}`
and finds the best constant (= the average)
approxmating the function on this interval.
We would to like to approximate the function by a regression line and not a
constant anymore. It means minimizing
:math:`\sum_i (y_i - X_i \beta)^2 \mathbb{1}_{ r_1 \leqslant x_i \leqslant r_2}`.
Doing this require to change the criterion used to split the space of feature
into buckets and the prediction function of the decision tree which now
needs to return a dot product.

.. GENERATED FROM PYTHON SOURCE LINES 311-319

.. code-block:: default


    fixed = False
    if fixed:
        # It does not work yet.
        piece = PiecewiseTreeRegressor(criterion="mselin", min_samples_leaf=100)
        piece.fit(X_train, y_train)









.. GENERATED FROM PYTHON SOURCE LINES 321-328

.. code-block:: default



    if fixed:
        pred = piece.predict(X_test)
        pred[:5]









.. GENERATED FROM PYTHON SOURCE LINES 330-339

.. code-block:: default



    if fixed:
        fig, ax = plt.subplots(1, 1)
        ax.plot(X_test[:, 0], y_test, ".", label="data")
        ax.plot(X_test[:, 0], pred, ".", label="predictions")
        ax.set_title("DecisionTreeRegressor\nwith criterion adapted to linear regression")
        ax.legend()








.. GENERATED FROM PYTHON SOURCE LINES 340-341

The coefficients for the linear regressions are kept into the following attribute:

.. GENERATED FROM PYTHON SOURCE LINES 341-347

.. code-block:: default



    if fixed:
        piece.betas_









.. GENERATED FROM PYTHON SOURCE LINES 348-349

Mapped to the following leaves:

.. GENERATED FROM PYTHON SOURCE LINES 349-355

.. code-block:: default



    if fixed:
        piece.leaves_index_, piece.leaves_mapping_









.. GENERATED FROM PYTHON SOURCE LINES 356-357

We can get the leave each observation falls into:

.. GENERATED FROM PYTHON SOURCE LINES 357-363

.. code-block:: default



    if fixed:
        piece.predict_leaves(X_test)[:5]









.. GENERATED FROM PYTHON SOURCE LINES 364-366

The training is quite slow as it is training many
linear regressions each time a split is evaluated.

.. GENERATED FROM PYTHON SOURCE LINES 366-372

.. code-block:: default



    if fixed:
        measure_time("piece.fit(X_train, y_train)", globals())









.. GENERATED FROM PYTHON SOURCE LINES 374-379

.. code-block:: default


    if fixed:
        measure_time("piece.fit(X_train3, y_train3)", globals())









.. GENERATED FROM PYTHON SOURCE LINES 380-396

It works but it is slow, slower than the slow implementation
of the standard criterion for decision tree regression.

Next
++++

PR `Model trees (M5P and co)
<https://github.com/scikit-learn/scikit-learn/issues/13106>`_
and issue `Model trees (M5P)
<https://github.com/scikit-learn/scikit-learn/pull/13732>`_
propose an implementation a piecewise regression with any kind of regression model.
It is based on `Building Model Trees
<https://github.com/ankonzoid/LearningX/tree/master/advanced_ML/model_tree>`_.
It fits many models to find the best splits and should be slower than this
implementation in the case of a decision tree regressor
associated with linear regressions.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 15.507 seconds)


.. _sphx_glr_download_auto_examples_plot_piecewise_linear_regression_criterion.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_piecewise_linear_regression_criterion.py <plot_piecewise_linear_regression_criterion.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_piecewise_linear_regression_criterion.ipynb <plot_piecewise_linear_regression_criterion.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
