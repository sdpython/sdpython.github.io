<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Export Phi-3.5-mini-instruct with draft_export" href="plot_exporter_exporter_draft_mode.html" /><link rel="prev" title="Do no use Module as inputs!" href="plot_exporter_exporter_inputs.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Export Phi-3.5-mini-instruct piece by piece - experimental-experiment 0.1.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">experimental-experiment 0.1.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">experimental-experiment 0.1.1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../design/index.html">Design</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../design/exporter.html">Custom Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/optimizer.html">Pattern Optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/backends.html">Dynamo Backends</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorial/index.html">Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/errors.html">Unexpected Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/docker.html">Start from a docker</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of API</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/gradient/index.html">.gradient</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of .gradient</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/gradient/ops/index.html">.gradient.ops</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of .gradient.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/gradient/ops/op_broadcast_gradient_args.html">.gradient.ops.op_broadcast_gradient_args</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/gradient/grad_helper.html">.gradient.grad_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/gradient/loss_helper.html">.gradient.loss_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">.reference</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of .reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">.reference.ops</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of .reference.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">.reference.quantized_tensor</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/convert/index.html">.convert</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of .convert</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/convert/convert_helper.html">.convert.convert_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/convert/ort_helper.html">.convert.ort_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/plotting/index.html">.plotting</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of .plotting</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/plotting/data.html">.plotting.data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/plotting/memory.html">.plotting.memory</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/skl/index.html">.skl</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of .skl</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/skl/convert.html">.skl.convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/skl/helpers.html">.skl.helpers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_interpreter/index.html">.torch_interpreter</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of .torch_interpreter</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_functions.html">.torch_interpreter._aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_functions_attention.html">.torch_interpreter._aten_functions_attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_non_aten_functions.html">.torch_interpreter._non_aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_methods.html">.torch_interpreter._aten_methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_doc_.html">.torch_interpreter._doc_</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_exceptions.html">.torch_interpreter._exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_prims_functions.html">.torch_interpreter._prims_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_torch_helper.html">.torch_interpreter._torch_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/aten_functions.html">.torch_interpreter.aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/aten_methods.html">.torch_interpreter.aten_methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/dispatcher.html">.torch_interpreter.dispatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/export_options.html">.torch_interpreter.export_options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/interpreter.html">.torch_interpreter.interpreter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/investigate_helper.html">.torch_interpreter.investigate_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/onnx_export.html">.torch_interpreter.onnx_export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/oxs_dispatcher.html">.torch_interpreter.oxs_dispatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/oxs_opset.html">.torch_interpreter.oxs_opset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/piece_by_piece.html">.torch_interpreter.piece_by_piece</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/piece_by_piece_serialize.html">.torch_interpreter.piece_by_piece_serialize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/tracing.html">.torch_interpreter.tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">.torch_models</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of .torch_models</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/diffusion_model_helper.html">.torch_models.diffusion_model_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/dump_helper.html">.torch_models.dump_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llama_helper.html">.torch_models.llama_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llm_model_helper.html">.torch_models.llm_model_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llm_model_setup.html">.torch_models.llm_model_setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/mistral_helper.html">.torch_models.mistral_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/phi3_helper.html">.torch_models.phi3_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/phi_helper.html">.torch_models.phi_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/training_helper.html">.torch_models.training_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/xbuilder/index.html">.xbuilder</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of .xbuilder</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_graph_builder_runtime.html">.xbuilder._graph_builder_runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_onnx_helper.html">.xbuilder._onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_shape_helper.html">.xbuilder._shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/expression_dimension.html">.xbuilder.expression_dimension</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/graph_builder.html">.xbuilder.graph_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/graph_builder_opset.html">.xbuilder.graph_builder_opset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/model_container.html">.xbuilder.model_container</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/optimization_options.html">.xbuilder.optimization_options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/reverse_graph_builder.html">.xbuilder.reverse_graph_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/shape_type_compute.html">.xbuilder.shape_type_compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/type_inference.html">.xbuilder.type_inference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/xoptim/index.html">.xoptim</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of .xoptim</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_investigation/index.html">.xoptim.patterns_investigation</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_investigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_investigation/element_wise.html">.xoptim.patterns_investigation.element_wise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_investigation/llm_patterns.html">.xoptim.patterns_investigation.llm_patterns</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_ml/index.html">.xoptim.patterns_ml</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_ml</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ml/tree_ensemble.html">.xoptim.patterns_ml.tree_ensemble</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_exp/index.html">.xoptim.patterns_exp</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_exp</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/binary_operators.html">.xoptim.patterns_exp.binary_operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/constant_of_shape_scatter_nd.html">.xoptim.patterns_exp.constant_of_shape_scatter_nd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/constants.html">.xoptim.patterns_exp.constants</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/simple_rotary.html">.xoptim.patterns_exp.simple_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/unary_operators.html">.xoptim.patterns_exp.unary_operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/where_replace.html">.xoptim.patterns_exp.where_replace</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns/index.html">.xoptim.patterns</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of .xoptim.patterns</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_any.html">.xoptim.patterns.onnx_any</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_cast.html">.xoptim.patterns.onnx_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_clip.html">.xoptim.patterns.onnx_clip</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_constants.html">.xoptim.patterns.onnx_constants</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_conv.html">.xoptim.patterns.onnx_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_dropout.html">.xoptim.patterns.onnx_dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_equal.html">.xoptim.patterns.onnx_equal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_expand.html">.xoptim.patterns.onnx_expand</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_functions.html">.xoptim.patterns.onnx_functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_layer_normalization.html">.xoptim.patterns.onnx_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_matmul.html">.xoptim.patterns.onnx_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_mul.html">.xoptim.patterns.onnx_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_reduce.html">.xoptim.patterns.onnx_reduce</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_reshape.html">.xoptim.patterns.onnx_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_rotary.html">.xoptim.patterns.onnx_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_slice.html">.xoptim.patterns.onnx_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_split.html">.xoptim.patterns.onnx_split</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_sub.html">.xoptim.patterns.onnx_sub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_sequence.html">.xoptim.patterns.onnx_sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_transpose.html">.xoptim.patterns.onnx_transpose</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_unsqueeze.html">.xoptim.patterns.onnx_unsqueeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_ort/index.html">.xoptim.patterns_ort</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_ort</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/activation.html">.xoptim.patterns_ort.activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/activation_grad.html">.xoptim.patterns_ort.activation_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/attention_patterns.html">.xoptim.patterns_ort.attention_patterns</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/batch_normalization.html">.xoptim.patterns_ort.batch_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/fused_conv.html">.xoptim.patterns_ort.fused_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/fused_matmul.html">.xoptim.patterns_ort.fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/gather_grad.html">.xoptim.patterns_ort.gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/llm_optim.html">.xoptim.patterns_ort.llm_optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/simplified_layer_normalization.html">.xoptim.patterns_ort.simplified_layer_normalization</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_fix/index.html">.xoptim.patterns_fix</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_fix</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_fix/add_reduction_scatter_nd.html">.xoptim.patterns_fix.add_reduction_scatter_nd</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/graph_builder_optim.html">.xoptim.graph_builder_optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/order_optim.html">.xoptim.order_optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/patterns_api.html">.xoptim.patterns_api</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/unfused.html">.xoptim.unfused</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_dynamo/index.html">.torch_dynamo</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><div class="visually-hidden">Toggle navigation of .torch_dynamo</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/_dynamo_exporter.html">.torch_dynamo._dynamo_exporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/backend_helper.html">.torch_dynamo.backend_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/debug_backend.html">.torch_dynamo.debug_backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/fast_backend.html">.torch_dynamo.fast_backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/partition.html">experimental_experiment.torch_dynamo.partition</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_bench/index.html">.torch_bench</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><div class="visually-hidden">Toggle navigation of .torch_bench</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner_agg.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner_agg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner_agg_helper.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner_agg_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_cmd.html">experimental_experiment.torch_bench._bash_bench_cmd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_model_runner.html">experimental_experiment.torch_bench._bash_bench_model_runner</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_models_helper.html">experimental_experiment.torch_bench._bash_bench_models_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_dummies.html">experimental_experiment.torch_bench._bash_bench_set_dummies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_explicit.html">experimental_experiment.torch_bench._bash_bench_set_explicit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_huggingface.html">experimental_experiment.torch_bench._bash_bench_set_huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_huggingface_big.html">experimental_experiment.torch_bench._bash_bench_set_huggingface_big</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_issues.html">experimental_experiment.torch_bench._bash_bench_set_issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_timm.html">experimental_experiment.torch_bench._bash_bench_set_timm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_torchbench.html">experimental_experiment.torch_bench._bash_bench_set_torchbench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_torchbench_ado.html">experimental_experiment.torch_bench._bash_bench_set_torchbench_ado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_untrained.html">experimental_experiment.torch_bench._bash_bench_untrained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_dort_cmd_common.html">experimental_experiment.torch_bench._dort_cmd_common</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_dort_cmd_common_models.html">experimental_experiment.torch_bench._dort_cmd_common_models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_agg.html">.torch_bench.bash_bench_agg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_explicit.html">.torch_bench.bash_bench_explicit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_huggingface.html">.torch_bench.bash_bench_huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_huggingface_big.html">.torch_bench.bash_bench_huggingface_big</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_issues.html">.torch_bench.bash_bench_issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_timm.html">.torch_bench.bash_bench_timm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_torchbench.html">.torch_bench.bash_bench_torchbench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_torchbench_ado.html">.torch_bench.bash_bench_torchbench_ado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_untrained.html">.torch_bench.bash_bench_untrained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/check_model.html">.torch_bench.check_model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_bench.html">.torch_bench.dort_bench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_bench_profile.html">.torch_bench.dort_bench_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_profile.html">.torch_bench.dort_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/export_model.html">.torch_bench.export_model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/export_model_helper.html">.torch_bench.export_model_helper</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/_bench_test.html">._bench_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/_command_lines_parser.html">._command_lines_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/args.html">.args</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/bench_run.html">.bench_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/checks.html">.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">.ext_test_case</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/helpers.html">.helpers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/memory_peak.html">.memory_peak</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/mini_onnx_builder.html">.mini_onnx_builder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/model_run.html">.model_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/onnx_tools.html">.onnx_tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ort_session.html">.ort_session</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/torch_test_helper.html">.torch_test_helper</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../galleries.html">Galleries of Examples and Recipes</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" role="switch" type="checkbox"/><label for="toctree-checkbox-23"><div class="visually-hidden">Toggle navigation of Galleries of Examples and Recipes</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/index.html">Examples Gallery</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" role="switch" type="checkbox"/><label for="toctree-checkbox-24"><div class="visually-hidden">Toggle navigation of Examples Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_custom_backend_101.html">101: A custom backend for torch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_linreg_101.html">101: Linear Regression and export to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_optimize_101.html">101: Onnx Model Optimization based on Pattern Rewriting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_rewrite_101.html">101: Onnx Model Rewriting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_profile_existing_onnx_101.html">101: Profile an existing model with onnxruntime</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_101.html">101: Some dummy examples with torch.export.export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_convolutation_matmul_102.html">102: Convolution and Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_onnxscript_102.html">102: Examples with onnxscript</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_custom_backend_llama_102.html">102: Fuse kernels in a small Llama Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_bench_102.html">102: Measure LLAMA speed</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_compile_102.html">102: Tweak onnx export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_dort_201.html">201: Evaluate DORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_aot_201.html">201: Evaluate DORT Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_201.html">201: Evaluate different ways to export a torch model to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_sklearn_201.html">201: Use torch to export a scikit-learn model into ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_export_301.html">301: Compares LLAMA exporters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_dort_301.html">301: Compares LLAMA exporters for onnxrt backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_model_to_python.html">Playground for big optimization pattern</a></li>
</ul>
</li>
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">Exporter Recipes Gallery</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" role="switch" type="checkbox"/><label for="toctree-checkbox-25"><div class="visually-hidden">Toggle navigation of Exporter Recipes Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_lost_dynamic_dimension.html">A dynamic dimension lost by torch.export.export</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_untrained_tinyllm.html">Check the exporter on a dummy from HuggingFace</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_inputs.html">Do no use Module as inputs!</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">Export Phi-3.5-mini-instruct piece by piece</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_draft_mode.html">Export Phi-3.5-mini-instruct with draft_export</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_reportibility.html">Export Phi-3.5-mini-instruct with report_exportability</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_with_dynamic_cache.html">Export a model using a custom type as input</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_scan_pdist.html">Export a model with a loop (scan)</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_infer_ds.html">Infer dynamic shapes before exporting</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_lr.html">Linear Regression and export to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_coverage.html">Measures the exporter success on many test cases</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_dynamic_shapes_auto.html">Use DYNAMIC or AUTO when dynamic shapes has constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_phi2.html">to_onnx and Phi-2</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_custom_ops_inplace.html">to_onnx and a custom operator inplace</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_custom_ops_fct.html">to_onnx and a custom operator registered with a function</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_scan_pdist.html">to_onnx and a model with a loop (scan)</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_cond.html">to_onnx and a model with a test</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_dynpad.html">to_onnx and padding one dimension to a mulitple of a constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_modules.html">to_onnx and submodules from LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_named_ds_auto.html">to_onnx: Rename Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_phi2.html">torch.onnx.export and Phi-2</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_custom_ops_inplace.html">torch.onnx.export and a custom operator inplace</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_custom_ops_fct.html">torch.onnx.export and a custom operator registered with a function</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_cond.html">torch.onnx.export and a model with a test</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_dynpad.html">torch.onnx.export and padding one dimension to a mulitple of a constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_named_ds_auto.html">torch.onnx.export: Rename Dynamic Shapes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../command_lines.html">Command Lines</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" role="switch" type="checkbox"/><label for="toctree-checkbox-26"><div class="visually-hidden">Toggle navigation of Command Lines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../bench/index.html">Benchmarks from the command line</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" role="switch" type="checkbox"/><label for="toctree-checkbox-27"><div class="visually-hidden">Toggle navigation of Benchmarks from the command line</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../bench/dort_bench.html">experimental_experiment.torch_bench.dort_bench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/dort_profile.html">experimental_experiment.torch_bench.dort_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/scripts.html">Interesting scripts or command lines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/bash_bench.html">Measuring the exporters on a short list of sets of models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tools/index.html">Tools from the command line</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" role="switch" type="checkbox"/><label for="toctree-checkbox-28"><div class="visually-hidden">Toggle navigation of Tools from the command line</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tools/lighten.html">python -m experimental_experiment lighten and unlighten</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/optimize.html">python -m experimental_experiment optimize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/print.html">python -m experimental_experiment print</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/run.html">python -m experimental_experiment run</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../miscellaneous/index.html">Miscellaneous</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" role="switch" type="checkbox"/><label for="toctree-checkbox-29"><div class="visually-hidden">Toggle navigation of Miscellaneous</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../miscellaneous/export_times.html">Export Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="../miscellaneous/long_outputs.html">Long Outputs uneasy to read</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../miscellaneous/models/index.html">Supported Models By the Custom Backend</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" role="switch" type="checkbox"/><label for="toctree-checkbox-30"><div class="visually-hidden">Toggle navigation of Supported Models By the Custom Backend</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../miscellaneous/models/phi.html">Phi</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_recipes/plot_exporter_exporter_phi35_piece.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-recipes-plot-exporter-exporter-phi35-piece-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="export-phi-3-5-mini-instruct-piece-by-piece">
<span id="l-plot-exporter-exporter-phi35-piece"></span><span id="sphx-glr-auto-recipes-plot-exporter-exporter-phi35-piece-py"></span><h1>Export Phi-3.5-mini-instruct piece by piece<a class="headerlink" href="#export-phi-3-5-mini-instruct-piece-by-piece" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://docs.pytorch.org/docs/main/export.html#torch.export.export" title="(in PyTorch vmain (2.8.0a0+gitd5f6422 ))"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a> often breaks on big models because there
are control flows or instructions breaking the propagation of
dynamic shapes (see ). The function usually gives an indication where
the model implementation can be fixed but in case, that is not possible,
we can try to export the model piece by piece: every module
is converted separately from its submodule. A model can be exported even
if one of its submodules cannot.</p>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading"></a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><span class="n">Any</span></a></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Dict</span></a></a></a></a>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch._export.tools</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.helpers.cache_helper</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">make_dynamic_cache</span></a></a></a></a>
<span class="kn">from</span> <span class="nn">experimental_experiment.helpers</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a>
<span class="kn">from</span> <span class="nn">experimental_experiment.torch_interpreter.piece_by_piece</span> <span class="kn">import</span> <span class="p">(</span>
    <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><span class="n">trace_execution_piece_by_piece</span></a></a></a></a><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">get_phi35_untrained</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Dict</span></a></a></a></a><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><span class="n">Any</span></a></a></a></a><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets a non initialized model with two sets of inputs and different shapes.</span>

<span class="sd">    :param batch_size: batch size</span>
<span class="sd">    :param kwargs: to overwrite the configuration, example ``num_hidden_layers=1``</span>
<span class="sd">    :return: dictionary</span>

<span class="sd">    See `Phi-3.5-mini-instruct/config.json</span>
<span class="sd">    &lt;https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/config.json&gt;`_.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;_name_or_path&quot;</span><span class="p">:</span> <span class="s2">&quot;Phi-3.5-mini-instruct&quot;</span><span class="p">,</span>
        <span class="s2">&quot;architectures&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Phi3ForCausalLM&quot;</span><span class="p">],</span>
        <span class="s2">&quot;attention_dropout&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;auto_map&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;AutoConfig&quot;</span><span class="p">:</span> <span class="s2">&quot;configuration_phi3.Phi3Config&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AutoModelForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;modeling_phi3.Phi3ForCausalLM&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;bos_token_id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;embd_pdrop&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;eos_token_id&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;hidden_act&quot;</span><span class="p">:</span> <span class="s2">&quot;silu&quot;</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">3072</span><span class="p">,</span>
        <span class="s2">&quot;initializer_range&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
        <span class="s2">&quot;intermediate_size&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;max_position_embeddings&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
        <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;phi3&quot;</span><span class="p">,</span>
        <span class="s2">&quot;num_attention_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;num_key_value_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;original_max_position_embeddings&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;pad_token_id&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;resid_pdrop&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;rms_norm_eps&quot;</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
        <span class="s2">&quot;rope_scaling&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;long_factor&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="mf">1.0800000429153442</span><span class="p">,</span>
                <span class="mf">1.1100000143051147</span><span class="p">,</span>
                <span class="mf">1.1399999856948853</span><span class="p">,</span>
                <span class="mf">1.340000033378601</span><span class="p">,</span>
                <span class="mf">1.5899999141693115</span><span class="p">,</span>
                <span class="mf">1.600000023841858</span><span class="p">,</span>
                <span class="mf">1.6200000047683716</span><span class="p">,</span>
                <span class="mf">2.620000123977661</span><span class="p">,</span>
                <span class="mf">3.2300000190734863</span><span class="p">,</span>
                <span class="mf">3.2300000190734863</span><span class="p">,</span>
                <span class="mf">4.789999961853027</span><span class="p">,</span>
                <span class="mf">7.400000095367432</span><span class="p">,</span>
                <span class="mf">7.700000286102295</span><span class="p">,</span>
                <span class="mf">9.09000015258789</span><span class="p">,</span>
                <span class="mf">12.199999809265137</span><span class="p">,</span>
                <span class="mf">17.670000076293945</span><span class="p">,</span>
                <span class="mf">24.46000099182129</span><span class="p">,</span>
                <span class="mf">28.57000160217285</span><span class="p">,</span>
                <span class="mf">30.420001983642578</span><span class="p">,</span>
                <span class="mf">30.840002059936523</span><span class="p">,</span>
                <span class="mf">32.590003967285156</span><span class="p">,</span>
                <span class="mf">32.93000411987305</span><span class="p">,</span>
                <span class="mf">42.320003509521484</span><span class="p">,</span>
                <span class="mf">44.96000289916992</span><span class="p">,</span>
                <span class="mf">50.340003967285156</span><span class="p">,</span>
                <span class="mf">50.45000457763672</span><span class="p">,</span>
                <span class="mf">57.55000305175781</span><span class="p">,</span>
                <span class="mf">57.93000411987305</span><span class="p">,</span>
                <span class="mf">58.21000289916992</span><span class="p">,</span>
                <span class="mf">60.1400032043457</span><span class="p">,</span>
                <span class="mf">62.61000442504883</span><span class="p">,</span>
                <span class="mf">62.62000274658203</span><span class="p">,</span>
                <span class="mf">62.71000289916992</span><span class="p">,</span>
                <span class="mf">63.1400032043457</span><span class="p">,</span>
                <span class="mf">63.1400032043457</span><span class="p">,</span>
                <span class="mf">63.77000427246094</span><span class="p">,</span>
                <span class="mf">63.93000411987305</span><span class="p">,</span>
                <span class="mf">63.96000289916992</span><span class="p">,</span>
                <span class="mf">63.970001220703125</span><span class="p">,</span>
                <span class="mf">64.02999877929688</span><span class="p">,</span>
                <span class="mf">64.06999969482422</span><span class="p">,</span>
                <span class="mf">64.08000183105469</span><span class="p">,</span>
                <span class="mf">64.12000274658203</span><span class="p">,</span>
                <span class="mf">64.41000366210938</span><span class="p">,</span>
                <span class="mf">64.4800033569336</span><span class="p">,</span>
                <span class="mf">64.51000213623047</span><span class="p">,</span>
                <span class="mf">64.52999877929688</span><span class="p">,</span>
                <span class="mf">64.83999633789062</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="s2">&quot;short_factor&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="mf">1.0</span><span class="p">,</span>
                <span class="mf">1.0199999809265137</span><span class="p">,</span>
                <span class="mf">1.0299999713897705</span><span class="p">,</span>
                <span class="mf">1.0299999713897705</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0699999332427979</span><span class="p">,</span>
                <span class="mf">1.0999999046325684</span><span class="p">,</span>
                <span class="mf">1.1099998950958252</span><span class="p">,</span>
                <span class="mf">1.1599998474121094</span><span class="p">,</span>
                <span class="mf">1.1599998474121094</span><span class="p">,</span>
                <span class="mf">1.1699998378753662</span><span class="p">,</span>
                <span class="mf">1.2899998426437378</span><span class="p">,</span>
                <span class="mf">1.339999794960022</span><span class="p">,</span>
                <span class="mf">1.679999828338623</span><span class="p">,</span>
                <span class="mf">1.7899998426437378</span><span class="p">,</span>
                <span class="mf">1.8199998140335083</span><span class="p">,</span>
                <span class="mf">1.8499997854232788</span><span class="p">,</span>
                <span class="mf">1.8799997568130493</span><span class="p">,</span>
                <span class="mf">1.9099997282028198</span><span class="p">,</span>
                <span class="mf">1.9399996995925903</span><span class="p">,</span>
                <span class="mf">1.9899996519088745</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0799996852874756</span><span class="p">,</span>
                <span class="mf">2.0899996757507324</span><span class="p">,</span>
                <span class="mf">2.189999580383301</span><span class="p">,</span>
                <span class="mf">2.2199995517730713</span><span class="p">,</span>
                <span class="mf">2.5899994373321533</span><span class="p">,</span>
                <span class="mf">2.729999542236328</span><span class="p">,</span>
                <span class="mf">2.749999523162842</span><span class="p">,</span>
                <span class="mf">2.8399994373321533</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;longrope&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;rope_theta&quot;</span><span class="p">:</span> <span class="mf">10000.0</span><span class="p">,</span>
        <span class="s2">&quot;sliding_window&quot;</span><span class="p">:</span> <span class="mi">262144</span><span class="p">,</span>
        <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;torch_dtype&quot;</span><span class="p">:</span> <span class="s2">&quot;bfloat16&quot;</span><span class="p">,</span>
        <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;attention_bias&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32064</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Phi3Config</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">transformers</span><span class="o">.</span><span class="n">Phi3ForCausalLM</span></a></a></a></a><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
    <a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">eval</span></a></a></a></a><span class="p">()</span>

    <span class="n">cache</span> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">make_dynamic_cache</span></a></a></a></a><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a></a><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span> <a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a></a><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">96</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">])</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">cache2</span> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">make_dynamic_cache</span></a></a></a></a><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a></a><span class="p">(</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span> <a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a></a><span class="p">(</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">96</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">])</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a></a></a></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">32064</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a></a><span class="p">),</span>
        <span class="n">attention_mask</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a></a></a></a><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">33</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a></a><span class="p">),</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
    <span class="p">)</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a></a></a></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">32064</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a></a><span class="p">),</span>
        <span class="n">attention_mask</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a></a></a></a><span class="p">((</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">35</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a></a><span class="p">),</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">cache2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a></a><span class="p">)</span>


<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a></a> <span class="o">=</span> <span class="n">get_phi35_untrained</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a></a><span class="p">[</span><span class="s2">&quot;inputs2&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dict(input_ids:T7s2x3,attention_mask:T7s2x33,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x96,T1s2x32x30x96], value_cache=#2[T1s2x32x30x96,T1s2x32x30x96]))
</pre></div>
</div>
</section>
<section id="dynamic-shapes">
<h2>Dynamic Shapes<a class="headerlink" href="#dynamic-shapes" title="Link to this heading"></a></h2>
<p>We want to infer the dynamic shapes from the two sets of inputs we gave.
For that, we use a function to trace the execution of the model
including its submodules. It is going to execute the model twice
with the two sets of inputs and stores every intermediate input and output.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">diag</span></a></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" title="experimental_experiment.torch_interpreter.piece_by_piece.trace_execution_piece_by_piece" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-function"><span class="n">trace_execution_piece_by_piece</span></a></a></a></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a></a><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[_trace_forward_execution] -trace-  M:__main__-Phi3ForCausalLM.forward
[_trace_forward_execution] -trace- .. M:model-Phi3Model.forward
[_trace_forward_execution] -trace- .... M:embed_tokens-Embedding.forward
[_trace_forward_execution] -trace- .... M:layers[0]-Phi3DecoderLayer.forward
[_trace_forward_execution] -trace- ...... M:self_attn-Phi3Attention.forward
[_trace_forward_execution] -trace- ........ M:o_proj-Linear.forward
[_trace_forward_execution] -trace- ........ M:qkv_proj-Linear.forward
[_trace_forward_execution] -trace- ...... M:mlp-Phi3MLP.forward
[_trace_forward_execution] -trace- ........ M:gate_up_proj-Linear.forward
[_trace_forward_execution] -trace- ........ M:down_proj-Linear.forward
[_trace_forward_execution] -trace- ........ M:activation_fn-SiLU.forward
[_trace_forward_execution] -trace- ...... M:input_layernorm-Phi3RMSNorm.forward
[_trace_forward_execution] -trace- ...... M:post_attention_layernorm-Phi3RMSNorm.forward
[_trace_forward_execution] -trace- ...... M:resid_attn_dropout-Dropout.forward
[_trace_forward_execution] -trace- ...... M:resid_mlp_dropout-Dropout.forward
[_trace_forward_execution] -trace- .... M:layers[1]-Phi3DecoderLayer.forward
[_trace_forward_execution] -trace- ...... M:self_attn-Phi3Attention.forward
[_trace_forward_execution] -trace- ........ M:o_proj-Linear.forward
[_trace_forward_execution] -trace- ........ M:qkv_proj-Linear.forward
[_trace_forward_execution] -trace- ...... M:mlp-Phi3MLP.forward
[_trace_forward_execution] -trace- ........ M:gate_up_proj-Linear.forward
[_trace_forward_execution] -trace- ........ M:down_proj-Linear.forward
[_trace_forward_execution] -trace- ........ M:activation_fn-SiLU.forward
[_trace_forward_execution] -trace- ...... M:input_layernorm-Phi3RMSNorm.forward
[_trace_forward_execution] -trace- ...... M:post_attention_layernorm-Phi3RMSNorm.forward
[_trace_forward_execution] -trace- ...... M:resid_attn_dropout-Dropout.forward
[_trace_forward_execution] -trace- ...... M:resid_mlp_dropout-Dropout.forward
[_trace_forward_execution] -trace- .... M:norm-Phi3RMSNorm.forward
[_trace_forward_execution] -trace- .... M:rotary_emb-Phi3RotaryEmbedding.forward
[_trace_forward_execution] -trace- .. M:lm_head-Linear.forward
[trace_execution_piece_by_piece] run with dict(args:(),kwargs:dict(input_ids:T7s2x3,attention_mask:T7s2x33,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x96,T1s2x32x30x96], value_cache=#2[T1s2x32x30x96,T1s2x32x30x96])))
[__main__:Phi3ForCausalLM] &gt; **dict(input_ids:T7r2,attention_mask:T7r2,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
[model:Phi3Model]   &gt; **dict(input_ids:T7r2,attention_mask:T7r2,position_ids:None,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),inputs_embeds:None,use_cache:None,output_attentions:bool,output_hidden_states:bool,cache_position:None)
[embed_tokens:Embedding]     &gt; T7r2
[embed_tokens:Embedding]     &lt; T1r3
[rotary_emb:Phi3RotaryEmbedding]     &gt; *(T1r3,T7r2)
[rotary_emb:Phi3RotaryEmbedding]     &lt; *(T1r3,T1r3)
[layers[0]:Phi3DecoderLayer]     &gt; *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[input_layernorm:Phi3RMSNorm]       &gt; T1r3
[input_layernorm:Phi3RMSNorm]       &lt; T1r3
[self_attn:Phi3Attention]       &gt; **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[qkv_proj:Linear]         &gt; T1r3
[qkv_proj:Linear]         &lt; T1r3
[o_proj:Linear]         &gt; T1r3
[o_proj:Linear]         &lt; T1r3
[self_attn:Phi3Attention]       &lt; *(T1r3,None)
[resid_attn_dropout:Dropout]       &gt; T1r3
[resid_attn_dropout:Dropout]       &lt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &gt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &lt; T1r3
[mlp:Phi3MLP]       &gt; T1r3
[gate_up_proj:Linear]         &gt; T1r3
[gate_up_proj:Linear]         &lt; T1r3
[activation_fn:SiLU]         &gt; T1r3
[activation_fn:SiLU]         &lt; T1r3
[down_proj:Linear]         &gt; T1r3
[down_proj:Linear]         &lt; T1r3
[mlp:Phi3MLP]       &lt; T1r3
[resid_mlp_dropout:Dropout]       &gt; T1r3
[resid_mlp_dropout:Dropout]       &lt; T1r3
[layers[0]:Phi3DecoderLayer]     &lt; *(T1r3,)
[layers[1]:Phi3DecoderLayer]     &gt; *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[input_layernorm:Phi3RMSNorm]       &gt; T1r3
[input_layernorm:Phi3RMSNorm]       &lt; T1r3
[self_attn:Phi3Attention]       &gt; **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[qkv_proj:Linear]         &gt; T1r3
[qkv_proj:Linear]         &lt; T1r3
[o_proj:Linear]         &gt; T1r3
[o_proj:Linear]         &lt; T1r3
[self_attn:Phi3Attention]       &lt; *(T1r3,None)
[resid_attn_dropout:Dropout]       &gt; T1r3
[resid_attn_dropout:Dropout]       &lt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &gt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &lt; T1r3
[mlp:Phi3MLP]       &gt; T1r3
[gate_up_proj:Linear]         &gt; T1r3
[gate_up_proj:Linear]         &lt; T1r3
[activation_fn:SiLU]         &gt; T1r3
[activation_fn:SiLU]         &lt; T1r3
[down_proj:Linear]         &gt; T1r3
[down_proj:Linear]         &lt; T1r3
[mlp:Phi3MLP]       &lt; T1r3
[resid_mlp_dropout:Dropout]       &gt; T1r3
[resid_mlp_dropout:Dropout]       &lt; T1r3
[layers[1]:Phi3DecoderLayer]     &lt; *(T1r3,)
[norm:Phi3RMSNorm]     &gt; T1r3
[norm:Phi3RMSNorm]     &lt; T1r3
[model:Phi3Model]   &lt; *BaseModelOutputWithPast(last_hidden_state:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
[lm_head:Linear]   &gt; T1r3
[lm_head:Linear]   &lt; T1r3
[__main__:Phi3ForCausalLM] &lt; *CausalLMOutputWithPast(logits:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
[trace_execution_piece_by_piece] run with dict(args:(),kwargs:dict(input_ids:T7s3x4,attention_mask:T7s3x35,past_key_values:DynamicCache(key_cache=#2[T1s3x32x31x96,T1s3x32x31x96], value_cache=#2[T1s3x32x31x96,T1s3x32x31x96])))
[__main__:Phi3ForCausalLM] &gt; **dict(input_ids:T7r2,attention_mask:T7r2,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
[model:Phi3Model]   &gt; **dict(input_ids:T7r2,attention_mask:T7r2,position_ids:None,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),inputs_embeds:None,use_cache:None,output_attentions:bool,output_hidden_states:bool,cache_position:None)
[embed_tokens:Embedding]     &gt; T7r2
[embed_tokens:Embedding]     &lt; T1r3
[rotary_emb:Phi3RotaryEmbedding]     &gt; *(T1r3,T7r2)
[rotary_emb:Phi3RotaryEmbedding]     &lt; *(T1r3,T1r3)
[layers[0]:Phi3DecoderLayer]     &gt; *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[input_layernorm:Phi3RMSNorm]       &gt; T1r3
[input_layernorm:Phi3RMSNorm]       &lt; T1r3
[self_attn:Phi3Attention]       &gt; **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[qkv_proj:Linear]         &gt; T1r3
[qkv_proj:Linear]         &lt; T1r3
[o_proj:Linear]         &gt; T1r3
[o_proj:Linear]         &lt; T1r3
[self_attn:Phi3Attention]       &lt; *(T1r3,None)
[resid_attn_dropout:Dropout]       &gt; T1r3
[resid_attn_dropout:Dropout]       &lt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &gt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &lt; T1r3
[mlp:Phi3MLP]       &gt; T1r3
[gate_up_proj:Linear]         &gt; T1r3
[gate_up_proj:Linear]         &lt; T1r3
[activation_fn:SiLU]         &gt; T1r3
[activation_fn:SiLU]         &lt; T1r3
[down_proj:Linear]         &gt; T1r3
[down_proj:Linear]         &lt; T1r3
[mlp:Phi3MLP]       &lt; T1r3
[resid_mlp_dropout:Dropout]       &gt; T1r3
[resid_mlp_dropout:Dropout]       &lt; T1r3
[layers[0]:Phi3DecoderLayer]     &lt; *(T1r3,)
[layers[1]:Phi3DecoderLayer]     &gt; *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[input_layernorm:Phi3RMSNorm]       &gt; T1r3
[input_layernorm:Phi3RMSNorm]       &lt; T1r3
[self_attn:Phi3Attention]       &gt; **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
[qkv_proj:Linear]         &gt; T1r3
[qkv_proj:Linear]         &lt; T1r3
[o_proj:Linear]         &gt; T1r3
[o_proj:Linear]         &lt; T1r3
[self_attn:Phi3Attention]       &lt; *(T1r3,None)
[resid_attn_dropout:Dropout]       &gt; T1r3
[resid_attn_dropout:Dropout]       &lt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &gt; T1r3
[post_attention_layernorm:Phi3RMSNorm]       &lt; T1r3
[mlp:Phi3MLP]       &gt; T1r3
[gate_up_proj:Linear]         &gt; T1r3
[gate_up_proj:Linear]         &lt; T1r3
[activation_fn:SiLU]         &gt; T1r3
[activation_fn:SiLU]         &lt; T1r3
[down_proj:Linear]         &gt; T1r3
[down_proj:Linear]         &lt; T1r3
[mlp:Phi3MLP]       &lt; T1r3
[resid_mlp_dropout:Dropout]       &gt; T1r3
[resid_mlp_dropout:Dropout]       &lt; T1r3
[layers[1]:Phi3DecoderLayer]     &lt; *(T1r3,)
[norm:Phi3RMSNorm]     &gt; T1r3
[norm:Phi3RMSNorm]     &lt; T1r3
[model:Phi3Model]   &lt; *BaseModelOutputWithPast(last_hidden_state:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
[lm_head:Linear]   &gt; T1r3
[lm_head:Linear]   &lt; T1r3
[__main__:Phi3ForCausalLM] &lt; *CausalLMOutputWithPast(logits:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
[trace_forward_execution] traced execution of model Phi3ForCausalLM
&gt;&gt;&gt; __main__: Phi3ForCausalLM
  &gt; ((),dict(input_ids:CT7s2x3[3876,15594:A9096.5],attention_mask:CT7s2x33[1,1:A1.0],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.400763034820557,4.259133338928223:A-0.004826152082167923],CT1s2x32x30x96[-4.0469818115234375,4.471982479095459:A0.002217433422628052]], value_cache=#2[CT1s2x32x30x96[-4.405541896820068,4.668274402618408:A0.002596911304140301],CT1s2x32x30x96[-4.310667037963867,4.2944207191467285:A0.00028137129325799627]])))
  &gt; ((),dict(input_ids:CT7s3x4[2392,29616:A20512.083333333332],attention_mask:CT7s3x35[1,1:A1.0],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.828048229217529,4.243164539337158:A0.00031101864580231346],CT1s3x32x31x96[-4.626999855041504,4.47794246673584:A-0.0025560081888748925]], value_cache=#2[CT1s3x32x31x96[-4.349869728088379,4.253993034362793:A0.0011117361406822917],CT1s3x32x31x96[-4.810003280639648,4.869333267211914:A-0.00027058590800271105]])))
    &gt;&gt;&gt; model: Phi3Model
      &gt; ((),dict(input_ids:CT7s2x3[3876,15594:A9096.5],attention_mask:CT7s2x33[1,1:A1.0],position_ids:None,past_key_values:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.400763034820557,4.259133338928223:A-0.004826152082167923],CT1s2x32x30x96[-4.0469818115234375,4.471982479095459:A0.002217433422628052]], value_cache=#2[CT1s2x32x30x96[-4.405541896820068,4.668274402618408:A0.002596911304140301],CT1s2x32x30x96[-4.310667037963867,4.2944207191467285:A0.00028137129325799627]]),inputs_embeds:None,use_cache:None,output_attentions:bool=False,output_hidden_states:bool=False,cache_position:None))
      &gt; ((),dict(input_ids:CT7s3x4[2392,29616:A20512.083333333332],attention_mask:CT7s3x35[1,1:A1.0],position_ids:None,past_key_values:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.828048229217529,4.243164539337158:A0.00031101864580231346],CT1s3x32x31x96[-4.626999855041504,4.47794246673584:A-0.0025560081888748925]], value_cache=#2[CT1s3x32x31x96[-4.349869728088379,4.253993034362793:A0.0011117361406822917],CT1s3x32x31x96[-4.810003280639648,4.869333267211914:A-0.00027058590800271105]]),inputs_embeds:None,use_cache:None,output_attentions:bool=False,output_hidden_states:bool=False,cache_position:None))
        &gt;&gt;&gt; embed_tokens: Embedding
          &gt; ((CT7s2x3[3876,15594:A9096.5],),{})
          &gt; ((CT7s3x4[2392,29616:A20512.083333333332],),{})
          &lt; (CT1s2x3x3072[-0.08262448012828827,0.07648692280054092:A0.00013174691782688906],)
          &lt; (CT1s3x4x3072[-0.08454269170761108,0.07825997471809387:A-2.2594149214932857e-05],)
        &lt;&lt;&lt;
        &gt;&gt;&gt; layers[0]: Phi3DecoderLayer
          &gt; ((CT1s2x3x3072[-0.08262448012828827,0.07648692280054092:A0.00013174691782688906],),dict(attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.400763034820557,4.259133338928223:A-0.004826152082167923],CT1s2x32x30x96[-4.0469818115234375,4.471982479095459:A0.002217433422628052]], value_cache=#2[CT1s2x32x30x96[-4.405541896820068,4.668274402618408:A0.002596911304140301],CT1s2x32x30x96[-4.310667037963867,4.2944207191467285:A0.00028137129325799627]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
          &gt; ((CT1s3x4x3072[-0.08454269170761108,0.07825997471809387:A-2.2594149214932857e-05],),dict(attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.828048229217529,4.243164539337158:A0.00031101864580231346],CT1s3x32x31x96[-4.626999855041504,4.47794246673584:A-0.0025560081888748925]], value_cache=#2[CT1s3x32x31x96[-4.349869728088379,4.253993034362793:A0.0011117361406822917],CT1s3x32x31x96[-4.810003280639648,4.869333267211914:A-0.00027058590800271105]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
            &gt;&gt;&gt; self_attn: Phi3Attention
              &gt; ((),dict(hidden_states:CT1s2x3x3072[-4.117458343505859,3.675187587738037:A0.006496929148884729],attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.400763034820557,4.259133338928223:A-0.004826152082167923],CT1s2x32x30x96[-4.0469818115234375,4.471982479095459:A0.002217433422628052]], value_cache=#2[CT1s2x32x30x96[-4.405541896820068,4.668274402618408:A0.002596911304140301],CT1s2x32x30x96[-4.310667037963867,4.2944207191467285:A0.00028137129325799627]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
              &gt; ((),dict(hidden_states:CT1s3x4x3072[-4.234438896179199,3.8390731811523438:A-0.0011698477338398423],attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.828048229217529,4.243164539337158:A0.00031101864580231346],CT1s3x32x31x96[-4.626999855041504,4.47794246673584:A-0.0025560081888748925]], value_cache=#2[CT1s3x32x31x96[-4.349869728088379,4.253993034362793:A0.0011117361406822917],CT1s3x32x31x96[-4.810003280639648,4.869333267211914:A-0.00027058590800271105]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                &gt;&gt;&gt; o_proj: Linear
                  &gt; ((CT1s2x3x3072[-1.7804882526397705,1.7273778915405273:A0.003596998118933412],),{})
                  &gt; ((CT1s3x4x3072[-3.1012768745422363,2.9759175777435303:A0.0021097844879451383],),{})
                  &lt; (CT1s2x3x3072[-1.6213274002075195,1.6112512350082397:A0.0015135138111317145],)
                  &lt; (CT1s3x4x3072[-1.6543391942977905,1.7866795063018799:A-0.0027640321040425736],)
                &lt;&lt;&lt;
                &gt;&gt;&gt; qkv_proj: Linear
                  &gt; ((CT1s2x3x3072[-4.117458343505859,3.675187587738037:A0.006496929148884729],),{})
                  &gt; ((CT1s3x4x3072[-4.234438896179199,3.8390731811523438:A-0.0011698477338398423],),{})
                  &lt; (CT1s2x3x9216[-4.603874683380127,4.854994297027588:A-0.007235225691783011],)
                  &lt; (CT1s3x4x9216[-5.057411193847656,4.701048374176025:A-0.0005786661612424487],)
                &lt;&lt;&lt;
              &lt; (CT1s2x3x3072[-1.6213274002075195,1.6112512350082397:A0.0015135138111317145],None)
              &lt; (CT1s3x4x3072[-1.6543391942977905,1.7866795063018799:A-0.0027640321040425736],None)
            &lt;&lt;&lt;
            &gt;&gt;&gt; mlp: Phi3MLP
              &gt; ((CT1s2x3x3072[-4.074462413787842,4.080641269683838:A0.004271472019650317],),{})
              &gt; ((CT1s3x4x3072[-4.160494804382324,4.450273036956787:A-0.007759615481418791],),{})
                &gt;&gt;&gt; gate_up_proj: Linear
                  &gt; ((CT1s2x3x3072[-4.074462413787842,4.080641269683838:A0.004271472019650317],),{})
                  &gt; ((CT1s3x4x3072[-4.160494804382324,4.450273036956787:A-0.007759615481418791],),{})
                  &lt; (CT1s2x3x16384[-4.665277481079102,4.764751434326172:A-0.003516147276544738],)
                  &lt; (CT1s3x4x16384[-4.765247821807861,4.7090325355529785:A-0.0005196295543422972],)
                &lt;&lt;&lt;
                &gt;&gt;&gt; down_proj: Linear
                  &gt; ((CT1s2x3x8192[-12.33570671081543,11.671303749084473:A-0.0007140373650187307],),{})
                  &gt; ((CT1s3x4x8192[-9.95230770111084,10.653127670288086:A-0.0001526525493489491],),{})
                  &lt; (CT1s2x3x3072[-5.095609188079834,5.600772857666016:A0.017001853358547377],)
                  &lt; (CT1s3x4x3072[-5.286900043487549,6.241175174713135:A0.00745626809162382],)
                &lt;&lt;&lt;
                &gt;&gt;&gt; activation_fn: SiLU
                  &gt; ((CT1s2x3x8192[-4.665277481079102,4.764751434326172:A-0.0084779705295972],),{})
                  &gt; ((CT1s3x4x8192[-4.765247821807861,4.606565952301025:A0.002571067230685268],),{})
                  &lt; (CT1s2x3x8192[-0.27846455574035645,4.724475383758545:A0.2441930089948916],)
                  &lt; (CT1s3x4x8192[-0.27846455574035645,4.561019420623779:A0.24662724596589647],)
                &lt;&lt;&lt;
              &lt; (CT1s2x3x3072[-5.095609188079834,5.600772857666016:A0.017001853358547377],)
              &lt; (CT1s3x4x3072[-5.286900043487549,6.241175174713135:A0.00745626809162382],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; input_layernorm: Phi3RMSNorm
              &gt; ((CT1s2x3x3072[-0.08262448012828827,0.07648692280054092:A0.00013174691782688906],),{})
              &gt; ((CT1s3x4x3072[-0.08454269170761108,0.07825997471809387:A-2.2594149214932857e-05],),{})
              &lt; (CT1s2x3x3072[-4.117458343505859,3.675187587738037:A0.006496929148884729],)
              &lt; (CT1s3x4x3072[-4.234438896179199,3.8390731811523438:A-0.0011698477338398423],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; post_attention_layernorm: Phi3RMSNorm
              &gt; ((CT1s2x3x3072[-1.59036123752594,1.5879346132278442:A0.0016452607151254345],),{})
              &gt; ((CT1s3x4x3072[-1.6508663892745972,1.791300892829895:A-0.002786626275726197],),{})
              &lt; (CT1s2x3x3072[-4.074462413787842,4.080641269683838:A0.004271472019650317],)
              &lt; (CT1s3x4x3072[-4.160494804382324,4.450273036956787:A-0.007759615481418791],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; resid_attn_dropout: Dropout
              &gt; ((CT1s2x3x3072[-1.6213274002075195,1.6112512350082397:A0.0015135138111317145],),{})
              &gt; ((CT1s3x4x3072[-1.6543391942977905,1.7866795063018799:A-0.0027640321040425736],),{})
              &lt; (CT1s2x3x3072[-1.6213274002075195,1.6112512350082397:A0.0015135138111317145],)
              &lt; (CT1s3x4x3072[-1.6543391942977905,1.7866795063018799:A-0.0027640321040425736],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; resid_mlp_dropout: Dropout
              &gt; ((CT1s2x3x3072[-5.095609188079834,5.600772857666016:A0.017001853358547377],),{})
              &gt; ((CT1s3x4x3072[-5.286900043487549,6.241175174713135:A0.00745626809162382],),{})
              &lt; (CT1s2x3x3072[-5.095609188079834,5.600772857666016:A0.017001853358547377],)
              &lt; (CT1s3x4x3072[-5.286900043487549,6.241175174713135:A0.00745626809162382],)
            &lt;&lt;&lt;
          &lt; (CT1s2x3x3072[-5.606287479400635,5.762608051300049:A0.018647113798023283],)
          &lt; (CT1s3x4x3072[-5.794071197509766,5.967385292053223:A0.004669641863668201],)
        &lt;&lt;&lt;
        &gt;&gt;&gt; layers[1]: Phi3DecoderLayer
          &gt; ((CT1s2x3x3072[-5.606287479400635,5.762608051300049:A0.018647113798023283],),dict(attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.061519622802734,4.763699054718018:A-0.005803250564235515],CT1s2x32x30x96[-4.0469818115234375,4.471982479095459:A0.002217433422628052]], value_cache=#2[CT1s2x32x33x96[-4.603874683380127,4.668274402618408:A0.0014253464397422568],CT1s2x32x30x96[-4.310667037963867,4.2944207191467285:A0.00028137129325799627]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
          &gt; ((CT1s3x4x3072[-5.794071197509766,5.967385292053223:A0.004669641863668201],),dict(attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.101267337799072,5.592326641082764:A0.0004619398607301088],CT1s3x32x31x96[-4.626999855041504,4.47794246673584:A-0.0025560081888748925]], value_cache=#2[CT1s3x32x35x96[-5.057411193847656,4.3394317626953125:A0.00042293995440766975],CT1s3x32x31x96[-4.810003280639648,4.869333267211914:A-0.00027058590800271105]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
            &gt;&gt;&gt; self_attn: Phi3Attention
              &gt; ((),dict(hidden_states:CT1s2x3x3072[-4.0454277992248535,4.159030914306641:A0.013164340832402624],attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.061519622802734,4.763699054718018:A-0.005803250564235515],CT1s2x32x30x96[-4.0469818115234375,4.471982479095459:A0.002217433422628052]], value_cache=#2[CT1s2x32x33x96[-4.603874683380127,4.668274402618408:A0.0014253464397422568],CT1s2x32x30x96[-4.310667037963867,4.2944207191467285:A0.00028137129325799627]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
              &gt; ((),dict(hidden_states:CT1s3x4x3072[-3.9066245555877686,4.0234808921813965:A0.003327302300785028],attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.101267337799072,5.592326641082764:A0.0004619398607301088],CT1s3x32x31x96[-4.626999855041504,4.47794246673584:A-0.0025560081888748925]], value_cache=#2[CT1s3x32x35x96[-5.057411193847656,4.3394317626953125:A0.00042293995440766975],CT1s3x32x31x96[-4.810003280639648,4.869333267211914:A-0.00027058590800271105]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                &gt;&gt;&gt; o_proj: Linear
                  &gt; ((CT1s2x3x3072[-2.426612615585327,2.5618062019348145:A0.0013105594592492447],),{})
                  &gt; ((CT1s3x4x3072[-2.194685935974121,1.6999558210372925:A0.0006558623772832772],),{})
                  &lt; (CT1s2x3x3072[-1.6817601919174194,1.8244985342025757:A0.007823421057183724],)
                  &lt; (CT1s3x4x3072[-1.51430082321167,1.9903841018676758:A-0.0005759481610236131],)
                &lt;&lt;&lt;
                &gt;&gt;&gt; qkv_proj: Linear
                  &gt; ((CT1s2x3x3072[-4.0454277992248535,4.159030914306641:A0.013164340832402624],),{})
                  &gt; ((CT1s3x4x3072[-3.9066245555877686,4.0234808921813965:A0.003327302300785028],),{})
                  &lt; (CT1s2x3x9216[-4.424978733062744,4.548888206481934:A0.007687200885247539],)
                  &lt; (CT1s3x4x9216[-5.294676780700684,4.829901695251465:A0.0011063482611651427],)
                &lt;&lt;&lt;
              &lt; (CT1s2x3x3072[-1.6817601919174194,1.8244985342025757:A0.007823421057183724],None)
              &lt; (CT1s3x4x3072[-1.51430082321167,1.9903841018676758:A-0.0005759481610236131],None)
            &lt;&lt;&lt;
            &gt;&gt;&gt; mlp: Phi3MLP
              &gt; ((CT1s2x3x3072[-3.9352612495422363,3.729083776473999:A0.018089336692754848],),{})
              &gt; ((CT1s3x4x3072[-3.7487435340881348,4.227113723754883:A0.002809795542497976],),{})
                &gt;&gt;&gt; gate_up_proj: Linear
                  &gt; ((CT1s2x3x3072[-3.9352612495422363,3.729083776473999:A0.018089336692754848],),{})
                  &gt; ((CT1s3x4x3072[-3.7487435340881348,4.227113723754883:A0.002809795542497976],),{})
                  &lt; (CT1s2x3x16384[-4.6556878089904785,4.95295524597168:A0.005004160443962273],)
                  &lt; (CT1s3x4x16384[-4.945455551147461,5.058211326599121:A-0.0019263709165381708],)
                &lt;&lt;&lt;
                &gt;&gt;&gt; down_proj: Linear
                  &gt; ((CT1s2x3x8192[-8.74767017364502,9.395020484924316:A0.0006071647130518922],),{})
                  &gt; ((CT1s3x4x8192[-13.272783279418945,10.04904842376709:A-0.0015269200680454332],),{})
                  &lt; (CT1s2x3x3072[-5.51132869720459,5.5960211753845215:A-0.0008730196218114846],)
                  &lt; (CT1s3x4x3072[-5.300881862640381,5.086551189422607:A-0.002262299616445615],)
                &lt;&lt;&lt;
                &gt;&gt;&gt; activation_fn: SiLU
                  &gt; ((CT1s2x3x8192[-4.588644027709961,4.672793388366699:A0.0015853615327661903],),{})
                  &gt; ((CT1s3x4x8192[-4.798925399780273,5.058211326599121:A-0.0037998580466028407],),{})
                  &lt; (CT1s2x3x8192[-0.27846455574035645,4.629525184631348:A0.24538704169494321],)
                  &lt; (CT1s3x4x8192[-0.27846455574035645,5.026259422302246:A0.24493035986874703],)
                &lt;&lt;&lt;
              &lt; (CT1s2x3x3072[-5.51132869720459,5.5960211753845215:A-0.0008730196218114846],)
              &lt; (CT1s3x4x3072[-5.300881862640381,5.086551189422607:A-0.002262299616445615],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; input_layernorm: Phi3RMSNorm
              &gt; ((CT1s2x3x3072[-5.606287479400635,5.762608051300049:A0.018647113798023283],),{})
              &gt; ((CT1s3x4x3072[-5.794071197509766,5.967385292053223:A0.004669641863668201],),{})
              &lt; (CT1s2x3x3072[-4.0454277992248535,4.159030914306641:A0.013164340832402624],)
              &lt; (CT1s3x4x3072[-3.9066245555877686,4.0234808921813965:A0.003327302300785028],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; post_attention_layernorm: Phi3RMSNorm
              &gt; ((CT1s2x3x3072[-5.665860176086426,5.33676815032959:A0.02647053424324339],),{})
              &gt; ((CT1s3x4x3072[-5.680884838104248,6.474119186401367:A0.004093693750544642],),{})
              &lt; (CT1s2x3x3072[-3.9352612495422363,3.729083776473999:A0.018089336692754848],)
              &lt; (CT1s3x4x3072[-3.7487435340881348,4.227113723754883:A0.002809795542497976],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; resid_attn_dropout: Dropout
              &gt; ((CT1s2x3x3072[-1.6817601919174194,1.8244985342025757:A0.007823421057183724],),{})
              &gt; ((CT1s3x4x3072[-1.51430082321167,1.9903841018676758:A-0.0005759481610236131],),{})
              &lt; (CT1s2x3x3072[-1.6817601919174194,1.8244985342025757:A0.007823421057183724],)
              &lt; (CT1s3x4x3072[-1.51430082321167,1.9903841018676758:A-0.0005759481610236131],)
            &lt;&lt;&lt;
            &gt;&gt;&gt; resid_mlp_dropout: Dropout
              &gt; ((CT1s2x3x3072[-5.51132869720459,5.5960211753845215:A-0.0008730196218114846],),{})
              &gt; ((CT1s3x4x3072[-5.300881862640381,5.086551189422607:A-0.002262299616445615],),{})
              &lt; (CT1s2x3x3072[-5.51132869720459,5.5960211753845215:A-0.0008730196218114846],)
              &lt; (CT1s3x4x3072[-5.300881862640381,5.086551189422607:A-0.002262299616445615],)
            &lt;&lt;&lt;
          &lt; (CT1s2x3x3072[-7.9020490646362305,8.29597282409668:A0.025597514293015895],)
          &lt; (CT1s3x4x3072[-8.080930709838867,8.061180114746094:A0.0018313946399454533],)
        &lt;&lt;&lt;
        &gt;&gt;&gt; norm: Phi3RMSNorm
          &gt; ((CT1s2x3x3072[-7.9020490646362305,8.29597282409668:A0.025597514293015895],),{})
          &gt; ((CT1s3x4x3072[-8.080930709838867,8.061180114746094:A0.0018313946399454533],),{})
          &lt; (CT1s2x3x3072[-4.05719518661499,4.155795574188232:A0.012949258255610453],)
          &lt; (CT1s3x4x3072[-3.9973807334899902,3.9562745094299316:A0.0008511715070787332],)
        &lt;&lt;&lt;
        &gt;&gt;&gt; rotary_emb: Phi3RotaryEmbedding
          &gt; ((CT1s2x3x3072[-0.08262448012828827,0.07648692280054092:A0.00013174691782688906],CT7s1x3[30,32:A31.0]),{})
          &gt; ((CT1s3x4x3072[-0.08454269170761108,0.07825997471809387:A-2.2594149214932857e-05],CT7s1x4[31,34:A32.5]),{})
          &lt; (CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])
          &lt; (CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])
        &lt;&lt;&lt;
      &lt; (dict(last_hidden_state:CT1s2x3x3072[-4.05719518661499,4.155795574188232:A0.012949258255610453],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.061519622802734,4.763699054718018:A-0.005803250564235515],CT1s2x32x33x96[-5.262935161590576,5.203064441680908:A0.001793874462405861]], value_cache=#2[CT1s2x32x33x96[-4.603874683380127,4.668274402618408:A0.0014253464397422568],CT1s2x32x33x96[-4.3247175216674805,4.2944207191467285:A0.0008252018333532062]])),)
      &lt; (dict(last_hidden_state:CT1s3x4x3072[-3.9973807334899902,3.9562745094299316:A0.0008511715070787332],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.101267337799072,5.592326641082764:A0.0004619398607301088],CT1s3x32x35x96[-6.038402080535889,5.146603584289551:A-0.0029978743856185967]], value_cache=#2[CT1s3x32x35x96[-5.057411193847656,4.3394317626953125:A0.00042293995440766975],CT1s3x32x35x96[-5.294676780700684,4.869333267211914:A3.3132155645670344e-05]])),)
    &lt;&lt;&lt;
    &gt;&gt;&gt; lm_head: Linear
      &gt; ((CT1s2x3x3072[-4.05719518661499,4.155795574188232:A0.012949258255610453],),{})
      &gt; ((CT1s3x4x3072[-3.9973807334899902,3.9562745094299316:A0.0008511715070787332],),{})
      &lt; (CT1s2x3x32064[-4.982978820800781,4.97531270980835:A5.55359560734437e-05],)
      &lt; (CT1s3x4x32064[-5.064220428466797,5.067513465881348:A-0.0019615298296192804],)
    &lt;&lt;&lt;
  &lt; (dict(logits:CT1s2x3x32064[-4.982978820800781,4.97531270980835:A5.55359560734437e-05],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.061519622802734,4.763699054718018:A-0.005803250564235515],CT1s2x32x33x96[-5.262935161590576,5.203064441680908:A0.001793874462405861]], value_cache=#2[CT1s2x32x33x96[-4.603874683380127,4.668274402618408:A0.0014253464397422568],CT1s2x32x33x96[-4.3247175216674805,4.2944207191467285:A0.0008252018333532062]])),)
  &lt; (dict(logits:CT1s3x4x32064[-5.064220428466797,5.067513465881348:A-0.0019615298296192804],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.101267337799072,5.592326641082764:A0.0004619398607301088],CT1s3x32x35x96[-6.038402080535889,5.146603584289551:A-0.0029978743856185967]], value_cache=#2[CT1s3x32x35x96[-5.057411193847656,4.3394317626953125:A0.00042293995440766975],CT1s3x32x35x96[-5.294676780700684,4.869333267211914:A3.3132155645670344e-05]])),)
&lt;&lt;&lt;
[_untrace_forward_execution]  M:__main__-Phi3ForCausalLM
[_untrace_forward_execution] .. M:model-Phi3Model
[_untrace_forward_execution] .... M:embed_tokens-Embedding
[_untrace_forward_execution] .... M:layers[0]-Phi3DecoderLayer
[_untrace_forward_execution] ...... M:self_attn-Phi3Attention
[_untrace_forward_execution] ........ M:o_proj-Linear
[_untrace_forward_execution] ........ M:qkv_proj-Linear
[_untrace_forward_execution] ...... M:mlp-Phi3MLP
[_untrace_forward_execution] ........ M:gate_up_proj-Linear
[_untrace_forward_execution] ........ M:down_proj-Linear
[_untrace_forward_execution] ........ M:activation_fn-SiLU
[_untrace_forward_execution] ...... M:input_layernorm-Phi3RMSNorm
[_untrace_forward_execution] ...... M:post_attention_layernorm-Phi3RMSNorm
[_untrace_forward_execution] ...... M:resid_attn_dropout-Dropout
[_untrace_forward_execution] ...... M:resid_mlp_dropout-Dropout
[_untrace_forward_execution] .... M:layers[1]-Phi3DecoderLayer
[_untrace_forward_execution] ...... M:self_attn-Phi3Attention
[_untrace_forward_execution] ........ M:o_proj-Linear
[_untrace_forward_execution] ........ M:qkv_proj-Linear
[_untrace_forward_execution] ...... M:mlp-Phi3MLP
[_untrace_forward_execution] ........ M:gate_up_proj-Linear
[_untrace_forward_execution] ........ M:down_proj-Linear
[_untrace_forward_execution] ........ M:activation_fn-SiLU
[_untrace_forward_execution] ...... M:input_layernorm-Phi3RMSNorm
[_untrace_forward_execution] ...... M:post_attention_layernorm-Phi3RMSNorm
[_untrace_forward_execution] ...... M:resid_attn_dropout-Dropout
[_untrace_forward_execution] ...... M:resid_mlp_dropout-Dropout
[_untrace_forward_execution] .... M:norm-Phi3RMSNorm
[_untrace_forward_execution] .... M:rotary_emb-Phi3RotaryEmbedding
[_untrace_forward_execution] .. M:lm_head-Linear
</pre></div>
</div>
<p>Now we keep in memory every input/output for the submodules,
we can guess the dynamic shapes for every of them.
The final ones:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.guess_dynamic_shapes" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><span class="n">diag</span><span class="o">.</span><span class="n">guess_dynamic_shapes</span></a></a></a></a><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The dynamic shapes are:&quot;</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span></a></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dynamic_shapes</span></a></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The dynamic shapes are:
((),
 {&#39;attention_mask&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                 min=None,
                                 max=None,
                                 _factory=True),
                     1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                 min=None,
                                 max=None,
                                 _factory=True)},
  &#39;input_ids&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                            min=None,
                            max=None,
                            _factory=True),
                1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                            min=None,
                            max=None,
                            _factory=True)},
  &#39;past_key_values&#39;: [[{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True),
                        2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True)},
                       {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True),
                        2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True)}],
                      [{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True),
                        2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True)},
                       {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True),
                        2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;,
                                    min=None,
                                    max=None,
                                    _factory=True)}]]})
</pre></div>
</div>
<p>And all the dynamic shapes all along the traced submodules.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><span class="n">diag</span><span class="o">.</span><span class="n">pretty_text</span></a></a></a></a><span class="p">(</span>
        <span class="n">with_dynamic_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">with_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_device</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;_DimHint.DYNAMIC: 3&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;DYN&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; __main__: Phi3ForCausalLM
  DS=((), {&#39;attention_mask&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;input_ids&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;past_key_values&#39;: [[{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}], [{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}]]})
    &gt;&gt;&gt; model: Phi3Model
      DS=((), {&#39;attention_mask&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;cache_position&#39;: None, &#39;input_ids&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;inputs_embeds&#39;: None, &#39;output_attentions&#39;: None, &#39;output_hidden_states&#39;: None, &#39;past_key_values&#39;: [[{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}], [{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}]], &#39;position_ids&#39;: None, &#39;use_cache&#39;: None})
        &gt;&gt;&gt; embed_tokens: Embedding: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
        &gt;&gt;&gt; layers[0]: Phi3DecoderLayer
          DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {&#39;attention_mask&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 3: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;cache_position&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;output_attentions&#39;: None, &#39;past_key_value&#39;: [[{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}], [{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}]], &#39;position_embeddings&#39;: ({1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}), &#39;position_ids&#39;: {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;use_cache&#39;: None})
            &gt;&gt;&gt; self_attn: Phi3Attention
              DS=((), {&#39;attention_mask&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 3: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;cache_position&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;hidden_states&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;output_attentions&#39;: None, &#39;past_key_value&#39;: [[{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}], [{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}]], &#39;position_embeddings&#39;: ({1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}), &#39;position_ids&#39;: {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;use_cache&#39;: None})
                &gt;&gt;&gt; o_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
                &gt;&gt;&gt; qkv_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; mlp: Phi3MLP
              DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {})
                &gt;&gt;&gt; gate_up_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
                &gt;&gt;&gt; down_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
                &gt;&gt;&gt; activation_fn: SiLU: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; input_layernorm: Phi3RMSNorm: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &gt;&gt;&gt; post_attention_layernorm: Phi3RMSNorm: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &gt;&gt;&gt; resid_attn_dropout: Dropout: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &gt;&gt;&gt; resid_mlp_dropout: Dropout: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
        &lt;&lt;&lt;
        &gt;&gt;&gt; layers[1]: Phi3DecoderLayer
          DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {&#39;attention_mask&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 3: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;cache_position&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;output_attentions&#39;: None, &#39;past_key_value&#39;: [[{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}], [{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}]], &#39;position_embeddings&#39;: ({1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}), &#39;position_ids&#39;: {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;use_cache&#39;: None})
            &gt;&gt;&gt; self_attn: Phi3Attention
              DS=((), {&#39;attention_mask&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 3: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;cache_position&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;hidden_states&#39;: {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;output_attentions&#39;: None, &#39;past_key_value&#39;: [[{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}], [{0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 2: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}]], &#39;position_embeddings&#39;: ({1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}), &#39;position_ids&#39;: {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, &#39;use_cache&#39;: None})
                &gt;&gt;&gt; o_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
                &gt;&gt;&gt; qkv_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; mlp: Phi3MLP
              DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {})
                &gt;&gt;&gt; gate_up_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
                &gt;&gt;&gt; down_proj: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
                &gt;&gt;&gt; activation_fn: SiLU: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; input_layernorm: Phi3RMSNorm: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &gt;&gt;&gt; post_attention_layernorm: Phi3RMSNorm: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &gt;&gt;&gt; resid_attn_dropout: Dropout: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
            &gt;&gt;&gt; resid_mlp_dropout: Dropout: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
        &lt;&lt;&lt;
        &gt;&gt;&gt; norm: Phi3RMSNorm: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
        &gt;&gt;&gt; rotary_emb: Phi3RotaryEmbedding: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}, {1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)}), {}) &lt;&lt;&lt;
    &lt;&lt;&lt;
    &gt;&gt;&gt; lm_head: Linear: DS=(({0: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True), 1: _DimHint(type=&lt;_DimHintType.DYNAMIC: 3&gt;, min=None, max=None, _factory=True)},), {}) &lt;&lt;&lt;
&lt;&lt;&lt;
</pre></div>
</div>
</section>
<section id="evaluate-the-export">
<h2>Evaluate the export<a class="headerlink" href="#evaluate-the-export" title="Link to this heading"></a></h2>
<p>In many cases, the export (to <a class="reference external" href="https://docs.pytorch.org/docs/main/fx.html#torch.fx.Graph" title="(in PyTorch vmain (2.8.0a0+gitd5f6422 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.fx.Graph</span></code></a>, to ONNX)
does not work on the first try. We need a way to understand
how much the model can be exported. It can be used to evaluate
the how much code needs to be rewritten or patched to be exportable.
The verbosity can be increase to show dynamic shapes, results
of the discrepancies.
Lets display the module and its submodule first.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.pretty_text" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><span class="n">diag</span><span class="o">.</span><span class="n">pretty_text</span></a></a></a></a><span class="p">(</span>
        <span class="n">with_dynamic_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_device</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">with_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; __main__: Phi3ForCausalLM
    &gt;&gt;&gt; model: Phi3Model
        &gt;&gt;&gt; embed_tokens: Embedding &lt;&lt;&lt;
        &gt;&gt;&gt; layers[0]: Phi3DecoderLayer
            &gt;&gt;&gt; self_attn: Phi3Attention
                &gt;&gt;&gt; o_proj: Linear &lt;&lt;&lt;
                &gt;&gt;&gt; qkv_proj: Linear &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; mlp: Phi3MLP
                &gt;&gt;&gt; gate_up_proj: Linear &lt;&lt;&lt;
                &gt;&gt;&gt; down_proj: Linear &lt;&lt;&lt;
                &gt;&gt;&gt; activation_fn: SiLU &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; input_layernorm: Phi3RMSNorm &lt;&lt;&lt;
            &gt;&gt;&gt; post_attention_layernorm: Phi3RMSNorm &lt;&lt;&lt;
            &gt;&gt;&gt; resid_attn_dropout: Dropout &lt;&lt;&lt;
            &gt;&gt;&gt; resid_mlp_dropout: Dropout &lt;&lt;&lt;
        &lt;&lt;&lt;
        &gt;&gt;&gt; layers[1]: Phi3DecoderLayer
            &gt;&gt;&gt; self_attn: Phi3Attention
                &gt;&gt;&gt; o_proj: Linear &lt;&lt;&lt;
                &gt;&gt;&gt; qkv_proj: Linear &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; mlp: Phi3MLP
                &gt;&gt;&gt; gate_up_proj: Linear &lt;&lt;&lt;
                &gt;&gt;&gt; down_proj: Linear &lt;&lt;&lt;
                &gt;&gt;&gt; activation_fn: SiLU &lt;&lt;&lt;
            &lt;&lt;&lt;
            &gt;&gt;&gt; input_layernorm: Phi3RMSNorm &lt;&lt;&lt;
            &gt;&gt;&gt; post_attention_layernorm: Phi3RMSNorm &lt;&lt;&lt;
            &gt;&gt;&gt; resid_attn_dropout: Dropout &lt;&lt;&lt;
            &gt;&gt;&gt; resid_mlp_dropout: Dropout &lt;&lt;&lt;
        &lt;&lt;&lt;
        &gt;&gt;&gt; norm: Phi3RMSNorm &lt;&lt;&lt;
        &gt;&gt;&gt; rotary_emb: Phi3RotaryEmbedding &lt;&lt;&lt;
    &lt;&lt;&lt;
    &gt;&gt;&gt; lm_head: Linear &lt;&lt;&lt;
&lt;&lt;&lt;
</pre></div>
</div>
<p>The we try to export to see the submodule failing the whole model.
We can pickle the failing model and restore it to speedup
the refactoring to make it work.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----------------------&quot;</span><span class="p">)</span>
<a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExport" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.try_export" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><span class="n">diag</span><span class="o">.</span><span class="n">try_export</span></a></a></a></a><span class="p">(</span>
    <span class="n">exporter</span><span class="o">=</span><span class="s2">&quot;fx&quot;</span><span class="p">,</span>
    <span class="n">use_dynamic_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">exporter_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>----------------------




def forward(self, arg0_1: &quot;f32[32064, 3072]&quot;, arg1_1: &quot;f32[3072, 3072]&quot;, arg2_1: &quot;f32[9216, 3072]&quot;, arg3_1: &quot;f32[16384, 3072]&quot;, arg4_1: &quot;f32[3072, 8192]&quot;, arg5_1: &quot;f32[3072]&quot;, arg6_1: &quot;f32[3072]&quot;, arg7_1: &quot;f32[3072, 3072]&quot;, arg8_1: &quot;f32[9216, 3072]&quot;, arg9_1: &quot;f32[16384, 3072]&quot;, arg10_1: &quot;f32[3072, 8192]&quot;, arg11_1: &quot;f32[3072]&quot;, arg12_1: &quot;f32[3072]&quot;, arg13_1: &quot;f32[3072]&quot;, arg14_1: &quot;f32[32064, 3072]&quot;, arg15_1: &quot;f32[48]&quot;, arg16_1: &quot;i64[s47, s2]&quot;, arg17_1: &quot;i64[s47, s10]&quot;, arg18_1: &quot;f32[s89, 32, s66, 96]&quot;, arg19_1: &quot;f32[s14, 32, s80, 96]&quot;, arg20_1: &quot;f32[s62, 32, s96, 96]&quot;, arg21_1: &quot;f32[s58, 32, s81, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(
    embedding: &quot;f32[s47, s2, 3072]&quot; = torch.ops.aten.embedding.default(arg0_1, arg16_1, 32000);  arg0_1 = embedding = None

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    sym_numel_default: &quot;Sym(3072*s66*s89)&quot; = torch.ops.aten.sym_numel.default(arg18_1)
    ne: &quot;Sym(True)&quot; = sym_numel_default != 0;  ne = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:455 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s66)&quot; = torch.ops.aten.sym_size.int(arg18_1, 2);  arg18_1 = None
    sym_size_int_1: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(arg16_1, 1)
    add: &quot;Sym(s2 + s66)&quot; = sym_size_int + sym_size_int_1

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:454 in forward, code: cache_position = torch.arange(
    arange: &quot;i64[s2]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:459 in forward, code: position_ids = cache_position.unsqueeze(0)
    unsqueeze: &quot;i64[1, s2]&quot; = torch.ops.aten.unsqueeze.default(arange, 0)

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    ne_1: &quot;Sym(True)&quot; = sym_numel_default != 0;  sym_numel_default = ne_1 = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:461 in forward, code: causal_mask = self._update_causal_mask(
    add_1: &quot;Sym(s2 + s66)&quot; = sym_size_int_1 + sym_size_int;  sym_size_int = None
    lt: &quot;Sym(s2 + s66 &lt; 262144)&quot; = add_1 &lt; 262144;  add_1 = lt = None
    sym_size_int_2: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(arg17_1, 1)
    full: &quot;f32[s2, s10]&quot; = torch.ops.aten.full.default([sym_size_int_1, sym_size_int_2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_1: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1])
    gt: &quot;b8[s2, s10]&quot; = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
    arange_2: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape_1: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
    sub: &quot;i64[s2, 1]&quot; = torch.ops.aten.sub.Tensor(reshape_1, 262144);  reshape_1 = None
    le: &quot;b8[s2, s10]&quot; = torch.ops.aten.le.Tensor(arange_2, sub);  arange_2 = sub = None
    bitwise_or_: &quot;b8[s2, s10]&quot; = torch.ops.aten.bitwise_or_.Tensor(gt, le);  gt = le = None
    mul_: &quot;f32[s2, s10]&quot; = torch.ops.aten.mul_.Tensor(full, bitwise_or_);  full = bitwise_or_ = None
    unsqueeze_1: &quot;f32[1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
    unsqueeze_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
    eq: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_1 == 9223372036854775807;  sym_size_int_1 = eq = None
    slice_1: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
    eq_1: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_1 = None
    slice_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_1, 3, 0, 9223372036854775807)
    sym_size_int_3: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg16_1, 0);  arg16_1 = None
    expand: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.expand.default(slice_2, [sym_size_int_3, 1, -1, -1])
    clone: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.clone.default(expand);  expand = None
    gt_1: &quot;Sym(False)&quot; = sym_size_int_2 &gt; sym_size_int_2;  gt_1 = None
    slice_3: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_4: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_3, 1)
    slice_5: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_4, 2);  slice_4 = None
    slice_6: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_5, 3, None, sym_size_int_2)
    sym_size_int_4: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg17_1, 0)
    eq_2: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_4 == 9223372036854775807;  sym_size_int_4 = eq_2 = None
    slice_7: &quot;i64[s47, s10]&quot; = torch.ops.aten.slice.Tensor(arg17_1, 0, 0, 9223372036854775807);  arg17_1 = None
    unsqueeze_3: &quot;i64[s47, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(slice_7, 1);  slice_7 = None
    unsqueeze_4: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
    eq_3: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_3 = None
    slice_8: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_4, 3, 0, 9223372036854775807);  unsqueeze_4 = None
    to: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.to.dtype_layout(slice_8, dtype = torch.int64, layout = torch.strided, device = device(type=&#39;cpu&#39;));  slice_8 = None
    add_2: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.add.Tensor(slice_6, to);  to = None
    eq_4: &quot;b8[s47, 1, s2, s10]&quot; = torch.ops.aten.eq.Scalar(add_2, 0);  add_2 = None
    slice_9: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_10: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_9, 1);  slice_9 = None
    slice_11: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_10, 2);  slice_10 = None
    slice_12: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_11, 3, None, sym_size_int_2);  slice_11 = None
    masked_fill: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.masked_fill.Scalar(slice_12, eq_4, -3.4028234663852886e+38);  slice_12 = eq_4 = None
    eq_5: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_3 == 9223372036854775807;  sym_size_int_3 = eq_5 = None
    slice_13: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807);  clone = None
    slice_14: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_13, 1, 0, 9223372036854775807)
    sym_size_int_5: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_1, 2);  slice_1 = None
    eq_6: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_5 == 9223372036854775807;  sym_size_int_5 = eq_6 = None
    slice_15: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_14, 2, 0, 9223372036854775807);  slice_14 = None
    sym_size_int_6: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_2, 3);  slice_2 = None
    eq_7: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_2;  sym_size_int_2 = eq_7 = None
    sym_size_int_7: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_13, 0);  slice_13 = None
    sym_size_int_8: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_3, 0);  slice_3 = None
    eq_8: &quot;Sym(True)&quot; = sym_size_int_7 == sym_size_int_8;  sym_size_int_7 = sym_size_int_8 = eq_8 = None
    sym_size_int_9: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_15, 2)
    sym_size_int_10: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_5, 2);  slice_5 = None
    eq_9: &quot;Sym(True)&quot; = sym_size_int_9 == sym_size_int_10;  sym_size_int_9 = sym_size_int_10 = eq_9 = None
    sym_size_int_11: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_6, 3);  slice_6 = None
    eq_10: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_11;  sym_size_int_6 = sym_size_int_11 = eq_10 = None
    copy_: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.copy_.default(slice_15, masked_fill);  slice_15 = masked_fill = copy_ = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:468 in forward, code: position_embeddings = self.rotary_emb(hidden_states, position_ids)
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    max_1: &quot;i64[]&quot; = torch.ops.aten.max.default(unsqueeze);  unsqueeze = None
    add_3: &quot;i64[]&quot; = torch.ops.aten.add.Tensor(max_1, 1);  max_1 = None
    gt_2: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(add_3, 4096);  add_3 = None
    ne_2: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt_2, 0);  gt_2 = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne_2);  ne_2 = item = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None




def forward(self, arg0_1: &quot;f32[32064, 3072]&quot;, arg1_1: &quot;f32[3072, 3072]&quot;, arg2_1: &quot;f32[9216, 3072]&quot;, arg3_1: &quot;f32[16384, 3072]&quot;, arg4_1: &quot;f32[3072, 8192]&quot;, arg5_1: &quot;f32[3072]&quot;, arg6_1: &quot;f32[3072]&quot;, arg7_1: &quot;f32[3072, 3072]&quot;, arg8_1: &quot;f32[9216, 3072]&quot;, arg9_1: &quot;f32[16384, 3072]&quot;, arg10_1: &quot;f32[3072, 8192]&quot;, arg11_1: &quot;f32[3072]&quot;, arg12_1: &quot;f32[3072]&quot;, arg13_1: &quot;f32[3072]&quot;, arg14_1: &quot;f32[32064, 3072]&quot;, arg15_1: &quot;f32[48]&quot;, arg16_1: &quot;i64[s47, s2]&quot;, arg17_1: &quot;i64[s47, s10]&quot;, arg18_1: &quot;f32[s89, 32, s66, 96]&quot;, arg19_1: &quot;f32[s14, 32, s80, 96]&quot;, arg20_1: &quot;f32[s62, 32, s96, 96]&quot;, arg21_1: &quot;f32[s58, 32, s81, 96]&quot;):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(
    embedding: &quot;f32[s47, s2, 3072]&quot; = torch.ops.aten.embedding.default(arg0_1, arg16_1, 32000);  arg0_1 = embedding = None

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    sym_numel_default: &quot;Sym(3072*s66*s89)&quot; = torch.ops.aten.sym_numel.default(arg18_1)
    ne: &quot;Sym(True)&quot; = sym_numel_default != 0;  ne = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:455 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s66)&quot; = torch.ops.aten.sym_size.int(arg18_1, 2);  arg18_1 = None
    sym_size_int_1: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(arg16_1, 1)
    add: &quot;Sym(s2 + s66)&quot; = sym_size_int + sym_size_int_1

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:454 in forward, code: cache_position = torch.arange(
    arange: &quot;i64[s2]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:459 in forward, code: position_ids = cache_position.unsqueeze(0)
    unsqueeze: &quot;i64[1, s2]&quot; = torch.ops.aten.unsqueeze.default(arange, 0)

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    ne_1: &quot;Sym(True)&quot; = sym_numel_default != 0;  sym_numel_default = ne_1 = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:461 in forward, code: causal_mask = self._update_causal_mask(
    add_1: &quot;Sym(s2 + s66)&quot; = sym_size_int_1 + sym_size_int;  sym_size_int = None
    lt: &quot;Sym(s2 + s66 &lt; 262144)&quot; = add_1 &lt; 262144;  add_1 = lt = None
    sym_size_int_2: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(arg17_1, 1)
    full: &quot;f32[s2, s10]&quot; = torch.ops.aten.full.default([sym_size_int_1, sym_size_int_2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_1: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1])
    gt: &quot;b8[s2, s10]&quot; = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
    arange_2: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape_1: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
    sub: &quot;i64[s2, 1]&quot; = torch.ops.aten.sub.Tensor(reshape_1, 262144);  reshape_1 = None
    le: &quot;b8[s2, s10]&quot; = torch.ops.aten.le.Tensor(arange_2, sub);  arange_2 = sub = None
    bitwise_or_: &quot;b8[s2, s10]&quot; = torch.ops.aten.bitwise_or_.Tensor(gt, le);  gt = le = None
    mul_: &quot;f32[s2, s10]&quot; = torch.ops.aten.mul_.Tensor(full, bitwise_or_);  full = bitwise_or_ = None
    unsqueeze_1: &quot;f32[1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
    unsqueeze_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
    eq: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_1 == 9223372036854775807;  sym_size_int_1 = eq = None
    slice_1: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
    eq_1: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_1 = None
    slice_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_1, 3, 0, 9223372036854775807)
    sym_size_int_3: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg16_1, 0);  arg16_1 = None
    expand: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.expand.default(slice_2, [sym_size_int_3, 1, -1, -1])
    clone: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.clone.default(expand);  expand = None
    gt_1: &quot;Sym(False)&quot; = sym_size_int_2 &gt; sym_size_int_2;  gt_1 = None
    slice_3: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_4: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_3, 1)
    slice_5: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_4, 2);  slice_4 = None
    slice_6: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_5, 3, None, sym_size_int_2)
    sym_size_int_4: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg17_1, 0)
    eq_2: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_4 == 9223372036854775807;  sym_size_int_4 = eq_2 = None
    slice_7: &quot;i64[s47, s10]&quot; = torch.ops.aten.slice.Tensor(arg17_1, 0, 0, 9223372036854775807);  arg17_1 = None
    unsqueeze_3: &quot;i64[s47, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(slice_7, 1);  slice_7 = None
    unsqueeze_4: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
    eq_3: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_3 = None
    slice_8: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_4, 3, 0, 9223372036854775807);  unsqueeze_4 = None
    to: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.to.dtype_layout(slice_8, dtype = torch.int64, layout = torch.strided, device = device(type=&#39;cpu&#39;));  slice_8 = None
    add_2: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.add.Tensor(slice_6, to);  to = None
    eq_4: &quot;b8[s47, 1, s2, s10]&quot; = torch.ops.aten.eq.Scalar(add_2, 0);  add_2 = None
    slice_9: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_10: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_9, 1);  slice_9 = None
    slice_11: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_10, 2);  slice_10 = None
    slice_12: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_11, 3, None, sym_size_int_2);  slice_11 = None
    masked_fill: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.masked_fill.Scalar(slice_12, eq_4, -3.4028234663852886e+38);  slice_12 = eq_4 = None
    eq_5: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_3 == 9223372036854775807;  sym_size_int_3 = eq_5 = None
    slice_13: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807);  clone = None
    slice_14: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_13, 1, 0, 9223372036854775807)
    sym_size_int_5: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_1, 2);  slice_1 = None
    eq_6: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_5 == 9223372036854775807;  sym_size_int_5 = eq_6 = None
    slice_15: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_14, 2, 0, 9223372036854775807);  slice_14 = None
    sym_size_int_6: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_2, 3);  slice_2 = None
    eq_7: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_2;  sym_size_int_2 = eq_7 = None
    sym_size_int_7: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_13, 0);  slice_13 = None
    sym_size_int_8: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_3, 0);  slice_3 = None
    eq_8: &quot;Sym(True)&quot; = sym_size_int_7 == sym_size_int_8;  sym_size_int_7 = sym_size_int_8 = eq_8 = None
    sym_size_int_9: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_15, 2)
    sym_size_int_10: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_5, 2);  slice_5 = None
    eq_9: &quot;Sym(True)&quot; = sym_size_int_9 == sym_size_int_10;  sym_size_int_9 = sym_size_int_10 = eq_9 = None
    sym_size_int_11: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_6, 3);  slice_6 = None
    eq_10: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_11;  sym_size_int_6 = sym_size_int_11 = eq_10 = None
    copy_: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.copy_.default(slice_15, masked_fill);  slice_15 = masked_fill = copy_ = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:468 in forward, code: position_embeddings = self.rotary_emb(hidden_states, position_ids)
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    max_1: &quot;i64[]&quot; = torch.ops.aten.max.default(unsqueeze);  unsqueeze = None
    add_3: &quot;i64[]&quot; = torch.ops.aten.add.Tensor(max_1, 1);  max_1 = None
    gt_2: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(add_3, 4096);  add_3 = None
    ne_2: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt_2, 0);  gt_2 = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne_2);  ne_2 = item = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None

[try_export-FX]  M:__main__-Phi3ForCausalLM --- FAIL, step=EXPORT, reason=Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none) ---  --- Caused by: (_export/non_strict_utils.py:973 in __torch_function__) --- For more information, run with TORCH_LOGS=&quot;dynamic&quot; --- For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot; --- If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 --- For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing ---  --- For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 ---  --- The following call raised this error: ---   File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update ---     if seq_len &gt; original_max_position_embeddings: ---  ---  --- The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.[&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;~/github/experimental-experiment/experimental_experiment/torch_interpreter/piece_by_piece.py&quot;, line 1587, in _try_export_no_bypass_export\n    ep = torch.export.export(\n         ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/__init__.py&quot;, line 319, in export\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/__init__.py&quot;, line 286, in export\n    return _export(\n           ^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1159, in wrapper\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1125, in wrapper\n    ep = fn(*args, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 2172, in _export\n    ep = _export_for_training(\n         ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1159, in wrapper\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1125, in wrapper\n    ep = fn(*args, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 2033, in _export_for_training\n    export_artifact = export_func(\n                      ^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1975, in _non_strict_export\n    aten_export_artifact = _to_aten_func(  # type: ignore[operator]\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1760, in _export_to_aten_ir_make_fx\n    gm, graph_signature = transform(_make_fx_helper)(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1901, in _aot_export_non_strict\n    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1679, in _make_fx_helper\n    gm = make_fx(\n         ^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2290, in wrapped\n    return make_fx_tracer.trace(f, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2228, in trace\n    return self._trace_inner(f, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2199, in _trace_inner\n    t = dispatch_trace(\n        ^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_compile.py&quot;, line 51, in inner\n    return disable_fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&quot;, line 893, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1223, in dispatch_trace\n    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1787, in trace\n    res = super().trace(root, concrete_args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&quot;, line 893, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace\n    (self.create_arg(fn(*args)),),\n                     ^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1278, in wrapped\n    out = f(*tensors)  # type:ignore[call-arg]\n          ^^^^^^^^^^^\n&#39;, &#39;  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1583, in wrapped_fn\n    return tuple(flat_fn(*args))\n                 ^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn\n    tree_out = fn(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call\n    out = mod(*args[params_len:], **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1885, in forward\n    tree_out = mod(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/utils/generic.py&quot;, line 969, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/models/phi3/modeling_phi3.py&quot;, line 744, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n                                       ^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/utils/generic.py&quot;, line 969, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/models/phi3/modeling_phi3.py&quot;, line 468, in forward\n    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/utils/_contextlib.py&quot;, line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 86, in wrapper\n    longrope_frequency_update(self, position_ids, device=x.device)\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update\n    if seq_len &gt; original_max_position_embeddings:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1326, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1373, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_export/non_strict_utils.py&quot;, line 973, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py&quot;, line 536, in guard_bool\n    r = self.evaluate()\n        ^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py&quot;, line 510, in evaluate\n    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6857, in evaluate_sym_node\n    return self.evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6876, in evaluate_expr\n    return self._inner_evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper\n    return retlog(fn(*args, **kwargs))\n                  ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6892, in _inner_evaluate_expr\n    return self._evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7160, in _evaluate_expr\n    raise self._make_data_dependent_error(\n&#39;, &#39;torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)\n\nCaused by: (_export/non_strict_utils.py:973 in __torch_function__)\nFor more information, run with TORCH_LOGS=&quot;dynamic&quot;\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n\nThe following call raised this error:\n  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update\n    if seq_len &gt; original_max_position_embeddings:\n\n\nThe error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n&#39;]



def forward(self, arg0_1: &quot;f32[32064, 3072]&quot;, arg1_1: &quot;f32[3072, 3072]&quot;, arg2_1: &quot;f32[9216, 3072]&quot;, arg3_1: &quot;f32[16384, 3072]&quot;, arg4_1: &quot;f32[3072, 8192]&quot;, arg5_1: &quot;f32[3072]&quot;, arg6_1: &quot;f32[3072]&quot;, arg7_1: &quot;f32[3072, 3072]&quot;, arg8_1: &quot;f32[9216, 3072]&quot;, arg9_1: &quot;f32[16384, 3072]&quot;, arg10_1: &quot;f32[3072, 8192]&quot;, arg11_1: &quot;f32[3072]&quot;, arg12_1: &quot;f32[3072]&quot;, arg13_1: &quot;f32[3072]&quot;, arg14_1: &quot;f32[48]&quot;, arg15_1: &quot;i64[s47, s2]&quot;, arg16_1: &quot;i64[s47, s10]&quot;, arg17_1, arg18_1: &quot;f32[s89, 32, s66, 96]&quot;, arg19_1: &quot;f32[s14, 32, s80, 96]&quot;, arg20_1: &quot;f32[s62, 32, s96, 96]&quot;, arg21_1: &quot;f32[s58, 32, s81, 96]&quot;, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(
    embedding: &quot;f32[s47, s2, 3072]&quot; = torch.ops.aten.embedding.default(arg0_1, arg15_1, 32000);  arg0_1 = embedding = None

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    sym_numel_default: &quot;Sym(3072*s66*s89)&quot; = torch.ops.aten.sym_numel.default(arg18_1)
    ne: &quot;Sym(True)&quot; = sym_numel_default != 0;  ne = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:455 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s66)&quot; = torch.ops.aten.sym_size.int(arg18_1, 2);  arg18_1 = None
    sym_size_int_1: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(arg15_1, 1)
    add: &quot;Sym(s2 + s66)&quot; = sym_size_int + sym_size_int_1

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:454 in forward, code: cache_position = torch.arange(
    arange: &quot;i64[s2]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:459 in forward, code: position_ids = cache_position.unsqueeze(0)
    unsqueeze: &quot;i64[1, s2]&quot; = torch.ops.aten.unsqueeze.default(arange, 0)

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    ne_1: &quot;Sym(True)&quot; = sym_numel_default != 0;  sym_numel_default = ne_1 = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:461 in forward, code: causal_mask = self._update_causal_mask(
    add_1: &quot;Sym(s2 + s66)&quot; = sym_size_int_1 + sym_size_int;  sym_size_int = None
    lt: &quot;Sym(s2 + s66 &lt; 262144)&quot; = add_1 &lt; 262144;  add_1 = lt = None
    sym_size_int_2: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(arg16_1, 1)
    full: &quot;f32[s2, s10]&quot; = torch.ops.aten.full.default([sym_size_int_1, sym_size_int_2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_1: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1])
    gt: &quot;b8[s2, s10]&quot; = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
    arange_2: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape_1: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
    sub: &quot;i64[s2, 1]&quot; = torch.ops.aten.sub.Tensor(reshape_1, 262144);  reshape_1 = None
    le: &quot;b8[s2, s10]&quot; = torch.ops.aten.le.Tensor(arange_2, sub);  arange_2 = sub = None
    bitwise_or_: &quot;b8[s2, s10]&quot; = torch.ops.aten.bitwise_or_.Tensor(gt, le);  gt = le = None
    mul_: &quot;f32[s2, s10]&quot; = torch.ops.aten.mul_.Tensor(full, bitwise_or_);  full = bitwise_or_ = None
    unsqueeze_1: &quot;f32[1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
    unsqueeze_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
    eq: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_1 == 9223372036854775807;  sym_size_int_1 = eq = None
    slice_1: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
    eq_1: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_1 = None
    slice_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_1, 3, 0, 9223372036854775807)
    sym_size_int_3: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg15_1, 0);  arg15_1 = None
    expand: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.expand.default(slice_2, [sym_size_int_3, 1, -1, -1])
    clone: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.clone.default(expand);  expand = None
    gt_1: &quot;Sym(False)&quot; = sym_size_int_2 &gt; sym_size_int_2;  gt_1 = None
    slice_3: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_4: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_3, 1)
    slice_5: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_4, 2);  slice_4 = None
    slice_6: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_5, 3, None, sym_size_int_2)
    sym_size_int_4: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg16_1, 0)
    eq_2: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_4 == 9223372036854775807;  sym_size_int_4 = eq_2 = None
    slice_7: &quot;i64[s47, s10]&quot; = torch.ops.aten.slice.Tensor(arg16_1, 0, 0, 9223372036854775807);  arg16_1 = None
    unsqueeze_3: &quot;i64[s47, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(slice_7, 1);  slice_7 = None
    unsqueeze_4: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
    eq_3: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_3 = None
    slice_8: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_4, 3, 0, 9223372036854775807);  unsqueeze_4 = None
    to: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.to.dtype_layout(slice_8, dtype = torch.int64, layout = torch.strided, device = device(type=&#39;cpu&#39;));  slice_8 = None
    add_2: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.add.Tensor(slice_6, to);  to = None
    eq_4: &quot;b8[s47, 1, s2, s10]&quot; = torch.ops.aten.eq.Scalar(add_2, 0);  add_2 = None
    slice_9: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_10: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_9, 1);  slice_9 = None
    slice_11: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_10, 2);  slice_10 = None
    slice_12: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_11, 3, None, sym_size_int_2);  slice_11 = None
    masked_fill: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.masked_fill.Scalar(slice_12, eq_4, -3.4028234663852886e+38);  slice_12 = eq_4 = None
    eq_5: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_3 == 9223372036854775807;  sym_size_int_3 = eq_5 = None
    slice_13: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807);  clone = None
    slice_14: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_13, 1, 0, 9223372036854775807)
    sym_size_int_5: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_1, 2);  slice_1 = None
    eq_6: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_5 == 9223372036854775807;  sym_size_int_5 = eq_6 = None
    slice_15: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_14, 2, 0, 9223372036854775807);  slice_14 = None
    sym_size_int_6: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_2, 3);  slice_2 = None
    eq_7: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_2;  sym_size_int_2 = eq_7 = None
    sym_size_int_7: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_13, 0);  slice_13 = None
    sym_size_int_8: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_3, 0);  slice_3 = None
    eq_8: &quot;Sym(True)&quot; = sym_size_int_7 == sym_size_int_8;  sym_size_int_7 = sym_size_int_8 = eq_8 = None
    sym_size_int_9: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_15, 2)
    sym_size_int_10: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_5, 2);  slice_5 = None
    eq_9: &quot;Sym(True)&quot; = sym_size_int_9 == sym_size_int_10;  sym_size_int_9 = sym_size_int_10 = eq_9 = None
    sym_size_int_11: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_6, 3);  slice_6 = None
    eq_10: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_11;  sym_size_int_6 = sym_size_int_11 = eq_10 = None
    copy_: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.copy_.default(slice_15, masked_fill);  slice_15 = masked_fill = copy_ = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:468 in forward, code: position_embeddings = self.rotary_emb(hidden_states, position_ids)
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    max_1: &quot;i64[]&quot; = torch.ops.aten.max.default(unsqueeze);  unsqueeze = None
    add_3: &quot;i64[]&quot; = torch.ops.aten.add.Tensor(max_1, 1);  max_1 = None
    gt_2: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(add_3, 4096);  add_3 = None
    ne_2: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt_2, 0);  gt_2 = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne_2);  ne_2 = item = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None




def forward(self, arg0_1: &quot;f32[32064, 3072]&quot;, arg1_1: &quot;f32[3072, 3072]&quot;, arg2_1: &quot;f32[9216, 3072]&quot;, arg3_1: &quot;f32[16384, 3072]&quot;, arg4_1: &quot;f32[3072, 8192]&quot;, arg5_1: &quot;f32[3072]&quot;, arg6_1: &quot;f32[3072]&quot;, arg7_1: &quot;f32[3072, 3072]&quot;, arg8_1: &quot;f32[9216, 3072]&quot;, arg9_1: &quot;f32[16384, 3072]&quot;, arg10_1: &quot;f32[3072, 8192]&quot;, arg11_1: &quot;f32[3072]&quot;, arg12_1: &quot;f32[3072]&quot;, arg13_1: &quot;f32[3072]&quot;, arg14_1: &quot;f32[48]&quot;, arg15_1: &quot;i64[s47, s2]&quot;, arg16_1: &quot;i64[s47, s10]&quot;, arg17_1, arg18_1: &quot;f32[s89, 32, s66, 96]&quot;, arg19_1: &quot;f32[s14, 32, s80, 96]&quot;, arg20_1: &quot;f32[s62, 32, s96, 96]&quot;, arg21_1: &quot;f32[s58, 32, s81, 96]&quot;, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1):
     # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(
    embedding: &quot;f32[s47, s2, 3072]&quot; = torch.ops.aten.embedding.default(arg0_1, arg15_1, 32000);  arg0_1 = embedding = None

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    sym_numel_default: &quot;Sym(3072*s66*s89)&quot; = torch.ops.aten.sym_numel.default(arg18_1)
    ne: &quot;Sym(True)&quot; = sym_numel_default != 0;  ne = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:455 in forward, code: past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
    sym_size_int: &quot;Sym(s66)&quot; = torch.ops.aten.sym_size.int(arg18_1, 2);  arg18_1 = None
    sym_size_int_1: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(arg15_1, 1)
    add: &quot;Sym(s2 + s66)&quot; = sym_size_int + sym_size_int_1

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:454 in forward, code: cache_position = torch.arange(
    arange: &quot;i64[s2]&quot; = torch.ops.aten.arange.start(sym_size_int, add, device = device(type=&#39;cpu&#39;), pin_memory = False);  add = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:459 in forward, code: position_ids = cache_position.unsqueeze(0)
    unsqueeze: &quot;i64[1, s2]&quot; = torch.ops.aten.unsqueeze.default(arange, 0)

     # File: ~/vv/this312/lib/python3.12/site-packages/torch/__init__.py:435 in __bool__, code: return builtins.bool(self != 0)
    ne_1: &quot;Sym(True)&quot; = sym_numel_default != 0;  sym_numel_default = ne_1 = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:461 in forward, code: causal_mask = self._update_causal_mask(
    add_1: &quot;Sym(s2 + s66)&quot; = sym_size_int_1 + sym_size_int;  sym_size_int = None
    lt: &quot;Sym(s2 + s66 &lt; 262144)&quot; = add_1 &lt; 262144;  add_1 = lt = None
    sym_size_int_2: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(arg16_1, 1)
    full: &quot;f32[s2, s10]&quot; = torch.ops.aten.full.default([sym_size_int_1, sym_size_int_2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
    arange_1: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1])
    gt: &quot;b8[s2, s10]&quot; = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
    arange_2: &quot;i64[s10]&quot; = torch.ops.aten.arange.default(sym_size_int_2, device = device(type=&#39;cpu&#39;), pin_memory = False)
    reshape_1: &quot;i64[s2, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
    sub: &quot;i64[s2, 1]&quot; = torch.ops.aten.sub.Tensor(reshape_1, 262144);  reshape_1 = None
    le: &quot;b8[s2, s10]&quot; = torch.ops.aten.le.Tensor(arange_2, sub);  arange_2 = sub = None
    bitwise_or_: &quot;b8[s2, s10]&quot; = torch.ops.aten.bitwise_or_.Tensor(gt, le);  gt = le = None
    mul_: &quot;f32[s2, s10]&quot; = torch.ops.aten.mul_.Tensor(full, bitwise_or_);  full = bitwise_or_ = None
    unsqueeze_1: &quot;f32[1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
    unsqueeze_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
    eq: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_1 == 9223372036854775807;  sym_size_int_1 = eq = None
    slice_1: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
    eq_1: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_1 = None
    slice_2: &quot;f32[1, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_1, 3, 0, 9223372036854775807)
    sym_size_int_3: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg15_1, 0);  arg15_1 = None
    expand: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.expand.default(slice_2, [sym_size_int_3, 1, -1, -1])
    clone: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.clone.default(expand);  expand = None
    gt_1: &quot;Sym(False)&quot; = sym_size_int_2 &gt; sym_size_int_2;  gt_1 = None
    slice_3: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_4: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_3, 1)
    slice_5: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_4, 2);  slice_4 = None
    slice_6: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_5, 3, None, sym_size_int_2)
    sym_size_int_4: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(arg16_1, 0)
    eq_2: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_4 == 9223372036854775807;  sym_size_int_4 = eq_2 = None
    slice_7: &quot;i64[s47, s10]&quot; = torch.ops.aten.slice.Tensor(arg16_1, 0, 0, 9223372036854775807);  arg16_1 = None
    unsqueeze_3: &quot;i64[s47, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(slice_7, 1);  slice_7 = None
    unsqueeze_4: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
    eq_3: &quot;Sym(Eq(s10, 9223372036854775807))&quot; = sym_size_int_2 == 9223372036854775807;  eq_3 = None
    slice_8: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_4, 3, 0, 9223372036854775807);  unsqueeze_4 = None
    to: &quot;i64[s47, 1, 1, s10]&quot; = torch.ops.aten.to.dtype_layout(slice_8, dtype = torch.int64, layout = torch.strided, device = device(type=&#39;cpu&#39;));  slice_8 = None
    add_2: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.add.Tensor(slice_6, to);  to = None
    eq_4: &quot;b8[s47, 1, s2, s10]&quot; = torch.ops.aten.eq.Scalar(add_2, 0);  add_2 = None
    slice_9: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone)
    slice_10: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_9, 1);  slice_9 = None
    slice_11: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_10, 2);  slice_10 = None
    slice_12: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_11, 3, None, sym_size_int_2);  slice_11 = None
    masked_fill: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.masked_fill.Scalar(slice_12, eq_4, -3.4028234663852886e+38);  slice_12 = eq_4 = None
    eq_5: &quot;Sym(Eq(s47, 9223372036854775807))&quot; = sym_size_int_3 == 9223372036854775807;  sym_size_int_3 = eq_5 = None
    slice_13: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807);  clone = None
    slice_14: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_13, 1, 0, 9223372036854775807)
    sym_size_int_5: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_1, 2);  slice_1 = None
    eq_6: &quot;Sym(Eq(s2, 9223372036854775807))&quot; = sym_size_int_5 == 9223372036854775807;  sym_size_int_5 = eq_6 = None
    slice_15: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.slice.Tensor(slice_14, 2, 0, 9223372036854775807);  slice_14 = None
    sym_size_int_6: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_2, 3);  slice_2 = None
    eq_7: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_2;  sym_size_int_2 = eq_7 = None
    sym_size_int_7: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_13, 0);  slice_13 = None
    sym_size_int_8: &quot;Sym(s47)&quot; = torch.ops.aten.sym_size.int(slice_3, 0);  slice_3 = None
    eq_8: &quot;Sym(True)&quot; = sym_size_int_7 == sym_size_int_8;  sym_size_int_7 = sym_size_int_8 = eq_8 = None
    sym_size_int_9: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_15, 2)
    sym_size_int_10: &quot;Sym(s2)&quot; = torch.ops.aten.sym_size.int(slice_5, 2);  slice_5 = None
    eq_9: &quot;Sym(True)&quot; = sym_size_int_9 == sym_size_int_10;  sym_size_int_9 = sym_size_int_10 = eq_9 = None
    sym_size_int_11: &quot;Sym(s10)&quot; = torch.ops.aten.sym_size.int(slice_6, 3);  slice_6 = None
    eq_10: &quot;Sym(True)&quot; = sym_size_int_6 == sym_size_int_11;  sym_size_int_6 = sym_size_int_11 = eq_10 = None
    copy_: &quot;f32[s47, 1, s2, s10]&quot; = torch.ops.aten.copy_.default(slice_15, masked_fill);  slice_15 = masked_fill = copy_ = None

     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:468 in forward, code: position_embeddings = self.rotary_emb(hidden_states, position_ids)
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    max_1: &quot;i64[]&quot; = torch.ops.aten.max.default(unsqueeze);  unsqueeze = None
    add_3: &quot;i64[]&quot; = torch.ops.aten.add.Tensor(max_1, 1);  max_1 = None
    gt_2: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(add_3, 4096);  add_3 = None
    ne_2: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt_2, 0);  gt_2 = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne_2);  ne_2 = item = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None

[try_export-FX] .. M:model-Phi3Model --- FAIL, step=EXPORT, reason=Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none) ---  --- Caused by: (_export/non_strict_utils.py:973 in __torch_function__) --- For more information, run with TORCH_LOGS=&quot;dynamic&quot; --- For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot; --- If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 --- For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing ---  --- For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 ---  --- The following call raised this error: ---   File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update ---     if seq_len &gt; original_max_position_embeddings: ---  ---  --- The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.[&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;~/github/experimental-experiment/experimental_experiment/torch_interpreter/piece_by_piece.py&quot;, line 1587, in _try_export_no_bypass_export\n    ep = torch.export.export(\n         ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/__init__.py&quot;, line 319, in export\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/__init__.py&quot;, line 286, in export\n    return _export(\n           ^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1159, in wrapper\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1125, in wrapper\n    ep = fn(*args, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 2172, in _export\n    ep = _export_for_training(\n         ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1159, in wrapper\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1125, in wrapper\n    ep = fn(*args, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 2033, in _export_for_training\n    export_artifact = export_func(\n                      ^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1975, in _non_strict_export\n    aten_export_artifact = _to_aten_func(  # type: ignore[operator]\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1760, in _export_to_aten_ir_make_fx\n    gm, graph_signature = transform(_make_fx_helper)(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1901, in _aot_export_non_strict\n    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1679, in _make_fx_helper\n    gm = make_fx(\n         ^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2290, in wrapped\n    return make_fx_tracer.trace(f, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2228, in trace\n    return self._trace_inner(f, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2199, in _trace_inner\n    t = dispatch_trace(\n        ^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_compile.py&quot;, line 51, in inner\n    return disable_fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&quot;, line 893, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1223, in dispatch_trace\n    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1787, in trace\n    res = super().trace(root, concrete_args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&quot;, line 893, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace\n    (self.create_arg(fn(*args)),),\n                     ^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1278, in wrapped\n    out = f(*tensors)  # type:ignore[call-arg]\n          ^^^^^^^^^^^\n&#39;, &#39;  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1583, in wrapped_fn\n    return tuple(flat_fn(*args))\n                 ^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn\n    tree_out = fn(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call\n    out = mod(*args[params_len:], **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1885, in forward\n    tree_out = mod(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/utils/generic.py&quot;, line 969, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/models/phi3/modeling_phi3.py&quot;, line 468, in forward\n    position_embeddings = self.rotary_emb(hidden_states, position_ids)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/utils/_contextlib.py&quot;, line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 86, in wrapper\n    longrope_frequency_update(self, position_ids, device=x.device)\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update\n    if seq_len &gt; original_max_position_embeddings:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1326, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1373, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_export/non_strict_utils.py&quot;, line 973, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py&quot;, line 536, in guard_bool\n    r = self.evaluate()\n        ^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py&quot;, line 510, in evaluate\n    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6857, in evaluate_sym_node\n    return self.evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6876, in evaluate_expr\n    return self._inner_evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper\n    return retlog(fn(*args, **kwargs))\n                  ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6892, in _inner_evaluate_expr\n    return self._evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7160, in _evaluate_expr\n    raise self._make_data_dependent_error(\n&#39;, &#39;torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)\n\nCaused by: (_export/non_strict_utils.py:973 in __torch_function__)\nFor more information, run with TORCH_LOGS=&quot;dynamic&quot;\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n\nThe following call raised this error:\n  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update\n    if seq_len &gt; original_max_position_embeddings:\n\n\nThe error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n&#39;]
~/github/onnx-diagnostic/onnx_diagnostic/helpers/helper.py:1285: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  float(diff.max()),
[try_export-FX] .... M:embed_tokens-Embedding --- OK:
[try_export-FX] .... M:layers[0]-Phi3DecoderLayer --- OK:
[try_export-FX] .... M:layers[1]-Phi3DecoderLayer --- OK:
[try_export-FX] .... M:norm-Phi3RMSNorm --- OK:



def forward(self, arg0_1: &quot;f32[48]&quot;, arg1_1: &quot;f32[s35, s16, 3072]&quot;, arg2_1: &quot;i64[1, s43]&quot;):
    # No stacktrace found for following nodes
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    max_1: &quot;i64[]&quot; = torch.ops.aten.max.default(arg2_1);  arg2_1 = None
    add: &quot;i64[]&quot; = torch.ops.aten.add.Tensor(max_1, 1);  max_1 = None
    gt: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(add, 4096);  add = None
    ne: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt, 0);  gt = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne);  ne = item = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None




def forward(self, arg0_1: &quot;f32[48]&quot;, arg1_1: &quot;f32[s35, s16, 3072]&quot;, arg2_1: &quot;i64[1, s43]&quot;):
    # No stacktrace found for following nodes
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    max_1: &quot;i64[]&quot; = torch.ops.aten.max.default(arg2_1);  arg2_1 = None
    add: &quot;i64[]&quot; = torch.ops.aten.add.Tensor(max_1, 1);  max_1 = None
    gt: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(add, 4096);  add = None
    ne: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt, 0);  gt = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne);  ne = item = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None

[try_export-FX] .... M:rotary_emb-Phi3RotaryEmbedding --- FAIL, step=EXPORT, reason=Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none) ---  --- Caused by: (_export/non_strict_utils.py:973 in __torch_function__) --- For more information, run with TORCH_LOGS=&quot;dynamic&quot; --- For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot; --- If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 --- For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing ---  --- For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 ---  --- The following call raised this error: ---   File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update ---     if seq_len &gt; original_max_position_embeddings: ---  ---  --- The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.[&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;~/github/experimental-experiment/experimental_experiment/torch_interpreter/piece_by_piece.py&quot;, line 1587, in _try_export_no_bypass_export\n    ep = torch.export.export(\n         ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/__init__.py&quot;, line 319, in export\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/__init__.py&quot;, line 286, in export\n    return _export(\n           ^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1159, in wrapper\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1125, in wrapper\n    ep = fn(*args, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 2172, in _export\n    ep = _export_for_training(\n         ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1159, in wrapper\n    raise e\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1125, in wrapper\n    ep = fn(*args, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 2033, in _export_for_training\n    export_artifact = export_func(\n                      ^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1975, in _non_strict_export\n    aten_export_artifact = _to_aten_func(  # type: ignore[operator]\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1760, in _export_to_aten_ir_make_fx\n    gm, graph_signature = transform(_make_fx_helper)(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1901, in _aot_export_non_strict\n    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1679, in _make_fx_helper\n    gm = make_fx(\n         ^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2290, in wrapped\n    return make_fx_tracer.trace(f, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2228, in trace\n    return self._trace_inner(f, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2199, in _trace_inner\n    t = dispatch_trace(\n        ^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_compile.py&quot;, line 51, in inner\n    return disable_fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&quot;, line 893, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1223, in dispatch_trace\n    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1787, in trace\n    res = super().trace(root, concrete_args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&quot;, line 893, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace\n    (self.create_arg(fn(*args)),),\n                     ^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1278, in wrapped\n    out = f(*tensors)  # type:ignore[call-arg]\n          ^^^^^^^^^^^\n&#39;, &#39;  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1583, in wrapped_fn\n    return tuple(flat_fn(*args))\n                 ^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn\n    tree_out = fn(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call\n    out = mod(*args[params_len:], **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/export/_trace.py&quot;, line 1885, in forward\n    tree_out = mod(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper\n    return self.call_module(mod, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1857, in call_module\n    return Tracer.call_module(self, m, forward, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module\n    ret_val = forward(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward\n    return _orig_module_call(mod, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;, line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/utils/_contextlib.py&quot;, line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 86, in wrapper\n    longrope_frequency_update(self, position_ids, device=x.device)\n&#39;, &#39;  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update\n    if seq_len &gt; original_max_position_embeddings:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1326, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1373, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/_export/non_strict_utils.py&quot;, line 973, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py&quot;, line 536, in guard_bool\n    r = self.evaluate()\n        ^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py&quot;, line 510, in evaluate\n    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6857, in evaluate_sym_node\n    return self.evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6876, in evaluate_expr\n    return self._inner_evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper\n    return retlog(fn(*args, **kwargs))\n                  ^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6892, in _inner_evaluate_expr\n    return self._evaluate_expr(\n           ^^^^^^^^^^^^^^^^^^^^\n&#39;, &#39;  File &quot;~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7160, in _evaluate_expr\n    raise self._make_data_dependent_error(\n&#39;, &#39;torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)\n\nCaused by: (_export/non_strict_utils.py:973 in __torch_function__)\nFor more information, run with TORCH_LOGS=&quot;dynamic&quot;\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n\nThe following call raised this error:\n  File &quot;~/github/transformers/src/transformers/modeling_rope_utils.py&quot;, line 50, in longrope_frequency_update\n    if seq_len &gt; original_max_position_embeddings:\n\n\nThe error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n&#39;]
[try_export-FX] .... M:rotary_emb-Phi3RotaryEmbedding --- FAIL: Could not guard on data-depend...
[try_export-FX] .. M:lm_head-Linear --- OK:
</pre></div>
</div>
<p>Lets display a report.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;success: </span><span class="si">{</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" title="experimental_experiment.torch_interpreter.piece_by_piece.StatusExportCode" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span><span class="o">.</span><span class="n">status</span></a></a></a></a><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/piece_by_piece.html#experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" title="experimental_experiment.torch_interpreter.piece_by_piece.ModelDiagnoseOutput.get_export_report" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter-piece_by_piece sphx-glr-backref-type-py-method"><span class="n">diag</span><span class="o">.</span><span class="n">get_export_report</span></a></a></a></a><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>success: 2
__main__                         Phi3ForCausalLM       FAIL -- step=EXPORT, reason=&#39;Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: n...&#39;
..model                          Phi3Model             FAIL -- step=EXPORT, reason=&#39;Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: n...&#39;
....embed_tokens                 Embedding             OK -- ExportedProgram
....layers[0]                    Phi3DecoderLayer      OK -- ExportedProgram
......self_attn                  Phi3Attention         &lt;OK-2i-0&gt;
........o_proj                   Linear                &lt;OK-2i-0&gt;
........qkv_proj                 Linear                &lt;OK-2i-0&gt;
......mlp                        Phi3MLP               &lt;OK-2i-0&gt;
........gate_up_proj             Linear                &lt;OK-2i-0&gt;
........down_proj                Linear                &lt;OK-2i-0&gt;
........activation_fn            SiLU                  &lt;OK-2i-0&gt;
......input_layernorm            Phi3RMSNorm           &lt;OK-2i-0&gt;
......post_attention_layernorm   Phi3RMSNorm           &lt;OK-2i-0&gt;
......resid_attn_dropout         Dropout               &lt;OK-2i-0&gt;
......resid_mlp_dropout          Dropout               &lt;OK-2i-0&gt;
....layers[1]                    Phi3DecoderLayer      OK -- ExportedProgram
......self_attn                  Phi3Attention         &lt;OK-2i-0&gt;
........o_proj                   Linear                &lt;OK-2i-0&gt;
........qkv_proj                 Linear                &lt;OK-2i-0&gt;
......mlp                        Phi3MLP               &lt;OK-2i-0&gt;
........gate_up_proj             Linear                &lt;OK-2i-0&gt;
........down_proj                Linear                &lt;OK-2i-0&gt;
........activation_fn            SiLU                  &lt;OK-2i-0&gt;
......input_layernorm            Phi3RMSNorm           &lt;OK-2i-0&gt;
......post_attention_layernorm   Phi3RMSNorm           &lt;OK-2i-0&gt;
......resid_attn_dropout         Dropout               &lt;OK-2i-0&gt;
......resid_mlp_dropout          Dropout               &lt;OK-2i-0&gt;
....norm                         Phi3RMSNorm           OK -- ExportedProgram
....rotary_emb                   Phi3RotaryEmbedding   FAIL -- step=EXPORT, reason=&#39;Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: n...&#39;
..lm_head                        Linear                OK -- ExportedProgram
</pre></div>
</div>
</section>
<section id="replace-the-failing-module-by-a-custom-op">
<h2>Replace the failing module by a custom op<a class="headerlink" href="#replace-the-failing-module-by-a-custom-op" title="Link to this heading"></a></h2>
<p>The main module is not exportable because one piece cannot be exported.
But maybe if we assume it works, maybe everything else is working.
So lets try to replace this class by a custom op.
This will be something for another example.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 7.551 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-recipes-plot-exporter-exporter-phi35-piece-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6aa9aa07e36ac1aa1643da0bf3e38f0d/plot_exporter_exporter_phi35_piece.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_exporter_exporter_phi35_piece.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/267efdcf585b83af5aa72b6b95817131/plot_exporter_exporter_phi35_piece.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_exporter_exporter_phi35_piece.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/1b8e64221afbb8509ca0eff85282e344/plot_exporter_exporter_phi35_piece.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_exporter_exporter_phi35_piece.zip</span></code></a></p>
</div>
</div>
<p class="rubric">Related examples</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Tries torch._export.tools.report_exportability."><img alt="" src="../_images/sphx_glr_plot_exporter_exporter_reportibility_thumb.png" />
<p><a class="reference internal" href="plot_exporter_exporter_reportibility.html#sphx-glr-auto-recipes-plot-exporter-exporter-reportibility-py"><span class="std std-ref">Export Phi-3.5-mini-instruct with report_exportability</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export Phi-3.5-mini-instruct with report_exportability</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Tries torch.export._draft_export.draft_export."><img alt="" src="../_images/sphx_glr_plot_exporter_exporter_draft_mode_thumb.png" />
<p><a class="reference internal" href="plot_exporter_exporter_draft_mode.html#sphx-glr-auto-recipes-plot-exporter-exporter-draft-mode-py"><span class="std std-ref">Export Phi-3.5-mini-instruct with draft_export</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export Phi-3.5-mini-instruct with draft_export</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Exports model Phi-2. We use a dummy model. The main difficulty is to set the dynamic shapes properly. If there is an issue, you can go to the following line: torch/fx/experimental/symbolic_shapes.py#L5965 and look for log.info(&quot;set_replacement %s = %s (%s) %s&quot;, a, tgt, msg, tgt_bound) and add before or after, something like:"><img alt="" src="../_images/sphx_glr_plot_exporter_recipes_c_phi2_thumb.png" />
<p><a class="reference internal" href="plot_exporter_recipes_c_phi2.html#sphx-glr-auto-recipes-plot-exporter-recipes-c-phi2-py"><span class="std std-ref">to_onnx and Phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">to_onnx and Phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Exports model Phi-2. We use a dummy model. The main difficulty is to set the dynamic shapes properly."><img alt="" src="../_images/sphx_glr_plot_exporter_recipes_oe_phi2_thumb.png" />
<p><a class="reference internal" href="plot_exporter_recipes_oe_phi2.html#sphx-glr-auto-recipes-plot-exporter-recipes-oe-phi2-py"><span class="std std-ref">torch.onnx.export and Phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">torch.onnx.export and Phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Dynamic shapes need to be specified to get a model able to cope with different dimensions. Input rank are expected to be the same but the dimension may change. The user has the ability to set them up or to call a function able to infer them from two sets of inputs having different values for the dynamic dimensions."><img alt="" src="../_images/sphx_glr_plot_exporter_exporter_infer_ds_thumb.png" />
<p><a class="reference internal" href="plot_exporter_exporter_infer_ds.html#sphx-glr-auto-recipes-plot-exporter-exporter-infer-ds-py"><span class="std std-ref">Infer dynamic shapes before exporting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Infer dynamic shapes before exporting</div>
</div></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="plot_exporter_exporter_draft_mode.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Export Phi-3.5-mini-instruct with draft_export</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_exporter_exporter_inputs.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Do no use Module as inputs!</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023-2024
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Export Phi-3.5-mini-instruct piece by piece</a><ul>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#dynamic-shapes">Dynamic Shapes</a></li>
<li><a class="reference internal" href="#evaluate-the-export">Evaluate the export</a></li>
<li><a class="reference internal" href="#replace-the-failing-module-by-a-custom-op">Replace the failing module by a custom op</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=1a9ffd16"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    </body>
</html>