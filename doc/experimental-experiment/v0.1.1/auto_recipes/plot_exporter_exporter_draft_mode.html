<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Export Phi-3.5-mini-instruct with report_exportability" href="plot_exporter_exporter_reportibility.html" /><link rel="prev" title="Export Phi-3.5-mini-instruct piece by piece" href="plot_exporter_exporter_phi35_piece.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Export Phi-3.5-mini-instruct with draft_export - experimental-experiment 0.1.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">experimental-experiment 0.1.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">experimental-experiment 0.1.1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../design/index.html">Design</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../design/exporter.html">Custom Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/optimizer.html">Pattern Optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/backends.html">Dynamo Backends</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorial/index.html">Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/errors.html">Unexpected Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/docker.html">Start from a docker</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorial/exported.html">Supported Model Signatures</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Supported Model Signatures</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/exported_program.html">Exported Programs with Static Shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/exported_program_dynamic.html">Exported Programs with Dynamic Shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/exported_onnx.html">Exported into ONNX with Static Shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/exported_onnx_dynamic.html">Exported into ONNX with Dynamic Shapes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of API</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/gradient/index.html">.gradient</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of .gradient</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/gradient/ops/index.html">.gradient.ops</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of .gradient.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/gradient/ops/op_broadcast_gradient_args.html">.gradient.ops.op_broadcast_gradient_args</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/gradient/grad_helper.html">.gradient.grad_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/gradient/loss_helper.html">.gradient.loss_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">.reference</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of .reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">.reference.ops</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of .reference.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_complex.html">.reference.ops.op_complex</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_average_pool.html">.reference.ops.op_qlinear_average_pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_qlinear_conv.html">.reference.ops.op_qlinear_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_skip_layer_normalization.html">.reference.ops.op_skip_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">.reference.ort_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/quantized_tensor.html">.reference.quantized_tensor</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/convert/index.html">.convert</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of .convert</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/convert/convert_helper.html">.convert.convert_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/convert/ort_helper.html">.convert.ort_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/plotting/index.html">.plotting</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of .plotting</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/plotting/data.html">.plotting.data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/plotting/memory.html">.plotting.memory</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/skl/index.html">.skl</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of .skl</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/skl/convert.html">.skl.convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/skl/helpers.html">.skl.helpers</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_interpreter/index.html">.torch_interpreter</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of .torch_interpreter</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_functions.html">.torch_interpreter._aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_functions_attention.html">.torch_interpreter._aten_functions_attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_non_aten_functions.html">.torch_interpreter._non_aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_methods.html">.torch_interpreter._aten_methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_doc_.html">.torch_interpreter._doc_</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_exceptions.html">.torch_interpreter._exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_prims_functions.html">.torch_interpreter._prims_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_torch_helper.html">.torch_interpreter._torch_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/aten_functions.html">.torch_interpreter.aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/aten_methods.html">.torch_interpreter.aten_methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/dispatcher.html">.torch_interpreter.dispatcher</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/torch_interpreter/eval/index.html">.torch_interpreter.eval</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of .torch_interpreter.eval</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/torch_interpreter/eval/model_cases.html">.torch_interpreter.eval.model_cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/export_options.html">.torch_interpreter.export_options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/interpreter.html">.torch_interpreter.interpreter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/investigate_helper.html">.torch_interpreter.investigate_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/onnx_export.html">.torch_interpreter.onnx_export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/oxs_dispatcher.html">.torch_interpreter.oxs_dispatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/oxs_opset.html">.torch_interpreter.oxs_opset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/piece_by_piece.html">.torch_interpreter.piece_by_piece</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/piece_by_piece_serialize.html">.torch_interpreter.piece_by_piece_serialize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/tracing.html">.torch_interpreter.tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">.torch_models</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of .torch_models</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/diffusion_model_helper.html">.torch_models.diffusion_model_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/dump_helper.html">.torch_models.dump_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llama_helper.html">.torch_models.llama_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llm_model_helper.html">.torch_models.llm_model_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llm_model_setup.html">.torch_models.llm_model_setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/mistral_helper.html">.torch_models.mistral_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/phi3_helper.html">.torch_models.phi3_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/phi_helper.html">.torch_models.phi_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/training_helper.html">.torch_models.training_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/xbuilder/index.html">.xbuilder</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of .xbuilder</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_graph_builder_runtime.html">.xbuilder._graph_builder_runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_onnx_helper.html">.xbuilder._onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_shape_helper.html">.xbuilder._shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/expression_dimension.html">.xbuilder.expression_dimension</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/graph_builder.html">.xbuilder.graph_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/graph_builder_opset.html">.xbuilder.graph_builder_opset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/model_container.html">.xbuilder.model_container</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/optimization_options.html">.xbuilder.optimization_options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/reverse_graph_builder.html">.xbuilder.reverse_graph_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/shape_type_compute.html">.xbuilder.shape_type_compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/type_inference.html">.xbuilder.type_inference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/xoptim/index.html">.xoptim</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of .xoptim</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_investigation/index.html">.xoptim.patterns_investigation</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_investigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_investigation/element_wise.html">.xoptim.patterns_investigation.element_wise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_investigation/llm_patterns.html">.xoptim.patterns_investigation.llm_patterns</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_ml/index.html">.xoptim.patterns_ml</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_ml</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ml/tree_ensemble.html">.xoptim.patterns_ml.tree_ensemble</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_exp/index.html">.xoptim.patterns_exp</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_exp</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/binary_operators.html">.xoptim.patterns_exp.binary_operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/constant_of_shape_scatter_nd.html">.xoptim.patterns_exp.constant_of_shape_scatter_nd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/constants.html">.xoptim.patterns_exp.constants</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/simple_rotary.html">.xoptim.patterns_exp.simple_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/unary_operators.html">.xoptim.patterns_exp.unary_operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/where_replace.html">.xoptim.patterns_exp.where_replace</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns/index.html">.xoptim.patterns</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle navigation of .xoptim.patterns</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_any.html">.xoptim.patterns.onnx_any</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_cast.html">.xoptim.patterns.onnx_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_clip.html">.xoptim.patterns.onnx_clip</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_constants.html">.xoptim.patterns.onnx_constants</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_conv.html">.xoptim.patterns.onnx_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_dropout.html">.xoptim.patterns.onnx_dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_equal.html">.xoptim.patterns.onnx_equal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_expand.html">.xoptim.patterns.onnx_expand</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_functions.html">.xoptim.patterns.onnx_functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_layer_normalization.html">.xoptim.patterns.onnx_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_matmul.html">.xoptim.patterns.onnx_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_mul.html">.xoptim.patterns.onnx_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_reduce.html">.xoptim.patterns.onnx_reduce</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_reshape.html">.xoptim.patterns.onnx_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_rotary.html">.xoptim.patterns.onnx_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_slice.html">.xoptim.patterns.onnx_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_split.html">.xoptim.patterns.onnx_split</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_sub.html">.xoptim.patterns.onnx_sub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_sequence.html">.xoptim.patterns.onnx_sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_transpose.html">.xoptim.patterns.onnx_transpose</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_unsqueeze.html">.xoptim.patterns.onnx_unsqueeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_ort/index.html">.xoptim.patterns_ort</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_ort</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/activation.html">.xoptim.patterns_ort.activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/activation_grad.html">.xoptim.patterns_ort.activation_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/attention_patterns.html">.xoptim.patterns_ort.attention_patterns</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/batch_normalization.html">.xoptim.patterns_ort.batch_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/fused_conv.html">.xoptim.patterns_ort.fused_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/fused_matmul.html">.xoptim.patterns_ort.fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/gather_grad.html">.xoptim.patterns_ort.gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/llm_optim.html">.xoptim.patterns_ort.llm_optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/simplified_layer_normalization.html">.xoptim.patterns_ort.simplified_layer_normalization</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_fix/index.html">.xoptim.patterns_fix</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><div class="visually-hidden">Toggle navigation of .xoptim.patterns_fix</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_fix/add_reduction_scatter_nd.html">.xoptim.patterns_fix.add_reduction_scatter_nd</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/graph_builder_optim.html">.xoptim.graph_builder_optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/order_optim.html">.xoptim.order_optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/patterns_api.html">.xoptim.patterns_api</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/unfused.html">.xoptim.unfused</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_dynamo/index.html">.torch_dynamo</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" role="switch" type="checkbox"/><label for="toctree-checkbox-23"><div class="visually-hidden">Toggle navigation of .torch_dynamo</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/_dynamo_exporter.html">.torch_dynamo._dynamo_exporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/backend_helper.html">.torch_dynamo.backend_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/debug_backend.html">.torch_dynamo.debug_backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/fast_backend.html">.torch_dynamo.fast_backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/partition.html">experimental_experiment.torch_dynamo.partition</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_bench/index.html">.torch_bench</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" role="switch" type="checkbox"/><label for="toctree-checkbox-24"><div class="visually-hidden">Toggle navigation of .torch_bench</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner_agg.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner_agg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner_agg_helper.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner_agg_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_cmd.html">experimental_experiment.torch_bench._bash_bench_cmd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_model_runner.html">experimental_experiment.torch_bench._bash_bench_model_runner</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_models_helper.html">experimental_experiment.torch_bench._bash_bench_models_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_dummies.html">experimental_experiment.torch_bench._bash_bench_set_dummies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_explicit.html">experimental_experiment.torch_bench._bash_bench_set_explicit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_huggingface.html">experimental_experiment.torch_bench._bash_bench_set_huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_huggingface_big.html">experimental_experiment.torch_bench._bash_bench_set_huggingface_big</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_issues.html">experimental_experiment.torch_bench._bash_bench_set_issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_timm.html">experimental_experiment.torch_bench._bash_bench_set_timm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_torchbench.html">experimental_experiment.torch_bench._bash_bench_set_torchbench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_torchbench_ado.html">experimental_experiment.torch_bench._bash_bench_set_torchbench_ado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_untrained.html">experimental_experiment.torch_bench._bash_bench_untrained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_dort_cmd_common.html">experimental_experiment.torch_bench._dort_cmd_common</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_dort_cmd_common_models.html">experimental_experiment.torch_bench._dort_cmd_common_models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_agg.html">.torch_bench.bash_bench_agg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_explicit.html">.torch_bench.bash_bench_explicit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_huggingface.html">.torch_bench.bash_bench_huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_huggingface_big.html">.torch_bench.bash_bench_huggingface_big</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_issues.html">.torch_bench.bash_bench_issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_timm.html">.torch_bench.bash_bench_timm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_torchbench.html">.torch_bench.bash_bench_torchbench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_torchbench_ado.html">.torch_bench.bash_bench_torchbench_ado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_untrained.html">.torch_bench.bash_bench_untrained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/check_model.html">.torch_bench.check_model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_bench.html">.torch_bench.dort_bench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_bench_profile.html">.torch_bench.dort_bench_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_profile.html">.torch_bench.dort_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/export_model.html">.torch_bench.export_model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/export_model_helper.html">.torch_bench.export_model_helper</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/_bench_test.html">._bench_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/_command_lines_parser.html">._command_lines_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/args.html">.args</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/bench_run.html">.bench_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/checks.html">.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">.ext_test_case</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/helpers.html">.helpers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/memory_peak.html">.memory_peak</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/mini_onnx_builder.html">.mini_onnx_builder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/model_run.html">.model_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/onnx_tools.html">.onnx_tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ort_session.html">.ort_session</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/torch_test_helper.html">.torch_test_helper</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../galleries.html">Galleries of Examples and Recipes</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" role="switch" type="checkbox"/><label for="toctree-checkbox-25"><div class="visually-hidden">Toggle navigation of Galleries of Examples and Recipes</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/index.html">Examples Gallery</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" role="switch" type="checkbox"/><label for="toctree-checkbox-26"><div class="visually-hidden">Toggle navigation of Examples Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_custom_backend_101.html">101: A custom backend for torch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_linreg_101.html">101: Linear Regression and export to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_optimize_101.html">101: Onnx Model Optimization based on Pattern Rewriting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_rewrite_101.html">101: Onnx Model Rewriting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_profile_existing_onnx_101.html">101: Profile an existing model with onnxruntime</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_101.html">101: Some dummy examples with torch.export.export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_convolutation_matmul_102.html">102: Convolution and Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_onnxscript_102.html">102: Examples with onnxscript</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_custom_backend_llama_102.html">102: Fuse kernels in a small Llama Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_bench_102.html">102: Measure LLAMA speed</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_compile_102.html">102: Tweak onnx export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_dort_201.html">201: Evaluate DORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_aot_201.html">201: Evaluate DORT Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_201.html">201: Evaluate different ways to export a torch model to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_sklearn_201.html">201: Use torch to export a scikit-learn model into ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_export_301.html">301: Compares LLAMA exporters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_dort_301.html">301: Compares LLAMA exporters for onnxrt backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_model_to_python.html">Playground for big optimization pattern</a></li>
</ul>
</li>
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">Exporter Recipes Gallery</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" role="switch" type="checkbox"/><label for="toctree-checkbox-27"><div class="visually-hidden">Toggle navigation of Exporter Recipes Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_lost_dynamic_dimension.html">A dynamic dimension lost by torch.export.export</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_untrained_tinyllm.html">Check the exporter on a dummy from HuggingFace</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_inputs.html">Do no use Module as inputs!</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_phi35_piece.html">Export Phi-3.5-mini-instruct piece by piece</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">Export Phi-3.5-mini-instruct with draft_export</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_reportibility.html">Export Phi-3.5-mini-instruct with report_exportability</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_with_dynamic_cache.html">Export a model using a custom type as input</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_scan_pdist.html">Export a model with a loop (scan)</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_infer_ds.html">Infer dynamic shapes before exporting</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_lr.html">Linear Regression and export to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_coverage.html">Measures the exporter success on many test cases</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_exporter_dynamic_shapes_auto.html">Use DYNAMIC or AUTO when dynamic shapes has constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_phi2.html">to_onnx and Phi-2</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_custom_ops_inplace.html">to_onnx and a custom operator inplace</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_custom_ops_fct.html">to_onnx and a custom operator registered with a function</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_scan_pdist.html">to_onnx and a model with a loop (scan)</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_cond.html">to_onnx and a model with a test</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_dynpad.html">to_onnx and padding one dimension to a mulitple of a constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_modules.html">to_onnx and submodules from LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_named_ds_auto.html">to_onnx: Rename Dynamic Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_phi2.html">torch.onnx.export and Phi-2</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_custom_ops_inplace.html">torch.onnx.export and a custom operator inplace</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_custom_ops_fct.html">torch.onnx.export and a custom operator registered with a function</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_cond.html">torch.onnx.export and a model with a test</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_dynpad.html">torch.onnx.export and padding one dimension to a mulitple of a constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_oe_named_ds_auto.html">torch.onnx.export: Rename Dynamic Shapes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../command_lines.html">Command Lines</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" role="switch" type="checkbox"/><label for="toctree-checkbox-28"><div class="visually-hidden">Toggle navigation of Command Lines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../bench/index.html">Benchmarks from the command line</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" role="switch" type="checkbox"/><label for="toctree-checkbox-29"><div class="visually-hidden">Toggle navigation of Benchmarks from the command line</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../bench/dort_bench.html">experimental_experiment.torch_bench.dort_bench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/dort_profile.html">experimental_experiment.torch_bench.dort_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/scripts.html">Interesting scripts or command lines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/bash_bench.html">Measuring the exporters on a short list of sets of models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tools/index.html">Tools from the command line</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" role="switch" type="checkbox"/><label for="toctree-checkbox-30"><div class="visually-hidden">Toggle navigation of Tools from the command line</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tools/lighten.html">python -m experimental_experiment lighten and unlighten</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/optimize.html">python -m experimental_experiment optimize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/print.html">python -m experimental_experiment print</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/run.html">python -m experimental_experiment run</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../miscellaneous/index.html">Miscellaneous</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" role="switch" type="checkbox"/><label for="toctree-checkbox-31"><div class="visually-hidden">Toggle navigation of Miscellaneous</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../miscellaneous/export_times.html">Export Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="../miscellaneous/long_outputs.html">Long Outputs uneasy to read</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../miscellaneous/models/index.html">Supported Models By the Custom Backend</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" role="switch" type="checkbox"/><label for="toctree-checkbox-32"><div class="visually-hidden">Toggle navigation of Supported Models By the Custom Backend</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../miscellaneous/models/phi.html">Phi</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_recipes/plot_exporter_exporter_draft_mode.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-recipes-plot-exporter-exporter-draft-mode-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="export-phi-3-5-mini-instruct-with-draft-export">
<span id="l-plot-exporter-exporter-draft-export"></span><span id="sphx-glr-auto-recipes-plot-exporter-exporter-draft-mode-py"></span><h1>Export Phi-3.5-mini-instruct with draft_export<a class="headerlink" href="#export-phi-3-5-mini-instruct-with-draft-export" title="Link to this heading">¶</a></h1>
<p>Tries <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export._draft_export.draft_export()</span></code>.</p>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <a href="https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stderr" title="contextlib.redirect_stderr" class="sphx-glr-backref-module-contextlib sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stderr" title="contextlib.redirect_stderr" class="sphx-glr-backref-module-contextlib sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stderr" title="contextlib.redirect_stderr" class="sphx-glr-backref-module-contextlib sphx-glr-backref-type-py-function"><span class="n">redirect_stderr</span></a></a></a>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">StringIO</span></a></a></a>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><span class="n">Any</span></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Dict</span></a></a></a>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.export._draft_export</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">experimental_experiment.helpers</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.helpers.cache_helper</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">make_dynamic_cache</span></a></a></a>
<span class="kn">from</span> <span class="nn">onnx_diagnostic.torch_export_patches</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">register_additional_serialization_functions</span></a></a></a>


<span class="k">def</span> <span class="nf">get_phi35_untrained</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="typing.Dict" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Dict</span></a></a></a><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><a href="https://docs.python.org/3/library/typing.html#typing.Any" title="typing.Any" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><span class="n">Any</span></a></a></a><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets a non initialized model with two sets of inputs and different shapes.</span>

<span class="sd">    :param batch_size: batch size</span>
<span class="sd">    :param kwargs: to overwrite the configuration, example ``num_hidden_layers=1``</span>
<span class="sd">    :return: dictionary</span>

<span class="sd">    See `Phi-3.5-mini-instruct/config.json</span>
<span class="sd">    &lt;https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/config.json&gt;`_.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;_name_or_path&quot;</span><span class="p">:</span> <span class="s2">&quot;Phi-3.5-mini-instruct&quot;</span><span class="p">,</span>
        <span class="s2">&quot;architectures&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Phi3ForCausalLM&quot;</span><span class="p">],</span>
        <span class="s2">&quot;attention_dropout&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;auto_map&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;AutoConfig&quot;</span><span class="p">:</span> <span class="s2">&quot;configuration_phi3.Phi3Config&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AutoModelForCausalLM&quot;</span><span class="p">:</span> <span class="s2">&quot;modeling_phi3.Phi3ForCausalLM&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;bos_token_id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;embd_pdrop&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;eos_token_id&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;hidden_act&quot;</span><span class="p">:</span> <span class="s2">&quot;silu&quot;</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">3072</span><span class="p">,</span>
        <span class="s2">&quot;initializer_range&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
        <span class="s2">&quot;intermediate_size&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;max_position_embeddings&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
        <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;phi3&quot;</span><span class="p">,</span>
        <span class="s2">&quot;num_attention_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;num_key_value_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;original_max_position_embeddings&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;pad_token_id&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;resid_pdrop&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;rms_norm_eps&quot;</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
        <span class="s2">&quot;rope_scaling&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;long_factor&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="mf">1.0800000429153442</span><span class="p">,</span>
                <span class="mf">1.1100000143051147</span><span class="p">,</span>
                <span class="mf">1.1399999856948853</span><span class="p">,</span>
                <span class="mf">1.340000033378601</span><span class="p">,</span>
                <span class="mf">1.5899999141693115</span><span class="p">,</span>
                <span class="mf">1.600000023841858</span><span class="p">,</span>
                <span class="mf">1.6200000047683716</span><span class="p">,</span>
                <span class="mf">2.620000123977661</span><span class="p">,</span>
                <span class="mf">3.2300000190734863</span><span class="p">,</span>
                <span class="mf">3.2300000190734863</span><span class="p">,</span>
                <span class="mf">4.789999961853027</span><span class="p">,</span>
                <span class="mf">7.400000095367432</span><span class="p">,</span>
                <span class="mf">7.700000286102295</span><span class="p">,</span>
                <span class="mf">9.09000015258789</span><span class="p">,</span>
                <span class="mf">12.199999809265137</span><span class="p">,</span>
                <span class="mf">17.670000076293945</span><span class="p">,</span>
                <span class="mf">24.46000099182129</span><span class="p">,</span>
                <span class="mf">28.57000160217285</span><span class="p">,</span>
                <span class="mf">30.420001983642578</span><span class="p">,</span>
                <span class="mf">30.840002059936523</span><span class="p">,</span>
                <span class="mf">32.590003967285156</span><span class="p">,</span>
                <span class="mf">32.93000411987305</span><span class="p">,</span>
                <span class="mf">42.320003509521484</span><span class="p">,</span>
                <span class="mf">44.96000289916992</span><span class="p">,</span>
                <span class="mf">50.340003967285156</span><span class="p">,</span>
                <span class="mf">50.45000457763672</span><span class="p">,</span>
                <span class="mf">57.55000305175781</span><span class="p">,</span>
                <span class="mf">57.93000411987305</span><span class="p">,</span>
                <span class="mf">58.21000289916992</span><span class="p">,</span>
                <span class="mf">60.1400032043457</span><span class="p">,</span>
                <span class="mf">62.61000442504883</span><span class="p">,</span>
                <span class="mf">62.62000274658203</span><span class="p">,</span>
                <span class="mf">62.71000289916992</span><span class="p">,</span>
                <span class="mf">63.1400032043457</span><span class="p">,</span>
                <span class="mf">63.1400032043457</span><span class="p">,</span>
                <span class="mf">63.77000427246094</span><span class="p">,</span>
                <span class="mf">63.93000411987305</span><span class="p">,</span>
                <span class="mf">63.96000289916992</span><span class="p">,</span>
                <span class="mf">63.970001220703125</span><span class="p">,</span>
                <span class="mf">64.02999877929688</span><span class="p">,</span>
                <span class="mf">64.06999969482422</span><span class="p">,</span>
                <span class="mf">64.08000183105469</span><span class="p">,</span>
                <span class="mf">64.12000274658203</span><span class="p">,</span>
                <span class="mf">64.41000366210938</span><span class="p">,</span>
                <span class="mf">64.4800033569336</span><span class="p">,</span>
                <span class="mf">64.51000213623047</span><span class="p">,</span>
                <span class="mf">64.52999877929688</span><span class="p">,</span>
                <span class="mf">64.83999633789062</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="s2">&quot;short_factor&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="mf">1.0</span><span class="p">,</span>
                <span class="mf">1.0199999809265137</span><span class="p">,</span>
                <span class="mf">1.0299999713897705</span><span class="p">,</span>
                <span class="mf">1.0299999713897705</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0499999523162842</span><span class="p">,</span>
                <span class="mf">1.0699999332427979</span><span class="p">,</span>
                <span class="mf">1.0999999046325684</span><span class="p">,</span>
                <span class="mf">1.1099998950958252</span><span class="p">,</span>
                <span class="mf">1.1599998474121094</span><span class="p">,</span>
                <span class="mf">1.1599998474121094</span><span class="p">,</span>
                <span class="mf">1.1699998378753662</span><span class="p">,</span>
                <span class="mf">1.2899998426437378</span><span class="p">,</span>
                <span class="mf">1.339999794960022</span><span class="p">,</span>
                <span class="mf">1.679999828338623</span><span class="p">,</span>
                <span class="mf">1.7899998426437378</span><span class="p">,</span>
                <span class="mf">1.8199998140335083</span><span class="p">,</span>
                <span class="mf">1.8499997854232788</span><span class="p">,</span>
                <span class="mf">1.8799997568130493</span><span class="p">,</span>
                <span class="mf">1.9099997282028198</span><span class="p">,</span>
                <span class="mf">1.9399996995925903</span><span class="p">,</span>
                <span class="mf">1.9899996519088745</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0199997425079346</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0299997329711914</span><span class="p">,</span>
                <span class="mf">2.0799996852874756</span><span class="p">,</span>
                <span class="mf">2.0899996757507324</span><span class="p">,</span>
                <span class="mf">2.189999580383301</span><span class="p">,</span>
                <span class="mf">2.2199995517730713</span><span class="p">,</span>
                <span class="mf">2.5899994373321533</span><span class="p">,</span>
                <span class="mf">2.729999542236328</span><span class="p">,</span>
                <span class="mf">2.749999523162842</span><span class="p">,</span>
                <span class="mf">2.8399994373321533</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;longrope&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;rope_theta&quot;</span><span class="p">:</span> <span class="mf">10000.0</span><span class="p">,</span>
        <span class="s2">&quot;sliding_window&quot;</span><span class="p">:</span> <span class="mi">262144</span><span class="p">,</span>
        <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;torch_dtype&quot;</span><span class="p">:</span> <span class="s2">&quot;bfloat16&quot;</span><span class="p">,</span>
        <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;attention_bias&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32064</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Phi3Config</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">transformers</span><span class="o">.</span><span class="n">Phi3ForCausalLM</span></a></a></a><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
    <a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><a href="https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">eval</span></a></a></a><span class="p">()</span>

    <span class="n">cache</span> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">make_dynamic_cache</span></a></a></a><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span> <a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">96</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">])</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">cache2</span> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/helpers/cache_helper.html#onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" title="onnx_diagnostic.helpers.cache_helper.make_dynamic_cache" class="sphx-glr-backref-module-onnx_diagnostic-helpers-cache_helper sphx-glr-backref-type-py-function"><span class="n">make_dynamic_cache</span></a></a></a><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a><span class="p">(</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span> <a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a></a></a><span class="p">(</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">96</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">])</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a></a></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">32064</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a><span class="p">),</span>
        <span class="n">attention_mask</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a></a></a><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">33</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a><span class="p">),</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
    <span class="p">)</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a></a></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">32064</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a><span class="p">),</span>
        <span class="n">attention_mask</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://docs.pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a></a></a><span class="p">((</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">35</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a><span class="p">),</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">cache2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a><span class="p">)</span>


<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a> <span class="o">=</span> <span class="n">get_phi35_untrained</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs2</span></a></a></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a></a></a><span class="p">[</span><span class="s2">&quot;inputs2&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.string_type" title="experimental_experiment.helpers.string_type" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">string_type</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a><span class="p">,</span> <span class="n">with_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>dict(input_ids:T7s2x3,attention_mask:T7s2x33,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x96,T1s2x32x30x96], value_cache=#2[T1s2x32x30x96,T1s2x32x30x96]))
</pre></div>
</div>
</section>
<section id="draft-export">
<h2>Draft Export<a class="headerlink" href="#draft-export" title="Link to this heading">¶</a></h2>
<p>The function we want to try.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">err</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">StringIO</span></a></a></a><span class="p">()</span>
<span class="k">with</span> <a href="https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stderr" title="contextlib.redirect_stderr" class="sphx-glr-backref-module-contextlib sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stderr" title="contextlib.redirect_stderr" class="sphx-glr-backref-module-contextlib sphx-glr-backref-type-py-function"><a href="https://docs.python.org/3/library/contextlib.html#contextlib.redirect_stderr" title="contextlib.redirect_stderr" class="sphx-glr-backref-module-contextlib sphx-glr-backref-type-py-function"><span class="n">redirect_stderr</span></a></a></a><span class="p">(</span><span class="n">err</span><span class="p">),</span> <a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-diagnostic/dev/api/torch_export_patches/index.html#onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" title="onnx_diagnostic.torch_export_patches.register_additional_serialization_functions" class="sphx-glr-backref-module-onnx_diagnostic-torch_export_patches sphx-glr-backref-type-py-function"><span class="n">register_additional_serialization_functions</span></a></a></a><span class="p">():</span>
    <a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">_draft_export</span><span class="o">.</span><span class="n">draft_export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">inputs</span></a></a></a><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Errors if any.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>
</pre></div>
</div>
<p>Let’s print the report.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a><span class="o">.</span><span class="n">_report</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>###################################################################################################
WARNING: 1 issue(s) found during export, and it was not able to soundly produce a graph.
Please follow the instructions to fix the errors.
###################################################################################################

1. Data dependent error.
    When exporting, we were unable to evaluate the value of `Eq(u0, 1)`.
    This was encountered 1 times.
    This occurred at the following user stacktrace:
        File ~/vv/this312/lib/python3.12/site-packages/torch/utils/_contextlib.py, lineno 116, in decorate_context
        File ~/github/transformers/src/transformers/modeling_rope_utils.py, lineno 86, in wrapper
        File ~/github/transformers/src/transformers/modeling_rope_utils.py, lineno 50, in longrope_frequency_update
            if seq_len &gt; original_max_position_embeddings:

        Locals:
            seq_len: [&#39;Tensor(shape: torch.Size([]), stride: (), storage_offset: 0)&#39;]
            original_max_position_embeddings: [None]

    And the following framework stacktrace:
        File ~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py, lineno 1326, in __torch_function__
        File ~/vv/this312/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py, lineno 1373, in __torch_function__
        File ~/vv/this312/lib/python3.12/site-packages/torch/_export/non_strict_utils.py, lineno 973, in __torch_function__
            return func(*args, **kwargs)

    As a result, it was specialized to a constant (e.g. `0` in the 1st occurrence), and asserts were inserted into the graph.

    Please add `torch._check(...)` to the original code to assert this data-dependent assumption.
    Please refer to https://docs.google.com/document/d/1kZ_BbB3JnoLbUZleDT6635dHs88ZVYId8jT-yTFgf3A/edit#heading=h.boi2xurpqa0o for more details.
</pre></div>
</div>
<p>And the exported program.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_embed_tokens_weight: &quot;f32[32064, 3072]&quot;, p_model_layers_0_self_attn_o_proj_weight: &quot;f32[3072, 3072]&quot;, p_model_layers_0_self_attn_qkv_proj_weight: &quot;f32[9216, 3072]&quot;, p_model_layers_0_mlp_gate_up_proj_weight: &quot;f32[16384, 3072]&quot;, p_model_layers_0_mlp_down_proj_weight: &quot;f32[3072, 8192]&quot;, p_model_layers_0_input_layernorm_weight: &quot;f32[3072]&quot;, p_model_layers_0_post_attention_layernorm_weight: &quot;f32[3072]&quot;, p_model_layers_1_self_attn_o_proj_weight: &quot;f32[3072, 3072]&quot;, p_model_layers_1_self_attn_qkv_proj_weight: &quot;f32[9216, 3072]&quot;, p_model_layers_1_mlp_gate_up_proj_weight: &quot;f32[16384, 3072]&quot;, p_model_layers_1_mlp_down_proj_weight: &quot;f32[3072, 8192]&quot;, p_model_layers_1_input_layernorm_weight: &quot;f32[3072]&quot;, p_model_layers_1_post_attention_layernorm_weight: &quot;f32[3072]&quot;, p_model_norm_weight: &quot;f32[3072]&quot;, p_lm_head_weight: &quot;f32[32064, 3072]&quot;, b_model_rotary_emb_inv_freq: &quot;f32[48]&quot;, c_model_rotary_emb_lifted_tensor_0: &quot;f32[48]&quot;, input_ids: &quot;i64[2, 3]&quot;, attention_mask: &quot;i64[2, 33]&quot;, past_key_values_key_cache_0: &quot;f32[2, 32, 30, 96]&quot;, past_key_values_key_cache_1: &quot;f32[2, 32, 30, 96]&quot;, past_key_values_value_cache_0: &quot;f32[2, 32, 30, 96]&quot;, past_key_values_value_cache_1: &quot;f32[2, 32, 30, 96]&quot;):
             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(
            embedding: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids, 32000);  p_model_embed_tokens_weight = input_ids = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:454 in forward, code: cache_position = torch.arange(
            arange: &quot;i64[3]&quot; = torch.ops.aten.arange.start(30, 33, device = device(type=&#39;cpu&#39;), pin_memory = False)

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:459 in forward, code: position_ids = cache_position.unsqueeze(0)
            unsqueeze: &quot;i64[1, 3]&quot; = torch.ops.aten.unsqueeze.default(arange, 0)

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:461 in forward, code: causal_mask = self._update_causal_mask(
            full: &quot;f32[3, 33]&quot; = torch.ops.aten.full.default([3, 33], -3.4028234663852886e+38, dtype = torch.float32, device = device(type=&#39;cpu&#39;), pin_memory = False)
            arange_1: &quot;i64[33]&quot; = torch.ops.aten.arange.default(33, device = device(type=&#39;cpu&#39;), pin_memory = False)
            reshape: &quot;i64[3, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1])
            gt: &quot;b8[3, 33]&quot; = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
            arange_2: &quot;i64[33]&quot; = torch.ops.aten.arange.default(33, device = device(type=&#39;cpu&#39;), pin_memory = False)
            reshape_1: &quot;i64[3, 1]&quot; = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
            sub: &quot;i64[3, 1]&quot; = torch.ops.aten.sub.Tensor(reshape_1, 262144);  reshape_1 = None
            le: &quot;b8[3, 33]&quot; = torch.ops.aten.le.Tensor(arange_2, sub);  arange_2 = sub = None
            bitwise_or_: &quot;b8[3, 33]&quot; = torch.ops.aten.bitwise_or_.Tensor(gt, le);  gt = le = None
            mul_: &quot;f32[3, 33]&quot; = torch.ops.aten.mul_.Tensor(full, bitwise_or_);  full = bitwise_or_ = None
            unsqueeze_1: &quot;f32[1, 3, 33]&quot; = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
            unsqueeze_2: &quot;f32[1, 1, 3, 33]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
            slice_1: &quot;f32[1, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
            slice_2: &quot;f32[1, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_1, 3, 0, 9223372036854775807);  slice_1 = None
            expand: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.expand.default(slice_2, [2, 1, -1, -1]);  slice_2 = None
            clone: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.clone.default(expand);  expand = None
            slice_3: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(clone)
            slice_4: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_3, 1);  slice_3 = None
            slice_5: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_4, 2);  slice_4 = None
            slice_6: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_5, 3, None, 33);  slice_5 = None
            slice_7: &quot;i64[2, 33]&quot; = torch.ops.aten.slice.Tensor(attention_mask, 0, 0, 9223372036854775807);  attention_mask = None
            unsqueeze_3: &quot;i64[2, 1, 33]&quot; = torch.ops.aten.unsqueeze.default(slice_7, 1);  slice_7 = None
            unsqueeze_4: &quot;i64[2, 1, 1, 33]&quot; = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
            slice_8: &quot;i64[2, 1, 1, 33]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_4, 3, 0, 9223372036854775807);  unsqueeze_4 = None
            _assert_tensor_metadata_default = torch.ops.aten._assert_tensor_metadata.default(slice_8, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default = None
            to: &quot;i64[2, 1, 1, 33]&quot; = torch.ops.aten.to.dtype_layout(slice_8, dtype = torch.int64, layout = torch.strided, device = device(type=&#39;cpu&#39;));  slice_8 = None
            add: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.add.Tensor(slice_6, to);  slice_6 = to = None
            eq: &quot;b8[2, 1, 3, 33]&quot; = torch.ops.aten.eq.Scalar(add, 0);  add = None
            slice_9: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(clone)
            slice_10: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_9, 1);  slice_9 = None
            slice_11: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_10, 2);  slice_10 = None
            slice_12: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_11, 3, None, 33);  slice_11 = None
            masked_fill: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.masked_fill.Scalar(slice_12, eq, -3.4028234663852886e+38);  slice_12 = eq = None
            slice_13: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
            slice_14: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_13, 1, 0, 9223372036854775807);  slice_13 = None
            slice_15: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_14, 2, 0, 9223372036854775807);  slice_14 = None
            copy_: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.copy_.default(slice_15, masked_fill);  slice_15 = masked_fill = copy_ = None

            # No stacktrace found for following nodes
            submod_3 = self.submod_1
            wrap_with_set_grad_enabled = torch.ops.higher_order.wrap_with_set_grad_enabled(False, submod_3, unsqueeze, c_model_rotary_emb_lifted_tensor_0);  submod_3 = unsqueeze = c_model_rotary_emb_lifted_tensor_0 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:385 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
            to_7: &quot;f32[1, 3, 96]&quot; = wrap_with_set_grad_enabled[0]
            to_8: &quot;f32[1, 3, 96]&quot; = wrap_with_set_grad_enabled[1];  wrap_with_set_grad_enabled = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:239 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_8 = torch.ops.aten._assert_tensor_metadata.default(embedding, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_8 = None
            to_9: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:240 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_9, 2)
            mean: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:241 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_2: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
            mul_2: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(to_9, rsqrt);  rsqrt = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:242 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_9 = torch.ops.aten._assert_tensor_metadata.default(mul_2, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_9 = None
            to_10: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(mul_2, torch.float32);  mul_2 = None
            mul_3: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_10);  p_model_layers_0_input_layernorm_weight = to_10 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[2, 3, 9216]&quot; = torch.ops.aten.linear.default(mul_3, p_model_layers_0_self_attn_qkv_proj_weight);  mul_3 = p_model_layers_0_self_attn_qkv_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:184 in forward, code: query_states = qkv[..., :query_pos]
            slice_19: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(linear, 2, 0, 3072)

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:185 in forward, code: key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]
            slice_20: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(linear, 2, 3072, 6144)

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:186 in forward, code: value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]
            slice_21: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(linear, 2, 6144, 9223372036854775807);  linear = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:188 in forward, code: query_states = query_states.view(hidden_shape).transpose(1, 2)
            view: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.view.default(slice_19, [2, 3, -1, 96]);  slice_19 = None
            transpose_1: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.transpose.int(view, 1, 2);  view = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:189 in forward, code: key_states = key_states.view(hidden_shape).transpose(1, 2)
            view_1: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.view.default(slice_20, [2, 3, -1, 96]);  slice_20 = None
            transpose_2: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:190 in forward, code: value_states = value_states.view(hidden_shape).transpose(1, 2)
            view_2: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.view.default(slice_21, [2, 3, -1, 96]);  slice_21 = None
            transpose_3: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:193 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_8: &quot;f32[1, 1, 3, 96]&quot; = torch.ops.aten.unsqueeze.default(to_7, 1)
            unsqueeze_9: &quot;f32[1, 1, 3, 96]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1)
            alias: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.alias.default(transpose_1)
            slice_22: &quot;f32[2, 32, 3, 0]&quot; = torch.ops.aten.slice.Tensor(transpose_1, 3, 96, 9223372036854775807);  transpose_1 = None
            alias_1: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.alias.default(transpose_2)
            slice_23: &quot;f32[2, 32, 3, 0]&quot; = torch.ops.aten.slice.Tensor(transpose_2, 3, 96, 9223372036854775807);  transpose_2 = None
            mul_4: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(alias, unsqueeze_8)
            slice_24: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias, 3, 0, 48)
            slice_25: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias, 3, 48, 9223372036854775807);  alias = None
            neg: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.neg.default(slice_25);  slice_25 = None
            cat_1: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([neg, slice_24], -1);  neg = slice_24 = None
            mul_5: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_9);  cat_1 = None
            add_3: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            cat_2: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([add_3, slice_22], -1);  add_3 = slice_22 = None
            mul_6: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(alias_1, unsqueeze_8);  unsqueeze_8 = None
            slice_26: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias_1, 3, 0, 48)
            slice_27: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias_1, 3, 48, 9223372036854775807);  alias_1 = None
            neg_1: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.neg.default(slice_27);  slice_27 = None
            cat_3: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([neg_1, slice_26], -1);  neg_1 = slice_26 = None
            mul_7: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(cat_3, unsqueeze_9);  cat_3 = unsqueeze_9 = None
            add_4: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None
            cat_4: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([add_4, slice_23], -1);  add_4 = slice_23 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:198 in forward, code: key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_5: &quot;f32[2, 32, 33, 96]&quot; = torch.ops.aten.cat.default([past_key_values_key_cache_0, cat_4], -2);  past_key_values_key_cache_0 = cat_4 = None
            cat_6: &quot;f32[2, 32, 33, 96]&quot; = torch.ops.aten.cat.default([past_key_values_value_cache_0, transpose_3], -2);  past_key_values_value_cache_0 = transpose_3 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:210 in forward, code: attn_output, attn_weights = attention_interface(
            slice_28: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(clone)
            slice_29: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_28, 1);  slice_28 = None
            slice_30: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_29, 2);  slice_29 = None
            slice_31: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_30, 3, None, 33);  slice_30 = None
            scaled_dot_product_attention: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.scaled_dot_product_attention.default(cat_2, cat_5, cat_6, slice_31, scale = 0.10206207261596575);  cat_2 = slice_31 = None
            transpose_4: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None
            contiguous: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.contiguous.default(transpose_4);  transpose_4 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:222 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.reshape.default(contiguous, [2, 3, -1]);  contiguous = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_1: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.linear.default(reshape_2, p_model_layers_0_self_attn_o_proj_weight);  reshape_2 = p_model_layers_0_self_attn_o_proj_weight = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            dropout: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.dropout.default(linear_1, 0.0, False);  linear_1 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:310 in forward, code: hidden_states = residual + self.resid_attn_dropout(hidden_states)  # main diff with Llama
            add_5: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.add.Tensor(to_9, dropout);  to_9 = dropout = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:239 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_10 = torch.ops.aten._assert_tensor_metadata.default(add_5, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_10 = None
            to_11: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(add_5, torch.float32);  add_5 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:240 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_11, 2)
            mean_1: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:241 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_6: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
            mul_8: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(to_11, rsqrt_1);  rsqrt_1 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:242 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_11 = torch.ops.aten._assert_tensor_metadata.default(mul_8, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_11 = None
            to_12: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(mul_8, torch.float32);  mul_8 = None
            mul_9: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_12);  p_model_layers_0_post_attention_layernorm_weight = to_12 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_2: &quot;f32[2, 3, 16384]&quot; = torch.ops.aten.linear.default(mul_9, p_model_layers_0_mlp_gate_up_proj_weight);  mul_9 = p_model_layers_0_mlp_gate_up_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:69 in forward, code: gate, up_states = up_states.chunk(2, dim=-1)
            chunk = torch.ops.aten.chunk.default(linear_2, 2, -1);  linear_2 = None
            getitem_10: &quot;f32[2, 3, 8192]&quot; = chunk[0]
            getitem_11: &quot;f32[2, 3, 8192]&quot; = chunk[1];  chunk = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/activation.py:434 in forward, code: return F.silu(input, inplace=self.inplace)
            silu: &quot;f32[2, 3, 8192]&quot; = torch.ops.aten.silu.default(getitem_10);  getitem_10 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:70 in forward, code: up_states = up_states * self.activation_fn(gate)
            mul_10: &quot;f32[2, 3, 8192]&quot; = torch.ops.aten.mul.Tensor(getitem_11, silu);  getitem_11 = silu = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_3: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.linear.default(mul_10, p_model_layers_0_mlp_down_proj_weight);  mul_10 = p_model_layers_0_mlp_down_proj_weight = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            dropout_1: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.dropout.default(linear_3, 0.0, False);  linear_3 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:315 in forward, code: hidden_states = residual + self.resid_mlp_dropout(hidden_states)  # main diff with Llama
            add_7: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.add.Tensor(to_11, dropout_1);  to_11 = dropout_1 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:239 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_12 = torch.ops.aten._assert_tensor_metadata.default(add_7, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_12 = None
            to_13: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(add_7, torch.float32);  add_7 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:240 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_13, 2)
            mean_2: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:241 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_8: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
            mul_11: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(to_13, rsqrt_2);  rsqrt_2 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:242 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_13 = torch.ops.aten._assert_tensor_metadata.default(mul_11, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_13 = None
            to_14: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(mul_11, torch.float32);  mul_11 = None
            mul_12: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_1_input_layernorm_weight, to_14);  p_model_layers_1_input_layernorm_weight = to_14 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_4: &quot;f32[2, 3, 9216]&quot; = torch.ops.aten.linear.default(mul_12, p_model_layers_1_self_attn_qkv_proj_weight);  mul_12 = p_model_layers_1_self_attn_qkv_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:184 in forward, code: query_states = qkv[..., :query_pos]
            slice_32: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(linear_4, 2, 0, 3072)

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:185 in forward, code: key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]
            slice_33: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(linear_4, 2, 3072, 6144)

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:186 in forward, code: value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]
            slice_34: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(linear_4, 2, 6144, 9223372036854775807);  linear_4 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:188 in forward, code: query_states = query_states.view(hidden_shape).transpose(1, 2)
            view_3: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.view.default(slice_32, [2, 3, -1, 96]);  slice_32 = None
            transpose_5: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:189 in forward, code: key_states = key_states.view(hidden_shape).transpose(1, 2)
            view_4: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.view.default(slice_33, [2, 3, -1, 96]);  slice_33 = None
            transpose_6: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:190 in forward, code: value_states = value_states.view(hidden_shape).transpose(1, 2)
            view_5: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.view.default(slice_34, [2, 3, -1, 96]);  slice_34 = None
            transpose_7: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:193 in forward, code: query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
            unsqueeze_10: &quot;f32[1, 1, 3, 96]&quot; = torch.ops.aten.unsqueeze.default(to_7, 1);  to_7 = None
            unsqueeze_11: &quot;f32[1, 1, 3, 96]&quot; = torch.ops.aten.unsqueeze.default(to_8, 1);  to_8 = None
            alias_2: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.alias.default(transpose_5)
            slice_35: &quot;f32[2, 32, 3, 0]&quot; = torch.ops.aten.slice.Tensor(transpose_5, 3, 96, 9223372036854775807);  transpose_5 = None
            alias_3: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.alias.default(transpose_6)
            slice_36: &quot;f32[2, 32, 3, 0]&quot; = torch.ops.aten.slice.Tensor(transpose_6, 3, 96, 9223372036854775807);  transpose_6 = None
            mul_13: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(alias_2, unsqueeze_10)
            slice_37: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias_2, 3, 0, 48)
            slice_38: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias_2, 3, 48, 9223372036854775807);  alias_2 = None
            neg_2: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.neg.default(slice_38);  slice_38 = None
            cat_7: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([neg_2, slice_37], -1);  neg_2 = slice_37 = None
            mul_14: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(cat_7, unsqueeze_11);  cat_7 = None
            add_9: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.add.Tensor(mul_13, mul_14);  mul_13 = mul_14 = None
            cat_8: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([add_9, slice_35], -1);  add_9 = slice_35 = None
            mul_15: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(alias_3, unsqueeze_10);  unsqueeze_10 = None
            slice_39: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias_3, 3, 0, 48)
            slice_40: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.slice.Tensor(alias_3, 3, 48, 9223372036854775807);  alias_3 = None
            neg_3: &quot;f32[2, 32, 3, 48]&quot; = torch.ops.aten.neg.default(slice_40);  slice_40 = None
            cat_9: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([neg_3, slice_39], -1);  neg_3 = slice_39 = None
            mul_16: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.mul.Tensor(cat_9, unsqueeze_11);  cat_9 = unsqueeze_11 = None
            add_10: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.add.Tensor(mul_15, mul_16);  mul_15 = mul_16 = None
            cat_10: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.cat.default([add_10, slice_36], -1);  add_10 = slice_36 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:198 in forward, code: key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
            cat_11: &quot;f32[2, 32, 33, 96]&quot; = torch.ops.aten.cat.default([past_key_values_key_cache_1, cat_10], -2);  past_key_values_key_cache_1 = cat_10 = None
            cat_12: &quot;f32[2, 32, 33, 96]&quot; = torch.ops.aten.cat.default([past_key_values_value_cache_1, transpose_7], -2);  past_key_values_value_cache_1 = transpose_7 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:210 in forward, code: attn_output, attn_weights = attention_interface(
            slice_41: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(clone);  clone = None
            slice_42: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_41, 1);  slice_41 = None
            slice_43: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_42, 2);  slice_42 = None
            slice_44: &quot;f32[2, 1, 3, 33]&quot; = torch.ops.aten.slice.Tensor(slice_43, 3, None, 33);  slice_43 = None
            scaled_dot_product_attention_1: &quot;f32[2, 32, 3, 96]&quot; = torch.ops.aten.scaled_dot_product_attention.default(cat_8, cat_11, cat_12, slice_44, scale = 0.10206207261596575);  cat_8 = slice_44 = None
            transpose_8: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None
            contiguous_1: &quot;f32[2, 3, 32, 96]&quot; = torch.ops.aten.contiguous.default(transpose_8);  transpose_8 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:222 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_3: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.reshape.default(contiguous_1, [2, 3, -1]);  contiguous_1 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_5: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.linear.default(reshape_3, p_model_layers_1_self_attn_o_proj_weight);  reshape_3 = p_model_layers_1_self_attn_o_proj_weight = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            dropout_2: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:310 in forward, code: hidden_states = residual + self.resid_attn_dropout(hidden_states)  # main diff with Llama
            add_11: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.add.Tensor(to_13, dropout_2);  to_13 = dropout_2 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:239 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_14 = torch.ops.aten._assert_tensor_metadata.default(add_11, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_14 = None
            to_15: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(add_11, torch.float32);  add_11 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:240 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_4: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_15, 2)
            mean_3: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:241 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_12: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.add.Tensor(mean_3, 1e-05);  mean_3 = None
            rsqrt_3: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
            mul_17: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(to_15, rsqrt_3);  rsqrt_3 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:242 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_15 = torch.ops.aten._assert_tensor_metadata.default(mul_17, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_15 = None
            to_16: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(mul_17, torch.float32);  mul_17 = None
            mul_18: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(p_model_layers_1_post_attention_layernorm_weight, to_16);  p_model_layers_1_post_attention_layernorm_weight = to_16 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_6: &quot;f32[2, 3, 16384]&quot; = torch.ops.aten.linear.default(mul_18, p_model_layers_1_mlp_gate_up_proj_weight);  mul_18 = p_model_layers_1_mlp_gate_up_proj_weight = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:69 in forward, code: gate, up_states = up_states.chunk(2, dim=-1)
            chunk_1 = torch.ops.aten.chunk.default(linear_6, 2, -1);  linear_6 = None
            getitem_12: &quot;f32[2, 3, 8192]&quot; = chunk_1[0]
            getitem_13: &quot;f32[2, 3, 8192]&quot; = chunk_1[1];  chunk_1 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/activation.py:434 in forward, code: return F.silu(input, inplace=self.inplace)
            silu_1: &quot;f32[2, 3, 8192]&quot; = torch.ops.aten.silu.default(getitem_12);  getitem_12 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:70 in forward, code: up_states = up_states * self.activation_fn(gate)
            mul_19: &quot;f32[2, 3, 8192]&quot; = torch.ops.aten.mul.Tensor(getitem_13, silu_1);  getitem_13 = silu_1 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_7: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.linear.default(mul_19, p_model_layers_1_mlp_down_proj_weight);  mul_19 = p_model_layers_1_mlp_down_proj_weight = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            dropout_3: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.dropout.default(linear_7, 0.0, False);  linear_7 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:315 in forward, code: hidden_states = residual + self.resid_mlp_dropout(hidden_states)  # main diff with Llama
            add_13: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.add.Tensor(to_15, dropout_3);  to_15 = dropout_3 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:239 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _assert_tensor_metadata_default_16 = torch.ops.aten._assert_tensor_metadata.default(add_13, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_16 = None
            to_17: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(add_13, torch.float32);  add_13 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:240 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_5: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.pow.Tensor_Scalar(to_17, 2)
            mean_4: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.mean.dim(pow_5, [-1], True);  pow_5 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:241 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_14: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.add.Tensor(mean_4, 1e-05);  mean_4 = None
            rsqrt_4: &quot;f32[2, 3, 1]&quot; = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
            mul_20: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(to_17, rsqrt_4);  to_17 = rsqrt_4 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:242 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _assert_tensor_metadata_default_17 = torch.ops.aten._assert_tensor_metadata.default(mul_20, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_17 = None
            to_18: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.to.dtype(mul_20, torch.float32);  mul_20 = None
            mul_21: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_18);  p_model_norm_weight = to_18 = None

             # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:760 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
            slice_45: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(mul_21);  mul_21 = None
            slice_46: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(slice_45, 1, 0);  slice_45 = None
            slice_47: &quot;f32[2, 3, 3072]&quot; = torch.ops.aten.slice.Tensor(slice_46, 2);  slice_46 = None

             # File: ~/vv/this312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear_8: &quot;f32[2, 3, 32064]&quot; = torch.ops.aten.linear.default(slice_47, p_lm_head_weight);  slice_47 = p_lm_head_weight = None
            return (linear_8, cat_5, cat_11, cat_6, cat_12)

        class submod_1(torch.nn.Module):
            def forward(self, unsqueeze: &quot;i64[1, 3]&quot;, c_model_rotary_emb_lifted_tensor_0: &quot;f32[48]&quot;):
                 # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:468 in forward, code: position_embeddings = self.rotary_emb(hidden_states, position_ids)
                max_1: &quot;i64[]&quot; = torch.ops.aten.max.default(unsqueeze)
                add_1: &quot;i64[]&quot; = torch.ops.aten.add.Tensor(max_1, 1);  max_1 = None
                gt_1: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(add_1, 4096);  add_1 = None
                ne: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt_1, 0);  gt_1 = None
                item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne);  ne = item = None
                to_1: &quot;f32[48]&quot; = torch.ops.aten.to.dtype_layout(c_model_rotary_emb_lifted_tensor_0, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  c_model_rotary_emb_lifted_tensor_0 = None

                 # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:375 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
                unsqueeze_5: &quot;f32[1, 48]&quot; = torch.ops.aten.unsqueeze.default(to_1, 0);  to_1 = None
                slice_16: &quot;f32[1, 48]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_5, 1, 0, 9223372036854775807);  unsqueeze_5 = None
                unsqueeze_6: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.unsqueeze.default(slice_16, 2);  slice_16 = None
                _assert_tensor_metadata_default_1 = torch.ops.aten._assert_tensor_metadata.default(unsqueeze_6, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_1 = None
                to_2: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.to.dtype(unsqueeze_6, torch.float32);  unsqueeze_6 = None
                expand_1: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.expand.default(to_2, [1, -1, 1]);  to_2 = None
                _assert_tensor_metadata_default_2 = torch.ops.aten._assert_tensor_metadata.default(expand_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_2 = None
                to_3: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.to.dtype_layout(expand_1, dtype = torch.float32, layout = torch.strided, device = device(type=&#39;cpu&#39;));  expand_1 = None

                 # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:376 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
                slice_17: &quot;i64[1, 3]&quot; = torch.ops.aten.slice.Tensor(unsqueeze, 0, 0, 9223372036854775807);  unsqueeze = None
                unsqueeze_7: &quot;i64[1, 1, 3]&quot; = torch.ops.aten.unsqueeze.default(slice_17, 1);  slice_17 = None
                slice_18: &quot;i64[1, 1, 3]&quot; = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
                _assert_tensor_metadata_default_3 = torch.ops.aten._assert_tensor_metadata.default(slice_18, dtype = torch.int64, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_3 = None
                to_4: &quot;f32[1, 1, 3]&quot; = torch.ops.aten.to.dtype(slice_18, torch.float32);  slice_18 = None

                # No stacktrace found for following nodes
                submod_3 = self.submod_1
                wrap_with_autocast = torch.ops.higher_order.wrap_with_autocast(&#39;cpu&#39;, torch.bfloat16, False, False, submod_3, to_3, to_4);  submod_3 = to_3 = to_4 = None

                 # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:382 in forward, code: cos = emb.cos() * self.attention_scaling
                mul: &quot;f32[1, 3, 96]&quot; = wrap_with_autocast[0]

                 # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:383 in forward, code: sin = emb.sin() * self.attention_scaling
                mul_1: &quot;f32[1, 3, 96]&quot; = wrap_with_autocast[1];  wrap_with_autocast = None

                 # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:385 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
                _assert_tensor_metadata_default_6 = torch.ops.aten._assert_tensor_metadata.default(mul, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_6 = None
                to_7: &quot;f32[1, 3, 96]&quot; = torch.ops.aten.to.dtype(mul, torch.float32);  mul = None
                _assert_tensor_metadata_default_7 = torch.ops.aten._assert_tensor_metadata.default(mul_1, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_7 = None
                to_8: &quot;f32[1, 3, 96]&quot; = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
                return (to_7, to_8)

            class submod_1(torch.nn.Module):
                def forward(self, to_3: &quot;f32[1, 48, 1]&quot;, to_4: &quot;f32[1, 1, 3]&quot;):
                     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:380 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
                    _assert_tensor_metadata_default_4 = torch.ops.aten._assert_tensor_metadata.default(to_3, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_4 = None
                    to_5: &quot;f32[1, 48, 1]&quot; = torch.ops.aten.to.dtype(to_3, torch.float32);  to_3 = None
                    _assert_tensor_metadata_default_5 = torch.ops.aten._assert_tensor_metadata.default(to_4, dtype = torch.float32, device = device(type=&#39;cpu&#39;), layout = torch.strided);  _assert_tensor_metadata_default_5 = None
                    to_6: &quot;f32[1, 1, 3]&quot; = torch.ops.aten.to.dtype(to_4, torch.float32);  to_4 = None
                    matmul: &quot;f32[1, 48, 3]&quot; = torch.ops.aten.matmul.default(to_5, to_6);  to_5 = to_6 = None
                    transpose: &quot;f32[1, 3, 48]&quot; = torch.ops.aten.transpose.int(matmul, 1, 2);  matmul = None

                     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:381 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
                    cat: &quot;f32[1, 3, 96]&quot; = torch.ops.aten.cat.default([transpose, transpose], -1);  transpose = None

                     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:382 in forward, code: cos = emb.cos() * self.attention_scaling
                    cos: &quot;f32[1, 3, 96]&quot; = torch.ops.aten.cos.default(cat)
                    mul: &quot;f32[1, 3, 96]&quot; = torch.ops.aten.mul.Tensor(cos, 1.1902380714238083);  cos = None

                     # File: ~/github/transformers/src/transformers/models/phi3/modeling_phi3.py:383 in forward, code: sin = emb.sin() * self.attention_scaling
                    sin: &quot;f32[1, 3, 96]&quot; = torch.ops.aten.sin.default(cat);  cat = None
                    mul_1: &quot;f32[1, 3, 96]&quot; = torch.ops.aten.mul.Tensor(sin, 1.1902380714238083);  sin = None
                    return (mul, mul_1)

Graph signature:
    # inputs
    p_model_embed_tokens_weight: PARAMETER target=&#39;model.embed_tokens.weight&#39;
    p_model_layers_0_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.o_proj.weight&#39;
    p_model_layers_0_self_attn_qkv_proj_weight: PARAMETER target=&#39;model.layers.0.self_attn.qkv_proj.weight&#39;
    p_model_layers_0_mlp_gate_up_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.gate_up_proj.weight&#39;
    p_model_layers_0_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.0.mlp.down_proj.weight&#39;
    p_model_layers_0_input_layernorm_weight: PARAMETER target=&#39;model.layers.0.input_layernorm.weight&#39;
    p_model_layers_0_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.0.post_attention_layernorm.weight&#39;
    p_model_layers_1_self_attn_o_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.o_proj.weight&#39;
    p_model_layers_1_self_attn_qkv_proj_weight: PARAMETER target=&#39;model.layers.1.self_attn.qkv_proj.weight&#39;
    p_model_layers_1_mlp_gate_up_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.gate_up_proj.weight&#39;
    p_model_layers_1_mlp_down_proj_weight: PARAMETER target=&#39;model.layers.1.mlp.down_proj.weight&#39;
    p_model_layers_1_input_layernorm_weight: PARAMETER target=&#39;model.layers.1.input_layernorm.weight&#39;
    p_model_layers_1_post_attention_layernorm_weight: PARAMETER target=&#39;model.layers.1.post_attention_layernorm.weight&#39;
    p_model_norm_weight: PARAMETER target=&#39;model.norm.weight&#39;
    p_lm_head_weight: PARAMETER target=&#39;lm_head.weight&#39;
    b_model_rotary_emb_inv_freq: BUFFER target=&#39;model.rotary_emb.inv_freq&#39; persistent=False
    c_model_rotary_emb_lifted_tensor_0: CONSTANT_TENSOR target=&#39;model.rotary_emb.lifted_tensor_0&#39;
    input_ids: USER_INPUT
    attention_mask: USER_INPUT
    past_key_values_key_cache_0: USER_INPUT
    past_key_values_key_cache_1: USER_INPUT
    past_key_values_value_cache_0: USER_INPUT
    past_key_values_value_cache_1: USER_INPUT

    # outputs
    linear_8: USER_OUTPUT
    cat_5: USER_OUTPUT
    cat_11: USER_OUTPUT
    cat_6: USER_OUTPUT
    cat_12: USER_OUTPUT

Range constraints: {u0: VR[0, 1]}
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 15.820 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-recipes-plot-exporter-exporter-draft-mode-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/fac2194c537b3717f092996e7e7733be/plot_exporter_exporter_draft_mode.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_exporter_exporter_draft_mode.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/aac26f61eede1fe98c5fe635f05a0493/plot_exporter_exporter_draft_mode.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_exporter_exporter_draft_mode.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/88f32d571ba83bdfcaf75dd65bede13a/plot_exporter_exporter_draft_mode.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_exporter_exporter_draft_mode.zip</span></code></a></p>
</div>
</div>
<p class="rubric">Related examples</p>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Tries torch._export.tools.report_exportability."><img alt="" src="../_images/sphx_glr_plot_exporter_exporter_reportibility_thumb.png" />
<p><a class="reference internal" href="plot_exporter_exporter_reportibility.html#sphx-glr-auto-recipes-plot-exporter-exporter-reportibility-py"><span class="std std-ref">Export Phi-3.5-mini-instruct with report_exportability</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export Phi-3.5-mini-instruct with report_exportability</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="torch.export.export often breaks on big models because there are control flows or instructions breaking the propagation of dynamic shapes (see ...). The function usually gives an indication where the model implementation can be fixed but in case, that is not possible, we can try to export the model piece by piece: every module is converted separately from its submodule. A model can be exported even if one of its submodules cannot."><img alt="" src="../_images/sphx_glr_plot_exporter_exporter_phi35_piece_thumb.png" />
<p><a class="reference internal" href="plot_exporter_exporter_phi35_piece.html#sphx-glr-auto-recipes-plot-exporter-exporter-phi35-piece-py"><span class="std std-ref">Export Phi-3.5-mini-instruct piece by piece</span></a></p>
  <div class="sphx-glr-thumbnail-title">Export Phi-3.5-mini-instruct piece by piece</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Exports model Phi-2. We use a dummy model. The main difficulty is to set the dynamic shapes properly."><img alt="" src="../_images/sphx_glr_plot_exporter_recipes_oe_phi2_thumb.png" />
<p><a class="reference internal" href="plot_exporter_recipes_oe_phi2.html#sphx-glr-auto-recipes-plot-exporter-recipes-oe-phi2-py"><span class="std std-ref">torch.onnx.export and Phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">torch.onnx.export and Phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Exports model Phi-2. We use a dummy model. The main difficulty is to set the dynamic shapes properly. If there is an issue, you can go to the following line: torch/fx/experimental/symbolic_shapes.py#L5965 and look for log.info(&quot;set_replacement %s = %s (%s) %s&quot;, a, tgt, msg, tgt_bound) and add before or after, something like:"><img alt="" src="../_images/sphx_glr_plot_exporter_recipes_c_phi2_thumb.png" />
<p><a class="reference internal" href="plot_exporter_recipes_c_phi2.html#sphx-glr-auto-recipes-plot-exporter-recipes-c-phi2-py"><span class="std std-ref">to_onnx and Phi-2</span></a></p>
  <div class="sphx-glr-thumbnail-title">to_onnx and Phi-2</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Every conversion task must be tested on a large scale. One huge source of model is HuggingFace. We focus on the model Tiny-LLM. To avoid downloading any weigths, we write a function creating a random model based on the same architecture."><img alt="" src="../_images/sphx_glr_plot_exporter_exporter_untrained_tinyllm_thumb.png" />
<p><a class="reference internal" href="plot_exporter_exporter_untrained_tinyllm.html#sphx-glr-auto-recipes-plot-exporter-exporter-untrained-tinyllm-py"><span class="std std-ref">Check the exporter on a dummy from HuggingFace</span></a></p>
  <div class="sphx-glr-thumbnail-title">Check the exporter on a dummy from HuggingFace</div>
</div></div><p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="plot_exporter_exporter_reportibility.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Export Phi-3.5-mini-instruct with report_exportability</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_exporter_exporter_phi35_piece.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Export Phi-3.5-mini-instruct piece by piece</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023-2024
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Export Phi-3.5-mini-instruct with draft_export</a><ul>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#draft-export">Draft Export</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=1a9ffd16"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    </body>
</html>