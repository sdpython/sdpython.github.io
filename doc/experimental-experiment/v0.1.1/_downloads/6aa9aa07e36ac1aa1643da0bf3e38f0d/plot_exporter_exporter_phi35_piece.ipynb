{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export Phi-3.5-mini-instruct piece by piece\n\n:func:`torch.export.export` often breaks on big models because there\nare control flows or instructions breaking the propagation of\ndynamic shapes (see ...). The function usually gives an indication where\nthe model implementation can be fixed but in case, that is not possible,\nwe can try to export the model piece by piece: every module\nis converted separately from its submodule. A model can be exported even\nif one of its submodules cannot.\n\n## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pprint\nfrom typing import Any, Dict\nimport torch\nimport torch._export.tools\nimport transformers\nfrom onnx_diagnostic.helpers.cache_helper import make_dynamic_cache\nfrom experimental_experiment.helpers import string_type\nfrom experimental_experiment.torch_interpreter.piece_by_piece import (\n    trace_execution_piece_by_piece,\n)\n\n\ndef get_phi35_untrained(batch_size: int = 2, **kwargs) -> Dict[str, Any]:\n    \"\"\"\n    Gets a non initialized model with two sets of inputs and different shapes.\n\n    :param batch_size: batch size\n    :param kwargs: to overwrite the configuration, example ``num_hidden_layers=1``\n    :return: dictionary\n\n    See `Phi-3.5-mini-instruct/config.json\n    <https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/config.json>`_.\n    \"\"\"\n    config = {\n        \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n        \"architectures\": [\"Phi3ForCausalLM\"],\n        \"attention_dropout\": 0.0,\n        \"auto_map\": {\n            \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n            \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\",\n        },\n        \"bos_token_id\": 1,\n        \"embd_pdrop\": 0.0,\n        \"eos_token_id\": 32000,\n        \"hidden_act\": \"silu\",\n        \"hidden_size\": 3072,\n        \"initializer_range\": 0.02,\n        \"intermediate_size\": 8192,\n        \"max_position_embeddings\": 131072,\n        \"model_type\": \"phi3\",\n        \"num_attention_heads\": 32,\n        \"num_hidden_layers\": 32,\n        \"num_key_value_heads\": 32,\n        \"original_max_position_embeddings\": 4096,\n        \"pad_token_id\": 32000,\n        \"resid_pdrop\": 0.0,\n        \"rms_norm_eps\": 1e-05,\n        \"rope_scaling\": {\n            \"long_factor\": [\n                1.0800000429153442,\n                1.1100000143051147,\n                1.1399999856948853,\n                1.340000033378601,\n                1.5899999141693115,\n                1.600000023841858,\n                1.6200000047683716,\n                2.620000123977661,\n                3.2300000190734863,\n                3.2300000190734863,\n                4.789999961853027,\n                7.400000095367432,\n                7.700000286102295,\n                9.09000015258789,\n                12.199999809265137,\n                17.670000076293945,\n                24.46000099182129,\n                28.57000160217285,\n                30.420001983642578,\n                30.840002059936523,\n                32.590003967285156,\n                32.93000411987305,\n                42.320003509521484,\n                44.96000289916992,\n                50.340003967285156,\n                50.45000457763672,\n                57.55000305175781,\n                57.93000411987305,\n                58.21000289916992,\n                60.1400032043457,\n                62.61000442504883,\n                62.62000274658203,\n                62.71000289916992,\n                63.1400032043457,\n                63.1400032043457,\n                63.77000427246094,\n                63.93000411987305,\n                63.96000289916992,\n                63.970001220703125,\n                64.02999877929688,\n                64.06999969482422,\n                64.08000183105469,\n                64.12000274658203,\n                64.41000366210938,\n                64.4800033569336,\n                64.51000213623047,\n                64.52999877929688,\n                64.83999633789062,\n            ],\n            \"short_factor\": [\n                1.0,\n                1.0199999809265137,\n                1.0299999713897705,\n                1.0299999713897705,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0699999332427979,\n                1.0999999046325684,\n                1.1099998950958252,\n                1.1599998474121094,\n                1.1599998474121094,\n                1.1699998378753662,\n                1.2899998426437378,\n                1.339999794960022,\n                1.679999828338623,\n                1.7899998426437378,\n                1.8199998140335083,\n                1.8499997854232788,\n                1.8799997568130493,\n                1.9099997282028198,\n                1.9399996995925903,\n                1.9899996519088745,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0799996852874756,\n                2.0899996757507324,\n                2.189999580383301,\n                2.2199995517730713,\n                2.5899994373321533,\n                2.729999542236328,\n                2.749999523162842,\n                2.8399994373321533,\n            ],\n            \"type\": \"longrope\",\n        },\n        \"rope_theta\": 10000.0,\n        \"sliding_window\": 262144,\n        \"tie_word_embeddings\": False,\n        \"torch_dtype\": \"bfloat16\",\n        \"use_cache\": True,\n        \"attention_bias\": False,\n        \"vocab_size\": 32064,\n    }\n    config.update(**kwargs)\n    conf = transformers.Phi3Config(**config)\n    model = transformers.Phi3ForCausalLM(conf)\n    model.eval()\n\n    cache = make_dynamic_cache(\n        [\n            (torch.randn(batch_size, 32, 30, 96), torch.randn(batch_size, 32, 30, 96))\n            for i in range(config[\"num_hidden_layers\"])\n        ]\n    )\n    cache2 = make_dynamic_cache(\n        [\n            (torch.randn(batch_size + 1, 32, 31, 96), torch.randn(batch_size + 1, 32, 31, 96))\n            for i in range(config[\"num_hidden_layers\"])\n        ]\n    )\n\n    inputs = dict(\n        input_ids=torch.randint(0, 32064, (batch_size, 3)).to(torch.int64),\n        attention_mask=torch.ones((batch_size, 33)).to(torch.int64),\n        past_key_values=cache,\n    )\n    inputs2 = dict(\n        input_ids=torch.randint(0, 32064, (batch_size + 1, 4)).to(torch.int64),\n        attention_mask=torch.ones((batch_size + 1, 35)).to(torch.int64),\n        past_key_values=cache2,\n    )\n    return dict(inputs=inputs, model=model, inputs2=inputs2)\n\n\ndata = get_phi35_untrained(num_hidden_layers=2)\nmodel, inputs, inputs2 = data[\"model\"], data[\"inputs\"], data[\"inputs2\"]\n\nprint(string_type(inputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Shapes\n\nWe want to infer the dynamic shapes from the two sets of inputs we gave.\nFor that, we use a function to trace the execution of the model\nincluding its submodules. It is going to execute the model twice\nwith the two sets of inputs and stores every intermediate input and output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diag = trace_execution_piece_by_piece(model, [inputs, inputs2], verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we keep in memory every input/output for the submodules,\nwe can guess the dynamic shapes for every of them.\nThe final ones:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dynamic_shapes = diag.guess_dynamic_shapes()\nprint(\"The dynamic shapes are:\")\npprint.pprint(dynamic_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And all the dynamic shapes all along the traced submodules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    diag.pretty_text(\n        with_dynamic_shape=True,\n        with_shape=False,\n        with_min_max=False,\n        with_device=False,\n        with_inputs=False,\n    ).replace(\"<_DimHint.DYNAMIC: 3>\", \"DYN\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the export\n\nIn many cases, the export (to :class:`torch.fx.Graph`, to ONNX)\ndoes not work on the first try. We need a way to understand\nhow much the model can be exported. It can be used to evaluate\nthe how much code needs to be rewritten or patched to be exportable.\nThe verbosity can be increase to show dynamic shapes, results\nof the discrepancies.\nLet's display the module and its submodule first.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    diag.pretty_text(\n        with_dynamic_shape=False,\n        with_shape=False,\n        with_min_max=False,\n        with_device=False,\n        with_inputs=False,\n    )\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The we try to export to see the submodule failing the whole model.\nWe can pickle the failing model and restore it to speedup\nthe refactoring to make it work.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"----------------------\")\nep = diag.try_export(\n    exporter=\"fx\",\n    use_dynamic_shapes=True,\n    exporter_kwargs=dict(strict=False),\n    verbose=1,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's display a report.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"success: {ep.status}\")\nprint(diag.get_export_report())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace the failing module by a custom op\n\nThe main module is not exportable because one piece cannot be exported.\nBut maybe if we assume it works, maybe everything else is working.\nSo let's try to replace this class by a custom op.\nThis will be something for another example.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}