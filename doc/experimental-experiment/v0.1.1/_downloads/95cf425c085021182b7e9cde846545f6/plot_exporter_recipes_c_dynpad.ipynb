{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# to_onnx and padding one dimension to a mulitple of a constant\n\nThis is a frequent task which does not play well with dynamic shapes.\nLet's see how to avoid using :func:`torch.cond`.\n\n## A model with a test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nfrom onnx_array_api.plotting.graphviz_helper import plot_dot\nimport torch\nfrom onnx_diagnostic.helpers import max_diff\nfrom onnx_diagnostic.helpers.onnx_helper import pretty_onnx\nfrom experimental_experiment.reference import ExtendedReferenceEvaluator\nfrom experimental_experiment.torch_interpreter import to_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a model padding to a multiple of a constant.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PadToMultiple(torch.nn.Module):\n    def __init__(\n        self,\n        multiple: int,\n        dim: int = 0,\n    ):\n        super().__init__()\n        self.dim_to_pad = dim\n        self.multiple = multiple\n\n    def forward(self, x):\n        shape = x.shape\n        dim = x.shape[self.dim_to_pad]\n        next_dim = ((dim + self.multiple - 1) // self.multiple) * self.multiple\n        to_pad = next_dim - dim\n        pad = torch.zeros(\n            (*shape[: self.dim_to_pad], to_pad, *shape[self.dim_to_pad + 1 :]), dtype=x.dtype\n        )\n        return torch.cat([x, pad], dim=self.dim_to_pad)\n\n\nmodel = PadToMultiple(4, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check it runs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn((6, 7, 8))\ny = model(x)\nprint(f\"x.shape={x.shape}, y.shape={y.shape}\")\n\n# Let's check it runs on another example.\nx2 = torch.randn((6, 8, 8))\ny2 = model(x2)\nprint(f\"x2.shape={x2.shape}, y2.shape={y2.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export\n\nLet's defined the dynamic shapes and checks it exports.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DYNAMIC = torch.export.Dim.DYNAMIC\nep = torch.export.export(\n    model, (x,), dynamic_shapes=({0: DYNAMIC, 1: DYNAMIC, 2: DYNAMIC},), strict=False\n)\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also inline the local function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(model, (x,), dynamic_shapes=({0: \"batch\", 1: \"seq_len\", 2: \"num_frames\"},))\nprint(pretty_onnx(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onnx.save(onx, \"plot_exporter_recipes_c_dynpad.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation\n\nLet's validate the exported model a set of inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref = ExtendedReferenceEvaluator(onx)\ninputs = [\n    torch.randn((6, 8, 8)),\n    torch.randn((6, 7, 8)),\n    torch.randn((5, 8, 17)),\n    torch.randn((1, 24, 4)),\n    torch.randn((3, 9, 11)),\n]\nfor inp in inputs:\n    expected = model(inp)\n    got = ref.run(None, {\"x\": inp.numpy()})\n    diff = max_diff(expected, got[0])\n    print(f\"diff with shape={inp.shape} -> {expected.shape}: discrepancies={diff['abs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And visually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_dot(onx, figsize=(10, 12))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}