
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_torch_export_101.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_torch_export_101.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_torch_export_101.py:


.. _l-plot-torch-export-101:

=================================================
101: Some dummy examples with torch.export.export
=================================================

:func:`torch.export.export` behaviour in various situations.

Easy Case
=========

A simple model.

.. GENERATED FROM PYTHON SOURCE LINES 15-32

.. code-block:: Python


    import torch


    class Neuron(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x):
            z = self.linear(x)
            return torch.sigmoid(z)


    exported_program = torch.export.export(Neuron(), (torch.randn(1, 5),))
    print(exported_program.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %p_linear_weight : [num_users=1] = placeholder[target=p_linear_weight]
        %p_linear_bias : [num_users=1] = placeholder[target=p_linear_bias]
        %x : [num_users=1] = placeholder[target=x]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_linear_weight, %p_linear_bias), kwargs = {})
        %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%linear,), kwargs = {})
        return (sigmoid,)




.. GENERATED FROM PYTHON SOURCE LINES 33-40

With an integer as input
++++++++++++++++++++++++

As `torch.export.export <https://pytorch.org/docs/stable/export.html>`_
documentation, integer do not show up on the graph.
An exporter based on :func:`torch.export.export` cannot consider
the integer as an input.

.. GENERATED FROM PYTHON SOURCE LINES 40-55

.. code-block:: Python



    class NeuronIInt(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x: torch.Tensor, i_input: int):
            z = self.linear(x)
            return torch.sigmoid(z)[:, i_input]


    exported_program = torch.export.export(NeuronIInt(), (torch.randn(1, 5), 2))
    print(exported_program.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %p_linear_weight : [num_users=1] = placeholder[target=p_linear_weight]
        %p_linear_bias : [num_users=1] = placeholder[target=p_linear_bias]
        %x : [num_users=1] = placeholder[target=x]
        %i_input : [num_users=0] = placeholder[target=i_input]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_linear_weight, %p_linear_bias), kwargs = {})
        %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%linear,), kwargs = {})
        %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sigmoid,), kwargs = {})
        %select : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_1, 1, 2), kwargs = {})
        return (select,)




.. GENERATED FROM PYTHON SOURCE LINES 56-60

With an integer as input
++++++++++++++++++++++++

But if the integer is wrapped into a Tensor, it works.

.. GENERATED FROM PYTHON SOURCE LINES 60-78

.. code-block:: Python



    class NeuronIInt(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x: torch.Tensor, i_input):
            z = self.linear(x)
            return torch.sigmoid(z)[:, i_input]


    exported_program = torch.export.export(
        NeuronIInt(), (torch.randn(1, 5), torch.Tensor([2]).to(torch.int32))
    )
    print(exported_program.graph)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %p_linear_weight : [num_users=1] = placeholder[target=p_linear_weight]
        %p_linear_bias : [num_users=1] = placeholder[target=p_linear_bias]
        %x : [num_users=1] = placeholder[target=x]
        %i_input : [num_users=1] = placeholder[target=i_input]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_linear_weight, %p_linear_bias), kwargs = {})
        %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%linear,), kwargs = {})
        %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sigmoid, 0, 0, 9223372036854775807), kwargs = {})
        %index : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%slice_1, [None, %i_input]), kwargs = {})
        return (index,)




.. GENERATED FROM PYTHON SOURCE LINES 79-83

Wrapped
+++++++

Wrapped, it continues to work.

.. GENERATED FROM PYTHON SOURCE LINES 83-100

.. code-block:: Python



    class WrappedNeuronIInt(torch.nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model

        def forward(self, *args, **kwargs):
            return self.model.forward(*args, **kwargs)


    exported_program = torch.export.export(
        WrappedNeuronIInt(NeuronIInt()), (torch.randn(1, 5), torch.Tensor([2]).to(torch.int32))
    )
    print(exported_program.graph)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %p_model_linear_weight : [num_users=1] = placeholder[target=p_model_linear_weight]
        %p_model_linear_bias : [num_users=1] = placeholder[target=p_model_linear_bias]
        %args_0 : [num_users=1] = placeholder[target=args_0]
        %args_1 : [num_users=1] = placeholder[target=args_1]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%args_0, %p_model_linear_weight, %p_model_linear_bias), kwargs = {})
        %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%linear,), kwargs = {})
        %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sigmoid, 0, 0, 9223372036854775807), kwargs = {})
        %index : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%slice_1, [None, %args_1]), kwargs = {})
        return (index,)




.. GENERATED FROM PYTHON SOURCE LINES 101-106

List
++++

The last one does not export. An exporter based on
:func:`torch.export.export` cannot work.

.. GENERATED FROM PYTHON SOURCE LINES 106-133

.. code-block:: Python



    class NeuronNoneListInt(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x, yz, i_input):
            z = self.linear(x + yz[0] * yz[3])
            return torch.sigmoid(z)[:i_input]


    try:
        exported_program = torch.export.export(
            NeuronNoneListInt(),
            (
                torch.randn(1, 5),
                [torch.randn(1, 5), None, None, torch.randn(1, 5)],
                torch.Tensor([2]).to(torch.int32),
            ),
        )
        print(exported_program.graph)
    except (torch._dynamo.exc.Unsupported, RuntimeError) as e:
        print(f"-- an error {type(e)} occured:")
        print(e)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- an error <class 'RuntimeError'> occured:
    Overloaded torch operator invoked from Python failed to match any schema:
    aten::slice() Expected a value of type 'Optional[int]' for argument 'end' but instead found type 'FakeTensor'.
    Position: 3
    Value: FakeTensor(..., size=(1,), dtype=torch.int32)
    Declaration: aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)
    Cast error details: Unable to cast Python instance of type <class 'torch._subclasses.fake_tensor.FakeTensor'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)

    aten::slice() expected at most 4 argument(s) but received 5 argument(s). Declaration: aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]

    aten::slice() expected at most 4 argument(s) but received 5 argument(s). Declaration: aten::slice.str(str string, int? start=None, int? end=None, int step=1) -> str






.. GENERATED FROM PYTHON SOURCE LINES 134-138

Loops
+++++

Loops are not captured.

.. GENERATED FROM PYTHON SOURCE LINES 138-161

.. code-block:: Python



    class NeuronLoop(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x, xs):
            z = self.linear(x)
            for i in range(len(xs)):
                x += xs[i] * (i + 1)
            return z


    exported_program = torch.export.export(
        NeuronLoop(),
        (
            torch.randn(1, 5),
            [torch.randn(1, 5), torch.randn(1, 5)],
        ),
    )
    print(exported_program.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %p_linear_weight : [num_users=1] = placeholder[target=p_linear_weight]
        %p_linear_bias : [num_users=1] = placeholder[target=p_linear_bias]
        %x : [num_users=2] = placeholder[target=x]
        %xs_0 : [num_users=1] = placeholder[target=xs_0]
        %xs_1 : [num_users=1] = placeholder[target=xs_1]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_linear_weight, %p_linear_bias), kwargs = {})
        %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%xs_0, 1), kwargs = {})
        %add_ : [num_users=1] = call_function[target=torch.ops.aten.add_.Tensor](args = (%x, %mul), kwargs = {})
        %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%xs_1, 2), kwargs = {})
        %add__1 : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%add_, %mul_1), kwargs = {})
        return (linear,)




.. GENERATED FROM PYTHON SOURCE LINES 162-166

Export for training
+++++++++++++++++++

In that case, the weights are exported as inputs.

.. GENERATED FROM PYTHON SOURCE LINES 166-185

.. code-block:: Python



    class Neuron(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x):
            z = self.linear(x)
            return torch.sigmoid(z)


    print("-- training")
    mod = Neuron()
    mod.train()
    exported_program = torch.export.export_for_training(mod, (torch.randn(1, 5),))
    print(exported_program.graph)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- training
    graph():
        %p_linear_weight : [num_users=1] = placeholder[target=p_linear_weight]
        %p_linear_bias : [num_users=1] = placeholder[target=p_linear_bias]
        %x : [num_users=1] = placeholder[target=x]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_linear_weight, %p_linear_bias), kwargs = {})
        %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%linear,), kwargs = {})
        return (sigmoid,)




.. GENERATED FROM PYTHON SOURCE LINES 186-189

Preserve Modules
++++++++++++++++


.. GENERATED FROM PYTHON SOURCE LINES 189-211

.. code-block:: Python



    class Neuron(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x):
            z = self.linear(x)
            return torch.sigmoid(z)


    class NeuronNeuron(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.my_neuron = Neuron(n_dims, n_targets)

        def forward(self, x):
            z = self.my_neuron(x)
            return -z









.. GENERATED FROM PYTHON SOURCE LINES 212-213

The list of the modules.

.. GENERATED FROM PYTHON SOURCE LINES 213-218

.. code-block:: Python


    mod = NeuronNeuron()
    for item in mod.named_modules():
        print(item)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ('', NeuronNeuron(
      (my_neuron): Neuron(
        (linear): Linear(in_features=5, out_features=3, bias=True)
      )
    ))
    ('my_neuron', Neuron(
      (linear): Linear(in_features=5, out_features=3, bias=True)
    ))
    ('my_neuron.linear', Linear(in_features=5, out_features=3, bias=True))




.. GENERATED FROM PYTHON SOURCE LINES 219-220

The exported module did not change.

.. GENERATED FROM PYTHON SOURCE LINES 220-227

.. code-block:: Python


    print("-- preserved?")
    exported_program = torch.export.export(
        mod, (torch.randn(1, 5),), preserve_module_call_signature=("my_neuron",)
    )
    print(exported_program.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- preserved?
    graph():
        %p_my_neuron_linear_weight : [num_users=1] = placeholder[target=p_my_neuron_linear_weight]
        %p_my_neuron_linear_bias : [num_users=1] = placeholder[target=p_my_neuron_linear_bias]
        %x : [num_users=1] = placeholder[target=x]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_my_neuron_linear_weight, %p_my_neuron_linear_bias), kwargs = {})
        %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%linear,), kwargs = {})
        %neg : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%sigmoid,), kwargs = {})
        return (neg,)




.. GENERATED FROM PYTHON SOURCE LINES 228-229

And now?

.. GENERATED FROM PYTHON SOURCE LINES 229-237

.. code-block:: Python


    import torch.export._swap

    swapped_gm = torch.export._swap._swap_modules(exported_program, {"my_neuron": Neuron()})

    print("-- preserved?")
    print(swapped_gm.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/export/unflatten.py:872: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
      spec_node = gm.graph.get_attr(name)
    /home/xadupre/vv/this312/lib/python3.12/site-packages/torch/export/unflatten.py:864: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
      spec_node = gm.graph.get_attr(name)
    -- preserved?
    graph():
        %x_1 : [num_users=1] = placeholder[target=x]
        %_spec_0 : [num_users=1] = get_attr[target=_spec_0]
        %_spec_1 : [num_users=1] = get_attr[target=_spec_1]
        %_spec_2 : [num_users=1] = get_attr[target=_spec_2]
        %tree_flatten : [num_users=1] = call_function[target=torch.utils._pytree.tree_flatten](args = ((%x_1,),), kwargs = {})
        %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%tree_flatten, 0), kwargs = {})
        %x : [num_users=1] = call_function[target=operator.getitem](args = (%getitem, 0), kwargs = {})
        %tree_unflatten_1 : [num_users=1] = call_function[target=torch.utils._pytree.tree_unflatten](args = ([%x], %_spec_1), kwargs = {})
        %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%tree_unflatten_1, 0), kwargs = {})
        %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%getitem_1, 0), kwargs = {})
        %my_neuron : [num_users=1] = call_module[target=my_neuron](args = (%getitem_2,), kwargs = {})
        %tree_flatten_spec : [num_users=1] = call_function[target=torch.fx._pytree.tree_flatten_spec](args = (%my_neuron, %_spec_2), kwargs = {})
        %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%tree_flatten_spec, 0), kwargs = {})
        %neg : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%getitem_4,), kwargs = {})
        %tree_unflatten : [num_users=1] = call_function[target=torch.utils._pytree.tree_unflatten](args = ((%neg,), %_spec_0), kwargs = {})
        return tree_unflatten




.. GENERATED FROM PYTHON SOURCE LINES 238-240

Unfortunately this approach does not work well on big models
and it is a provite API.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.325 seconds)


.. _sphx_glr_download_auto_examples_plot_torch_export_101.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_torch_export_101.ipynb <plot_torch_export_101.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_torch_export_101.py <plot_torch_export_101.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_torch_export_101.zip <plot_torch_export_101.zip>`


.. include:: plot_torch_export_101.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
