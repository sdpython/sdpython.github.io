<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="A script to report a bug" href="example_bug.html" /><link rel="prev" title="Tries with Undocumented" href="torchtry.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2024.01.29 -->
        <title>Use the custom exporter in torch - experimental-experiment 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">experimental-experiment 0.1.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">experimental-experiment 0.1.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorial/index.html">Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tutorial/pytorch.html">pytorch and onnx</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of pytorch and onnx</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_linreg_101.html">101: Linear Regression and export to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_custom_backend_101.html">101: A custom backend for torch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_optimize_101.html">101: Graph Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_convolutation_matmul_102.html">102: Convolution and Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_bench_102.html">102: Measure LLAMA speed</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_201.html">201: Evaluate different ways to export a torch model to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_dort_201.html">201: Evaluate DORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_aot_201.html">201: Evaluate DORT Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_export_301.html">301: Compares LLAMA exporters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_dort_301.html">301: Compares LLAMA exporters for onnxrt backend</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tutorial/onnx.html">onnx</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of onnx</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_profile_existing_onnx_101.html">101: Profile an existing model with onnxruntime</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/errors.html">Frequent Exceptions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../design/index.html">Design</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../design/exporter.html">Custom Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/optimizer.html">Pattern Optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/backends.html">Dynamo Backends</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of API</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gradient.html">gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/reference.html">reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/graph_builder.html">graph_builder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/graph_builder_pattern.html">graph_builder_optim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/interpreter.html">interpreter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/onnx_export.html">onnx_export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/aten_function.html">aten_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/aten_method.html">aten_methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/prims_function.html">aten_prims</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/convert.html">convert_tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/torch_dynamo.html">torch_dynamo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/misc.html">Othersâ€¦</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/torch_helper.html">torch_models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/dimension.html">Dimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/torch_test.html">Testing</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_examples/index.html">Example gallery</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Example gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_optimize_101.html">101: Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_profile_existing_onnx_101.html">101: Profile an existing model with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_torch_linreg_101.html">101: Linear Regression and export to ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_torch_custom_backend_101.html">101: A custom backend for torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_convolutation_matmul_102.html">102: Convolution and Matrix Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_llama_diff_export_301.html">301: Compares LLAMA exporters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_llama_bench_102.html">102: Measure LLAMA speed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_llama_diff_dort_301.html">301: Compares LLAMA exporters for onnxrt backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_torch_aot_201.html">201: Evaluate DORT Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_torch_dort_201.html">201: Evaluate DORT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_torch_export_201.html">201: Evaluate different ways to export a torch model to ONNX</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Supported Models</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of Supported Models</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="torchtry.html">Tries with Undocumented</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Use the custom exporter in torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_bug.html">A script to report a bug</a></li>
<li class="toctree-l2"><a class="reference internal" href="llama.html">LLaMa</a></li>
<li class="toctree-l2"><a class="reference internal" href="mistral.html">Mistral</a></li>
<li class="toctree-l2"><a class="reference internal" href="phi.html">Phi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../bench/index.html">Benchmark from the command line</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of Benchmark from the command line</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../bench/dort_bench.html">experimental_experiment.torch_bench.dort_bench</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/dort_profile.html">experimental_experiment.torch_bench.dort_profile</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../times.html">Times</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../long_outputs.html">Long Outputs uneasy to read</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="use-the-custom-exporter-in-torch">
<h1>Use the custom exporter in torch<a class="headerlink" href="#use-the-custom-exporter-in-torch" title="Link to this heading">#</a></h1>
<p><em>Subject to change</em></p>
<section id="file-onnxruntime-py">
<h2>File <cite>onnxruntime.py</cite><a class="headerlink" href="#file-onnxruntime-py" title="Link to this heading">#</a></h2>
<p>This change enables the custom rewriter is an environment variable is enabled.
Look for substring <code class="docutils literal notranslate"><span class="pre">TODO:</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_ort_acclerated_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This function replaces GraphModule._wrapped_call in compiled model.</span>

<span class="sd">    The _wrapped_call is the underlying implementation of forward method. Replacing</span>
<span class="sd">    it means we delegate the computation to _ort_acclerated_call and therefore</span>
<span class="sd">    onnxruntime.InferenceSession.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cached_execution_info_per_session</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_all_ort_execution_info</span><span class="o">.</span><span class="n">search_reusable_session_execution_info</span><span class="p">(</span>
            <span class="n">graph_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">cached_execution_info_per_session</span><span class="p">:</span>
        <span class="n">onnx_session</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">session</span>
        <span class="n">input_names</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">input_names</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">output_names</span>
        <span class="n">input_value_infos</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">input_value_infos</span>
        <span class="n">output_value_infos</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">output_value_infos</span>
        <span class="n">input_devices</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">input_devices</span>
        <span class="n">output_devices</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">output_devices</span>
        <span class="n">prim_outputs</span> <span class="o">=</span> <span class="n">cached_execution_info_per_session</span><span class="o">.</span><span class="n">example_outputs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># It&#39;s first time seeing such as graph. Let&#39;s make a new session</span>
        <span class="c1"># (type: onnxruntime.InferenceSession) for it.</span>

        <span class="c1">##########################</span>
        <span class="c1"># TODO: Insert these lines</span>
        <span class="c1">##########################</span>

        <span class="n">use_other_rewriter</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ONNXRT_CHANGE_REWRITER&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_other_rewriter</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">experimental_experiment.torch_interpreter</span> <span class="kn">import</span> <span class="n">to_onnx</span>
            <span class="kn">from</span> <span class="nn">experimental_experiment.torch_interpreter._torch_models</span> <span class="kn">import</span> <span class="n">create_input_names</span>
            <span class="kn">from</span> <span class="nn">experimental_experiment.xbuilder</span> <span class="kn">import</span> <span class="n">OptimizationOptions</span>
            <span class="kn">from</span> <span class="nn">experimental_experiment.torch_interpreter.oxs_dispatcher</span> <span class="kn">import</span> <span class="n">OxsDispatcher</span>

            <span class="n">input_names</span> <span class="o">=</span> <span class="n">input_names</span> <span class="o">=</span> <span class="n">create_input_names</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
            <span class="n">dispatcher</span> <span class="o">=</span> <span class="n">OxsDispatcher</span><span class="p">()</span>
            <span class="n">target_opset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">onnx_registry</span><span class="o">.</span><span class="n">opset_version</span>
            <span class="n">options</span> <span class="o">=</span> <span class="n">OptimizationOptions</span><span class="p">(</span>
                <span class="n">remove_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">constant_folding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">patterns</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">onnx_model</span><span class="p">,</span> <span class="n">builder</span> <span class="o">=</span> <span class="n">to_onnx</span><span class="p">(</span>
                <span class="n">graph_module</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">),</span>
                <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span>
                <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">target_opset</span><span class="o">=</span><span class="n">target_opset</span><span class="p">,</span>
                <span class="n">return_builder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">dispatcher</span><span class="o">=</span><span class="n">dispatcher</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">def</span> <span class="nf">maybe_map_to_meta_val</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;val&quot;</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">meta</span><span class="p">:</span>
                    <span class="c1"># Select outputs with &quot;val&quot; information. Without &quot;val&quot;,</span>
                    <span class="c1"># it&#39;s not possible access output_arg.meta[&quot;val&quot;].device.</span>
                    <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">value</span>

            <span class="n">extracted_outputs</span> <span class="o">=</span> <span class="n">_extract_graph_module_outputs</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
            <span class="n">prim_outputs</span> <span class="o">=</span> <span class="n">_pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">maybe_map_to_meta_val</span><span class="p">,</span> <span class="n">extracted_outputs</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>

        <span class="c1">####################################</span>
        <span class="c1"># TODO: end of the insertion</span>
        <span class="c1"># TODO: indent what follows</span>
        <span class="c1">####################################</span>

            <span class="n">graph_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">_internal</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">passes</span><span class="o">.</span><span class="n">MovePlaceholderToFront</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">diagnostic_context</span><span class="p">,</span>
                <span class="n">graph_module</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
            <span class="c1"># Generate reference outputs. They are used to indicate output</span>
            <span class="c1"># tensors&#39; types and devices when calling ORT.</span>
            <span class="c1">#</span>
            <span class="c1"># WARNING: The downstream code should not change prim_outputs and</span>
            <span class="c1"># this backend should always produces output with schema identical to prim_outputs&#39;.</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">dynamic_shapes</span><span class="p">:</span>
                <span class="c1"># No pre-allocation when dynamic shape is enabled.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">preallocate_output</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">extracted_outputs</span> <span class="o">=</span> <span class="n">_extract_graph_module_outputs</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>

                <span class="k">def</span> <span class="nf">maybe_map_to_meta_val</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;val&quot;</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">meta</span><span class="p">:</span>
                        <span class="c1"># Select outputs with &quot;val&quot; information. Without &quot;val&quot;,</span>
                        <span class="c1"># it&#39;s not possible access output_arg.meta[&quot;val&quot;].device.</span>
                        <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">value</span>

                <span class="n">prim_outputs</span> <span class="o">=</span> <span class="n">_pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
                    <span class="n">maybe_map_to_meta_val</span><span class="p">,</span> <span class="n">extracted_outputs</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">prim_outputs</span> <span class="o">=</span> <span class="n">FakeTensorProp</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span>
                        <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;FakeTensorProb failed for </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">graph_module</span><span class="p">)</span>
                    <span class="c1"># When FakeTensorProp fails, it is not possible to preallocate output buffers</span>
                    <span class="c1"># because the output shapes are not inferred.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">preallocate_output</span> <span class="o">=</span> <span class="kc">False</span>

                    <span class="c1"># rethrow FakeTensorProb failure because it is not yet currently handled.</span>
                    <span class="k">raise</span>

            <span class="c1"># Create the object to iterate through the nodes in graph one-by-one</span>
            <span class="c1"># and calls the corresponding ONNX exporter for each node.</span>
            <span class="n">fx_interpreter</span> <span class="o">=</span> <span class="n">fx_onnx_interpreter</span><span class="o">.</span><span class="n">FxOnnxInterpreter</span><span class="p">(</span>
                <span class="n">diagnostic_context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">diagnostic_context</span>
            <span class="p">)</span>
            <span class="c1"># Cast FX variables if they will result schema-mismatch when searching</span>
            <span class="c1"># for ONNX operator. E.g., add(double_tensor, int_tensor) is fine in PyTorch,</span>
            <span class="c1"># but ONNX expects add(double_tensor, double_tensor).</span>
            <span class="n">graph_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">_internal</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">passes</span><span class="o">.</span><span class="n">InsertTypePromotion</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">diagnostic_context</span><span class="p">,</span> <span class="n">graph_module</span>
            <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
            <span class="c1"># Start the per-node exporting process. It&#39;s conceptually a for loop</span>
            <span class="c1"># scanning through the nodes in the graph.</span>
            <span class="n">exported</span> <span class="o">=</span> <span class="n">fx_interpreter</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                <span class="n">fx_graph_module</span><span class="o">=</span><span class="n">graph_module</span><span class="p">,</span>
                <span class="n">onnxfunction_dispatcher</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">onnxfunction_dispatcher</span><span class="p">,</span>
                <span class="n">op_level_debug</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">op_level_debug</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Convert the exported result to ONNX ModelProto.</span>
            <span class="n">onnx_model</span> <span class="o">=</span> <span class="n">exported</span><span class="o">.</span><span class="n">to_model_proto</span><span class="p">(</span>
                <span class="n">opset_version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_resolved_onnx_exporter_options</span><span class="o">.</span><span class="n">onnx_registry</span><span class="o">.</span><span class="n">opset_version</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1">####################################</span>
        <span class="c1"># TODO: end of the modification</span>
        <span class="c1">####################################</span>

        <span class="c1"># Modify ONNX model using pre-registered graph transforms.</span>
        <span class="c1"># They are in-place modifications for avoiding unnecessary</span>
        <span class="c1"># copy of ONNX initializers.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_options</span><span class="o">.</span><span class="n">pre_ort_model_transforms</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">transform</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_options</span><span class="o">.</span><span class="n">pre_ort_model_transforms</span><span class="p">:</span>
                <span class="n">transform</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>

        <span class="n">onnx_model_bytes</span> <span class="o">=</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ONNXRT_DUMP_PATH&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="c1"># If not empty, environment variable ONNXRT_DUMP_PATH defined the path</span>
            <span class="c1"># where generated onnx files should be stored.</span>
            <span class="c1"># This module keeps a global variables keeping track of the</span>
            <span class="c1"># stored models.</span>
            <span class="c1"># If ONNXRT_DUMP_PATH=&quot;dumped/dumped_model_&quot;</span>
            <span class="c1"># The first file name will be &#39;dumped/dumped_model_0.onnx&#39;.</span>
            <span class="c1"># For every dumped model, a text file &#39;dumped/dumped_model_0.txt&#39;</span>
            <span class="c1"># is created as well to contain the string representing the graph_module.</span>
            <span class="n">_dump_onnx_model</span><span class="p">(</span><span class="n">onnx_model_bytes</span><span class="p">,</span> <span class="n">graph_module</span><span class="o">=</span><span class="n">graph_module</span><span class="p">)</span>

        <span class="c1"># Initialize a ORT session to execute this ONNX model.</span>
        <span class="c1"># Note that TorchDynamo assumes all inputs/outputs are on the</span>
        <span class="c1"># same device, but it&#39;s subject to change (very likely with</span>
        <span class="c1"># dynamic shape support), so we add execution providers</span>
        <span class="c1"># based on the logic in _select_eps: (explicitly preferred EPs,</span>
        <span class="c1"># EPs inferred from inputs or graph, and the fallback default EP)/</span>
        <span class="c1">#</span>
        <span class="c1"># TODO(wschin): enable external allocators.</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/106867</span>
        <span class="n">onnx_session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
            <span class="n">path_or_bytes</span><span class="o">=</span><span class="n">onnx_model_bytes</span><span class="p">,</span>
            <span class="n">sess_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_options</span><span class="o">.</span><span class="n">ort_session_options</span><span class="p">,</span>
            <span class="n">providers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_select_eps</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># Cache ORT session. It&#39;s reused for the same &quot;graph_module&quot;.</span>
        <span class="c1"># Generate ONNX model and extract its input and output names.</span>
        <span class="n">input_names</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>
        <span class="n">output_names</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
        <span class="n">input_devices</span> <span class="o">=</span> <span class="n">_get_onnx_devices</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="c1"># Cache devices for inputs and outputs. They are used to invoke</span>
        <span class="c1"># ORT session. Output devices indicate where (e.g., GPU or CPU)</span>
        <span class="c1"># to store outputs</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prim_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">output_devices</span> <span class="o">=</span> <span class="n">_get_onnx_devices</span><span class="p">(</span><span class="n">prim_outputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_devices</span> <span class="o">=</span> <span class="n">_get_onnx_devices</span><span class="p">((</span><span class="n">prim_outputs</span><span class="p">,))</span>

        <span class="n">input_value_infos</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">input</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>
        <span class="n">output_value_infos</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>

        <span class="n">execution_info_per_session</span> <span class="o">=</span> <span class="n">OrtExecutionInfoPerSession</span><span class="p">(</span>
            <span class="n">session</span><span class="o">=</span><span class="n">onnx_session</span><span class="p">,</span>
            <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span>
            <span class="n">input_value_infos</span><span class="o">=</span><span class="n">input_value_infos</span><span class="p">,</span>
            <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span>
            <span class="n">output_value_infos</span><span class="o">=</span><span class="n">output_value_infos</span><span class="p">,</span>
            <span class="n">input_devices</span><span class="o">=</span><span class="n">input_devices</span><span class="p">,</span>
            <span class="n">output_devices</span><span class="o">=</span><span class="n">output_devices</span><span class="p">,</span>
            <span class="n">example_outputs</span><span class="o">=</span><span class="n">prim_outputs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_all_ort_execution_info</span><span class="o">.</span><span class="n">cache_session_execution_info</span><span class="p">(</span>
            <span class="n">graph_module</span><span class="p">,</span> <span class="n">execution_info_per_session</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">execution_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># ORT always returns a tuple of outputs. If the original output is a tensor,</span>
    <span class="c1"># ORT output&#39;s first element must be extracted and returned. Otherwise, type</span>
    <span class="c1"># mismatch may happen in downstream computation.</span>
    <span class="n">is_single_tensor_output</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prim_outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="n">normalized_prim_outputs</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">prim_outputs</span><span class="p">,)</span> <span class="k">if</span> <span class="n">is_single_tensor_output</span> <span class="k">else</span> <span class="n">prim_outputs</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalized_prim_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">normalized_prim_outputs</span>
    <span class="p">)</span>

    <span class="n">_nvtx_range_push</span><span class="p">(</span><span class="s2">&quot;run_onnx_session_with_ortvaluevector&quot;</span><span class="p">)</span>
    <span class="n">onnx_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">onnx_session</span><span class="p">,</span>
        <span class="n">input_names</span><span class="p">,</span>
        <span class="n">args</span><span class="p">,</span>
        <span class="n">input_devices</span><span class="p">,</span>
        <span class="n">output_names</span><span class="p">,</span>
        <span class="n">normalized_prim_outputs</span><span class="p">,</span>
        <span class="n">output_devices</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_options</span><span class="o">.</span><span class="n">preallocate_output</span><span class="p">,</span>
        <span class="n">input_value_infos</span><span class="p">,</span>
        <span class="n">normalized_prim_outputs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">_nvtx_range_pop</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assert_allclose_to_baseline</span><span class="p">:</span>
        <span class="c1"># Compute baseline.</span>
        <span class="n">baseline_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_prims</span><span class="o">.</span><span class="n">executor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span>
            <span class="n">graph_module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">executor</span><span class="o">=</span><span class="s2">&quot;aten&quot;</span>
        <span class="p">)</span>
        <span class="n">normalized_baseline_ouptuts</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">baseline_outputs</span><span class="p">,)</span> <span class="k">if</span> <span class="n">is_single_tensor_output</span> <span class="k">else</span> <span class="n">baseline_outputs</span>
        <span class="p">)</span>
        <span class="c1"># Ensure every output tensor is close to the corresponding baseline.</span>
        <span class="k">for</span> <span class="n">onnx_output</span><span class="p">,</span> <span class="n">baseline_output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">onnx_outputs</span><span class="p">,</span> <span class="n">normalized_baseline_ouptuts</span>
        <span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">onnx_output</span><span class="p">,</span> <span class="n">baseline_output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">onnx_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">is_single_tensor_output</span> <span class="k">else</span> <span class="n">onnx_outputs</span>
</pre></div>
</div>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h2>
<section id="baseline">
<h3>Baseline<a class="headerlink" href="#baseline" title="Link to this heading">#</a></h3>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.onnx</span>
<span class="kn">from</span> <span class="nn">experimental_experiment.torch_models.training_helper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">make_aot_ort</span><span class="p">,</span>
    <span class="n">train_loop</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">experimental_experiment.torch_models.dump_helper</span> <span class="kn">import</span> <span class="n">dump_onnx</span>

<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">LlamaConfig</span>
    <span class="kn">from</span> <span class="nn">transformers.models.llama.modeling_llama</span> <span class="kn">import</span> <span class="n">LlamaModel</span>


<span class="k">def</span> <span class="nf">ids_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="n">total_dims</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
        <span class="n">total_dims</span> <span class="o">*=</span> <span class="n">dim</span>

    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_dims</span><span class="p">):</span>
        <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>


<span class="n">config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">(</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="s2">&quot;eager&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LlamaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">ids_tensor</span><span class="p">([</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">],</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="n">input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>

<span class="n">local_aot_ort</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_aot_ort</span><span class="p">(</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rewrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
    <span class="n">optimized_mod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">local_aot_ort</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">dump_onnx</span><span class="p">(</span><span class="s2">&quot;dort-llama-ort&quot;</span><span class="p">,</span> <span class="n">folder</span><span class="o">=</span><span class="s2">&quot;dump_llama&quot;</span><span class="p">,</span> <span class="n">clean</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">train_loop</span><span class="p">(</span><span class="n">optimized_mod</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">_</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;dump_llama&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">_</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.onnx&quot;</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;exported model: </span><span class="si">{</span><span class="n">names</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NODES in </span><span class="si">{name!r}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">onx</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;dump_llama&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">onx</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">onx</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">op_type</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">input</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">output</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    [make_aot_ort] enable rewriting
    [optimize_model_proto] starts optimize with 372 nodes and 25 local functions
    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.
    [optimize_model_proto] optimize done in 0.29367939999974624 seconds.
    [optimize_model_proto] starts rewrite with 222 nodes and 7 local functions
    [optimize_model_proto] rewrite done in 0.004606500000591041 seconds with 222 nodes and 7 local functions
    [optimize_model_proto] starts inlining with 222 nodes and 7 local functions
    [optimize_model_proto] inlining done in 0.0033720999999786727 seconds with 236 nodes and 0 local functions
    [optimize_model_proto] starts optimize with 460 nodes and 23 local functions
    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.
    [optimize_model_proto] optimize done in 0.3823653999988892 seconds.
    [optimize_model_proto] starts rewrite with 296 nodes and 19 local functions
    [optimize_model_proto] rewrite done in 0.006795799999963492 seconds with 296 nodes and 19 local functions
    [optimize_model_proto] starts inlining with 296 nodes and 19 local functions
    [optimize_model_proto] inlining done in 0.005382500001360313 seconds with 336 nodes and 0 local functions
    ------------------------------------------
    exported model: [&#39;dort-llama-ort_1.onnx&#39;, &#39;dort-llama-ort_0.onnx&#39;]
    
    NODES in {name!r}
    1/336: Mul [&#39;embedding&#39;, &#39;rsqrt&#39;] -&gt; [&#39;mul&#39;]
    2/336: Constant [] -&gt; [&#39;_val_37&#39;]
    3/336: Pow [&#39;embedding&#39;, &#39;_val_37&#39;] -&gt; [&#39;pow_9&#39;]
    4/336: Constant [] -&gt; [&#39;_val_42&#39;]
    5/336: Constant [] -&gt; [&#39;_val_46&#39;]
    6/336: Constant [] -&gt; [&#39;_val_50&#39;]
    7/336: Constant [] -&gt; [&#39;_val_54&#39;]
    8/336: Slice [&#39;primals_12&#39;, &#39;_val_42&#39;, &#39;_val_46&#39;, &#39;_val_50&#39;, &#39;_val_54&#39;] -&gt; [&#39;slice_7&#39;]
    9/336: Constant [] -&gt; [&#39;_val_59&#39;]
    10/336: Constant [] -&gt; [&#39;_val_63&#39;]
    11/336: Constant [] -&gt; [&#39;_val_67&#39;]
    12/336: Constant [] -&gt; [&#39;_val_71&#39;]
    13/336: Slice [&#39;primals_13&#39;, &#39;_val_59&#39;, &#39;_val_63&#39;, &#39;_val_67&#39;, &#39;_val_71&#39;] -&gt; [&#39;slice_8&#39;]
    14/336: Constant [] -&gt; [&#39;aten_view_160_size_0&#39;]
    15/336: Reshape [&#39;mm_3&#39;, &#39;aten_view_160_size_0&#39;] -&gt; [&#39;view_18&#39;]
    16/336: Constant [] -&gt; [&#39;_val_75&#39;]
    17/336: Equal [&#39;primals_14&#39;, &#39;_val_75&#39;] -&gt; [&#39;eq&#39;]
    18/336: Constant [] -&gt; [&#39;aten_view_165_size_0&#39;]
    19/336: Reshape [&#39;mm_4&#39;, &#39;aten_view_165_size_0&#39;] -&gt; [&#39;view_20&#39;]
    20/336: Constant [] -&gt; [&#39;aten_view_167_size_0&#39;]
    21/336: Reshape [&#39;mm_5&#39;, &#39;aten_view_167_size_0&#39;] -&gt; [&#39;view_22&#39;]
    22/336: Constant [] -&gt; [&#39;aten_view_169_size_0&#39;]
    23/336: Reshape [&#39;mm_6&#39;, &#39;aten_view_169_size_0&#39;] -&gt; [&#39;view_24&#39;]
    24/336: Mul [&#39;tangents_1&#39;, &#39;primals_3&#39;] -&gt; [&#39;mul_12&#39;]
    25/336: Constant [] -&gt; [&#39;_val_90&#39;]
    26/336: Mul [&#39;pow_9&#39;, &#39;_val_90&#39;] -&gt; [&#39;mul_45&#39;]
    27/336: Transpose [&#39;slice_7&#39;] -&gt; [&#39;_val_92&#39;]
    28/336: Max [&#39;unsqueeze&#39;] -&gt; [&#39;_val_93&#39;]
    29/336: Shape [&#39;_val_93&#39;] -&gt; [&#39;_val_94&#39;]
    30/336: Expand [&#39;unsqueeze&#39;, &#39;_val_94&#39;] -&gt; [&#39;_val_95&#39;]
    31/336: Constant [] -&gt; [&#39;_val_96&#39;]
    32/336: Unsqueeze [&#39;_val_95&#39;, &#39;_val_96&#39;] -&gt; [&#39;_val_97&#39;]
    33/336: Concat [&#39;_val_97&#39;] -&gt; [&#39;_val_98&#39;]
    34/336: GatherND [&#39;_val_92&#39;, &#39;_val_98&#39;] -&gt; [&#39;_val_99&#39;]
    35/336: Transpose [&#39;_val_99&#39;] -&gt; [&#39;index&#39;]
    36/336: Transpose [&#39;slice_8&#39;] -&gt; [&#39;_val_101&#39;]
    37/336: Max [&#39;unsqueeze&#39;] -&gt; [&#39;_val_102&#39;]
    38/336: Shape [&#39;_val_102&#39;] -&gt; [&#39;_val_103&#39;]
    39/336: Expand [&#39;unsqueeze&#39;, &#39;_val_103&#39;] -&gt; [&#39;_val_104&#39;]
    40/336: Constant [] -&gt; [&#39;_val_105&#39;]
    41/336: Unsqueeze [&#39;_val_104&#39;, &#39;_val_105&#39;] -&gt; [&#39;_val_106&#39;]
    42/336: Concat [&#39;_val_106&#39;] -&gt; [&#39;_val_107&#39;]
    43/336: GatherND [&#39;_val_101&#39;, &#39;_val_107&#39;] -&gt; [&#39;_val_108&#39;]
    44/336: Transpose [&#39;_val_108&#39;] -&gt; [&#39;index_1&#39;]
    45/336: Constant [] -&gt; [&#39;alpha__1&#39;]
    46/336: Mul [&#39;view_18&#39;, &#39;alpha__1&#39;] -&gt; [&#39;other_1__1&#39;]
    47/336: Add [&#39;embedding&#39;, &#39;other_1__1&#39;] -&gt; [&#39;add_5&#39;]
    48/336: Constant [] -&gt; [&#39;aten_unsqueeze_197_dim_0&#39;]
    49/336: Unsqueeze [&#39;eq&#39;, &#39;aten_unsqueeze_197_dim_0&#39;] -&gt; [&#39;unsqueeze_9&#39;]
    50/336: Mul [&#39;view_20&#39;, &#39;sigmoid&#39;] -&gt; [&#39;mul_8&#39;]
    51/336: Mul [&#39;mul_12&#39;, &#39;rsqrt_2&#39;] -&gt; [&#39;mul_15&#39;]
    52/336: Constant [] -&gt; [&#39;fill&#39;]
    53/336: Constant [] -&gt; [&#39;aten_unsqueeze_205_dim_0&#39;]
    54/336: Unsqueeze [&#39;index&#39;, &#39;aten_unsqueeze_205_dim_0&#39;] -&gt; [&#39;unsqueeze_7&#39;]
    55/336: Constant [] -&gt; [&#39;aten_unsqueeze_206_dim_0&#39;]
    56/336: Unsqueeze [&#39;index_1&#39;, &#39;aten_unsqueeze_206_dim_0&#39;] -&gt; [&#39;unsqueeze_8&#39;]
    57/336: Mul [&#39;add_5&#39;, &#39;rsqrt_1&#39;] -&gt; [&#39;mul_6&#39;]
    58/336: Constant [] -&gt; [&#39;alpha__2&#39;]
    59/336: Mul [&#39;view_24&#39;, &#39;alpha__2&#39;] -&gt; [&#39;other_1__2&#39;]
    60/336: Add [&#39;add_5&#39;, &#39;other_1__2&#39;] -&gt; [&#39;add_7&#39;]
    61/336: Constant [] -&gt; [&#39;_val_123&#39;]
    62/336: Pow [&#39;add_5&#39;, &#39;_val_123&#39;] -&gt; [&#39;pow_7&#39;]
    63/336: Constant [] -&gt; [&#39;alpha__3&#39;]
    64/336: Mul [&#39;sigmoid&#39;, &#39;alpha__3&#39;] -&gt; [&#39;other_1__3&#39;]
    65/336: Sub [&#39;fill&#39;, &#39;other_1__3&#39;] -&gt; [&#39;sub&#39;]
    66/336: Mul [&#39;add_7&#39;, &#39;rsqrt_2&#39;] -&gt; [&#39;mul_10&#39;]
    67/336: Mul [&#39;mul_12&#39;, &#39;add_7&#39;] -&gt; [&#39;mul_14&#39;]
    68/336: Constant [] -&gt; [&#39;_val_131&#39;]
    69/336: Pow [&#39;add_7&#39;, &#39;_val_131&#39;] -&gt; [&#39;pow_5&#39;]
    70/336: Constant [] -&gt; [&#39;_val_133&#39;]
    71/336: Mul [&#39;pow_7&#39;, &#39;_val_133&#39;] -&gt; [&#39;mul_31&#39;]
    72/336: Mul [&#39;view_20&#39;, &#39;sub&#39;] -&gt; [&#39;mul_22&#39;]
    73/336: Constant [] -&gt; [&#39;scalar_tensor_default&#39;]
    74/336: Pow [&#39;rsqrt&#39;, &#39;scalar_tensor_default&#39;] -&gt; [&#39;pow_8&#39;]
    75/336: Mul [&#39;tangents_1&#39;, &#39;mul_10&#39;] -&gt; [&#39;mul_13&#39;]
    76/336: Constant [] -&gt; [&#39;_val_142&#39;]
    77/336: ReduceSum [&#39;mul_14&#39;, &#39;_val_142&#39;] -&gt; [&#39;sum_2&#39;]
    78/336: Constant [] -&gt; [&#39;_val_144&#39;]
    79/336: Mul [&#39;pow_5&#39;, &#39;_val_144&#39;] -&gt; [&#39;mul_18&#39;]
    80/336: Constant [] -&gt; [&#39;scalar_tensor_default_1&#39;]
    81/336: Pow [&#39;rsqrt_1&#39;, &#39;scalar_tensor_default_1&#39;] -&gt; [&#39;pow_6&#39;]
    82/336: Constant [] -&gt; [&#39;scalar_tensor_default_2&#39;]
    83/336: Pow [&#39;rsqrt_2&#39;, &#39;scalar_tensor_default_2&#39;] -&gt; [&#39;pow_4&#39;]
    84/336: Constant [] -&gt; [&#39;aten_add_240_other_1&#39;]
    85/336: Add [&#39;mul_22&#39;, &#39;aten_add_240_other_1&#39;] -&gt; [&#39;add_10&#39;]
    86/336: Constant [] -&gt; [&#39;_val_155&#39;]
    87/336: ReduceSum [&#39;mul_13&#39;, &#39;_val_155&#39;] -&gt; [&#39;sum_1&#39;]
    88/336: Constant [] -&gt; [&#39;_val_157&#39;]
    89/336: Mul [&#39;sum_2&#39;, &#39;_val_157&#39;] -&gt; [&#39;mul_16&#39;]
    90/336: Mul [&#39;sigmoid&#39;, &#39;add_10&#39;] -&gt; [&#39;mul_23&#39;]
    91/336: Constant [] -&gt; [&#39;aten_view_247_size_0&#39;]
    92/336: Reshape [&#39;sum_1&#39;, &#39;aten_view_247_size_0&#39;] -&gt; [&#39;view_25&#39;]
    93/336: Mul [&#39;mul_16&#39;, &#39;pow_4&#39;] -&gt; [&#39;mul_17&#39;]
    94/336: Constant [] -&gt; [&#39;aten_expand_250_size_1&#39;]
    95/336: Expand [&#39;mul_17&#39;, &#39;aten_expand_250_size_1&#39;] -&gt; [&#39;expand_7&#39;]
    96/336: Constant [] -&gt; [&#39;scalar_tensor_default_4&#39;]
    97/336: Div [&#39;expand_7&#39;, &#39;scalar_tensor_default_4&#39;] -&gt; [&#39;div_1&#39;]
    98/336: Mul [&#39;div_1&#39;, &#39;mul_18&#39;] -&gt; [&#39;mul_19&#39;]
    99/336: Constant [] -&gt; [&#39;alpha__4&#39;]
    100/336: Mul [&#39;mul_19&#39;, &#39;alpha__4&#39;] -&gt; [&#39;other_1__4&#39;]
    101/336: Add [&#39;mul_15&#39;, &#39;other_1__4&#39;] -&gt; [&#39;add_9&#39;]
    102/336: Constant [] -&gt; [&#39;aten_view_257_size_0&#39;]
    103/336: Reshape [&#39;add_9&#39;, &#39;aten_view_257_size_0&#39;] -&gt; [&#39;view_26&#39;]
    104/336: Transpose [&#39;view_26&#39;] -&gt; [&#39;t_7&#39;]
    105/336: MatMul [&#39;view_26&#39;, &#39;t_9&#39;] -&gt; [&#39;mm_8&#39;]
    106/336: MatMul [&#39;t_7&#39;, &#39;view_23&#39;] -&gt; [&#39;mm_7&#39;]
    107/336: Constant [] -&gt; [&#39;aten_view_262_size_0&#39;]
    108/336: Reshape [&#39;mm_8&#39;, &#39;aten_view_262_size_0&#39;] -&gt; [&#39;view_27&#39;]
    109/336: Transpose [&#39;mm_7&#39;] -&gt; [&#39;t_8&#39;]
    110/336: Mul [&#39;view_27&#39;, &#39;mul_8&#39;] -&gt; [&#39;mul_20&#39;]
    111/336: Mul [&#39;view_27&#39;, &#39;view_22&#39;] -&gt; [&#39;mul_21&#39;]
    112/336: Transpose [&#39;t_8&#39;] -&gt; [&#39;t_10&#39;]
    113/336: Constant [] -&gt; [&#39;aten_view_268_size_0&#39;]
    114/336: Reshape [&#39;mul_20&#39;, &#39;aten_view_268_size_0&#39;] -&gt; [&#39;view_28&#39;]
    115/336: Mul [&#39;mul_21&#39;, &#39;mul_23&#39;] -&gt; [&#39;mul_24&#39;]
    116/336: Transpose [&#39;view_28&#39;] -&gt; [&#39;t_11&#39;]
    117/336: MatMul [&#39;view_28&#39;, &#39;t_13&#39;] -&gt; [&#39;mm_10&#39;]
    118/336: Constant [] -&gt; [&#39;aten_view_273_size_0&#39;]
    119/336: Reshape [&#39;mul_24&#39;, &#39;aten_view_273_size_0&#39;] -&gt; [&#39;view_30&#39;]
    120/336: MatMul [&#39;t_11&#39;, &#39;view_19&#39;] -&gt; [&#39;mm_9&#39;]
    121/336: Constant [] -&gt; [&#39;aten_view_276_size_0&#39;]
    122/336: Reshape [&#39;mm_10&#39;, &#39;aten_view_276_size_0&#39;] -&gt; [&#39;view_29&#39;]
    123/336: Transpose [&#39;view_30&#39;] -&gt; [&#39;t_15&#39;]
    124/336: MatMul [&#39;view_30&#39;, &#39;t_17&#39;] -&gt; [&#39;mm_12&#39;]
    125/336: Transpose [&#39;mm_9&#39;] -&gt; [&#39;t_12&#39;]
    126/336: MatMul [&#39;t_15&#39;, &#39;view_19&#39;] -&gt; [&#39;mm_11&#39;]
    127/336: Constant [] -&gt; [&#39;aten_view_282_size_0&#39;]
    128/336: Reshape [&#39;mm_12&#39;, &#39;aten_view_282_size_0&#39;] -&gt; [&#39;view_31&#39;]
    129/336: Transpose [&#39;t_12&#39;] -&gt; [&#39;t_14&#39;]
    130/336: Transpose [&#39;mm_11&#39;] -&gt; [&#39;t_16&#39;]
    131/336: Constant [] -&gt; [&#39;alpha__5&#39;]
    132/336: Mul [&#39;view_31&#39;, &#39;alpha__5&#39;] -&gt; [&#39;other_1__5&#39;]
    133/336: Add [&#39;view_29&#39;, &#39;other_1__5&#39;] -&gt; [&#39;add_11&#39;]
    134/336: Transpose [&#39;t_16&#39;] -&gt; [&#39;t_18&#39;]
    135/336: Mul [&#39;add_11&#39;, &#39;primals_2&#39;] -&gt; [&#39;mul_25&#39;]
    136/336: Mul [&#39;add_11&#39;, &#39;mul_6&#39;] -&gt; [&#39;mul_26&#39;]
    137/336: Mul [&#39;mul_25&#39;, &#39;add_5&#39;] -&gt; [&#39;mul_27&#39;]
    138/336: Mul [&#39;mul_25&#39;, &#39;rsqrt_1&#39;] -&gt; [&#39;mul_28&#39;]
    139/336: Constant [] -&gt; [&#39;_val_205&#39;]
    140/336: ReduceSum [&#39;mul_26&#39;, &#39;_val_205&#39;] -&gt; [&#39;sum_3&#39;]
    141/336: Constant [] -&gt; [&#39;_val_207&#39;]
    142/336: ReduceSum [&#39;mul_27&#39;, &#39;_val_207&#39;] -&gt; [&#39;sum_4&#39;]
    143/336: Constant [] -&gt; [&#39;alpha__6&#39;]
    144/336: Mul [&#39;mul_28&#39;, &#39;alpha__6&#39;] -&gt; [&#39;other_1__6&#39;]
    145/336: Add [&#39;add_9&#39;, &#39;other_1__6&#39;] -&gt; [&#39;add_12&#39;]
    146/336: Constant [] -&gt; [&#39;aten_view_297_size_0&#39;]
    147/336: Reshape [&#39;sum_3&#39;, &#39;aten_view_297_size_0&#39;] -&gt; [&#39;view_32&#39;]
    148/336: Constant [] -&gt; [&#39;_val_212&#39;]
    149/336: Mul [&#39;sum_4&#39;, &#39;_val_212&#39;] -&gt; [&#39;mul_29&#39;]
    150/336: Mul [&#39;mul_29&#39;, &#39;pow_6&#39;] -&gt; [&#39;mul_30&#39;]
    151/336: Constant [] -&gt; [&#39;aten_expand_302_size_1&#39;]
    152/336: Expand [&#39;mul_30&#39;, &#39;aten_expand_302_size_1&#39;] -&gt; [&#39;expand_8&#39;]
    153/336: Constant [] -&gt; [&#39;scalar_tensor_default_5&#39;]
    154/336: Div [&#39;expand_8&#39;, &#39;scalar_tensor_default_5&#39;] -&gt; [&#39;div_2&#39;]
    155/336: Mul [&#39;div_2&#39;, &#39;mul_31&#39;] -&gt; [&#39;mul_32&#39;]
    156/336: Constant [] -&gt; [&#39;alpha__7&#39;]
    157/336: Mul [&#39;mul_32&#39;, &#39;alpha__7&#39;] -&gt; [&#39;other_1__7&#39;]
    158/336: Add [&#39;add_12&#39;, &#39;other_1__7&#39;] -&gt; [&#39;add_13&#39;]
    159/336: Constant [] -&gt; [&#39;aten_view_309_size_0&#39;]
    160/336: Reshape [&#39;add_13&#39;, &#39;aten_view_309_size_0&#39;] -&gt; [&#39;view_33&#39;]
    161/336: Transpose [&#39;view_33&#39;] -&gt; [&#39;t_19&#39;]
    162/336: MatMul [&#39;view_33&#39;, &#39;t_21&#39;] -&gt; [&#39;mm_14&#39;]
    163/336: MatMul [&#39;t_19&#39;, &#39;view_17&#39;] -&gt; [&#39;mm_13&#39;]
    164/336: Constant [] -&gt; [&#39;aten_view_314_size_0&#39;]
    165/336: Reshape [&#39;mm_14&#39;, &#39;aten_view_314_size_0&#39;] -&gt; [&#39;view_34&#39;]
    166/336: Transpose [&#39;mm_13&#39;] -&gt; [&#39;t_20&#39;]
    167/336: Constant [] -&gt; [&#39;aten_view_317_size_0&#39;]
    168/336: Reshape [&#39;view_34&#39;, &#39;aten_view_317_size_0&#39;] -&gt; [&#39;view_35&#39;]
    169/336: Transpose [&#39;t_20&#39;] -&gt; [&#39;t_22&#39;]
    170/336: Transpose [&#39;view_35&#39;] -&gt; [&#39;transpose_5&#39;]
    171/336: Constant [] -&gt; [&#39;aten_view_322_size_0&#39;]
    172/336: Reshape [&#39;transpose_5&#39;, &#39;aten_view_322_size_0&#39;] -&gt; [&#39;view_36&#39;]
    173/336: MatMul [&#39;transpose_6&#39;, &#39;view_36&#39;] -&gt; [&#39;bmm_2&#39;]
    174/336: MatMul [&#39;view_36&#39;, &#39;transpose_7&#39;] -&gt; [&#39;bmm_3&#39;]
    175/336: Constant [] -&gt; [&#39;aten_view_326_size_0&#39;]
    176/336: Reshape [&#39;bmm_2&#39;, &#39;aten_view_326_size_0&#39;] -&gt; [&#39;view_37&#39;]
    177/336: Constant [] -&gt; [&#39;aten_view_328_size_0&#39;]
    178/336: Reshape [&#39;bmm_3&#39;, &#39;aten_view_328_size_0&#39;] -&gt; [&#39;view_38&#39;]
    179/336: Constant [] -&gt; [&#39;alpha__8&#39;]
    180/336: Mul [&#39;view_37&#39;, &#39;alpha__8&#39;] -&gt; [&#39;other_1__8&#39;]
    181/336: Add [&#39;tangents_3&#39;, &#39;other_1__8&#39;] -&gt; [&#39;add_14&#39;]
    182/336: Mul [&#39;view_38&#39;, &#39;detach_13&#39;] -&gt; [&#39;mul_33&#39;]
    183/336: Transpose [&#39;add_14&#39;] -&gt; [&#39;transpose_11&#39;]
    184/336: Constant [] -&gt; [&#39;_val_246&#39;]
    185/336: ReduceSum [&#39;mul_33&#39;, &#39;_val_246&#39;] -&gt; [&#39;sum_5&#39;]
    186/336: Mul [&#39;detach_13&#39;, &#39;sum_5&#39;] -&gt; [&#39;mul_34&#39;]
    187/336: Constant [] -&gt; [&#39;aten_view_337_size_0&#39;]
    188/336: Reshape [&#39;transpose_11&#39;, &#39;aten_view_337_size_0&#39;] -&gt; [&#39;view_42&#39;]
    189/336: Constant [] -&gt; [&#39;alpha__9&#39;]
    190/336: Mul [&#39;mul_34&#39;, &#39;alpha__9&#39;] -&gt; [&#39;other_1__9&#39;]
    191/336: Sub [&#39;mul_33&#39;, &#39;other_1__9&#39;] -&gt; [&#39;sub_1&#39;]
    192/336: Constant [] -&gt; [&#39;aten_view_340_size_0&#39;]
    193/336: Reshape [&#39;view_42&#39;, &#39;aten_view_340_size_0&#39;] -&gt; [&#39;view_45&#39;]
    194/336: Constant [] -&gt; [&#39;_val_255&#39;]
    195/336: Div [&#39;sub_1&#39;, &#39;_val_255&#39;] -&gt; [&#39;div_3&#39;]
    196/336: Transpose [&#39;view_45&#39;] -&gt; [&#39;t_23&#39;]
    197/336: MatMul [&#39;view_45&#39;, &#39;t_25&#39;] -&gt; [&#39;mm_16&#39;]
    198/336: Constant [] -&gt; [&#39;aten_view_346_size_0&#39;]
    199/336: Reshape [&#39;div_3&#39;, &#39;aten_view_346_size_0&#39;] -&gt; [&#39;view_39&#39;]
    200/336: MatMul [&#39;t_23&#39;, &#39;view_1&#39;] -&gt; [&#39;mm_15&#39;]
    201/336: Constant [] -&gt; [&#39;aten_view_349_size_0&#39;]
    202/336: Reshape [&#39;mm_16&#39;, &#39;aten_view_349_size_0&#39;] -&gt; [&#39;view_46&#39;]
    203/336: MatMul [&#39;transpose_8&#39;, &#39;view_39&#39;] -&gt; [&#39;bmm_4&#39;]
    204/336: MatMul [&#39;view_39&#39;, &#39;transpose_9&#39;] -&gt; [&#39;bmm_5&#39;]
    205/336: Transpose [&#39;mm_15&#39;] -&gt; [&#39;t_24&#39;]
    206/336: Constant [] -&gt; [&#39;aten_view_354_size_0&#39;]
    207/336: Reshape [&#39;bmm_4&#39;, &#39;aten_view_354_size_0&#39;] -&gt; [&#39;view_40&#39;]
    208/336: Constant [] -&gt; [&#39;aten_view_356_size_0&#39;]
    209/336: Reshape [&#39;bmm_5&#39;, &#39;aten_view_356_size_0&#39;] -&gt; [&#39;view_41&#39;]
    210/336: Transpose [&#39;t_24&#39;] -&gt; [&#39;t_26&#39;]
    211/336: Transpose [&#39;view_40&#39;] -&gt; [&#39;transpose_10&#39;]
    212/336: Mul [&#39;view_41&#39;, &#39;unsqueeze_8&#39;] -&gt; [&#39;mul_37&#39;]
    213/336: Mul [&#39;view_41&#39;, &#39;unsqueeze_7&#39;] -&gt; [&#39;mul_38&#39;]
    214/336: Constant [] -&gt; [&#39;alpha__10&#39;]
    215/336: Mul [&#39;transpose_10&#39;, &#39;alpha__10&#39;] -&gt; [&#39;other_1__10&#39;]
    216/336: Add [&#39;tangents_2&#39;, &#39;other_1__10&#39;] -&gt; [&#39;add_15&#39;]
    217/336: Constant [] -&gt; [&#39;_val_279&#39;]
    218/336: Constant [] -&gt; [&#39;_val_283&#39;]
    219/336: Constant [] -&gt; [&#39;_val_287&#39;]
    220/336: Constant [] -&gt; [&#39;_val_291&#39;]
    221/336: Slice [&#39;mul_37&#39;, &#39;_val_279&#39;, &#39;_val_283&#39;, &#39;_val_287&#39;, &#39;_val_291&#39;] -&gt; [&#39;slice_15&#39;]
    222/336: Constant [] -&gt; [&#39;_val_296&#39;]
    223/336: Constant [] -&gt; [&#39;_val_300&#39;]
    224/336: Constant [] -&gt; [&#39;_val_304&#39;]
    225/336: Constant [] -&gt; [&#39;_val_308&#39;]
    226/336: Slice [&#39;mul_37&#39;, &#39;_val_296&#39;, &#39;_val_300&#39;, &#39;_val_304&#39;, &#39;_val_308&#39;] -&gt; [&#39;slice_16&#39;]
    227/336: Mul [&#39;add_15&#39;, &#39;unsqueeze_8&#39;] -&gt; [&#39;mul_35&#39;]
    228/336: Mul [&#39;add_15&#39;, &#39;unsqueeze_7&#39;] -&gt; [&#39;mul_36&#39;]
    229/336: Neg [&#39;slice_15&#39;] -&gt; [&#39;neg_3&#39;]
    230/336: Constant [] -&gt; [&#39;_val_318&#39;]
    231/336: Constant [] -&gt; [&#39;_val_322&#39;]
    232/336: Constant [] -&gt; [&#39;_val_326&#39;]
    233/336: Constant [] -&gt; [&#39;_val_330&#39;]
    234/336: Slice [&#39;mul_35&#39;, &#39;_val_318&#39;, &#39;_val_322&#39;, &#39;_val_326&#39;, &#39;_val_330&#39;] -&gt; [&#39;slice_13&#39;]
    235/336: Constant [] -&gt; [&#39;_val_335&#39;]
    236/336: Constant [] -&gt; [&#39;_val_339&#39;]
    237/336: Constant [] -&gt; [&#39;_val_343&#39;]
    238/336: Constant [] -&gt; [&#39;_val_347&#39;]
    239/336: Slice [&#39;mul_35&#39;, &#39;_val_335&#39;, &#39;_val_339&#39;, &#39;_val_343&#39;, &#39;_val_347&#39;] -&gt; [&#39;slice_14&#39;]
    240/336: Constant [] -&gt; [&#39;_val_366&#39;]
    241/336: Transpose [&#39;slice_16&#39;] -&gt; [&#39;_val_367&#39;]
    242/336: Constant [] -&gt; [&#39;_val_368&#39;]
    243/336: ScatterND [&#39;_val_368&#39;, &#39;_val_366&#39;, &#39;_val_367&#39;] -&gt; [&#39;_val_369&#39;]
    244/336: Transpose [&#39;_val_369&#39;] -&gt; [&#39;slice_scatter_3&#39;]
    245/336: Neg [&#39;slice_13&#39;] -&gt; [&#39;neg_2&#39;]
    246/336: Constant [] -&gt; [&#39;_val_389&#39;]
    247/336: Transpose [&#39;neg_3&#39;] -&gt; [&#39;_val_390&#39;]
    248/336: Constant [] -&gt; [&#39;_val_391&#39;]
    249/336: ScatterND [&#39;_val_391&#39;, &#39;_val_389&#39;, &#39;_val_390&#39;] -&gt; [&#39;_val_392&#39;]
    250/336: Transpose [&#39;_val_392&#39;] -&gt; [&#39;slice_scatter_2&#39;]
    251/336: Constant [] -&gt; [&#39;_val_411&#39;]
    252/336: Transpose [&#39;slice_14&#39;] -&gt; [&#39;_val_412&#39;]
    253/336: Constant [] -&gt; [&#39;_val_413&#39;]
    254/336: ScatterND [&#39;_val_413&#39;, &#39;_val_411&#39;, &#39;_val_412&#39;] -&gt; [&#39;_val_414&#39;]
    255/336: Transpose [&#39;_val_414&#39;] -&gt; [&#39;slice_scatter_1&#39;]
    256/336: Constant [] -&gt; [&#39;alpha__11&#39;]
    257/336: Mul [&#39;slice_scatter_3&#39;, &#39;alpha__11&#39;] -&gt; [&#39;other_1__11&#39;]
    258/336: Add [&#39;slice_scatter_2&#39;, &#39;other_1__11&#39;] -&gt; [&#39;add_18&#39;]
    259/336: Constant [] -&gt; [&#39;_val_432&#39;]
    260/336: Transpose [&#39;neg_2&#39;] -&gt; [&#39;_val_433&#39;]
    261/336: Constant [] -&gt; [&#39;_val_434&#39;]
    262/336: ScatterND [&#39;_val_434&#39;, &#39;_val_432&#39;, &#39;_val_433&#39;] -&gt; [&#39;_val_435&#39;]
    263/336: Transpose [&#39;_val_435&#39;] -&gt; [&#39;slice_scatter&#39;]
    264/336: Constant [] -&gt; [&#39;alpha__12&#39;]
    265/336: Mul [&#39;mul_38&#39;, &#39;alpha__12&#39;] -&gt; [&#39;other_1__12&#39;]
    266/336: Add [&#39;add_18&#39;, &#39;other_1__12&#39;] -&gt; [&#39;add_19&#39;]
    267/336: Constant [] -&gt; [&#39;alpha__13&#39;]
    268/336: Mul [&#39;slice_scatter_1&#39;, &#39;alpha__13&#39;] -&gt; [&#39;other_1__13&#39;]
    269/336: Add [&#39;slice_scatter&#39;, &#39;other_1__13&#39;] -&gt; [&#39;add_16&#39;]
    270/336: Transpose [&#39;add_19&#39;] -&gt; [&#39;transpose_13&#39;]
    271/336: Constant [] -&gt; [&#39;alpha__14&#39;]
    272/336: Mul [&#39;mul_36&#39;, &#39;alpha__14&#39;] -&gt; [&#39;other_1__14&#39;]
    273/336: Add [&#39;add_16&#39;, &#39;other_1__14&#39;] -&gt; [&#39;add_17&#39;]
    274/336: Transpose [&#39;add_17&#39;] -&gt; [&#39;transpose_12&#39;]
    275/336: Constant [] -&gt; [&#39;aten_view_530_size_0&#39;]
    276/336: Reshape [&#39;transpose_13&#39;, &#39;aten_view_530_size_0&#39;] -&gt; [&#39;view_44&#39;]
    277/336: Constant [] -&gt; [&#39;aten_view_533_size_0&#39;]
    278/336: Reshape [&#39;view_44&#39;, &#39;aten_view_533_size_0&#39;] -&gt; [&#39;view_49&#39;]
    279/336: Constant [] -&gt; [&#39;aten_view_535_size_0&#39;]
    280/336: Reshape [&#39;transpose_12&#39;, &#39;aten_view_535_size_0&#39;] -&gt; [&#39;view_43&#39;]
    281/336: Transpose [&#39;view_49&#39;] -&gt; [&#39;t_31&#39;]
    282/336: MatMul [&#39;view_49&#39;, &#39;t_33&#39;] -&gt; [&#39;mm_20&#39;]
    283/336: Constant [] -&gt; [&#39;aten_view_539_size_0&#39;]
    284/336: Reshape [&#39;view_43&#39;, &#39;aten_view_539_size_0&#39;] -&gt; [&#39;view_47&#39;]
    285/336: MatMul [&#39;t_31&#39;, &#39;view_1&#39;] -&gt; [&#39;mm_19&#39;]
    286/336: Constant [] -&gt; [&#39;aten_view_542_size_0&#39;]
    287/336: Reshape [&#39;mm_20&#39;, &#39;aten_view_542_size_0&#39;] -&gt; [&#39;view_50&#39;]
    288/336: Transpose [&#39;view_47&#39;] -&gt; [&#39;t_27&#39;]
    289/336: MatMul [&#39;view_47&#39;, &#39;t_29&#39;] -&gt; [&#39;mm_18&#39;]
    290/336: Transpose [&#39;mm_19&#39;] -&gt; [&#39;t_32&#39;]
    291/336: MatMul [&#39;t_27&#39;, &#39;view_1&#39;] -&gt; [&#39;mm_17&#39;]
    292/336: Constant [] -&gt; [&#39;aten_view_548_size_0&#39;]
    293/336: Reshape [&#39;mm_18&#39;, &#39;aten_view_548_size_0&#39;] -&gt; [&#39;view_48&#39;]
    294/336: Transpose [&#39;t_32&#39;] -&gt; [&#39;t_34&#39;]
    295/336: Transpose [&#39;mm_17&#39;] -&gt; [&#39;t_28&#39;]
    296/336: Constant [] -&gt; [&#39;alpha__15&#39;]
    297/336: Mul [&#39;view_48&#39;, &#39;alpha__15&#39;] -&gt; [&#39;other_1__15&#39;]
    298/336: Add [&#39;view_46&#39;, &#39;other_1__15&#39;] -&gt; [&#39;add_20&#39;]
    299/336: Transpose [&#39;t_28&#39;] -&gt; [&#39;t_30&#39;]
    300/336: Constant [] -&gt; [&#39;alpha__16&#39;]
    301/336: Mul [&#39;view_50&#39;, &#39;alpha__16&#39;] -&gt; [&#39;other_1__16&#39;]
    302/336: Add [&#39;add_20&#39;, &#39;other_1__16&#39;] -&gt; [&#39;add_21&#39;]
    303/336: Mul [&#39;add_21&#39;, &#39;primals_1&#39;] -&gt; [&#39;mul_39&#39;]
    304/336: Mul [&#39;add_21&#39;, &#39;mul&#39;] -&gt; [&#39;mul_40&#39;]
    305/336: Mul [&#39;mul_39&#39;, &#39;embedding&#39;] -&gt; [&#39;mul_41&#39;]
    306/336: Mul [&#39;mul_39&#39;, &#39;rsqrt&#39;] -&gt; [&#39;mul_42&#39;]
    307/336: Constant [] -&gt; [&#39;_val_472&#39;]
    308/336: ReduceSum [&#39;mul_40&#39;, &#39;_val_472&#39;] -&gt; [&#39;sum_6&#39;]
    309/336: Constant [] -&gt; [&#39;_val_474&#39;]
    310/336: ReduceSum [&#39;mul_41&#39;, &#39;_val_474&#39;] -&gt; [&#39;sum_7&#39;]
    311/336: Constant [] -&gt; [&#39;alpha__17&#39;]
    312/336: Mul [&#39;mul_42&#39;, &#39;alpha__17&#39;] -&gt; [&#39;other_1__17&#39;]
    313/336: Add [&#39;add_13&#39;, &#39;other_1__17&#39;] -&gt; [&#39;add_22&#39;]
    314/336: Constant [] -&gt; [&#39;aten_view_564_size_0&#39;]
    315/336: Reshape [&#39;sum_6&#39;, &#39;aten_view_564_size_0&#39;] -&gt; [&#39;view_51&#39;]
    316/336: Constant [] -&gt; [&#39;_val_479&#39;]
    317/336: Mul [&#39;sum_7&#39;, &#39;_val_479&#39;] -&gt; [&#39;mul_43&#39;]
    318/336: Mul [&#39;mul_43&#39;, &#39;pow_8&#39;] -&gt; [&#39;mul_44&#39;]
    319/336: Constant [] -&gt; [&#39;aten_expand_569_size_1&#39;]
    320/336: Expand [&#39;mul_44&#39;, &#39;aten_expand_569_size_1&#39;] -&gt; [&#39;expand_9&#39;]
    321/336: Constant [] -&gt; [&#39;scalar_tensor_default_6&#39;]
    322/336: Div [&#39;expand_9&#39;, &#39;scalar_tensor_default_6&#39;] -&gt; [&#39;div_4&#39;]
    323/336: Mul [&#39;div_4&#39;, &#39;mul_45&#39;] -&gt; [&#39;mul_46&#39;]
    324/336: Constant [] -&gt; [&#39;alpha__18&#39;]
    325/336: Mul [&#39;mul_46&#39;, &#39;alpha__18&#39;] -&gt; [&#39;other_1__18&#39;]
    326/336: Add [&#39;add_22&#39;, &#39;other_1__18&#39;] -&gt; [&#39;add_23&#39;]
    327/336: Constant [] -&gt; [&#39;aten_masked_fill_576_value_cast&#39;]
    328/336: Where [&#39;unsqueeze_9&#39;, &#39;aten_masked_fill_576_value_cast&#39;, &#39;add_23&#39;] -&gt; [&#39;masked_fill_3&#39;]
    329/336: Constant [] -&gt; [&#39;_val_491&#39;]
    330/336: ConstantOfShape [&#39;_val_491&#39;] -&gt; [&#39;aten_new_zeros_578_result&#39;]
    331/336: SequenceConstruct [&#39;primals_14&#39;] -&gt; [&#39;493&#39;]
    332/336: Constant [] -&gt; [&#39;int64_0__19&#39;]
    333/336: SequenceAt [&#39;493&#39;, &#39;int64_0__19&#39;] -&gt; [&#39;index__19&#39;]
    334/336: Constant [] -&gt; [&#39;int64_m1_1d__19&#39;]
    335/336: Unsqueeze [&#39;index__19&#39;, &#39;int64_m1_1d__19&#39;] -&gt; [&#39;new_index__19&#39;]
    336/336: ScatterND [&#39;aten_new_zeros_578_result&#39;, &#39;new_index__19&#39;, &#39;masked_fill_3&#39;] -&gt; [&#39;_unsafe_index_put&#39;]
    
    NODES in {name!r}
    1/236: Transpose [&#39;primals_9&#39;] -&gt; [&#39;t_4&#39;]
    2/236: Transpose [&#39;primals_8&#39;] -&gt; [&#39;t_3&#39;]
    3/236: Transpose [&#39;primals_10&#39;] -&gt; [&#39;t_5&#39;]
    4/236: Transpose [&#39;primals_11&#39;] -&gt; [&#39;t_6&#39;]
    5/236: Constant [] -&gt; [&#39;_val_22&#39;]
    6/236: Constant [] -&gt; [&#39;_val_26&#39;]
    7/236: Constant [] -&gt; [&#39;_val_30&#39;]
    8/236: Constant [] -&gt; [&#39;_val_34&#39;]
    9/236: Slice [&#39;primals_12&#39;, &#39;_val_22&#39;, &#39;_val_26&#39;, &#39;_val_30&#39;, &#39;_val_34&#39;] -&gt; [&#39;slice_7&#39;]
    10/236: Gather [&#39;primals_4&#39;, &#39;primals_14&#39;] -&gt; [&#39;embedding&#39;]
    11/236: Constant [] -&gt; [&#39;_val_44&#39;]
    12/236: Constant [] -&gt; [&#39;_val_45&#39;]
    13/236: Constant [] -&gt; [&#39;size_0__1&#39;]
    14/236: Constant [] -&gt; [&#39;fill_value_1__1&#39;]
    15/236: Expand [&#39;fill_value_1__1&#39;, &#39;size_0__1&#39;] -&gt; [&#39;full&#39;]
    16/236: Constant [] -&gt; [&#39;_val_58&#39;]
    17/236: Constant [] -&gt; [&#39;_val_62&#39;]
    18/236: Constant [] -&gt; [&#39;_val_66&#39;]
    19/236: Constant [] -&gt; [&#39;_val_70&#39;]
    20/236: Slice [&#39;primals_15&#39;, &#39;_val_58&#39;, &#39;_val_62&#39;, &#39;_val_66&#39;, &#39;_val_70&#39;] -&gt; [&#39;slice_3&#39;]
    21/236: Constant [] -&gt; [&#39;_val_75&#39;]
    22/236: Constant [] -&gt; [&#39;_val_79&#39;]
    23/236: Constant [] -&gt; [&#39;_val_83&#39;]
    24/236: Constant [] -&gt; [&#39;_val_87&#39;]
    25/236: Slice [&#39;primals_13&#39;, &#39;_val_75&#39;, &#39;_val_79&#39;, &#39;_val_83&#39;, &#39;_val_87&#39;] -&gt; [&#39;slice_8&#39;]
    26/236: Transpose [&#39;primals_5&#39;] -&gt; [&#39;t&#39;]
    27/236: Transpose [&#39;primals_6&#39;] -&gt; [&#39;t_1&#39;]
    28/236: Transpose [&#39;primals_7&#39;] -&gt; [&#39;t_2&#39;]
    29/236: Transpose [&#39;t_4&#39;] -&gt; [&#39;t_17&#39;]
    30/236: Transpose [&#39;t_3&#39;] -&gt; [&#39;t_21&#39;]
    31/236: Transpose [&#39;t_5&#39;] -&gt; [&#39;t_13&#39;]
    32/236: Transpose [&#39;t_6&#39;] -&gt; [&#39;t_9&#39;]
    33/236: Constant [] -&gt; [&#39;unsqueeze&#39;]
    34/236: Constant [] -&gt; [&#39;_val_97&#39;]
    35/236: Pow [&#39;embedding&#39;, &#39;_val_97&#39;] -&gt; [&#39;pow_1&#39;]
    36/236: Constant [] -&gt; [&#39;aten_unsqueeze_181_dim_0&#39;]
    37/236: Unsqueeze [&#39;slice_3&#39;, &#39;aten_unsqueeze_181_dim_0&#39;] -&gt; [&#39;unsqueeze_3&#39;]
    38/236: Transpose [&#39;t&#39;] -&gt; [&#39;t_33&#39;]
    39/236: Transpose [&#39;t_1&#39;] -&gt; [&#39;t_29&#39;]
    40/236: Transpose [&#39;t_2&#39;] -&gt; [&#39;t_25&#39;]
    41/236: Transpose [&#39;slice_7&#39;] -&gt; [&#39;_val_105&#39;]
    42/236: Constant [] -&gt; [&#39;_val_111&#39;]
    43/236: GatherND [&#39;_val_105&#39;, &#39;_val_111&#39;] -&gt; [&#39;_val_112&#39;]
    44/236: Transpose [&#39;_val_112&#39;] -&gt; [&#39;index&#39;]
    45/236: Transpose [&#39;slice_8&#39;] -&gt; [&#39;_val_114&#39;]
    46/236: Constant [] -&gt; [&#39;_val_120&#39;]
    47/236: GatherND [&#39;_val_114&#39;, &#39;_val_120&#39;] -&gt; [&#39;_val_121&#39;]
    48/236: Transpose [&#39;_val_121&#39;] -&gt; [&#39;index_1&#39;]
    49/236: Constant [] -&gt; [&#39;_val_123&#39;]
    50/236: ReduceMean [&#39;pow_1&#39;, &#39;_val_123&#39;] -&gt; [&#39;mean&#39;]
    51/236: Constant [] -&gt; [&#39;aten_unsqueeze_207_dim_0&#39;]
    52/236: Unsqueeze [&#39;unsqueeze_3&#39;, &#39;aten_unsqueeze_207_dim_0&#39;] -&gt; [&#39;unsqueeze_4&#39;]
    53/236: Constant [] -&gt; [&#39;aten_unsqueeze_208_dim_0&#39;]
    54/236: Unsqueeze [&#39;index&#39;, &#39;aten_unsqueeze_208_dim_0&#39;] -&gt; [&#39;unsqueeze_7&#39;]
    55/236: Constant [] -&gt; [&#39;aten_unsqueeze_209_dim_0&#39;]
    56/236: Unsqueeze [&#39;index_1&#39;, &#39;aten_unsqueeze_209_dim_0&#39;] -&gt; [&#39;unsqueeze_8&#39;]
    57/236: Constant [] -&gt; [&#39;aten_add_211_other_1&#39;]
    58/236: Add [&#39;mean&#39;, &#39;aten_add_211_other_1&#39;] -&gt; [&#39;add_1&#39;]
    59/236: Constant [] -&gt; [&#39;lt&#39;]
    60/236: Constant [] -&gt; [&#39;_val_136&#39;]
    61/236: Constant [] -&gt; [&#39;_val_140&#39;]
    62/236: Constant [] -&gt; [&#39;_val_144&#39;]
    63/236: Constant [] -&gt; [&#39;_val_148&#39;]
    64/236: Slice [&#39;unsqueeze_4&#39;, &#39;_val_136&#39;, &#39;_val_140&#39;, &#39;_val_144&#39;, &#39;_val_148&#39;] -&gt; [&#39;slice_4&#39;]
    65/236: Sqrt [&#39;add_1&#39;] -&gt; [&#39;aten_rsqrt_230_tmp&#39;]
    66/236: Reciprocal [&#39;aten_rsqrt_230_tmp&#39;] -&gt; [&#39;rsqrt&#39;]
    67/236: Constant [] -&gt; [&#39;aten_masked_fill_232_value_cast&#39;]
    68/236: Where [&#39;lt&#39;, &#39;aten_masked_fill_232_value_cast&#39;, &#39;full&#39;] -&gt; [&#39;masked_fill&#39;]
    69/236: Constant [] -&gt; [&#39;aten_expand_234_size_1&#39;]
    70/236: Expand [&#39;slice_4&#39;, &#39;aten_expand_234_size_1&#39;] -&gt; [&#39;expand_1&#39;]
    71/236: Mul [&#39;embedding&#39;, &#39;rsqrt&#39;] -&gt; [&#39;mul&#39;]
    72/236: Constant [] -&gt; [&#39;aten_unsqueeze_236_dim_0&#39;]
    73/236: Unsqueeze [&#39;masked_fill&#39;, &#39;aten_unsqueeze_236_dim_0&#39;] -&gt; [&#39;unsqueeze_5&#39;]
    74/236: Constant [] -&gt; [&#39;_val_157&#39;]
    75/236: Constant [] -&gt; [&#39;alpha__2&#39;]
    76/236: Mul [&#39;expand_1&#39;, &#39;alpha__2&#39;] -&gt; [&#39;tmp__2&#39;]
    77/236: Sub [&#39;_val_157&#39;, &#39;tmp__2&#39;] -&gt; [&#39;rsub&#39;]
    78/236: Mul [&#39;primals_1&#39;, &#39;mul&#39;] -&gt; [&#39;mul_1&#39;]
    79/236: Constant [] -&gt; [&#39;aten_unsqueeze_240_dim_0&#39;]
    80/236: Unsqueeze [&#39;unsqueeze_5&#39;, &#39;aten_unsqueeze_240_dim_0&#39;] -&gt; [&#39;unsqueeze_6&#39;]
    81/236: Cast [&#39;rsub&#39;] -&gt; [&#39;_to_copy&#39;]
    82/236: Constant [] -&gt; [&#39;aten_view_243_size_0&#39;]
    83/236: Reshape [&#39;mul_1&#39;, &#39;aten_view_243_size_0&#39;] -&gt; [&#39;view_1&#39;]
    84/236: Constant [] -&gt; [&#39;_val_167&#39;]
    85/236: Constant [] -&gt; [&#39;_val_171&#39;]
    86/236: Constant [] -&gt; [&#39;_val_175&#39;]
    87/236: Constant [] -&gt; [&#39;_val_179&#39;]
    88/236: Slice [&#39;unsqueeze_6&#39;, &#39;_val_167&#39;, &#39;_val_171&#39;, &#39;_val_175&#39;, &#39;_val_179&#39;] -&gt; [&#39;slice_5&#39;]
    89/236: Constant [] -&gt; [&#39;_val_181&#39;]
    90/236: Where [&#39;_to_copy&#39;, &#39;_val_181&#39;, &#39;rsub&#39;] -&gt; [&#39;masked_fill_1&#39;]
    91/236: MatMul [&#39;view_1&#39;, &#39;t&#39;] -&gt; [&#39;mm&#39;]
    92/236: MatMul [&#39;view_1&#39;, &#39;t_1&#39;] -&gt; [&#39;mm_1&#39;]
    93/236: MatMul [&#39;view_1&#39;, &#39;t_2&#39;] -&gt; [&#39;mm_2&#39;]
    94/236: Constant [] -&gt; [&#39;_val_189&#39;]
    95/236: Constant [] -&gt; [&#39;_val_193&#39;]
    96/236: Constant [] -&gt; [&#39;_val_197&#39;]
    97/236: Constant [] -&gt; [&#39;_val_201&#39;]
    98/236: Slice [&#39;slice_5&#39;, &#39;_val_189&#39;, &#39;_val_193&#39;, &#39;_val_197&#39;, &#39;_val_201&#39;] -&gt; [&#39;slice_6&#39;]
    99/236: Cast [&#39;masked_fill_1&#39;] -&gt; [&#39;_to_copy_1&#39;]
    100/236: Constant [] -&gt; [&#39;aten_view_285_size_0&#39;]
    101/236: Reshape [&#39;mm&#39;, &#39;aten_view_285_size_0&#39;] -&gt; [&#39;view_2&#39;]
    102/236: Constant [] -&gt; [&#39;aten_view_287_size_0&#39;]
    103/236: Reshape [&#39;mm_1&#39;, &#39;aten_view_287_size_0&#39;] -&gt; [&#39;view_4&#39;]
    104/236: Constant [] -&gt; [&#39;aten_view_289_size_0&#39;]
    105/236: Reshape [&#39;mm_2&#39;, &#39;aten_view_289_size_0&#39;] -&gt; [&#39;view_6&#39;]
    106/236: Constant [] -&gt; [&#39;aten_expand_291_size_1&#39;]
    107/236: Expand [&#39;slice_6&#39;, &#39;aten_expand_291_size_1&#39;] -&gt; [&#39;expand_2&#39;]
    108/236: Constant [] -&gt; [&#39;aten_view_293_size_0&#39;]
    109/236: Reshape [&#39;view_2&#39;, &#39;aten_view_293_size_0&#39;] -&gt; [&#39;view_7&#39;]
    110/236: Constant [] -&gt; [&#39;aten_view_295_size_0&#39;]
    111/236: Reshape [&#39;view_4&#39;, &#39;aten_view_295_size_0&#39;] -&gt; [&#39;view_8&#39;]
    112/236: Constant [] -&gt; [&#39;aten_view_297_size_0&#39;]
    113/236: Reshape [&#39;view_6&#39;, &#39;aten_view_297_size_0&#39;] -&gt; [&#39;view_9&#39;]
    114/236: Constant [] -&gt; [&#39;_val_218&#39;]
    115/236: Where [&#39;_to_copy_1&#39;, &#39;_val_218&#39;, &#39;expand_2&#39;] -&gt; [&#39;masked_fill_2&#39;]
    116/236: Transpose [&#39;view_7&#39;] -&gt; [&#39;transpose&#39;]
    117/236: Transpose [&#39;view_8&#39;] -&gt; [&#39;transpose_1&#39;]
    118/236: Transpose [&#39;view_9&#39;] -&gt; [&#39;transpose_2&#39;]
    119/236: Mul [&#39;transpose&#39;, &#39;unsqueeze_7&#39;] -&gt; [&#39;mul_2&#39;]
    120/236: Constant [] -&gt; [&#39;_val_227&#39;]
    121/236: Constant [] -&gt; [&#39;_val_231&#39;]
    122/236: Constant [] -&gt; [&#39;_val_235&#39;]
    123/236: Constant [] -&gt; [&#39;_val_239&#39;]
    124/236: Slice [&#39;transpose&#39;, &#39;_val_227&#39;, &#39;_val_231&#39;, &#39;_val_235&#39;, &#39;_val_239&#39;] -&gt; [&#39;slice_9&#39;]
    125/236: Constant [] -&gt; [&#39;_val_244&#39;]
    126/236: Constant [] -&gt; [&#39;_val_248&#39;]
    127/236: Constant [] -&gt; [&#39;_val_252&#39;]
    128/236: Constant [] -&gt; [&#39;_val_256&#39;]
    129/236: Slice [&#39;transpose&#39;, &#39;_val_244&#39;, &#39;_val_248&#39;, &#39;_val_252&#39;, &#39;_val_256&#39;] -&gt; [&#39;slice_10&#39;]
    130/236: Mul [&#39;transpose_1&#39;, &#39;unsqueeze_7&#39;] -&gt; [&#39;mul_4&#39;]
    131/236: Constant [] -&gt; [&#39;_val_262&#39;]
    132/236: Constant [] -&gt; [&#39;_val_266&#39;]
    133/236: Constant [] -&gt; [&#39;_val_270&#39;]
    134/236: Constant [] -&gt; [&#39;_val_274&#39;]
    135/236: Slice [&#39;transpose_1&#39;, &#39;_val_262&#39;, &#39;_val_266&#39;, &#39;_val_270&#39;, &#39;_val_274&#39;] -&gt; [&#39;slice_11&#39;]
    136/236: Constant [] -&gt; [&#39;_val_279&#39;]
    137/236: Constant [] -&gt; [&#39;_val_283&#39;]
    138/236: Constant [] -&gt; [&#39;_val_287&#39;]
    139/236: Constant [] -&gt; [&#39;_val_291&#39;]
    140/236: Slice [&#39;transpose_1&#39;, &#39;_val_279&#39;, &#39;_val_283&#39;, &#39;_val_287&#39;, &#39;_val_291&#39;] -&gt; [&#39;slice_12&#39;]
    141/236: Constant [] -&gt; [&#39;aten_expand_374_size_1&#39;]
    142/236: Expand [&#39;transpose_2&#39;, &#39;aten_expand_374_size_1&#39;] -&gt; [&#39;expand_6&#39;]
    143/236: Neg [&#39;slice_10&#39;] -&gt; [&#39;neg&#39;]
    144/236: Neg [&#39;slice_12&#39;] -&gt; [&#39;neg_1&#39;]
    145/236: Concat [&#39;neg&#39;, &#39;slice_9&#39;] -&gt; [&#39;cat&#39;]
    146/236: Concat [&#39;neg_1&#39;, &#39;slice_11&#39;] -&gt; [&#39;cat_1&#39;]
    147/236: Constant [] -&gt; [&#39;aten_view_383_size_0&#39;]
    148/236: Reshape [&#39;expand_6&#39;, &#39;aten_view_383_size_0&#39;] -&gt; [&#39;view_14&#39;]
    149/236: Mul [&#39;cat&#39;, &#39;unsqueeze_8&#39;] -&gt; [&#39;mul_3&#39;]
    150/236: Mul [&#39;cat_1&#39;, &#39;unsqueeze_8&#39;] -&gt; [&#39;mul_5&#39;]
    151/236: Transpose [&#39;view_14&#39;] -&gt; [&#39;transpose_7&#39;]
    152/236: Constant [] -&gt; [&#39;alpha__3&#39;]
    153/236: Mul [&#39;mul_3&#39;, &#39;alpha__3&#39;] -&gt; [&#39;other_1__3&#39;]
    154/236: Add [&#39;mul_2&#39;, &#39;other_1__3&#39;] -&gt; [&#39;add_2&#39;]
    155/236: Constant [] -&gt; [&#39;alpha__4&#39;]
    156/236: Mul [&#39;mul_5&#39;, &#39;alpha__4&#39;] -&gt; [&#39;other_1__4&#39;]
    157/236: Add [&#39;mul_4&#39;, &#39;other_1__4&#39;] -&gt; [&#39;add_3&#39;]
    158/236: Constant [] -&gt; [&#39;aten_expand_390_size_1&#39;]
    159/236: Expand [&#39;add_2&#39;, &#39;aten_expand_390_size_1&#39;] -&gt; [&#39;expand_3&#39;]
    160/236: Transpose [&#39;add_3&#39;] -&gt; [&#39;transpose_3&#39;]
    161/236: Constant [] -&gt; [&#39;aten_expand_394_size_1&#39;]
    162/236: Expand [&#39;transpose_3&#39;, &#39;aten_expand_394_size_1&#39;] -&gt; [&#39;expand_4&#39;]
    163/236: Constant [] -&gt; [&#39;aten_view_396_size_0&#39;]
    164/236: Reshape [&#39;expand_3&#39;, &#39;aten_view_396_size_0&#39;] -&gt; [&#39;view_10&#39;]
    165/236: Transpose [&#39;view_10&#39;] -&gt; [&#39;transpose_8&#39;]
    166/236: Constant [] -&gt; [&#39;aten_view_400_size_0&#39;]
    167/236: Reshape [&#39;expand_4&#39;, &#39;aten_view_400_size_0&#39;] -&gt; [&#39;view_11&#39;]
    168/236: MatMul [&#39;view_10&#39;, &#39;view_11&#39;] -&gt; [&#39;bmm&#39;]
    169/236: Transpose [&#39;view_11&#39;] -&gt; [&#39;transpose_9&#39;]
    170/236: Constant [] -&gt; [&#39;aten_view_404_size_0&#39;]
    171/236: Reshape [&#39;bmm&#39;, &#39;aten_view_404_size_0&#39;] -&gt; [&#39;view_12&#39;]
    172/236: Constant [] -&gt; [&#39;_val_325&#39;]
    173/236: Div [&#39;view_12&#39;, &#39;_val_325&#39;] -&gt; [&#39;div&#39;]
    174/236: Constant [] -&gt; [&#39;alpha__5&#39;]
    175/236: Mul [&#39;masked_fill_2&#39;, &#39;alpha__5&#39;] -&gt; [&#39;other_1__5&#39;]
    176/236: Add [&#39;div&#39;, &#39;other_1__5&#39;] -&gt; [&#39;add_4&#39;]
    177/236: Softmax [&#39;add_4&#39;] -&gt; [&#39;_softmax&#39;]
    178/236: Constant [] -&gt; [&#39;aten_expand_413_size_1&#39;]
    179/236: Expand [&#39;_softmax&#39;, &#39;aten_expand_413_size_1&#39;] -&gt; [&#39;expand_5&#39;]
    180/236: Constant [] -&gt; [&#39;aten_view_416_size_0&#39;]
    181/236: Reshape [&#39;expand_5&#39;, &#39;aten_view_416_size_0&#39;] -&gt; [&#39;view_13&#39;]
    182/236: Identity [&#39;_softmax&#39;] -&gt; [&#39;detach_13&#39;]
    183/236: MatMul [&#39;view_13&#39;, &#39;view_14&#39;] -&gt; [&#39;bmm_1&#39;]
    184/236: Transpose [&#39;view_13&#39;] -&gt; [&#39;transpose_6&#39;]
    185/236: Constant [] -&gt; [&#39;aten_view_421_size_0&#39;]
    186/236: Reshape [&#39;bmm_1&#39;, &#39;aten_view_421_size_0&#39;] -&gt; [&#39;view_15&#39;]
    187/236: Transpose [&#39;view_15&#39;] -&gt; [&#39;transpose_4&#39;]
    188/236: Constant [] -&gt; [&#39;aten_view_425_size_0&#39;]
    189/236: Reshape [&#39;transpose_4&#39;, &#39;aten_view_425_size_0&#39;] -&gt; [&#39;view_16&#39;]
    190/236: Constant [] -&gt; [&#39;aten_view_427_size_0&#39;]
    191/236: Reshape [&#39;view_16&#39;, &#39;aten_view_427_size_0&#39;] -&gt; [&#39;view_17&#39;]
    192/236: MatMul [&#39;view_17&#39;, &#39;t_3&#39;] -&gt; [&#39;mm_3&#39;]
    193/236: Constant [] -&gt; [&#39;aten_view_430_size_0&#39;]
    194/236: Reshape [&#39;mm_3&#39;, &#39;aten_view_430_size_0&#39;] -&gt; [&#39;view_18&#39;]
    195/236: Constant [] -&gt; [&#39;alpha__6&#39;]
    196/236: Mul [&#39;view_18&#39;, &#39;alpha__6&#39;] -&gt; [&#39;other_1__6&#39;]
    197/236: Add [&#39;embedding&#39;, &#39;other_1__6&#39;] -&gt; [&#39;add_5&#39;]
    198/236: Constant [] -&gt; [&#39;_val_352&#39;]
    199/236: Pow [&#39;add_5&#39;, &#39;_val_352&#39;] -&gt; [&#39;pow_2&#39;]
    200/236: Constant [] -&gt; [&#39;_val_354&#39;]
    201/236: ReduceMean [&#39;pow_2&#39;, &#39;_val_354&#39;] -&gt; [&#39;mean_1&#39;]
    202/236: Constant [] -&gt; [&#39;aten_add_437_other_1&#39;]
    203/236: Add [&#39;mean_1&#39;, &#39;aten_add_437_other_1&#39;] -&gt; [&#39;add_6&#39;]
    204/236: Sqrt [&#39;add_6&#39;] -&gt; [&#39;aten_rsqrt_438_tmp&#39;]
    205/236: Reciprocal [&#39;aten_rsqrt_438_tmp&#39;] -&gt; [&#39;rsqrt_1&#39;]
    206/236: Mul [&#39;add_5&#39;, &#39;rsqrt_1&#39;] -&gt; [&#39;mul_6&#39;]
    207/236: Mul [&#39;primals_2&#39;, &#39;mul_6&#39;] -&gt; [&#39;mul_7&#39;]
    208/236: Constant [] -&gt; [&#39;aten_view_442_size_0&#39;]
    209/236: Reshape [&#39;mul_7&#39;, &#39;aten_view_442_size_0&#39;] -&gt; [&#39;view_19&#39;]
    210/236: MatMul [&#39;view_19&#39;, &#39;t_4&#39;] -&gt; [&#39;mm_4&#39;]
    211/236: MatMul [&#39;view_19&#39;, &#39;t_5&#39;] -&gt; [&#39;mm_5&#39;]
    212/236: Constant [] -&gt; [&#39;aten_view_446_size_0&#39;]
    213/236: Reshape [&#39;mm_4&#39;, &#39;aten_view_446_size_0&#39;] -&gt; [&#39;view_20&#39;]
    214/236: Constant [] -&gt; [&#39;aten_view_448_size_0&#39;]
    215/236: Reshape [&#39;mm_5&#39;, &#39;aten_view_448_size_0&#39;] -&gt; [&#39;view_22&#39;]
    216/236: Sigmoid [&#39;view_20&#39;] -&gt; [&#39;sigmoid&#39;]
    217/236: Mul [&#39;view_20&#39;, &#39;sigmoid&#39;] -&gt; [&#39;mul_8&#39;]
    218/236: Mul [&#39;mul_8&#39;, &#39;view_22&#39;] -&gt; [&#39;mul_9&#39;]
    219/236: Constant [] -&gt; [&#39;aten_view_453_size_0&#39;]
    220/236: Reshape [&#39;mul_9&#39;, &#39;aten_view_453_size_0&#39;] -&gt; [&#39;view_23&#39;]
    221/236: MatMul [&#39;view_23&#39;, &#39;t_6&#39;] -&gt; [&#39;mm_6&#39;]
    222/236: Constant [] -&gt; [&#39;aten_view_456_size_0&#39;]
    223/236: Reshape [&#39;mm_6&#39;, &#39;aten_view_456_size_0&#39;] -&gt; [&#39;view_24&#39;]
    224/236: Constant [] -&gt; [&#39;alpha__7&#39;]
    225/236: Mul [&#39;view_24&#39;, &#39;alpha__7&#39;] -&gt; [&#39;other_1__7&#39;]
    226/236: Add [&#39;add_5&#39;, &#39;other_1__7&#39;] -&gt; [&#39;add_7&#39;]
    227/236: Constant [] -&gt; [&#39;_val_378&#39;]
    228/236: Pow [&#39;add_7&#39;, &#39;_val_378&#39;] -&gt; [&#39;pow_3&#39;]
    229/236: Constant [] -&gt; [&#39;_val_380&#39;]
    230/236: ReduceMean [&#39;pow_3&#39;, &#39;_val_380&#39;] -&gt; [&#39;mean_2&#39;]
    231/236: Constant [] -&gt; [&#39;aten_add_463_other_1&#39;]
    232/236: Add [&#39;mean_2&#39;, &#39;aten_add_463_other_1&#39;] -&gt; [&#39;add_8&#39;]
    233/236: Sqrt [&#39;add_8&#39;] -&gt; [&#39;aten_rsqrt_464_tmp&#39;]
    234/236: Reciprocal [&#39;aten_rsqrt_464_tmp&#39;] -&gt; [&#39;rsqrt_2&#39;]
    235/236: Mul [&#39;add_7&#39;, &#39;rsqrt_2&#39;] -&gt; [&#39;mul_10&#39;]
    236/236: Mul [&#39;primals_3&#39;, &#39;mul_10&#39;] -&gt; [&#39;mul_11&#39;]
    [runpythonerror]
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    2024-04-18 15:54:00,319 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue return_val due to large size 4194304.
    2024-04-18 15:54:00,319 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue full due to large size 4194304.
    2024-04-18 15:54:00,361 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue masked_fill due to large size 4194304.
    2024-04-18 15:54:00,364 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue return_val due to large size 4194304.
    2024-04-18 15:54:00,364 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue unsqueeze_5 due to large size 4194304.
    2024-04-18 15:54:00,366 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue return_val due to large size 4194304.
    2024-04-18 15:54:00,366 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue unsqueeze_6 due to large size 4194304.
    2024-04-18 15:54:00,370 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue slice_5 due to large size 4194304.
    2024-04-18 15:54:00,374 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue slice_6 due to large size 4194304.
    2024-04-18 15:54:00,380 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue return_val due to large size 8388608.
    2024-04-18 15:54:00,380 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue expand_2 due to large size 8388608.
    2024-04-18 15:54:00,489 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue return_val due to large size 4194304.
    2024-04-18 15:54:00,489 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue full due to large size 4194304.
    2024-04-18 15:54:00,496 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue masked_fill due to large size 4194304.
    2024-04-18 15:54:00,497 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue unsqueeze_5 due to large size 4194304.
    2024-04-18 15:54:00,498 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue unsqueeze_6 due to large size 4194304.
    2024-04-18 15:54:00,499 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue slice_5 due to large size 4194304.
    2024-04-18 15:54:00,499 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue slice_6 due to large size 4194304.
    2024-04-18 15:54:00,503 onnxscript.optimizer.constant_folding [WARNING] - Skip storing constant folded nvalue expand_2 due to large size 8388608.
    [0;93m2024-04-18 15:54:00.594316600 [W:onnxruntime:, graph.cc:3594 CleanUnusedInitializersAndNodeArgs] Removing initializer &#39;_val_45&#39;. It is not used by any node and should be removed from the model.[m
    [0;93m2024-04-18 15:54:00.594363200 [W:onnxruntime:, graph.cc:3594 CleanUnusedInitializersAndNodeArgs] Removing initializer &#39;_val_44&#39;. It is not used by any node and should be removed from the model.[m
</pre></div>
</div>
</section>
<section id="with-the-custom-exporter">
<h3>With the custom exporter<a class="headerlink" href="#with-the-custom-exporter" title="Link to this heading">#</a></h3>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># from onnx_array_api.plotting.text_plot import onnx_simple_text_plot</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.onnx</span>
<span class="kn">from</span> <span class="nn">experimental_experiment.torch_models.training_helper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">make_aot_ort</span><span class="p">,</span>
    <span class="n">train_loop</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">experimental_experiment.torch_models.dump_helper</span> <span class="kn">import</span> <span class="n">dump_onnx</span>

<span class="c1"># from experimental_experiment.torch_interpreter import to_onnx</span>

<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">LlamaConfig</span>
    <span class="kn">from</span> <span class="nn">transformers.models.llama.modeling_llama</span> <span class="kn">import</span> <span class="n">LlamaModel</span>


<span class="k">def</span> <span class="nf">ids_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="n">total_dims</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
        <span class="n">total_dims</span> <span class="o">*=</span> <span class="n">dim</span>

    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_dims</span><span class="p">):</span>
        <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>


<span class="n">config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">(</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="s2">&quot;eager&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LlamaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">ids_tensor</span><span class="p">([</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">],</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="n">input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ONNXRT_CHANGE_REWRITER&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>

<span class="n">local_aot_ort</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_aot_ort</span><span class="p">(</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rewrite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
    <span class="n">optimized_mod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">local_aot_ort</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">dump_onnx</span><span class="p">(</span><span class="s2">&quot;dort-llama-ort&quot;</span><span class="p">,</span> <span class="n">folder</span><span class="o">=</span><span class="s2">&quot;dump_llama&quot;</span><span class="p">,</span> <span class="n">clean</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">train_loop</span><span class="p">(</span><span class="n">optimized_mod</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">_</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;dump_llama&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">_</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.onnx&quot;</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;exported model: </span><span class="si">{</span><span class="n">names</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NODES in </span><span class="si">{name!r}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">onx</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;dump_llama&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">onx</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">onx</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">op_type</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">input</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">output</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ONNXRT_CHANGE_REWRITER&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;0&quot;</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    
    [runpythonerror]
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    [0;93m2024-04-18 15:54:08.846784400 [W:onnxruntime:, unsqueeze_elimination.cc:20 Apply] UnsqueezeElimination cannot remove node _inlfunc_aten_mean_dim_n1[m
    [0;93m2024-04-18 15:54:08.846878300 [W:onnxruntime:, unsqueeze_elimination.cc:20 Apply] UnsqueezeElimination cannot remove node _inlfunc_aten_mean_dim_token_67_n1[m
    [0;93m2024-04-18 15:54:08.846896900 [W:onnxruntime:, unsqueeze_elimination.cc:20 Apply] UnsqueezeElimination cannot remove node _inlfunc_aten_mean_dim_token_84_n1[m
</pre></div>
</div>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="example_bug.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">A script to report a bug</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="torchtry.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Tries with Undocumented</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023-2024
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Use the custom exporter in torch</a><ul>
<li><a class="reference internal" href="#file-onnxruntime-py">File <cite>onnxruntime.py</cite></a></li>
<li><a class="reference internal" href="#examples">Examples</a><ul>
<li><a class="reference internal" href="#baseline">Baseline</a></li>
<li><a class="reference internal" href="#with-the-custom-exporter">With the custom exporter</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=a1637f0b"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    </body>
</html>