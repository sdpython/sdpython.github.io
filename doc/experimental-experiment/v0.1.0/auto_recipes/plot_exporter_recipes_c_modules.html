<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Command Lines" href="../command_lines.html" /><link rel="prev" title="to_onnx and a custom operator registered with a function" href="plot_exporter_recipes_c_custom_ops_fct.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>to_onnx and submodules from LLMs - experimental-experiment 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">experimental-experiment 0.1.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">experimental-experiment 0.1.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../design/index.html">Design</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Design</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../design/exporter.html">Custom Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/optimizer.html">Pattern Optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design/backends.html">Dynamo Backends</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorial/index.html">Tutorial</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/pytorch.html">pytorch and onnx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/onnx.html">onnx</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tutorial/exporter_recipes.html">Exporter Recipes</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Exporter Recipes</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial/exporter_recipes_custom.html">Recipes with to_onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial/exporter_recipes_onnx_export.html">Recipes with torch.onnx.export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial/errors.html">Frequent Exceptions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/docker.html">Start from a docker</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of API</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/gradient/index.html">experimental_experiment.gradient</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of experimental_experiment.gradient</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/gradient/ops/index.html">experimental_experiment.gradient.ops</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of experimental_experiment.gradient.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/gradient/ops/op_broadcast_gradient_args.html">experimental_experiment.gradient.ops.op_broadcast_gradient_args</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/gradient/grad_helper.html">experimental_experiment.gradient.grad_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/gradient/loss_helper.html">experimental_experiment.gradient.loss_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/reference/index.html">experimental_experiment.reference</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of experimental_experiment.reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/reference/ops/index.html">experimental_experiment.reference.ops</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of experimental_experiment.reference.ops</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_add_add_mul_mul.html">experimental_experiment.reference.ops.op_add_add_mul_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_average_pool_grad.html">experimental_experiment.reference.ops.op_average_pool_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_cast_like.html">experimental_experiment.reference.ops.op_cast_like</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_concat.html">experimental_experiment.reference.ops.op_concat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_constant_of_shape.html">experimental_experiment.reference.ops.op_constant_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_fused_matmul.html">experimental_experiment.reference.ops.op_fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_gather_grad.html">experimental_experiment.reference.ops.op_gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_memcpy_host.html">experimental_experiment.reference.ops.op_memcpy_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_mul_sigmoid.html">experimental_experiment.reference.ops.op_mul_sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_negxplus1.html">experimental_experiment.reference.ops.op_negxplus1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_quick_gelu.html">experimental_experiment.reference.ops.op_quick_gelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_replace_zero.html">experimental_experiment.reference.ops.op_replace_zero</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_rotary.html">experimental_experiment.reference.ops.op_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatter_elements.html">experimental_experiment.reference.ops.op_scatter_elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_scatternd_of_shape.html">experimental_experiment.reference.ops.op_scatternd_of_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_simplified_layer_normalization.html">experimental_experiment.reference.ops.op_simplified_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_slice.html">experimental_experiment.reference.ops.op_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_transpose_cast.html">experimental_experiment.reference.ops.op_transpose_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/reference/ops/op_tri_matrix.html">experimental_experiment.reference.ops.op_tri_matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/evaluator.html">experimental_experiment.reference.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/reference/ort_evaluator.html">experimental_experiment.reference.ort_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/convert/index.html">experimental_experiment.convert</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of experimental_experiment.convert</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/convert/convert_helper.html">experimental_experiment.convert.convert_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/convert/ort_helper.html">experimental_experiment.convert.ort_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/plotting/index.html">experimental_experiment.plotting</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of experimental_experiment.plotting</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/plotting/data.html">experimental_experiment.plotting.data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/plotting/memory.html">experimental_experiment.plotting.memory</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_interpreter/index.html">experimental_experiment.torch_interpreter</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of experimental_experiment.torch_interpreter</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_functions.html">experimental_experiment.torch_interpreter._aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_functions_attention.html">experimental_experiment.torch_interpreter._aten_functions_attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_aten_methods.html">experimental_experiment.torch_interpreter._aten_methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_doc_.html">experimental_experiment.torch_interpreter._doc_</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_exceptions.html">experimental_experiment.torch_interpreter._exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_prims_functions.html">experimental_experiment.torch_interpreter._prims_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/_torch_helper.html">experimental_experiment.torch_interpreter._torch_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/aten_functions.html">experimental_experiment.torch_interpreter.aten_functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/aten_methods.html">experimental_experiment.torch_interpreter.aten_methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/dispatcher.html">experimental_experiment.torch_interpreter.dispatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/export_options.html">experimental_experiment.torch_interpreter.export_options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/interpreter.html">experimental_experiment.torch_interpreter.interpreter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/onnx_export.html">experimental_experiment.torch_interpreter.onnx_export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/onnx_export_errors.html">experimental_experiment.torch_interpreter.onnx_export_errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/oxs_dispatcher.html">experimental_experiment.torch_interpreter.oxs_dispatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_interpreter/oxs_opset.html">experimental_experiment.torch_interpreter.oxs_opset</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_models/index.html">experimental_experiment.torch_models</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of experimental_experiment.torch_models</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/diffusion_model_helper.html">experimental_experiment.torch_models.diffusion_model_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/dump_helper.html">experimental_experiment.torch_models.dump_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llama_helper.html">experimental_experiment.torch_models.llama_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/llm_model_helper.html">experimental_experiment.torch_models.llm_model_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/mistral_helper.html">experimental_experiment.torch_models.mistral_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/phi3_helper.html">experimental_experiment.torch_models.phi3_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/phi_helper.html">experimental_experiment.torch_models.phi_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_models/training_helper.html">experimental_experiment.torch_models.training_helper</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/xbuilder/index.html">experimental_experiment.xbuilder</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of experimental_experiment.xbuilder</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_dtype_helper.html">experimental_experiment.xbuilder._dtype_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_graph_builder_runtime.html">experimental_experiment.xbuilder._graph_builder_runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_onnx_helper.html">experimental_experiment.xbuilder._onnx_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/_shape_helper.html">experimental_experiment.xbuilder._shape_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/expression_dimension.html">experimental_experiment.xbuilder.expression_dimension</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/graph_builder.html">experimental_experiment.xbuilder.graph_builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/graph_builder_opset.html">experimental_experiment.xbuilder.graph_builder_opset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/model_container.html">experimental_experiment.xbuilder.model_container</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/optimization_options.html">experimental_experiment.xbuilder.optimization_options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/shape_type_compute.html">experimental_experiment.xbuilder.shape_type_compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xbuilder/type_inference.html">experimental_experiment.xbuilder.type_inference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/xoptim/index.html">experimental_experiment.xoptim</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of experimental_experiment.xoptim</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_investigation/index.html">experimental_experiment.xoptim.patterns_investigation</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle navigation of experimental_experiment.xoptim.patterns_investigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_investigation/element_wise.html">experimental_experiment.xoptim.patterns_investigation.element_wise</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_ml/index.html">experimental_experiment.xoptim.patterns_ml</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle navigation of experimental_experiment.xoptim.patterns_ml</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ml/tree_ensemble.html">experimental_experiment.xoptim.patterns_ml.tree_ensemble</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_exp/index.html">experimental_experiment.xoptim.patterns_exp</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle navigation of experimental_experiment.xoptim.patterns_exp</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/binary_operators.html">experimental_experiment.xoptim.patterns_exp.binary_operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/constant_of_shape_scatter_nd.html">experimental_experiment.xoptim.patterns_exp.constant_of_shape_scatter_nd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/constants.html">experimental_experiment.xoptim.patterns_exp.constants</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/simple_rotary.html">experimental_experiment.xoptim.patterns_exp.simple_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/unary_operators.html">experimental_experiment.xoptim.patterns_exp.unary_operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_exp/where_replace.html">experimental_experiment.xoptim.patterns_exp.where_replace</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns/index.html">experimental_experiment.xoptim.patterns</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle navigation of experimental_experiment.xoptim.patterns</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_any.html">experimental_experiment.xoptim.patterns.onnx_any</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_cast.html">experimental_experiment.xoptim.patterns.onnx_cast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_conv.html">experimental_experiment.xoptim.patterns.onnx_conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_dropout.html">experimental_experiment.xoptim.patterns.onnx_dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_equal.html">experimental_experiment.xoptim.patterns.onnx_equal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_expand.html">experimental_experiment.xoptim.patterns.onnx_expand</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_functions.html">experimental_experiment.xoptim.patterns.onnx_functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_layer_normalization.html">experimental_experiment.xoptim.patterns.onnx_layer_normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_matmul.html">experimental_experiment.xoptim.patterns.onnx_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_mul.html">experimental_experiment.xoptim.patterns.onnx_mul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_reduce.html">experimental_experiment.xoptim.patterns.onnx_reduce</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_reshape.html">experimental_experiment.xoptim.patterns.onnx_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_rotary.html">experimental_experiment.xoptim.patterns.onnx_rotary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_split.html">experimental_experiment.xoptim.patterns.onnx_split</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_sub.html">experimental_experiment.xoptim.patterns.onnx_sub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_transpose.html">experimental_experiment.xoptim.patterns.onnx_transpose</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns/onnx_unsqueeze.html">experimental_experiment.xoptim.patterns.onnx_unsqueeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_ort/index.html">experimental_experiment.xoptim.patterns_ort</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle navigation of experimental_experiment.xoptim.patterns_ort</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/activation.html">experimental_experiment.xoptim.patterns_ort.activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/activation_grad.html">experimental_experiment.xoptim.patterns_ort.activation_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/fused_matmul.html">experimental_experiment.xoptim.patterns_ort.fused_matmul</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/gather_grad.html">experimental_experiment.xoptim.patterns_ort.gather_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_ort/simplified_layer_normalization.html">experimental_experiment.xoptim.patterns_ort.simplified_layer_normalization</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../api/xoptim/patterns_fix/index.html">experimental_experiment.xoptim.patterns_fix</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle navigation of experimental_experiment.xoptim.patterns_fix</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/xoptim/patterns_fix/add_reduction_scatter_nd.html">experimental_experiment.xoptim.patterns_fix.add_reduction_scatter_nd</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/graph_builder_optim.html">experimental_experiment.xoptim.graph_builder_optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/order_optim.html">experimental_experiment.xoptim.order_optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/xoptim/patterns_api.html">experimental_experiment.xoptim.patterns_api</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_dynamo/index.html">experimental_experiment.torch_dynamo</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><div class="visually-hidden">Toggle navigation of experimental_experiment.torch_dynamo</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/_dynamo_exporter.html">experimental_experiment.torch_dynamo._dynamo_exporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/backend_helper.html">experimental_experiment.torch_dynamo.backend_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/debug_backend.html">experimental_experiment.torch_dynamo.debug_backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/dynger_backend.html">experimental_experiment.torch_dynamo.dynger_backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/fast_backend.html">experimental_experiment.torch_dynamo.fast_backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_dynamo/partition.html">experimental_experiment.torch_dynamo.partition</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/torch_bench/index.html">experimental_experiment.torch_bench</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><div class="visually-hidden">Toggle navigation of experimental_experiment.torch_bench</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner_agg.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner_agg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_benchmark_runner_agg_helper.html">experimental_experiment.torch_bench._bash_bench_benchmark_runner_agg_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_cmd.html">experimental_experiment.torch_bench._bash_bench_cmd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_model_runner.html">experimental_experiment.torch_bench._bash_bench_model_runner</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_models_helper.html">experimental_experiment.torch_bench._bash_bench_models_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_dummies.html">experimental_experiment.torch_bench._bash_bench_set_dummies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_explicit.html">experimental_experiment.torch_bench._bash_bench_set_explicit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_huggingface.html">experimental_experiment.torch_bench._bash_bench_set_huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_huggingface_big.html">experimental_experiment.torch_bench._bash_bench_set_huggingface_big</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_issues.html">experimental_experiment.torch_bench._bash_bench_set_issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_timm.html">experimental_experiment.torch_bench._bash_bench_set_timm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_torchbench.html">experimental_experiment.torch_bench._bash_bench_set_torchbench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_set_torchbench_ado.html">experimental_experiment.torch_bench._bash_bench_set_torchbench_ado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_bash_bench_suites.html">experimental_experiment.torch_bench._bash_bench_suites</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_dort_cmd_common.html">experimental_experiment.torch_bench._dort_cmd_common</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/_dort_cmd_common_models.html">experimental_experiment.torch_bench._dort_cmd_common_models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_agg.html">experimental_experiment.torch_bench.bash_bench_agg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_explicit.html">experimental_experiment.torch_bench.bash_bench_explicit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_huggingface.html">experimental_experiment.torch_bench.bash_bench_huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_huggingface_big.html">experimental_experiment.torch_bench.bash_bench_huggingface_big</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_issues.html">experimental_experiment.torch_bench.bash_bench_issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_timm.html">experimental_experiment.torch_bench.bash_bench_timm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_torchbench.html">experimental_experiment.torch_bench.bash_bench_torchbench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_torchbench_ado.html">experimental_experiment.torch_bench.bash_bench_torchbench_ado</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/bash_bench_untrained.html">experimental_experiment.torch_bench.bash_bench_untrained</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/check_model.html">experimental_experiment.torch_bench.check_model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_bench.html">experimental_experiment.torch_bench.dort_bench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_bench_profile.html">experimental_experiment.torch_bench.dort_bench_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/dort_profile.html">experimental_experiment.torch_bench.dort_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/export_model.html">experimental_experiment.torch_bench.export_model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/torch_bench/export_model_helper.html">experimental_experiment.torch_bench.export_model_helper</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/_bench_test.html">experimental_experiment._bench_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/_command_lines_parser.html">experimental_experiment._command_lines_parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/args.html">experimental_experiment.args</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/bench_run.html">experimental_experiment.bench_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/checks.html">experimental_experiment.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/ext_test_case.html">experimental_experiment.ext_test_case</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/helpers.html">experimental_experiment.helpers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/memory_peak.html">experimental_experiment.memory_peak</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/model_run.html">experimental_experiment.model_run</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/onnx_tools.html">experimental_experiment.onnx_tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/torch_test_helper.html">experimental_experiment.torch_test_helper</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../galleries.html">Galleries of Examples and Recipes</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" role="switch" type="checkbox"/><label for="toctree-checkbox-23"><div class="visually-hidden">Toggle navigation of Galleries of Examples and Recipes</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/index.html">Examples Gallery</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" role="switch" type="checkbox"/><label for="toctree-checkbox-24"><div class="visually-hidden">Toggle navigation of Examples Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_onnxscript_102.html">102: Examples with onnxscript</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_optimize_101.html">101: Graph Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_compile_102.html">102: Tweak onnx export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_profile_existing_onnx_101.html">101: Profile an existing model with onnxruntime</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_rewrite_101.html">101: Onnx Model Rewriting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_linreg_101.html">101: Linear Regression and export to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_custom_backend_101.html">101: A custom backend for torch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_101.html">101: Some dummy examples with torch.export.export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_convolutation_matmul_102.html">102: Convolution and Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_custom_backend_llama_102.html">102: Fuse kernels in a small Llama Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_export_301.html">301: Compares LLAMA exporters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_diff_dort_301.html">301: Compares LLAMA exporters for onnxrt backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_llama_bench_102.html">102: Measure LLAMA speed</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_aot_201.html">201: Evaluate DORT Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_dort_201.html">201: Evaluate DORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/plot_torch_export_201.html">201: Evaluate different ways to export a torch model to ONNX</a></li>
</ul>
</li>
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">Exporter Recipes Gallery</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" role="switch" type="checkbox"/><label for="toctree-checkbox-25"><div class="visually-hidden">Toggle navigation of Exporter Recipes Gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_cond.html">to_onnx and a model with a test</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_custom_ops_inplace.html">to_onnx and a custom operator inplace</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_exporter_recipes_c_custom_ops_fct.html">to_onnx and a custom operator registered with a function</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">to_onnx and submodules from LLMs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../command_lines.html">Command Lines</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" role="switch" type="checkbox"/><label for="toctree-checkbox-26"><div class="visually-hidden">Toggle navigation of Command Lines</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../bench/index.html">Benchmarks from the command line</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" role="switch" type="checkbox"/><label for="toctree-checkbox-27"><div class="visually-hidden">Toggle navigation of Benchmarks from the command line</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../bench/dort_bench.html">experimental_experiment.torch_bench.dort_bench</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/dort_profile.html">experimental_experiment.torch_bench.dort_profile</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/scripts.html">Interesting scripts or command lines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bench/bash_bench.html">Measuring the exporters on a short list of sets of models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tools/index.html">Tools from the command line</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" role="switch" type="checkbox"/><label for="toctree-checkbox-28"><div class="visually-hidden">Toggle navigation of Tools from the command line</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tools/lighten.html">python -m experimental_experiment lighten and unlighten</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/optimize.html">python -m experimental_experiment optimize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tools/run.html">python -m experimental_experiment run</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../miscellaneous/index.html">Miscellaneous</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" role="switch" type="checkbox"/><label for="toctree-checkbox-29"><div class="visually-hidden">Toggle navigation of Miscellaneous</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../miscellaneous/export_times.html">Export Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="../miscellaneous/long_outputs.html">Long Outputs uneasy to read</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../miscellaneous/models/index.html">Supported Models By the Custom Backend</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" role="switch" type="checkbox"/><label for="toctree-checkbox-30"><div class="visually-hidden">Toggle navigation of Supported Models By the Custom Backend</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../miscellaneous/models/llama.html">LLaMa</a></li>
<li class="toctree-l3"><a class="reference internal" href="../miscellaneous/models/mistral.html">Mistral</a></li>
<li class="toctree-l3"><a class="reference internal" href="../miscellaneous/models/phi.html">Phi</a></li>
<li class="toctree-l3"><a class="reference internal" href="../miscellaneous/models/phi3.html">Phi3</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/auto_recipes/plot_exporter_recipes_c_modules.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-recipes-plot-exporter-recipes-c-modules-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="to-onnx-and-submodules-from-llms">
<span id="l-plot-exporter-recipes-custom-modules"></span><span id="sphx-glr-auto-recipes-plot-exporter-recipes-c-modules-py"></span><h1>to_onnx and submodules from LLMs<a class="headerlink" href="#to-onnx-and-submodules-from-llms" title="Link to this heading">¶</a></h1>
<p>Big models are hard to read once converted into onnx.
Let’s see how to improve their readibility.
The code is inspired from
<a class="reference external" href="https://medium.com/&#64;msouza.os/llm-from-scratch-with-pytorch-9f21808c6319">LLM from scratch with Pytorch</a>.</p>
<section id="a-simple-llm">
<h2>A simple LLM<a class="headerlink" href="#a-simple-llm" title="Link to this heading">¶</a></h2>
<p>All comments were removed from the code to make it less verbose.
A few fixes were applied to the original code.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx.inliner</span> <span class="kn">import</span> <a href="https://onnx.ai/onnx/api/inliner.html#onnx.inliner.inline_local_functions" title="onnx.inliner.inline_local_functions" class="sphx-glr-backref-module-onnx-inliner sphx-glr-backref-type-py-function"><a href="https://onnx.ai/onnx/api/inliner.html#onnx.inliner.inline_local_functions" title="onnx.inliner.inline_local_functions" class="sphx-glr-backref-module-onnx-inliner sphx-glr-backref-type-py-function"><a href="https://onnx.ai/onnx/api/inliner.html#onnx.inliner.inline_local_functions" title="onnx.inliner.inline_local_functions" class="sphx-glr-backref-module-onnx-inliner sphx-glr-backref-type-py-function"><span class="n">inline_local_functions</span></a></a></a>
<span class="kn">from</span> <span class="nn">onnx_array_api.plotting.graphviz_helper</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/plotting.html#onnx_array_api.plotting.graphviz_helper.plot_dot" title="onnx_array_api.plotting.graphviz_helper.plot_dot" class="sphx-glr-backref-module-onnx_array_api-plotting-graphviz_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/plotting.html#onnx_array_api.plotting.graphviz_helper.plot_dot" title="onnx_array_api.plotting.graphviz_helper.plot_dot" class="sphx-glr-backref-module-onnx_array_api-plotting-graphviz_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/plotting.html#onnx_array_api.plotting.graphviz_helper.plot_dot" title="onnx_array_api.plotting.graphviz_helper.plot_dot" class="sphx-glr-backref-module-onnx_array_api-plotting-graphviz_helper sphx-glr-backref-type-py-function"><span class="n">plot_dot</span></a></a></a>
<span class="kn">from</span> <span class="nn">onnx_array_api.reference</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.compare_onnx_execution" title="onnx_array_api.reference.compare_onnx_execution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.compare_onnx_execution" title="onnx_array_api.reference.compare_onnx_execution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.compare_onnx_execution" title="onnx_array_api.reference.compare_onnx_execution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-function"><span class="n">compare_onnx_execution</span></a></a></a>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">onnxruntime</span> <span class="kn">import</span> <span class="n">InferenceSession</span>
<span class="kn">from</span> <span class="nn">experimental_experiment.torch_interpreter</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><span class="n">to_onnx</span></a></a></a>
<span class="kn">from</span> <span class="nn">experimental_experiment.helpers</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">pretty_onnx</span></a></a></a>
<span class="kn">from</span> <span class="nn">experimental_experiment.bench_run</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><span class="n">max_diff</span></a></a></a>
<span class="kn">from</span> <span class="nn">experimental_experiment.xbuilder</span> <span class="kn">import</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><span class="n">OptimizationOptions</span></a></a></a>


<span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a></a></a><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span></a></a></a><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span></a></a></a><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">word_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">word_pe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">word_emb</span> <span class="o">+</span> <span class="n">word_pe</span>


<span class="k">class</span> <span class="nc">AttentionBlock</span><span class="p">(</span><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a></a></a><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">ones</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a></a></a><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">context_size</span><span class="p">,</span> <span class="n">context_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><a href="https://pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">float</span></a></a></a><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;mask&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="o">=</span><a href="https://pytorch.org/docs/main/generated/torch.tril.html#torch.tril" title="torch.tril" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.tril.html#torch.tril" title="torch.tril" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.tril.html#torch.tril" title="torch.tril" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">tril</span></a></a></a><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">qk</span> <span class="o">=</span> <span class="n">query</span> <span class="o">@</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">C</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">qk</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
        <span class="n">attention</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span></a></a></a><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a></a></a><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">attention</span> <span class="o">@</span> <span class="n">value</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">MultiAttentionBlock</span><span class="p">(</span><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a></a></a><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span></a></a></a><span class="p">(</span>
            <span class="n">modules</span><span class="o">=</span><span class="p">[</span><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">AttentionBlock</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a></a></a><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">embedding_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">embedding_dim</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.cat.html#torch.cat" title="torch.cat" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.cat.html#torch.cat" title="torch.cat" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.cat.html#torch.cat" title="torch.cat" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">cat</span></a></a></a><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">attention</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">],</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a></a></a><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a></a></a><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.ReLU.html#torch.nn.ReLU" title="torch.nn.ReLU" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.ReLU.html#torch.nn.ReLU" title="torch.nn.ReLU" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.ReLU.html#torch.nn.ReLU" title="torch.nn.ReLU" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span></a></a></a><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a></a></a><span class="p">(</span><span class="n">ff_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a></a></a><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">MultiAttentionBlock</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">FeedForward</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span></a></a></a><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_2</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span></a></a></a><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span> <span class="o">+</span> <span class="n">x</span>

        <span class="n">attention_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_2</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
        <span class="n">ff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">attention_norm</span><span class="p">)</span>
        <span class="n">ff</span> <span class="o">=</span> <span class="n">ff</span> <span class="o">+</span> <span class="n">attention</span>

        <span class="k">return</span> <span class="n">ff</span>


<span class="k">class</span> <span class="nc">LLM</span><span class="p">(</span><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a></a></a><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">ff_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">Embedding</span></a></a></a><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">DecoderLayer</span></a></a></a><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">context_size</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">)</span>
        <a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span></a></a></a> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span></a></a></a>


<span class="n">llm</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><a href="https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">LLM</span></a></a></a><span class="p">()</span>
<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a></a></a> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a> <span class="o">=</span> <a href="https://pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a></a></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a></a></a><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">int64</span></a></a></a><span class="p">)</span>
<a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span></a></a></a> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output: shape=</span><span class="si">{</span><a href="https://pytorch.org/docs/main/size.html#torch.Size" title="torch.Size" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/size.html#torch.Size" title="torch.Size" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/size.html#torch.Size" title="torch.Size" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span><span class="o">.</span><span class="n">shape</span></a></a></a><span class="si">}</span><span class="s2">, min=</span><span class="si">{</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span></a></a></a><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s2">, max=</span><span class="si">{</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span></a></a></a><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>output: shape=torch.Size([1, 30, 16]), min=-3.9085302352905273, max=3.940504550933838
</pre></div>
</div>
</section>
<section id="first-conversion-to-onnx">
<h2>First conversion to ONNX<a class="headerlink" href="#first-conversion-to-onnx" title="Link to this heading">¶</a></h2>
<p>The conversion relies on <a class="reference external" href="https://pytorch.org/docs/main/export.html#torch.export.export" title="(in PyTorch vmain (2.6.0a0+gitbf1b8ad ))"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a>.
which gives:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a> <span class="o">=</span> <a href="https://pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a></a><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a><span class="o">.</span><span class="n">graph</span><span class="p">)</span>

<span class="c1"># Then function :func:`to_onnx &lt;experimental_experiment.torch_interpreter.to_onnx&gt;`</span>
<span class="c1"># converts it into ONNX.</span>

<a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><span class="n">to_onnx</span></a></a></a><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">pretty_onnx</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_embedding_embedding_weight : [num_users=1] = placeholder[target=p_embedding_embedding_weight]
    %p_embedding_pe_weight : [num_users=1] = placeholder[target=p_embedding_pe_weight]
    %p_decoder_norm_1_weight : [num_users=1] = placeholder[target=p_decoder_norm_1_weight]
    %p_decoder_norm_1_bias : [num_users=1] = placeholder[target=p_decoder_norm_1_bias]
    %p_decoder_attention_attention_0_query_weight : [num_users=1] = placeholder[target=p_decoder_attention_attention_0_query_weight]
    %p_decoder_attention_attention_0_key_weight : [num_users=1] = placeholder[target=p_decoder_attention_attention_0_key_weight]
    %p_decoder_attention_attention_0_value_weight : [num_users=1] = placeholder[target=p_decoder_attention_attention_0_value_weight]
    %p_decoder_attention_attention_1_query_weight : [num_users=1] = placeholder[target=p_decoder_attention_attention_1_query_weight]
    %p_decoder_attention_attention_1_key_weight : [num_users=1] = placeholder[target=p_decoder_attention_attention_1_key_weight]
    %p_decoder_attention_attention_1_value_weight : [num_users=1] = placeholder[target=p_decoder_attention_attention_1_value_weight]
    %p_decoder_attention_linear_weight : [num_users=1] = placeholder[target=p_decoder_attention_linear_weight]
    %p_decoder_attention_linear_bias : [num_users=1] = placeholder[target=p_decoder_attention_linear_bias]
    %p_decoder_norm_2_weight : [num_users=1] = placeholder[target=p_decoder_norm_2_weight]
    %p_decoder_norm_2_bias : [num_users=1] = placeholder[target=p_decoder_norm_2_bias]
    %p_decoder_feed_forward_linear_1_weight : [num_users=1] = placeholder[target=p_decoder_feed_forward_linear_1_weight]
    %p_decoder_feed_forward_linear_1_bias : [num_users=1] = placeholder[target=p_decoder_feed_forward_linear_1_bias]
    %p_decoder_feed_forward_linear_2_weight : [num_users=1] = placeholder[target=p_decoder_feed_forward_linear_2_weight]
    %p_decoder_feed_forward_linear_2_bias : [num_users=1] = placeholder[target=p_decoder_feed_forward_linear_2_bias]
    %b_decoder_attention_attention_0_mask : [num_users=1] = placeholder[target=b_decoder_attention_attention_0_mask]
    %b_decoder_attention_attention_1_mask : [num_users=1] = placeholder[target=b_decoder_attention_attention_1_mask]
    %input_ids : [num_users=2] = placeholder[target=input_ids]
    %embedding : [num_users=1] = call_function[target=torch.ops.aten.embedding.default](args = (%p_embedding_embedding_weight, %input_ids), kwargs = {})
    %embedding_1 : [num_users=1] = call_function[target=torch.ops.aten.embedding.default](args = (%p_embedding_pe_weight, %input_ids), kwargs = {})
    %add : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%embedding, %embedding_1), kwargs = {})
    %layer_norm : [num_users=6] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add, [16], %p_decoder_norm_1_weight, %p_decoder_norm_1_bias), kwargs = {})
    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_decoder_attention_attention_0_query_weight), kwargs = {})
    %linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_decoder_attention_attention_0_key_weight), kwargs = {})
    %linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_decoder_attention_attention_0_value_weight), kwargs = {})
    %transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%linear_1, -2, -1), kwargs = {})
    %matmul : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%linear, %transpose), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul, 0.25), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%b_decoder_attention_attention_0_mask, 0, 0, 30), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_1, 1, 0, 30), kwargs = {})
    %eq : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%slice_2, 0), kwargs = {})
    %masked_fill : [num_users=1] = call_function[target=torch.ops.aten.masked_fill.Scalar](args = (%mul, %eq, -inf), kwargs = {})
    %softmax : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%masked_fill, -1), kwargs = {})
    %matmul_1 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%softmax, %linear_2), kwargs = {})
    %linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_decoder_attention_attention_1_query_weight), kwargs = {})
    %linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_decoder_attention_attention_1_key_weight), kwargs = {})
    %linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_decoder_attention_attention_1_value_weight), kwargs = {})
    %transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%linear_4, -2, -1), kwargs = {})
    %matmul_2 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%linear_3, %transpose_1), kwargs = {})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%matmul_2, 0.25), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%b_decoder_attention_attention_1_mask, 0, 0, 30), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_3, 1, 0, 30), kwargs = {})
    %eq_1 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%slice_4, 0), kwargs = {})
    %masked_fill_1 : [num_users=1] = call_function[target=torch.ops.aten.masked_fill.Scalar](args = (%mul_1, %eq_1, -inf), kwargs = {})
    %softmax_1 : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%masked_fill_1, -1), kwargs = {})
    %matmul_3 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%softmax_1, %linear_5), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%matmul_1, %matmul_3], -1), kwargs = {})
    %linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%cat, %p_decoder_attention_linear_weight, %p_decoder_attention_linear_bias), kwargs = {})
    %add_1 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_6, %add), kwargs = {})
    %layer_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_1, [16], %p_decoder_norm_2_weight, %p_decoder_norm_2_bias), kwargs = {})
    %linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_1, %p_decoder_feed_forward_linear_1_weight, %p_decoder_feed_forward_linear_1_bias), kwargs = {})
    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%linear_7,), kwargs = {})
    %linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%relu, %p_decoder_feed_forward_linear_2_weight, %p_decoder_feed_forward_linear_2_bias), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_8, %add_1), kwargs = {})
    return (add_2,)
opset: domain=&#39;&#39; version=18
doc_string: large_model=False, inline=False, external_threshold=102...
input: name=&#39;input_ids&#39; type=dtype(&#39;int64&#39;) shape=[1, 30]
init: name=&#39;p_embedding_embedding_weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;p_embedding_pe_weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;p_decoder_norm_1_weight&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;p_decoder_norm_1_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;p_decoder_attention_attention_0_query_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;p_decoder_attention_attention_0_key_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;p_decoder_attention_attention_0_value_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;p_decoder_attention_attention_1_query_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;p_decoder_attention_attention_1_key_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;p_decoder_attention_attention_1_value_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;p_decoder_attention_linear_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 32)
init: name=&#39;p_decoder_attention_linear_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;p_decoder_norm_2_weight&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;p_decoder_norm_2_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;p_decoder_feed_forward_linear_1_weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 16)
init: name=&#39;p_decoder_feed_forward_linear_1_bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;p_decoder_feed_forward_linear_2_weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 128)
init: name=&#39;p_decoder_feed_forward_linear_2_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;b_decoder_attention_attention_0_mask&#39; type=dtype(&#39;float32&#39;) shape=(256, 256)
init: name=&#39;b_decoder_attention_attention_1_mask&#39; type=dtype(&#39;float32&#39;) shape=(256, 256)
init: name=&#39;init1_s_&#39; type=dtype(&#39;float32&#39;) shape=() -- array([0.25], dtype=float32)
init: name=&#39;init7_s1_1&#39; type=dtype(&#39;int64&#39;) shape=(1,) -- array([1])
init: name=&#39;init7_s1_0&#39; type=dtype(&#39;int64&#39;) shape=(1,) -- array([0])
init: name=&#39;init7_s1_30&#39; type=dtype(&#39;int64&#39;) shape=(1,) -- array([30])
init: name=&#39;init1_s_2&#39; type=dtype(&#39;float32&#39;) shape=() -- array([0.], dtype=float32)
init: name=&#39;init1_s1_3&#39; type=dtype(&#39;float32&#39;) shape=(1,) -- array([-inf], dtype=float32)
init: name=&#39;init1_s16_&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;init1_s16_2&#39; type=dtype(&#39;float32&#39;) shape=(16,)
Gather(p_embedding_embedding_weight, input_ids) -&gt; embedding
Gather(p_embedding_pe_weight, input_ids) -&gt; embedding_1
  Add(embedding, embedding_1) -&gt; add
Mul(init1_s16_, p_decoder_norm_1_weight) -&gt; LayerNormalizationScalePattern_init1_s16_
Mul(p_decoder_norm_1_weight, init1_s16_2) -&gt; LayerNormalizationScalePattern_init1_s16_2
  Add(LayerNormalizationScalePattern_init1_s16_2, p_decoder_norm_1_bias) -&gt; LayerNormalizationScalePattern_init1_s16_3
  LayerNormalization(add, LayerNormalizationScalePattern_init1_s16_, LayerNormalizationScalePattern_init1_s16_3, axis=-1, epsilon=0.00, stash_type=1) -&gt; _onx_add02
Transpose(p_decoder_attention_attention_0_query_weight, perm=[1,0]) -&gt; _onx_transpose0
  MatMul(_onx_add02, _onx_transpose0) -&gt; linear
Transpose(p_decoder_attention_attention_0_key_weight, perm=[1,0]) -&gt; _onx_transpose02
  MatMul(_onx_add02, _onx_transpose02) -&gt; linear_1
    Transpose(linear_1, perm=[0,2,1]) -&gt; transpose
    MatMul(linear, transpose) -&gt; matmul
Transpose(p_decoder_attention_attention_0_value_weight, perm=[1,0]) -&gt; _onx_transpose03
  MatMul(_onx_add02, _onx_transpose03) -&gt; linear_2
Reshape(init1_s_, init7_s1_1) -&gt; _onx_reshape0
  Mul(matmul, _onx_reshape0) -&gt; _onx_mul02
Slice(b_decoder_attention_attention_0_mask, init7_s1_0, init7_s1_30, init7_s1_0) -&gt; slice_1
  Slice(slice_1, init7_s1_0, init7_s1_30, init7_s1_1) -&gt; slice_2
Reshape(init1_s_2, init7_s1_1) -&gt; _onx_reshape02
  Equal(slice_2, _onx_reshape02) -&gt; eq
    Where(eq, init1_s1_3, _onx_mul02) -&gt; _onx_where0
      Softmax(_onx_where0, axis=-1) -&gt; softmax
    MatMul(softmax, linear_2) -&gt; matmul_1
Transpose(p_decoder_attention_attention_1_query_weight, perm=[1,0]) -&gt; _onx_transpose04
  MatMul(_onx_add02, _onx_transpose04) -&gt; linear_3
Transpose(p_decoder_attention_attention_1_key_weight, perm=[1,0]) -&gt; _onx_transpose05
  MatMul(_onx_add02, _onx_transpose05) -&gt; linear_4
    Transpose(linear_4, perm=[0,2,1]) -&gt; transpose_1
    MatMul(linear_3, transpose_1) -&gt; matmul_2
Transpose(p_decoder_attention_attention_1_value_weight, perm=[1,0]) -&gt; _onx_transpose06
  MatMul(_onx_add02, _onx_transpose06) -&gt; linear_5
Reshape(init1_s_, init7_s1_1) -&gt; _onx_reshape03
  Mul(matmul_2, _onx_reshape03) -&gt; _onx_mul03
Slice(b_decoder_attention_attention_1_mask, init7_s1_0, init7_s1_30, init7_s1_0) -&gt; slice_3
  Slice(slice_3, init7_s1_0, init7_s1_30, init7_s1_1) -&gt; slice_4
Reshape(init1_s_2, init7_s1_1) -&gt; _onx_reshape04
  Equal(slice_4, _onx_reshape04) -&gt; eq_1
    Where(eq_1, init1_s1_3, _onx_mul03) -&gt; _onx_where02
      Softmax(_onx_where02, axis=-1) -&gt; softmax_1
    MatMul(softmax_1, linear_5) -&gt; matmul_3
      Concat(matmul_1, matmul_3, axis=-1) -&gt; cat
Transpose(p_decoder_attention_linear_weight, perm=[1,0]) -&gt; _onx_transpose07
  MatMul(cat, _onx_transpose07) -&gt; _onx_matmul0
    Add(_onx_matmul0, p_decoder_attention_linear_bias) -&gt; linear_6
    Add(linear_6, add) -&gt; add_1
Mul(init1_s16_, p_decoder_norm_2_weight) -&gt; LayerNormalizationScalePattern_init1_s16_4
Mul(p_decoder_norm_2_weight, init1_s16_2) -&gt; LayerNormalizationScalePattern_init1_s16_5
  Add(LayerNormalizationScalePattern_init1_s16_5, p_decoder_norm_2_bias) -&gt; LayerNormalizationScalePattern_init1_s16_6
  LayerNormalization(add_1, LayerNormalizationScalePattern_init1_s16_4, LayerNormalizationScalePattern_init1_s16_6, axis=-1, epsilon=0.00, stash_type=1) -&gt; _onx_add04
Transpose(p_decoder_feed_forward_linear_1_weight, perm=[1,0]) -&gt; _onx_transpose08
  MatMul(_onx_add04, _onx_transpose08) -&gt; _onx_matmul02
    Add(_onx_matmul02, p_decoder_feed_forward_linear_1_bias) -&gt; linear_7
      Relu(linear_7) -&gt; relu
Transpose(p_decoder_feed_forward_linear_2_weight, perm=[1,0]) -&gt; _onx_transpose09
  MatMul(relu, _onx_transpose09) -&gt; _onx_matmul03
    Add(_onx_matmul03, p_decoder_feed_forward_linear_2_bias) -&gt; linear_8
      Add(linear_8, add_1) -&gt; output_0
output: name=&#39;output_0&#39; type=dtype(&#39;float32&#39;) shape=[1, 30, 16]
</pre></div>
</div>
<p>Let’s check there is no discrepancy.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">sess</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a></a></a><span class="o">.</span><span class="n">SerializeToString</span><span class="p">(),</span> <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;CPUExecutionProvider&quot;</span><span class="p">])</span>
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">feeds</span></a></a></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="o">=</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">feeds</span></a></a></a><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">diff</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><span class="n">max_diff</span></a></a></a><span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span></a></a></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output: shape=</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span><span class="o">.</span><span class="n">shape</span></a></a></a><span class="si">}</span><span class="s2">, min=</span><span class="si">{</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s2">, max=</span><span class="si">{</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;max discrepancy=</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">diff</span></a></a></a><span class="p">[</span><span class="s1">&#39;abs&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>output: shape=(1, 30, 16), min=-3.9085307121276855, max=3.940504312515259
max discrepancy=4.76837158203125e-07
</pre></div>
</div>
<p>Let’s save the ONNX model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">onnx</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a></a></a><span class="p">,</span> <span class="s2">&quot;plot_exporter_recipes_c_modules.inlined.onnx&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="onnx-with-submodules">
<h2>ONNX with submodules<a class="headerlink" href="#onnx-with-submodules" title="Link to this heading">¶</a></h2>
<p>Let’s produce an ONNX model with submodules.
Function <a class="reference internal" href="../api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_onnx</span></code></a>
is calling the function <a class="reference external" href="https://pytorch.org/docs/main/export.html#torch.export.unflatten.unflatten" title="(in PyTorch vmain (2.6.0a0+gitbf1b8ad ))"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.unflatten.unflatten()</span></code></a>
under the hood. The fx graph looks like the following.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a> <span class="o">=</span> <a href="https://pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><a href="https://pytorch.org/docs/main/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a></a></a><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">,))</span>
<span class="n">unflatten_ep</span> <span class="o">=</span> <a href="https://pytorch.org/docs/main/export.html#module-torch.export.unflatten" title="torch.export.unflatten" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-module"><a href="https://pytorch.org/docs/main/export.html#module-torch.export.unflatten" title="torch.export.unflatten" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-module"><a href="https://pytorch.org/docs/main/export.html#module-torch.export.unflatten" title="torch.export.unflatten" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-module"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">unflatten</span></a></a></a><span class="p">(</span><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ep</span></a></a></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://pytorch.org/docs/main/fx.html#torch.fx.Graph" title="torch.fx.Graph" class="sphx-glr-backref-module-torch-fx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/fx.html#torch.fx.Graph" title="torch.fx.Graph" class="sphx-glr-backref-module-torch-fx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/fx.html#torch.fx.Graph" title="torch.fx.Graph" class="sphx-glr-backref-module-torch-fx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">unflatten_ep</span><span class="o">.</span><span class="n">graph</span></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %input_ids : [num_users=1] = placeholder[target=input_ids]
    %embedding : [num_users=1] = call_module[target=embedding](args = (%input_ids,), kwargs = {})
    %decoder : [num_users=1] = call_module[target=decoder](args = (%embedding,), kwargs = {})
    return (decoder,)
</pre></div>
</div>
<p>The exported graph looks simpler and shows something like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">decoder</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_module</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">decoder</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">embedding</span><span class="p">,),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
</pre></div>
</div>
<p>It preserves the hierarchy but it does not necessarily preserves the signatures
of the initial modules. That’s was not one of our goals.
The tricky part is module called (<em>embedding</em>) is not an instance <code class="docutils literal notranslate"><span class="pre">Embedding</span></code>
but an instance of <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/export/unflatten.py#L116">InterpreterModule</a>
and contains the fx nodes contributing to the submodule and coming from the
previous graph.</p>
<p>Now the ONNX graph.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><span class="n">to_onnx</span></a></a></a><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">,),</span> <span class="n">export_modules_as_functions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">pretty_onnx</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
doc_string: large_model=False, inline=False, external_threshold=102...
input: name=&#39;input_ids&#39; type=dtype(&#39;int64&#39;) shape=[1, 30]
init: name=&#39;embedding.embedding.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;embedding.pe.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;mask&#39; type=dtype(&#39;float32&#39;) shape=(256, 256)
init: name=&#39;decoder.attention.attention.0.query.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.0.key.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.0.value.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;mask2&#39; type=dtype(&#39;float32&#39;) shape=(256, 256)
init: name=&#39;decoder.attention.attention.1.query.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.1.key.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.1.value.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.linear.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 32)
init: name=&#39;decoder.feed_forward.linear_1.weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 16)
init: name=&#39;decoder.feed_forward.linear_1.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;decoder.feed_forward.linear_2.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 128)
__main__.Embedding[aten_local_function](input_ids, embedding.pe.weight, embedding.embedding.weight) -&gt; embedding
  __main__.DecoderLayer[aten_local_function](embedding, mask2, mask, decoder.feed_forward.linear_2.weight, decoder.feed_forward.linear_1.weight, decoder.attention.linear.weight, decoder.attention.attention.1.value.weight, decoder.attention.attention.1.query.weight, decoder.attention.attention.1.key.weight, decoder.attention.attention.0.value.weight, decoder.attention.attention.0.query.weight, decoder.attention.attention.0.key.weight, decoder.feed_forward.linear_1.bias) -&gt; output_0
output: name=&#39;output_0&#39; type=dtype(&#39;float32&#39;) shape=[1, 30, 16]
----- function name=Embedding domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
input: &#39;input_ids&#39;
input: &#39;weight&#39;
Gather(weight, input_ids) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.Embedding domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;input_ids&#39;
input: &#39;embedding.pe.weight&#39;
input: &#39;embedding.embedding.weight&#39;
Embedding[aten_local_function](input_ids, embedding.embedding.weight) -&gt; embedding
Embedding[aten_local_function](input_ids, embedding.pe.weight) -&gt; pe
  Add(embedding, pe) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=LayerNorm domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;add&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
Constant(value=[1.0, 1.0,...) -&gt; init1_s16_
  Mul(init1_s16_, weight) -&gt; LayerNormalizationScalePattern_init1_s16_
Constant(value=[0.0, 0.0,...) -&gt; init1_s16_2
  Mul(weight, init1_s16_2) -&gt; LayerNormalizationScalePattern_init1_s16_2
    Add(LayerNormalizationScalePattern_init1_s16_2, bias) -&gt; LayerNormalizationScalePattern_init1_s16_3
    LayerNormalization(add, LayerNormalizationScalePattern_init1_s16_, LayerNormalizationScalePattern_init1_s16_3, axis=-1, epsilon=0.00, stash_type=1) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=Linear domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;layer_norm&#39;
input: &#39;weight&#39;
Transpose(weight, perm=[1,0]) -&gt; _onx_transpose0
  MatMul(layer_norm, _onx_transpose0) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.AttentionBlock domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;layer_norm&#39;
input: &#39;mask&#39;
input: &#39;decoder.attention.attention.0.value.weight&#39;
input: &#39;decoder.attention.attention.0.query.weight&#39;
input: &#39;decoder.attention.attention.0.key.weight&#39;
Constant(value=0.25) -&gt; init1_s_
Constant(value=[1]) -&gt; init7_s1_1
  Reshape(init1_s_, init7_s1_1) -&gt; _onx_reshape0
Constant(value=[0]) -&gt; init7_s1_0
Constant(value=[30]) -&gt; init7_s1_30
  Slice(mask, init7_s1_0, init7_s1_30, init7_s1_0) -&gt; slice_1
  Slice(slice_1, init7_s1_0, init7_s1_30, init7_s1_1) -&gt; slice_2
Constant(value=0.0) -&gt; init1_s_2
  Reshape(init1_s_2, init7_s1_1) -&gt; _onx_reshape02
    Equal(slice_2, _onx_reshape02) -&gt; eq
Constant(value=[-inf]) -&gt; init1_s1_
Linear[aten_local_function](layer_norm, decoder.attention.attention.0.query.weight) -&gt; query
Linear[aten_local_function](layer_norm, decoder.attention.attention.0.key.weight) -&gt; key
  Transpose(key, perm=[0,2,1]) -&gt; transpose
  MatMul(query, transpose) -&gt; matmul
    Mul(matmul, _onx_reshape0) -&gt; _onx_mul0
  Where(eq, init1_s1_, _onx_mul0) -&gt; _onx_where0
    Softmax(_onx_where0, axis=-1) -&gt; softmax
Linear[aten_local_function](layer_norm, decoder.attention.attention.0.value.weight) -&gt; value
  MatMul(softmax, value) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=Linear_2 domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;cat&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
Transpose(weight, perm=[1,0]) -&gt; _onx_transpose0
  MatMul(cat, _onx_transpose0) -&gt; _onx_matmul0
    Add(_onx_matmul0, bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.MultiAttentionBlock domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;layer_norm&#39;
input: &#39;mask2&#39;
input: &#39;mask&#39;
input: &#39;decoder.attention.linear.weight&#39;
input: &#39;decoder.attention.attention.1.value.weight&#39;
input: &#39;decoder.attention.attention.1.query.weight&#39;
input: &#39;decoder.attention.attention.1.key.weight&#39;
input: &#39;decoder.attention.attention.0.value.weight&#39;
input: &#39;decoder.attention.attention.0.query.weight&#39;
input: &#39;decoder.attention.attention.0.key.weight&#39;
Constant(value=[-0.150363...) -&gt; decoder.attention.linear.bias
__main__.AttentionBlock[aten_local_function](layer_norm, mask, decoder.attention.attention.0.value.weight, decoder.attention.attention.0.query.weight, decoder.attention.attention.0.key.weight) -&gt; attention_0
__main__.AttentionBlock[aten_local_function](layer_norm, mask2, decoder.attention.attention.1.value.weight, decoder.attention.attention.1.query.weight, decoder.attention.attention.1.key.weight) -&gt; attention_1
  Concat(attention_0, attention_1, axis=-1) -&gt; cat
  Linear_2[aten_local_function](cat, decoder.attention.linear.weight, decoder.attention.linear.bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=Linear_3 domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;layer_norm_1&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
Transpose(weight, perm=[1,0]) -&gt; _onx_transpose0
  MatMul(layer_norm_1, _onx_transpose0) -&gt; _onx_matmul0
    Add(_onx_matmul0, bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=ReLU domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;linear_7&#39;
Relu(linear_7) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.FeedForward domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;layer_norm_1&#39;
input: &#39;decoder.feed_forward.linear_2.weight&#39;
input: &#39;decoder.feed_forward.linear_1.weight&#39;
input: &#39;decoder.feed_forward.linear_1.bias&#39;
Constant(value=[0.0262538...) -&gt; decoder.feed_forward.linear_2.bias
Linear_3[aten_local_function](layer_norm_1, decoder.feed_forward.linear_1.weight, decoder.feed_forward.linear_1.bias) -&gt; linear_1
  ReLU[aten_local_function](linear_1) -&gt; relu
  Linear_3[aten_local_function](relu, decoder.feed_forward.linear_2.weight, decoder.feed_forward.linear_2.bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.DecoderLayer domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;add&#39;
input: &#39;mask2&#39;
input: &#39;mask&#39;
input: &#39;decoder.feed_forward.linear_2.weight&#39;
input: &#39;decoder.feed_forward.linear_1.weight&#39;
input: &#39;decoder.attention.linear.weight&#39;
input: &#39;decoder.attention.attention.1.value.weight&#39;
input: &#39;decoder.attention.attention.1.query.weight&#39;
input: &#39;decoder.attention.attention.1.key.weight&#39;
input: &#39;decoder.attention.attention.0.value.weight&#39;
input: &#39;decoder.attention.attention.0.query.weight&#39;
input: &#39;decoder.attention.attention.0.key.weight&#39;
input: &#39;decoder.feed_forward.linear_1.bias&#39;
Constant(value=[1.0, 1.0,...) -&gt; decoder.norm_1.weight
Constant(value=[0.0, 0.0,...) -&gt; decoder.norm_1.bias
  LayerNorm[aten_local_function](add, decoder.norm_1.weight, decoder.norm_1.bias) -&gt; norm_1
    __main__.MultiAttentionBlock[aten_local_function](norm_1, mask2, mask, decoder.attention.linear.weight, decoder.attention.attention.1.value.weight, decoder.attention.attention.1.query.weight, decoder.attention.attention.1.key.weight, decoder.attention.attention.0.value.weight, decoder.attention.attention.0.query.weight, decoder.attention.attention.0.key.weight) -&gt; attention
      Add(attention, add) -&gt; add_1
Constant(value=[1.0, 1.0,...) -&gt; decoder.norm_2.weight
Constant(value=[0.0, 0.0,...) -&gt; decoder.norm_2.bias
  LayerNorm[aten_local_function](add_1, decoder.norm_2.weight, decoder.norm_2.bias) -&gt; norm_2
    __main__.FeedForward[aten_local_function](norm_2, decoder.feed_forward.linear_2.weight, decoder.feed_forward.linear_1.weight, decoder.feed_forward.linear_1.bias) -&gt; feed_forward
      Add(feed_forward, add_1) -&gt; output
output: name=&#39;output&#39; type=? shape=?
</pre></div>
</div>
<p>We check again there is no new discrepancies.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">sess</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module</span></a></a></a><span class="o">.</span><span class="n">SerializeToString</span><span class="p">(),</span> <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;CPUExecutionProvider&quot;</span><span class="p">])</span>
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">feeds</span></a></a></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="o">=</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">feeds</span></a></a></a><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">diff</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/bench_run.html#experimental_experiment.bench_run.max_diff" title="experimental_experiment.bench_run.max_diff" class="sphx-glr-backref-module-experimental_experiment-bench_run sphx-glr-backref-type-py-function"><span class="n">max_diff</span></a></a></a><span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y</span></a></a></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output: shape=</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span><span class="o">.</span><span class="n">shape</span></a></a></a><span class="si">}</span><span class="s2">, min=</span><span class="si">{</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s2">, max=</span><span class="si">{</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">got</span></a></a></a><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;max discrepancy=</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">diff</span></a></a></a><span class="p">[</span><span class="s1">&#39;abs&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>output: shape=(1, 30, 16), min=-3.9085307121276855, max=3.940504312515259
max discrepancy=4.76837158203125e-07
</pre></div>
</div>
<p>Let’s save the ONNX model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">onnx</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module</span></a></a></a><span class="p">,</span> <span class="s2">&quot;plot_exporter_recipes_c_modules.module.onnx&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>And visually.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/plotting.html#onnx_array_api.plotting.graphviz_helper.plot_dot" title="onnx_array_api.plotting.graphviz_helper.plot_dot" class="sphx-glr-backref-module-onnx_array_api-plotting-graphviz_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/plotting.html#onnx_array_api.plotting.graphviz_helper.plot_dot" title="onnx_array_api.plotting.graphviz_helper.plot_dot" class="sphx-glr-backref-module-onnx_array_api-plotting-graphviz_helper sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/plotting.html#onnx_array_api.plotting.graphviz_helper.plot_dot" title="onnx_array_api.plotting.graphviz_helper.plot_dot" class="sphx-glr-backref-module-onnx_array_api-plotting-graphviz_helper sphx-glr-backref-type-py-function"><span class="n">plot_dot</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module</span></a></a></a><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_exporter_recipes_c_modules_001.png" srcset="../_images/sphx_glr_plot_exporter_recipes_c_modules_001.png" alt="plot exporter recipes c modules" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;Axes: &gt;
</pre></div>
</div>
</section>
<section id="inlining">
<h2>Inlining<a class="headerlink" href="#inlining" title="Link to this heading">¶</a></h2>
<p>The ONNX graph can still be inline after this.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_inlined</span></a></a></a> <span class="o">=</span> <a href="https://onnx.ai/onnx/api/inliner.html#onnx.inliner.inline_local_functions" title="onnx.inliner.inline_local_functions" class="sphx-glr-backref-module-onnx-inliner sphx-glr-backref-type-py-function"><a href="https://onnx.ai/onnx/api/inliner.html#onnx.inliner.inline_local_functions" title="onnx.inliner.inline_local_functions" class="sphx-glr-backref-module-onnx-inliner sphx-glr-backref-type-py-function"><a href="https://onnx.ai/onnx/api/inliner.html#onnx.inliner.inline_local_functions" title="onnx.inliner.inline_local_functions" class="sphx-glr-backref-module-onnx-inliner sphx-glr-backref-type-py-function"><span class="n">inline_local_functions</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module</span></a></a></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">pretty_onnx</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_inlined</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
doc_string: large_model=False, inline=False, external_threshold=102...
input: name=&#39;input_ids&#39; type=dtype(&#39;int64&#39;) shape=[1, 30]
init: name=&#39;embedding.embedding.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;embedding.pe.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;mask&#39; type=dtype(&#39;float32&#39;) shape=(256, 256)
init: name=&#39;decoder.attention.attention.0.query.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.0.key.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.0.value.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;mask2&#39; type=dtype(&#39;float32&#39;) shape=(256, 256)
init: name=&#39;decoder.attention.attention.1.query.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.1.key.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.attention.1.value.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;decoder.attention.linear.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 32)
init: name=&#39;decoder.feed_forward.linear_1.weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 16)
init: name=&#39;decoder.feed_forward.linear_1.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;decoder.feed_forward.linear_2.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 128)
Constant(value=[1]) -&gt; init7_s1_1__11
Gather(embedding.embedding.weight, input_ids) -&gt; embedding__1
Gather(embedding.pe.weight, input_ids) -&gt; pe__1
  Add(embedding__1, pe__1) -&gt; embedding
Constant(value=[1.0, 1.0,...) -&gt; decoder.norm_1.weight__4
Constant(value=[0.0, 0.0,...) -&gt; decoder.norm_1.bias__4
Constant(value=[1.0, 1.0,...) -&gt; decoder.norm_2.weight__4
Constant(value=[0.0, 0.0,...) -&gt; decoder.norm_2.bias__4
Constant(value=[1.0, 1.0,...) -&gt; init1_s16___5
  Mul(init1_s16___5, decoder.norm_1.weight__4) -&gt; LayerNormalizationScalePattern_init1_s16___5
Constant(value=[0.0, 0.0,...) -&gt; init1_s16_2__5
  Mul(decoder.norm_1.weight__4, init1_s16_2__5) -&gt; LayerNormalizationScalePattern_init1_s16_2__5
  Add(LayerNormalizationScalePattern_init1_s16_2__5, decoder.norm_1.bias__4) -&gt; LayerNormalizationScalePattern_init1_s16_3__5
    LayerNormalization(embedding, LayerNormalizationScalePattern_init1_s16___5, LayerNormalizationScalePattern_init1_s16_3__5, axis=-1, epsilon=0.00, stash_type=1) -&gt; norm_1__4
Constant(value=[-0.150363...) -&gt; decoder.attention.linear.bias__6
Constant(value=0.25) -&gt; init1_s___7
Constant(value=[1]) -&gt; init7_s1_1__7
  Reshape(init1_s___7, init7_s1_1__7) -&gt; _onx_reshape0__7
Constant(value=[0]) -&gt; init7_s1_0__7
Constant(value=[30]) -&gt; init7_s1_30__7
  Slice(mask, init7_s1_0__7, init7_s1_30__7, init7_s1_0__7) -&gt; slice_1__7
  Slice(slice_1__7, init7_s1_0__7, init7_s1_30__7, init7_s1_1__7) -&gt; slice_2__7
Constant(value=0.0) -&gt; init1_s_2__7
  Reshape(init1_s_2__7, init7_s1_1__7) -&gt; _onx_reshape02__7
    Equal(slice_2__7, _onx_reshape02__7) -&gt; eq__7_2
Constant(value=[-inf]) -&gt; init1_s1___7
Transpose(decoder.attention.attention.0.query.weight, perm=[1,0]) -&gt; _onx_transpose0__8
  MatMul(norm_1__4, _onx_transpose0__8) -&gt; query__7
Transpose(decoder.attention.attention.0.key.weight, perm=[1,0]) -&gt; _onx_transpose0__9
  MatMul(norm_1__4, _onx_transpose0__9) -&gt; key__7
    Transpose(key__7, perm=[0,2,1]) -&gt; transpose__7_1
    MatMul(query__7, transpose__7_1) -&gt; matmul__7
    Mul(matmul__7, _onx_reshape0__7) -&gt; _onx_mul0__7
  Where(eq__7_2, init1_s1___7, _onx_mul0__7) -&gt; _onx_where0__7
    Softmax(_onx_where0__7, axis=-1) -&gt; softmax__7
Transpose(decoder.attention.attention.0.value.weight, perm=[1,0]) -&gt; _onx_transpose0__10
  MatMul(norm_1__4, _onx_transpose0__10) -&gt; value__7
    MatMul(softmax__7, value__7) -&gt; attention_0__6
Constant(value=0.25) -&gt; init1_s___11
  Reshape(init1_s___11, init7_s1_1__11) -&gt; _onx_reshape0__11
Constant(value=[0]) -&gt; init7_s1_0__11
Constant(value=[30]) -&gt; init7_s1_30__11
  Slice(mask2, init7_s1_0__11, init7_s1_30__11, init7_s1_0__11) -&gt; slice_1__11
  Slice(slice_1__11, init7_s1_0__11, init7_s1_30__11, init7_s1_1__11) -&gt; slice_2__11
Constant(value=0.0) -&gt; init1_s_2__11
  Reshape(init1_s_2__11, init7_s1_1__11) -&gt; _onx_reshape02__11
    Equal(slice_2__11, _onx_reshape02__11) -&gt; eq__11_4
Constant(value=[-inf]) -&gt; init1_s1___11
Transpose(decoder.attention.attention.1.query.weight, perm=[1,0]) -&gt; _onx_transpose0__12
  MatMul(norm_1__4, _onx_transpose0__12) -&gt; query__11
Transpose(decoder.attention.attention.1.key.weight, perm=[1,0]) -&gt; _onx_transpose0__13
  MatMul(norm_1__4, _onx_transpose0__13) -&gt; key__11
    Transpose(key__11, perm=[0,2,1]) -&gt; transpose__11_3
    MatMul(query__11, transpose__11_3) -&gt; matmul__11
    Mul(matmul__11, _onx_reshape0__11) -&gt; _onx_mul0__11
  Where(eq__11_4, init1_s1___11, _onx_mul0__11) -&gt; _onx_where0__11
    Softmax(_onx_where0__11, axis=-1) -&gt; softmax__11
Transpose(decoder.attention.attention.1.value.weight, perm=[1,0]) -&gt; _onx_transpose0__14
  MatMul(norm_1__4, _onx_transpose0__14) -&gt; value__11
    MatMul(softmax__11, value__11) -&gt; attention_1__6
      Concat(attention_0__6, attention_1__6, axis=-1) -&gt; cat__6_0
Transpose(decoder.attention.linear.weight, perm=[1,0]) -&gt; _onx_transpose0__15
  MatMul(cat__6_0, _onx_transpose0__15) -&gt; _onx_matmul0__15
  Add(_onx_matmul0__15, decoder.attention.linear.bias__6) -&gt; attention__4
    Add(attention__4, embedding) -&gt; add_1__4
Constant(value=[1.0, 1.0,...) -&gt; init1_s16___16
  Mul(init1_s16___16, decoder.norm_2.weight__4) -&gt; LayerNormalizationScalePattern_init1_s16___16
Constant(value=[0.0, 0.0,...) -&gt; init1_s16_2__16
  Mul(decoder.norm_2.weight__4, init1_s16_2__16) -&gt; LayerNormalizationScalePattern_init1_s16_2__16
  Add(LayerNormalizationScalePattern_init1_s16_2__16, decoder.norm_2.bias__4) -&gt; LayerNormalizationScalePattern_init1_s16_3__16
    LayerNormalization(add_1__4, LayerNormalizationScalePattern_init1_s16___16, LayerNormalizationScalePattern_init1_s16_3__16, axis=-1, epsilon=0.00, stash_type=1) -&gt; norm_2__4
Constant(value=[0.0262538...) -&gt; decoder.feed_forward.linear_2.bias__17
Transpose(decoder.feed_forward.linear_1.weight, perm=[1,0]) -&gt; _onx_transpose0__18
  MatMul(norm_2__4, _onx_transpose0__18) -&gt; _onx_matmul0__18
    Add(_onx_matmul0__18, decoder.feed_forward.linear_1.bias) -&gt; linear_1__17
      Relu(linear_1__17) -&gt; relu__17
Transpose(decoder.feed_forward.linear_2.weight, perm=[1,0]) -&gt; _onx_transpose0__20
  MatMul(relu__17, _onx_transpose0__20) -&gt; _onx_matmul0__20
  Add(_onx_matmul0__20, decoder.feed_forward.linear_2.bias__17) -&gt; feed_forward__4
    Add(feed_forward__4, add_1__4) -&gt; output_0
output: name=&#39;output_0&#39; type=dtype(&#39;float32&#39;) shape=[1, 30, 16]
</pre></div>
</div>
</section>
<section id="optimizations">
<h2>Optimizations<a class="headerlink" href="#optimizations" title="Link to this heading">¶</a></h2>
<p>The ONNX graph produced by the exporter without any optimization is very verbose
and less efficient. That’s why some optimizations are made to the model by default.
It is also possible to introduce kernels implemented in <a class="reference external" href="https://onnxruntime.ai/">onnxruntime</a>.
Let’s how it goes.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_optimized</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><span class="n">to_onnx</span></a></a></a><span class="p">(</span>
    <span class="n">llm</span><span class="p">,</span>
    <span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">,),</span>
    <span class="n">options</span><span class="o">=</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><span class="n">OptimizationOptions</span></a></a></a><span class="p">(</span>
        <span class="n">patterns</span><span class="o">=</span><span class="s2">&quot;default+onnxruntime&quot;</span><span class="p">,</span> <span class="n">constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">pretty_onnx</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_optimized</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[GraphBuilder.optimize] start with 75 nodes
[GraphBuilder.optimize] #patterns=51
[GraphBuilder.remove_unused] 4/46remove_initializer:p_decoder_attention_attention_0_query_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 5/46remove_initializer:p_decoder_attention_attention_0_key_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 6/46remove_initializer:p_decoder_attention_attention_0_value_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 7/46remove_initializer:p_decoder_attention_attention_1_query_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 8/46remove_initializer:p_decoder_attention_attention_1_key_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 9/46remove_initializer:p_decoder_attention_attention_1_value_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 10/46remove_initializer:p_decoder_attention_linear_weight:torch.float32[torch.Size([16, 32])]
[GraphBuilder.remove_unused] 14/46remove_initializer:p_decoder_feed_forward_linear_1_weight:torch.float32[torch.Size([128, 16])]
[GraphBuilder.remove_unused] 16/46remove_initializer:p_decoder_feed_forward_linear_2_weight:torch.float32[torch.Size([16, 128])]
[GraphBuilder.remove_unused] 18/46remove_initializer:b_decoder_attention_attention_0_mask:torch.float32[torch.Size([256, 256])]
[GraphBuilder.remove_unused] 19/46remove_initializer:b_decoder_attention_attention_1_mask:torch.float32[torch.Size([256, 256])]
[GraphBuilder.remove_unused] 23/46remove_initializer:init1_s_:float32[()]
[GraphBuilder.remove_unused] 24/46remove_initializer:init7_s1_1:int64[(1,)]
[GraphBuilder.remove_unused] 25/46remove_initializer:init7_s1_0:int64[(1,)]
[GraphBuilder.remove_unused] 26/46remove_initializer:init7_s1_30:int64[(1,)]
[GraphBuilder.remove_unused] 27/46remove_initializer:init1_s_2:float32[()]
[GraphBuilder.remove_unused] 33/46remove_initializer:slice_1:torch.float32[torch.Size([30, 256])]
[GraphBuilder.remove_unused] 40/46remove_initializer:slice_3:torch.float32[torch.Size([30, 256])]
[GraphBuilderPatternOptimization.optimize] start with 53 nodes, 28 initializers, 51 patterns, priorities=[0, 1, 2, 3]
[GraphBuilderPatternOptimization.optimize] use pattern   1/51 - P0 - BatchNormalizationPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   2/51 - P0 - BatchNormalizationTrainingPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   3/51 - P0 - CastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   4/51 - P0 - ConvBiasNullPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   5/51 - P0 - ExpandPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   6/51 - P0 - GeluErfPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   7/51 - P0 - GeluOrtPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   8/51 - P0 - GeluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   9/51 - P0 - IdentityPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  10/51 - P0 - LeakyReluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  11/51 - P0 - ReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  12/51 - P0 - ReshapeReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  13/51 - P0 - SameChildrenPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  14/51 - P0 - SoftmaxCrossEntropyLossCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  15/51 - P0 - TransposeReshapeTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  16/51 - P0 - TransposeTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  17/51 - P0 - UnsqueezeUnsqueezePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  18/51 - P1 - BiasGeluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  19/51 - P1 - CastCastBinaryPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  20/51 - P1 - CastLayerNormalizationCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  21/51 - P1 - CastOpCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  22/51 - P1 - ComputationCastOpCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  23/51 - P1 - DropoutPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  24/51 - P1 - ExpandBroadcastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  25/51 - P1 - ExpandSwapPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  26/51 - P1 - FastGeluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  27/51 - P1 - GemmTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  28/51 - P1 - LayerNormalizationPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  29/51 - P1 - LayerNormalizationScalePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  30/51 - P1 - MatMulAddPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  31/51 - P1 - MatMulReshape2Of3Pattern()
[GraphBuilderPatternOptimization.optimize] use pattern  32/51 - P1 - MulMulMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  33/51 - P1 - MulMulMulScalarPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  34/51 - P1 - ReduceReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  35/51 - P1 - ReduceSumNormalizePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  36/51 - P1 - Reshape2Of3Pattern()
[GraphBuilderPatternOptimization.optimize] use pattern  37/51 - P1 - ReshapeMatMulReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  38/51 - P1 - ReshapeReshapeBinaryPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  39/51 - P1 - RotaryConcatPartPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  40/51 - P1 - SimplifiedLayerNormalizationPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  41/51 - P1 - SlicesSplitPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  42/51 - P1 - SoftmaxGradPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  43/51 - P1 - Sub1MulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  44/51 - P1 - SwitchOrderBinaryPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  45/51 - P1 - TransposeMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  46/51 - P1 - TransposeReshapeMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  47/51 - P1 - UnsqueezeEqualPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  48/51 - P2 - FusedMatMulDivPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  49/51 - P2 - FusedMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  50/51 - P3 - FusedMatMulTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  51/51 - P3 - FusedMatMulx2Pattern()
[GraphBuilderPatternOptimization.optimize] iteration 0: 53 nodes, priority=0
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*CastPattern - time=0.014 | max_time=SoftmaxCrossEntropyLossCastPattern:0.004
[GraphBuilderPatternOptimization.optimize] iteration 1: 51 nodes, priority=0
[GraphBuilderPatternOptimization.optimize] increase priority to 1
[GraphBuilderPatternOptimization.optimize] iteration 2: 51 nodes, priority=1
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*LayerNormalizationPattern - time=0.009 | max_time=IdentityPattern:0.001
[GraphBuilderPatternOptimization.optimize] iteration 3: 39 nodes, priority=1
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*LayerNormalizationScalePattern - time=0.008 | max_time=IdentityPattern:0.001
[GraphBuilderPatternOptimization.optimize] iteration 4: 41 nodes, priority=1
[GraphBuilderPatternOptimization.optimize] increase priority to 2
[GraphBuilderPatternOptimization.optimize] iteration 5: 41 nodes, priority=2
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*FusedMatMulPattern - time=0.006 | max_time=IdentityPattern:0.001
[GraphBuilderPatternOptimization.optimize] iteration 6: 37 nodes, priority=2
[GraphBuilderPatternOptimization.optimize] increase priority to 3
[GraphBuilderPatternOptimization.optimize] iteration 7: 37 nodes, priority=3
[GraphBuilderPatternOptimization.optimize] done after 8 iterations with 37 nodes in 0.076
    STAT apply_CastPattern +2 -2 #it=1 maxmatch=1 i=2 - time=0.0002351299990550615
    STAT apply_FusedMatMulPattern +2 -6 #it=1 maxmatch=1 i=2 - time=0.0014661570021416992
    STAT apply_LayerNormalizationPattern +2 -14 #it=1 maxmatch=1 i=2 - time=0.001612208998267306
    STAT apply_LayerNormalizationScalePattern +8 -6 #it=1 maxmatch=1 i=2 - time=0.001656857999478234
    STAT build_for_pattern +0 -0 #it=8 maxmatch=0 i=0 - time=0.0034324219996051397
    STAT check_pattern_00 +0 -0 #it=1 maxmatch=0 i=0 - time=0.00028798499988624826
    STAT check_pattern_A0 +0 -0 #it=4 maxmatch=0 i=0 - time=0.001610279992746655
    STAT check_pattern_B0 +0 -0 #it=3 maxmatch=0 i=0 - time=0.000420615997427376
    STAT match_BatchNormalizationPattern +0 -0 #it=8 maxmatch=0 i=0 - time=0.0005751800017606001
    STAT match_BatchNormalizationTrainingPattern +0 -0 #it=8 maxmatch=0 i=0 - time=0.0005249440000625327
    STAT match_BiasGeluPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0004153130066697486
    STAT match_CastCastBinaryPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.0012884470052085817
    STAT match_CastLayerNormalizationCastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.0005035309950471856
    STAT match_CastOpCastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.0010113780044775922
    STAT match_CastPattern +0 -0 #it=8 maxmatch=2 i=2 - time=0.0005228279951552395
    STAT match_ComputationCastOpCastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.0007516890036640689
    STAT match_ConvBiasNullPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.00047723000170663
    STAT match_DropoutPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.0002968260014313273
    STAT match_ExpandBroadcastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.000539689001016086
    STAT match_ExpandPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0004626949958037585
    STAT match_ExpandSwapPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.00037213200630503707
    STAT match_FastGeluPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00044710899601341225
    STAT match_FusedMatMulDivPattern +0 -0 #it=3 maxmatch=2 i=0 - time=0.0003920269991795067
    STAT match_FusedMatMulPattern +0 -0 #it=3 maxmatch=2 i=2 - time=0.0008664269953442272
    STAT match_FusedMatMulTransposePattern +0 -0 #it=1 maxmatch=0 i=0 - time=6.403199950000271e-05
    STAT match_FusedMatMulx2Pattern +0 -0 #it=1 maxmatch=0 i=0 - time=6.986199878156185e-05
    STAT match_GeluErfPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.004822103001060896
    STAT match_GeluOrtPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0055566909941262566
    STAT match_GeluPattern +0 -0 #it=8 maxmatch=2 i=0 - time=2.0358995243441314e-05
    STAT match_GemmTransposePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00031650299933971837
    STAT match_IdentityPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.008763080000790069
    STAT match_LayerNormalizationPattern +0 -0 #it=6 maxmatch=2 i=2 - time=0.000779076995968353
    STAT match_LayerNormalizationScalePattern +0 -0 #it=6 maxmatch=2 i=2 - time=0.0005384139985835645
    STAT match_LeakyReluPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.005380797992984299
    STAT match_MatMulAddPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0007936110050650313
    STAT match_MatMulReshape2Of3Pattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0013552689924836159
    STAT match_MulMulMatMulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0008081600026343949
    STAT match_MulMulMulScalarPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0004969670007994864
    STAT match_ReduceReshapePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0004736139999295119
    STAT match_ReduceSumNormalizePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00033958600397454575
    STAT match_Reshape2Of3Pattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.001289054001972545
    STAT match_ReshapeMatMulReshapePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.000834900994959753
    STAT match_ReshapePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0005168720017536543
    STAT match_ReshapeReshapeBinaryPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0007207430062408093
    STAT match_ReshapeReshapePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0004518020068644546
    STAT match_RotaryConcatPartPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0005238180019659922
    STAT match_SameChildrenPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0017317149977316149
    STAT match_SimplifiedLayerNormalizationPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0006951350005692802
    STAT match_SlicesSplitPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00043415199252194725
    STAT match_SoftmaxCrossEntropyLossCastPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.007078238002577564
    STAT match_SoftmaxGradPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0004059839993715286
    STAT match_Sub1MulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0006120830003055744
    STAT match_SwitchOrderBinaryPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.001375295003526844
    STAT match_TransposeMatMulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0008386039953620639
    STAT match_TransposeReshapeMatMulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.000900467002793448
    STAT match_TransposeReshapeTransposePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0006130619985924568
    STAT match_TransposeTransposePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0006817969988333061
    STAT match_UnsqueezeEqualPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0007598710035381373
    STAT match_UnsqueezeUnsqueezePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0005397129934863187
    STAT remove_identity_nodes +2 -4 #it=3 maxmatch=0 i=0 - time=0.0008773940026003402
--MODEL: 37 nodes, 1 inputs, 1 outputs, 30 initializers--
     INPUT:   1 x 7t
    OUTPUT:   1 x 1t
      INIT:  29 x 1t
      INIT:   1 x 7t
      NODE:   8 x Add
      NODE:   1 x Concat
      NODE:   2 x Equal
      NODE:   2 x Gather
      NODE:   2 x LayerNormalization
      NODE:  11 x MatMul
      NODE:   4 x Mul
      NODE:   1 x Relu
      NODE:   2 x Softmax
      NODE:   2 x Where
      NODE:   2 x com.microsoft.FusedMatMul
--MODEL: 37 nodes, 1 inputs, 1 outputs, 30 initializers--DETAILED--
     INPUT:   1 x 7t[1x30]
    OUTPUT:   1 x 1t[1x30x16]
      INIT:   2 x 1t[1024x16]
      INIT:   1 x 1t[128]
      INIT:   1 x 1t[128x16]
      INIT:   8 x 1t[16]
      INIT:   1 x 1t[16x128]
      INIT:   6 x 1t[16x16]
      INIT:   7 x 1t[1]
      INIT:   2 x 1t[30x30]
      INIT:   1 x 1t[32x16]
      INIT:   1 x 7t[1]
      NODE:   2 x Add -SIG- 1t[16], 1t[16]
      NODE:   1 x Add -SIG- 1t[1x30x128], 1t[128]
      NODE:   2 x Add -SIG- 1t[1x30x16], 1t[16]
      NODE:   3 x Add -SIG- 1t[1x30x16], 1t[1x30x16]
      NODE:   1 x Concat -SIG- 1t[1x30x16], 1t[1x30x16]
      NODE:   2 x Equal -SIG- 1t[30x30], 1t[1]
      NODE:   2 x Gather -SIG- 1t[1024x16], 7t[1x30]
      NODE:   2 x LayerNormalization -SIG- 1t[1x30x16], 1t[16], 1t[16]
      NODE:   1 x MatMul -SIG- 1t[1x30x128], 1t[128x16]
      NODE:   1 x MatMul -SIG- 1t[1x30x16], 1t[16x128]
      NODE:   6 x MatMul -SIG- 1t[1x30x16], 1t[16x16]
      NODE:   2 x MatMul -SIG- 1t[1x30x30], 1t[1x30x16]
      NODE:   1 x MatMul -SIG- 1t[1x30x32], 1t[32x16]
      NODE:   4 x Mul -SIG- 1t[16], 1t[16]
      NODE:   1 x Relu -SIG- 1t[1x30x128]
      NODE:   2 x Softmax -SIG- 1t[1x30x30]
      NODE:   2 x Where -SIG- 9t[30x30], 1t[1], 1t[1x30x30]
      NODE:   2 x com.microsoft.FusedMatMul -SIG- 1t[1x30x16], 1t[1x30x16]
[GraphBuilder.remove_unused] 9/30remove_initializer:init7_s1_-1:int64[(1,)]
[GraphBuilder.remove_unused] 10/30remove_initializer:init1_s1_:float32[(1,)]
[GraphBuilder.remove_unused] 11/30remove_initializer:init1_s1_2:float32[(1,)]
[GraphBuilder.remove_unused] 16/30remove_initializer:_onx_reshape0:float32[(1,)]
[GraphBuilder.remove_unused] 22/30remove_initializer:_onx_reshape03:float32[(1,)]
[GraphBuilder.remove_unused] 2/31remove_initializer:p_decoder_norm_1_weight:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 3/31remove_initializer:p_decoder_norm_1_bias:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 5/31remove_initializer:p_decoder_norm_2_weight:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 6/31remove_initializer:p_decoder_norm_2_bias:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 23/31remove_initializer:init1_s16_:float32[(16,)]
[GraphBuilder.remove_unused] 24/31remove_initializer:init1_s16_2:float32[(16,)]
[GraphBuilder.remove_unused] 26/31remove_initializer:LayerNormalizationScalePattern_init1_s16_2:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 29/31remove_initializer:LayerNormalizationScalePattern_init1_s16_5:torch.float32[torch.Size([16])]
[GraphBuilder.optimize] done with 31 nodes in 0.094
opset: domain=&#39;&#39; version=18
opset: domain=&#39;com.microsoft&#39; version=1
doc_string: large_model=False, inline=False, external_threshold=102...
input: name=&#39;input_ids&#39; type=dtype(&#39;int64&#39;) shape=[1, 30]
init: name=&#39;p_embedding_embedding_weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;p_embedding_pe_weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;p_decoder_attention_linear_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;p_decoder_feed_forward_linear_1_bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;p_decoder_feed_forward_linear_2_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;init1_s1_3&#39; type=dtype(&#39;float32&#39;) shape=(1,) -- array([-inf], dtype=float32)
init: name=&#39;_onx_transpose0&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose02&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose03&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;slice_2&#39; type=dtype(&#39;float32&#39;) shape=(30, 30)
init: name=&#39;_onx_reshape02&#39; type=dtype(&#39;float32&#39;) shape=(1,) -- array([0.], dtype=float32)
init: name=&#39;_onx_transpose04&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose05&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose06&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;slice_4&#39; type=dtype(&#39;float32&#39;) shape=(30, 30)
init: name=&#39;_onx_reshape04&#39; type=dtype(&#39;float32&#39;) shape=(1,) -- array([0.], dtype=float32)
init: name=&#39;_onx_transpose07&#39; type=dtype(&#39;float32&#39;) shape=(32, 16)
init: name=&#39;_onx_transpose08&#39; type=dtype(&#39;float32&#39;) shape=(16, 128)
init: name=&#39;_onx_transpose09&#39; type=dtype(&#39;float32&#39;) shape=(128, 16)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_3&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_4&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_6&#39; type=dtype(&#39;float32&#39;) shape=(16,)
Equal(slice_2, _onx_reshape02) -&gt; eq
Gather(p_embedding_embedding_weight, input_ids) -&gt; embedding
Gather(p_embedding_pe_weight, input_ids) -&gt; embedding_1
  Add(embedding, embedding_1) -&gt; add
    LayerNormalization(add, LayerNormalizationScalePattern_init1_s16_, LayerNormalizationScalePattern_init1_s16_3, axis=-1, epsilon=0.00, stash_type=1) -&gt; _onx_add02
      MatMul(_onx_add02, _onx_transpose0) -&gt; linear
MatMul(_onx_add02, _onx_transpose02) -&gt; linear_1
  FusedMatMul[com.microsoft](linear, linear_1, alpha=0.25, transA=0, transB=1, transBatchA=0, transBatchB=0) -&gt; _onx_mul02
  Where(eq, init1_s1_3, _onx_mul02) -&gt; _onx_where0
    Softmax(_onx_where0, axis=-1) -&gt; softmax
MatMul(_onx_add02, _onx_transpose03) -&gt; linear_2
  MatMul(softmax, linear_2) -&gt; matmul_1
MatMul(_onx_add02, _onx_transpose04) -&gt; linear_3
MatMul(_onx_add02, _onx_transpose05) -&gt; linear_4
  FusedMatMul[com.microsoft](linear_3, linear_4, alpha=0.25, transA=0, transB=1, transBatchA=0, transBatchB=0) -&gt; _onx_mul03
MatMul(_onx_add02, _onx_transpose06) -&gt; linear_5
Equal(slice_4, _onx_reshape04) -&gt; eq_1
  Where(eq_1, init1_s1_3, _onx_mul03) -&gt; _onx_where02
    Softmax(_onx_where02, axis=-1) -&gt; softmax_1
  MatMul(softmax_1, linear_5) -&gt; matmul_3
    Concat(matmul_1, matmul_3, axis=-1) -&gt; cat
      MatMul(cat, _onx_transpose07) -&gt; _onx_matmul0
        Add(_onx_matmul0, p_decoder_attention_linear_bias) -&gt; linear_6
    Add(linear_6, add) -&gt; add_1
      LayerNormalization(add_1, LayerNormalizationScalePattern_init1_s16_4, LayerNormalizationScalePattern_init1_s16_6, axis=-1, epsilon=0.00, stash_type=1) -&gt; _onx_add04
        MatMul(_onx_add04, _onx_transpose08) -&gt; _onx_matmul02
          Add(_onx_matmul02, p_decoder_feed_forward_linear_1_bias) -&gt; linear_7
            Relu(linear_7) -&gt; relu
              MatMul(relu, _onx_transpose09) -&gt; _onx_matmul03
                Add(_onx_matmul03, p_decoder_feed_forward_linear_2_bias) -&gt; linear_8
      Add(linear_8, add_1) -&gt; output_0
output: name=&#39;output_0&#39; type=dtype(&#39;float32&#39;) shape=[1, 30, 16]
</pre></div>
</div>
<p>This shows a kernel <code class="docutils literal notranslate"><span class="pre">FusedMatMul[com.microsoft]</span></code> which implement a kernel equivalent Gemm
but working for any tensors, not only 2D.
How does it work on the model which keeps exports the moduels as local functions?
The optimizer optimizes every local function independantly.
We reduce the verbosity…</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module_optimized</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><span class="n">to_onnx</span></a></a></a><span class="p">(</span>
    <span class="n">llm</span><span class="p">,</span>
    <span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">,),</span>
    <span class="n">options</span><span class="o">=</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><span class="n">OptimizationOptions</span></a></a></a><span class="p">(</span><span class="n">patterns</span><span class="o">=</span><span class="s2">&quot;default+onnxruntime&quot;</span><span class="p">,</span> <span class="n">constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">export_modules_as_functions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">pretty_onnx</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_module_optimized</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
doc_string: large_model=False, inline=False, external_threshold=102...
input: name=&#39;input_ids&#39; type=dtype(&#39;int64&#39;) shape=[1, 30]
init: name=&#39;embedding.embedding.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;embedding.pe.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;_onx_transpose0&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose02&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose03&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;slice_2&#39; type=dtype(&#39;float32&#39;) shape=(30, 30)
init: name=&#39;_onx_transpose04&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose022&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose032&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;slice_4&#39; type=dtype(&#39;float32&#39;) shape=(30, 30)
init: name=&#39;_onx_transpose05&#39; type=dtype(&#39;float32&#39;) shape=(32, 16)
init: name=&#39;decoder.feed_forward.linear_1.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;_onx_transpose06&#39; type=dtype(&#39;float32&#39;) shape=(16, 128)
init: name=&#39;_onx_transpose023&#39; type=dtype(&#39;float32&#39;) shape=(128, 16)
__main__.Embedding[aten_local_function](input_ids, embedding.pe.weight, embedding.embedding.weight) -&gt; embedding
  __main__.DecoderLayer[aten_local_function](embedding, _onx_transpose06, _onx_transpose023, slice_4, slice_2, _onx_transpose05, _onx_transpose04, _onx_transpose032, _onx_transpose03, _onx_transpose022, _onx_transpose02, _onx_transpose0, decoder.feed_forward.linear_1.bias) -&gt; output_0
output: name=&#39;output_0&#39; type=dtype(&#39;float32&#39;) shape=[1, 30, 16]
----- function name=Embedding domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
input: &#39;input_ids&#39;
input: &#39;weight&#39;
Gather(weight, input_ids) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.Embedding domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;input_ids&#39;
input: &#39;embedding.pe.weight&#39;
input: &#39;embedding.embedding.weight&#39;
Embedding[aten_local_function](input_ids, embedding.embedding.weight) -&gt; embedding
Embedding[aten_local_function](input_ids, embedding.pe.weight) -&gt; pe
  Add(embedding, pe) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=LayerNorm domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;add&#39;
Constant(value=[1.0, 1.0,...) -&gt; LayerNormalizationScalePattern_init1_s16_
Constant(value=[0.0, 0.0,...) -&gt; LayerNormalizationScalePattern_init1_s16_3
  LayerNormalization(add, LayerNormalizationScalePattern_init1_s16_, LayerNormalizationScalePattern_init1_s16_3, axis=-1, epsilon=0.00, stash_type=1) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=Linear domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
input: &#39;layer_norm&#39;
input: &#39;_onx_transpose0&#39;
MatMul(layer_norm, _onx_transpose0) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.AttentionBlock domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
input: &#39;layer_norm&#39;
input: &#39;slice_2&#39;
input: &#39;_onx_transpose03&#39;
input: &#39;_onx_transpose02&#39;
input: &#39;_onx_transpose0&#39;
Constant(value=[-inf]) -&gt; init1_s1_
Constant(value=[0.0]) -&gt; _onx_reshape02
  Equal(slice_2, _onx_reshape02) -&gt; eq
Linear[aten_local_function](layer_norm, _onx_transpose0) -&gt; query
Linear[aten_local_function](layer_norm, _onx_transpose02) -&gt; key
  FusedMatMul[com.microsoft](query, key, alpha=0.25, transA=0, transB=1, transBatchA=0, transBatchB=0) -&gt; _onx_mul0
  Where(eq, init1_s1_, _onx_mul0) -&gt; _onx_where0
    Softmax(_onx_where0, axis=-1) -&gt; softmax
Linear[aten_local_function](layer_norm, _onx_transpose03) -&gt; value
  MatMul(softmax, value) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=Linear_2 domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
input: &#39;cat&#39;
input: &#39;_onx_transpose0&#39;
input: &#39;bias&#39;
MatMul(cat, _onx_transpose0) -&gt; _onx_matmul0
  Add(_onx_matmul0, bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.MultiAttentionBlock domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
input: &#39;layer_norm&#39;
input: &#39;slice_4&#39;
input: &#39;slice_2&#39;
input: &#39;_onx_transpose05&#39;
input: &#39;_onx_transpose04&#39;
input: &#39;_onx_transpose032&#39;
input: &#39;_onx_transpose03&#39;
input: &#39;_onx_transpose022&#39;
input: &#39;_onx_transpose02&#39;
input: &#39;_onx_transpose0&#39;
Constant(value=[-0.150363...) -&gt; decoder.attention.linear.bias
__main__.AttentionBlock[aten_local_function](layer_norm, slice_2, _onx_transpose03, _onx_transpose02, _onx_transpose0) -&gt; attention_0
__main__.AttentionBlock[aten_local_function](layer_norm, slice_4, _onx_transpose032, _onx_transpose022, _onx_transpose04) -&gt; attention_1
  Concat(attention_0, attention_1, axis=-1) -&gt; cat
  Linear_2[aten_local_function](cat, _onx_transpose05, decoder.attention.linear.bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=Linear_3 domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
input: &#39;layer_norm_1&#39;
input: &#39;_onx_transpose0&#39;
input: &#39;bias&#39;
MatMul(layer_norm_1, _onx_transpose0) -&gt; _onx_matmul0
  Add(_onx_matmul0, bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=ReLU domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
input: &#39;linear_7&#39;
Relu(linear_7) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.FeedForward domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
input: &#39;layer_norm_1&#39;
input: &#39;_onx_transpose02&#39;
input: &#39;_onx_transpose0&#39;
input: &#39;decoder.feed_forward.linear_1.bias&#39;
Constant(value=[0.0262538...) -&gt; decoder.feed_forward.linear_2.bias
Linear_3[aten_local_function](layer_norm_1, _onx_transpose0, decoder.feed_forward.linear_1.bias) -&gt; linear_1
  ReLU[aten_local_function](linear_1) -&gt; relu
  Linear_3[aten_local_function](relu, _onx_transpose02, decoder.feed_forward.linear_2.bias) -&gt; output
output: name=&#39;output&#39; type=? shape=?
----- function name=__main__.DecoderLayer domain=aten_local_function
----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
opset: domain=&#39;&#39; version=18
opset: domain=&#39;aten_local_function&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
input: &#39;add&#39;
input: &#39;_onx_transpose06&#39;
input: &#39;_onx_transpose023&#39;
input: &#39;slice_4&#39;
input: &#39;slice_2&#39;
input: &#39;_onx_transpose05&#39;
input: &#39;_onx_transpose04&#39;
input: &#39;_onx_transpose032&#39;
input: &#39;_onx_transpose03&#39;
input: &#39;_onx_transpose022&#39;
input: &#39;_onx_transpose02&#39;
input: &#39;_onx_transpose0&#39;
input: &#39;decoder.feed_forward.linear_1.bias&#39;
LayerNorm[aten_local_function](add) -&gt; norm_1
  __main__.MultiAttentionBlock[aten_local_function](norm_1, slice_4, slice_2, _onx_transpose05, _onx_transpose04, _onx_transpose032, _onx_transpose03, _onx_transpose022, _onx_transpose02, _onx_transpose0) -&gt; attention
    Add(attention, add) -&gt; add_1
      LayerNorm[aten_local_function](add_1) -&gt; norm_2
        __main__.FeedForward[aten_local_function](norm_2, _onx_transpose023, _onx_transpose06, decoder.feed_forward.linear_1.bias) -&gt; feed_forward
      Add(feed_forward, add_1) -&gt; output
output: name=&#39;output&#39; type=? shape=?
</pre></div>
</div>
<p>It seems to be working as well on this simple case even though the optimizers were
not tested on such models. However, keeping the submodule information might be useful
to implement optimizer for a fmaily of models sharing the same components.</p>
</section>
<section id="optimizations-for-cuda">
<h2>Optimizations for CUDA<a class="headerlink" href="#optimizations-for-cuda" title="Link to this heading">¶</a></h2>
<p>The optimizer may have a different behaviour knowning the model is running on CUDA.
It may use different kernels and do different optimization if needed.
That may not be the good place to do it as the runtime may choose to run one kernel on CPU,
another one on CUDA. The current optimization does not know that and
is not able to decide which provider would be more useful for some kernels.
This coudl even be decided at runtime.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_cuda_optimized</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/torch_interpreter/index.html#experimental_experiment.torch_interpreter.to_onnx" title="experimental_experiment.torch_interpreter.to_onnx" class="sphx-glr-backref-module-experimental_experiment-torch_interpreter sphx-glr-backref-type-py-function"><span class="n">to_onnx</span></a></a></a><span class="p">(</span>
    <span class="n">llm</span><span class="p">,</span>
    <span class="p">(</span><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_ids</span></a></a></a><span class="p">,),</span>
    <span class="n">options</span><span class="o">=</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/xbuilder/index.html#experimental_experiment.xbuilder.OptimizationOptions" title="experimental_experiment.xbuilder.OptimizationOptions" class="sphx-glr-backref-module-experimental_experiment-xbuilder sphx-glr-backref-type-py-class"><span class="n">OptimizationOptions</span></a></a></a><span class="p">(</span>
        <span class="n">patterns</span><span class="o">=</span><span class="s2">&quot;default+onnxruntime&quot;</span><span class="p">,</span> <span class="n">constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">processor</span><span class="o">=</span><span class="s2">&quot;CUDA&quot;</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/experimental-experiment/dev/api/helpers.html#experimental_experiment.helpers.pretty_onnx" title="experimental_experiment.helpers.pretty_onnx" class="sphx-glr-backref-module-experimental_experiment-helpers sphx-glr-backref-type-py-function"><span class="n">pretty_onnx</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_cuda_optimized</span></a></a></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[GraphBuilder.optimize] start with 75 nodes
[GraphBuilder.optimize] #patterns=51
[GraphBuilder.remove_unused] 4/46remove_initializer:p_decoder_attention_attention_0_query_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 5/46remove_initializer:p_decoder_attention_attention_0_key_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 6/46remove_initializer:p_decoder_attention_attention_0_value_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 7/46remove_initializer:p_decoder_attention_attention_1_query_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 8/46remove_initializer:p_decoder_attention_attention_1_key_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 9/46remove_initializer:p_decoder_attention_attention_1_value_weight:torch.float32[torch.Size([16, 16])]
[GraphBuilder.remove_unused] 10/46remove_initializer:p_decoder_attention_linear_weight:torch.float32[torch.Size([16, 32])]
[GraphBuilder.remove_unused] 14/46remove_initializer:p_decoder_feed_forward_linear_1_weight:torch.float32[torch.Size([128, 16])]
[GraphBuilder.remove_unused] 16/46remove_initializer:p_decoder_feed_forward_linear_2_weight:torch.float32[torch.Size([16, 128])]
[GraphBuilder.remove_unused] 18/46remove_initializer:b_decoder_attention_attention_0_mask:torch.float32[torch.Size([256, 256])]
[GraphBuilder.remove_unused] 19/46remove_initializer:b_decoder_attention_attention_1_mask:torch.float32[torch.Size([256, 256])]
[GraphBuilder.remove_unused] 23/46remove_initializer:init1_s_:float32[()]
[GraphBuilder.remove_unused] 24/46remove_initializer:init7_s1_1:int64[(1,)]
[GraphBuilder.remove_unused] 25/46remove_initializer:init7_s1_0:int64[(1,)]
[GraphBuilder.remove_unused] 26/46remove_initializer:init7_s1_30:int64[(1,)]
[GraphBuilder.remove_unused] 27/46remove_initializer:init1_s_2:float32[()]
[GraphBuilder.remove_unused] 33/46remove_initializer:slice_1:torch.float32[torch.Size([30, 256])]
[GraphBuilder.remove_unused] 40/46remove_initializer:slice_3:torch.float32[torch.Size([30, 256])]
[GraphBuilderPatternOptimization.optimize] start with 53 nodes, 28 initializers, 51 patterns, priorities=[0, 1, 2, 3]
[GraphBuilderPatternOptimization.optimize] use pattern   1/51 - P0 - BatchNormalizationPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   2/51 - P0 - BatchNormalizationTrainingPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   3/51 - P0 - CastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   4/51 - P0 - ConvBiasNullPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   5/51 - P0 - ExpandPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   6/51 - P0 - GeluErfPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   7/51 - P0 - GeluOrtPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   8/51 - P0 - GeluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern   9/51 - P0 - IdentityPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  10/51 - P0 - LeakyReluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  11/51 - P0 - ReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  12/51 - P0 - ReshapeReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  13/51 - P0 - SameChildrenPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  14/51 - P0 - SoftmaxCrossEntropyLossCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  15/51 - P0 - TransposeReshapeTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  16/51 - P0 - TransposeTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  17/51 - P0 - UnsqueezeUnsqueezePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  18/51 - P1 - BiasGeluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  19/51 - P1 - CastCastBinaryPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  20/51 - P1 - CastLayerNormalizationCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  21/51 - P1 - CastOpCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  22/51 - P1 - ComputationCastOpCastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  23/51 - P1 - DropoutPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  24/51 - P1 - ExpandBroadcastPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  25/51 - P1 - ExpandSwapPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  26/51 - P1 - FastGeluPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  27/51 - P1 - GemmTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  28/51 - P1 - LayerNormalizationPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  29/51 - P1 - LayerNormalizationScalePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  30/51 - P1 - MatMulAddPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  31/51 - P1 - MatMulReshape2Of3Pattern()
[GraphBuilderPatternOptimization.optimize] use pattern  32/51 - P1 - MulMulMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  33/51 - P1 - MulMulMulScalarPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  34/51 - P1 - ReduceReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  35/51 - P1 - ReduceSumNormalizePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  36/51 - P1 - Reshape2Of3Pattern()
[GraphBuilderPatternOptimization.optimize] use pattern  37/51 - P1 - ReshapeMatMulReshapePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  38/51 - P1 - ReshapeReshapeBinaryPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  39/51 - P1 - RotaryConcatPartPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  40/51 - P1 - SimplifiedLayerNormalizationPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  41/51 - P1 - SlicesSplitPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  42/51 - P1 - SoftmaxGradPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  43/51 - P1 - Sub1MulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  44/51 - P1 - SwitchOrderBinaryPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  45/51 - P1 - TransposeMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  46/51 - P1 - TransposeReshapeMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  47/51 - P1 - UnsqueezeEqualPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  48/51 - P2 - FusedMatMulDivPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  49/51 - P2 - FusedMatMulPattern()
[GraphBuilderPatternOptimization.optimize] use pattern  50/51 - P3 - FusedMatMulTransposePattern()
[GraphBuilderPatternOptimization.optimize] use pattern  51/51 - P3 - FusedMatMulx2Pattern()
[GraphBuilderPatternOptimization.optimize] iteration 0: 53 nodes, priority=0
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*CastPattern - time=0.013 | max_time=SoftmaxCrossEntropyLossCastPattern:0.003
[GraphBuilderPatternOptimization.optimize] iteration 1: 51 nodes, priority=0
[GraphBuilderPatternOptimization.optimize] increase priority to 1
[GraphBuilderPatternOptimization.optimize] iteration 2: 51 nodes, priority=1
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*LayerNormalizationPattern - time=0.005 | max_time=IdentityPattern:0.001
[GraphBuilderPatternOptimization.optimize] iteration 3: 39 nodes, priority=1
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*LayerNormalizationScalePattern - time=0.004 | max_time=IdentityPattern:0.001
[GraphBuilderPatternOptimization.optimize] iteration 4: 41 nodes, priority=1
[GraphBuilderPatternOptimization.optimize] increase priority to 2
[GraphBuilderPatternOptimization.optimize] iteration 5: 41 nodes, priority=2
[GraphBuilderPatternOptimization.optimize] applies 2 matches, 2*FusedMatMulPattern - time=0.007 | max_time=IdentityPattern:0.001
[GraphBuilderPatternOptimization.optimize] iteration 6: 37 nodes, priority=2
[GraphBuilderPatternOptimization.optimize] increase priority to 3
[GraphBuilderPatternOptimization.optimize] iteration 7: 37 nodes, priority=3
[GraphBuilderPatternOptimization.optimize] done after 8 iterations with 37 nodes in 0.059
    STAT apply_CastPattern +2 -2 #it=1 maxmatch=1 i=2 - time=0.00033199400058947504
    STAT apply_FusedMatMulPattern +2 -6 #it=1 maxmatch=1 i=2 - time=0.00044636500024353154
    STAT apply_LayerNormalizationPattern +2 -14 #it=1 maxmatch=1 i=2 - time=0.0004514319989539217
    STAT apply_LayerNormalizationScalePattern +8 -6 #it=1 maxmatch=1 i=2 - time=0.0007248299989441875
    STAT build_for_pattern +0 -0 #it=8 maxmatch=0 i=0 - time=0.0022589520012843423
    STAT check_pattern_00 +0 -0 #it=1 maxmatch=0 i=0 - time=0.00031031700200401247
    STAT check_pattern_A0 +0 -0 #it=4 maxmatch=0 i=0 - time=0.0009253140015061945
    STAT check_pattern_B0 +0 -0 #it=3 maxmatch=0 i=0 - time=0.00036864899811916985
    STAT match_BatchNormalizationPattern +0 -0 #it=8 maxmatch=0 i=0 - time=0.0004700450044765603
    STAT match_BatchNormalizationTrainingPattern +0 -0 #it=8 maxmatch=0 i=0 - time=0.000425509009801317
    STAT match_BiasGeluPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0002755290006462019
    STAT match_CastCastBinaryPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.000751970994315343
    STAT match_CastLayerNormalizationCastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.00033706999602145515
    STAT match_CastOpCastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.0007624669997312594
    STAT match_CastPattern +0 -0 #it=8 maxmatch=2 i=2 - time=0.00047389600513270125
    STAT match_ComputationCastOpCastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.000455398992926348
    STAT match_ConvBiasNullPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.00040974299918161705
    STAT match_DropoutPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.00021438399926410057
    STAT match_ExpandBroadcastPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.00023231499289977364
    STAT match_ExpandPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.00040036199789028615
    STAT match_ExpandSwapPattern +0 -0 #it=6 maxmatch=0 i=0 - time=0.00022855099814478308
    STAT match_FastGeluPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00030827800583210774
    STAT match_FusedMatMulDivPattern +0 -0 #it=3 maxmatch=2 i=0 - time=0.0003567460007616319
    STAT match_FusedMatMulPattern +0 -0 #it=3 maxmatch=2 i=2 - time=0.0007394730018859264
    STAT match_FusedMatMulTransposePattern +0 -0 #it=1 maxmatch=0 i=0 - time=5.263699858915061e-05
    STAT match_FusedMatMulx2Pattern +0 -0 #it=1 maxmatch=0 i=0 - time=6.395899981725961e-05
    STAT match_GeluErfPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0036905640008626506
    STAT match_GeluOrtPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.004693301001680084
    STAT match_GeluPattern +0 -0 #it=8 maxmatch=2 i=0 - time=1.3928998669143766e-05
    STAT match_GemmTransposePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00035539600139600225
    STAT match_IdentityPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.007192787001258694
    STAT match_LayerNormalizationPattern +0 -0 #it=6 maxmatch=2 i=2 - time=0.00045746899559162557
    STAT match_LayerNormalizationScalePattern +0 -0 #it=6 maxmatch=2 i=2 - time=0.000513541996042477
    STAT match_LeakyReluPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.004610444000718417
    STAT match_MatMulAddPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0006537789995491039
    STAT match_MatMulReshape2Of3Pattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0011999769958492834
    STAT match_MulMulMatMulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.000663203994918149
    STAT match_MulMulMulScalarPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0005202160064072814
    STAT match_ReduceReshapePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0004569890006678179
    STAT match_ReduceSumNormalizePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00030697799593326636
    STAT match_Reshape2Of3Pattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0012463210005080327
    STAT match_ReshapeMatMulReshapePattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0006260909940465353
    STAT match_ReshapePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0006521489995066077
    STAT match_ReshapeReshapeBinaryPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0006813899999542627
    STAT match_ReshapeReshapePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0004642429994419217
    STAT match_RotaryConcatPartPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0005402189999585971
    STAT match_SameChildrenPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0013755860018136445
    STAT match_SimplifiedLayerNormalizationPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.000347028995747678
    STAT match_SlicesSplitPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0003770200055441819
    STAT match_SoftmaxCrossEntropyLossCastPattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.006253109993849648
    STAT match_SoftmaxGradPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.00026055299895233475
    STAT match_Sub1MulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0003796259989030659
    STAT match_SwitchOrderBinaryPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0010040179986390285
    STAT match_TransposeMatMulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0009082119977392722
    STAT match_TransposeReshapeMatMulPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0006176680035423487
    STAT match_TransposeReshapeTransposePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0005275410003378056
    STAT match_TransposeTransposePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.0004835229992750101
    STAT match_UnsqueezeEqualPattern +0 -0 #it=6 maxmatch=2 i=0 - time=0.0004754610054078512
    STAT match_UnsqueezeUnsqueezePattern +0 -0 #it=8 maxmatch=2 i=0 - time=0.00039690500125288963
    STAT remove_identity_nodes +2 -4 #it=3 maxmatch=0 i=0 - time=0.0007931020008982159
--MODEL: 37 nodes, 1 inputs, 1 outputs, 30 initializers--
     INPUT:   1 x 7t
    OUTPUT:   1 x 1t
      INIT:  29 x 1t
      INIT:   1 x 7t
      NODE:   8 x Add
      NODE:   1 x Concat
      NODE:   2 x Equal
      NODE:   2 x Gather
      NODE:   2 x LayerNormalization
      NODE:  11 x MatMul
      NODE:   4 x Mul
      NODE:   1 x Relu
      NODE:   2 x Softmax
      NODE:   2 x Where
      NODE:   2 x com.microsoft.FusedMatMul
--MODEL: 37 nodes, 1 inputs, 1 outputs, 30 initializers--DETAILED--
     INPUT:   1 x 7t[1x30]
    OUTPUT:   1 x 1t[1x30x16]
      INIT:   2 x 1t[1024x16]
      INIT:   1 x 1t[128]
      INIT:   1 x 1t[128x16]
      INIT:   8 x 1t[16]
      INIT:   1 x 1t[16x128]
      INIT:   6 x 1t[16x16]
      INIT:   7 x 1t[1]
      INIT:   2 x 1t[30x30]
      INIT:   1 x 1t[32x16]
      INIT:   1 x 7t[1]
      NODE:   2 x Add -SIG- 1t[16], 1t[16]
      NODE:   1 x Add -SIG- 1t[1x30x128], 1t[128]
      NODE:   2 x Add -SIG- 1t[1x30x16], 1t[16]
      NODE:   3 x Add -SIG- 1t[1x30x16], 1t[1x30x16]
      NODE:   1 x Concat -SIG- 1t[1x30x16], 1t[1x30x16]
      NODE:   2 x Equal -SIG- 1t[30x30], 1t[1]
      NODE:   2 x Gather -SIG- 1t[1024x16], 7t[1x30]
      NODE:   2 x LayerNormalization -SIG- 1t[1x30x16], 1t[16], 1t[16]
      NODE:   1 x MatMul -SIG- 1t[1x30x128], 1t[128x16]
      NODE:   1 x MatMul -SIG- 1t[1x30x16], 1t[16x128]
      NODE:   6 x MatMul -SIG- 1t[1x30x16], 1t[16x16]
      NODE:   2 x MatMul -SIG- 1t[1x30x30], 1t[1x30x16]
      NODE:   1 x MatMul -SIG- 1t[1x30x32], 1t[32x16]
      NODE:   4 x Mul -SIG- 1t[16], 1t[16]
      NODE:   1 x Relu -SIG- 1t[1x30x128]
      NODE:   2 x Softmax -SIG- 1t[1x30x30]
      NODE:   2 x Where -SIG- 9t[30x30], 1t[1], 1t[1x30x30]
      NODE:   2 x com.microsoft.FusedMatMul -SIG- 1t[1x30x16], 1t[1x30x16]
[GraphBuilder.remove_unused] 9/30remove_initializer:init7_s1_-1:int64[(1,)]
[GraphBuilder.remove_unused] 10/30remove_initializer:init1_s1_:float32[(1,)]
[GraphBuilder.remove_unused] 11/30remove_initializer:init1_s1_2:float32[(1,)]
[GraphBuilder.remove_unused] 16/30remove_initializer:_onx_reshape0:float32[(1,)]
[GraphBuilder.remove_unused] 22/30remove_initializer:_onx_reshape03:float32[(1,)]
[GraphBuilder.remove_unused] 2/31remove_initializer:p_decoder_norm_1_weight:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 3/31remove_initializer:p_decoder_norm_1_bias:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 5/31remove_initializer:p_decoder_norm_2_weight:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 6/31remove_initializer:p_decoder_norm_2_bias:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 23/31remove_initializer:init1_s16_:float32[(16,)]
[GraphBuilder.remove_unused] 24/31remove_initializer:init1_s16_2:float32[(16,)]
[GraphBuilder.remove_unused] 26/31remove_initializer:LayerNormalizationScalePattern_init1_s16_2:torch.float32[torch.Size([16])]
[GraphBuilder.remove_unused] 29/31remove_initializer:LayerNormalizationScalePattern_init1_s16_5:torch.float32[torch.Size([16])]
[GraphBuilder.optimize] done with 31 nodes in 0.074
opset: domain=&#39;&#39; version=18
opset: domain=&#39;com.microsoft&#39; version=1
doc_string: large_model=False, inline=False, external_threshold=102...
input: name=&#39;input_ids&#39; type=dtype(&#39;int64&#39;) shape=[1, 30]
init: name=&#39;p_embedding_embedding_weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;p_embedding_pe_weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 16)
init: name=&#39;p_decoder_attention_linear_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;p_decoder_feed_forward_linear_1_bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;p_decoder_feed_forward_linear_2_bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;init1_s1_3&#39; type=dtype(&#39;float32&#39;) shape=(1,) -- array([-inf], dtype=float32)
init: name=&#39;_onx_transpose0&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose02&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose03&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;slice_2&#39; type=dtype(&#39;float32&#39;) shape=(30, 30)
init: name=&#39;_onx_reshape02&#39; type=dtype(&#39;float32&#39;) shape=(1,) -- array([0.], dtype=float32)
init: name=&#39;_onx_transpose04&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose05&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;_onx_transpose06&#39; type=dtype(&#39;float32&#39;) shape=(16, 16)
init: name=&#39;slice_4&#39; type=dtype(&#39;float32&#39;) shape=(30, 30)
init: name=&#39;_onx_reshape04&#39; type=dtype(&#39;float32&#39;) shape=(1,) -- array([0.], dtype=float32)
init: name=&#39;_onx_transpose07&#39; type=dtype(&#39;float32&#39;) shape=(32, 16)
init: name=&#39;_onx_transpose08&#39; type=dtype(&#39;float32&#39;) shape=(16, 128)
init: name=&#39;_onx_transpose09&#39; type=dtype(&#39;float32&#39;) shape=(128, 16)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_3&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_4&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;LayerNormalizationScalePattern_init1_s16_6&#39; type=dtype(&#39;float32&#39;) shape=(16,)
Equal(slice_2, _onx_reshape02) -&gt; eq
Gather(p_embedding_embedding_weight, input_ids) -&gt; embedding
Gather(p_embedding_pe_weight, input_ids) -&gt; embedding_1
  Add(embedding, embedding_1) -&gt; add
    LayerNormalization(add, LayerNormalizationScalePattern_init1_s16_, LayerNormalizationScalePattern_init1_s16_3, axis=-1, epsilon=0.00, stash_type=1) -&gt; _onx_add02
      MatMul(_onx_add02, _onx_transpose0) -&gt; linear
MatMul(_onx_add02, _onx_transpose02) -&gt; linear_1
  FusedMatMul[com.microsoft](linear, linear_1, alpha=0.25, transA=0, transB=1, transBatchA=0, transBatchB=0) -&gt; _onx_mul02
  Where(eq, init1_s1_3, _onx_mul02) -&gt; _onx_where0
    Softmax(_onx_where0, axis=-1) -&gt; softmax
MatMul(_onx_add02, _onx_transpose03) -&gt; linear_2
  MatMul(softmax, linear_2) -&gt; matmul_1
MatMul(_onx_add02, _onx_transpose04) -&gt; linear_3
MatMul(_onx_add02, _onx_transpose05) -&gt; linear_4
  FusedMatMul[com.microsoft](linear_3, linear_4, alpha=0.25, transA=0, transB=1, transBatchA=0, transBatchB=0) -&gt; _onx_mul03
MatMul(_onx_add02, _onx_transpose06) -&gt; linear_5
Equal(slice_4, _onx_reshape04) -&gt; eq_1
  Where(eq_1, init1_s1_3, _onx_mul03) -&gt; _onx_where02
    Softmax(_onx_where02, axis=-1) -&gt; softmax_1
  MatMul(softmax_1, linear_5) -&gt; matmul_3
    Concat(matmul_1, matmul_3, axis=-1) -&gt; cat
      MatMul(cat, _onx_transpose07) -&gt; _onx_matmul0
        Add(_onx_matmul0, p_decoder_attention_linear_bias) -&gt; linear_6
    Add(linear_6, add) -&gt; add_1
      LayerNormalization(add_1, LayerNormalizationScalePattern_init1_s16_4, LayerNormalizationScalePattern_init1_s16_6, axis=-1, epsilon=0.00, stash_type=1) -&gt; _onx_add04
        MatMul(_onx_add04, _onx_transpose08) -&gt; _onx_matmul02
          Add(_onx_matmul02, p_decoder_feed_forward_linear_1_bias) -&gt; linear_7
            Relu(linear_7) -&gt; relu
              MatMul(relu, _onx_transpose09) -&gt; _onx_matmul03
                Add(_onx_matmul03, p_decoder_feed_forward_linear_2_bias) -&gt; linear_8
      Add(linear_8, add_1) -&gt; output_0
output: name=&#39;output_0&#39; type=dtype(&#39;float32&#39;) shape=[1, 30, 16]
</pre></div>
</div>
</section>
<section id="comparison-optimized-and-not-optimized">
<h2>Comparison optimized and not optimized?<a class="headerlink" href="#comparison-optimized-and-not-optimized" title="Link to this heading">¶</a></h2>
<p>The following tools is trying to match the node and shape inference
from two models. If they are not too different, the functions
is able to find out the differences. We can use to see
which operators were fused into bigger ones only implemented by
<a class="reference external" href="https://onnxruntime.ai/">onnxruntime</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res1</span></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res2</span></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">align</span></a></a></a><span class="p">,</span> <a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.DistanceExecution" title="onnx_array_api.reference.DistanceExecution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.DistanceExecution" title="onnx_array_api.reference.DistanceExecution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.DistanceExecution" title="onnx_array_api.reference.DistanceExecution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dc</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.compare_onnx_execution" title="onnx_array_api.reference.compare_onnx_execution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.compare_onnx_execution" title="onnx_array_api.reference.compare_onnx_execution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-function"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.compare_onnx_execution" title="onnx_array_api.reference.compare_onnx_execution" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-function"><span class="n">compare_onnx_execution</span></a></a></a><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a></a></a><span class="p">,</span> <a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx_optimized</span></a></a></a><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------&quot;</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a></a></a> <span class="o">=</span> <a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.DistanceExecution.to_str" title="onnx_array_api.reference.DistanceExecution.to_str" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.DistanceExecution.to_str" title="onnx_array_api.reference.DistanceExecution.to_str" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-method"><a href="https://sdpython.github.io/doc/onnx-array-api/dev/api/reference.html#onnx_array_api.reference.DistanceExecution.to_str" title="onnx_array_api.reference.DistanceExecution.to_str" class="sphx-glr-backref-module-onnx_array_api-reference sphx-glr-backref-type-py-method"><span class="n">dc</span><span class="o">.</span><span class="n">to_str</span></a></a></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res1</span></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">res2</span></a></a></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">align</span></a></a></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a></a></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[compare_onnx_execution] generate inputs
[compare_onnx_execution] execute with 1 inputs
[compare_onnx_execution] execute first model
[compare_onnx_execution] got 88 results
[compare_onnx_execution] execute second model
[compare_onnx_execution] got 88 results (first model)
[compare_onnx_execution] got 56 results (second model)
[compare_onnx_execution] compute edit distance
[compare_onnx_execution] got 90 pairs
[compare_onnx_execution] done
------------
001 = | INITIA float32  2:1024x16            BORW                 p_ | INITIA float32  2:1024x16            BORW                 p_
002 = | INITIA float32  2:1024x16            UUVW                 p_ | INITIA float32  2:1024x16            UUVW                 p_
003 ~ | INITIA float32  1:16                 EEEE                 p_ | INITIA float32  1:16                 AAAA                 p_
004 ~ | INITIA float32  1:16                 AAAA                 p_ | INITIA float32  1:128                AAAA                 p_
005 ~ | INITIA float32  2:16x16              AAAA                 p_ | INITIA float32  1:16                 AAAA                 p_
006 ~ | INITIA float32  2:16x16              AAAA                 p_ | INITIA float32  1:1                  ?AAA                 in
007 - | INITIA float32  2:16x16              YAAC                 p_ |
008 ~ | INITIA float32  2:16x16              AAZB                 p_ | INITIA float32  2:16x16              CAZA                 _o
009 ~ | INITIA float32  2:16x16              AABA                 p_ | INITIA float32  2:16x16              AAAA                 _o
010 ~ | INITIA float32  2:16x16              ZZAA                 p_ | INITIA float32  2:16x16              AAAA                 _o
011 ~ | INITIA float32  2:16x32              AAYA                 p_ | INITIA float32  2:30x30              KGSP                 sl
012 ~ | INITIA float32  1:16                 AAAA                 p_ | INITIA float32  1:1                  AAAA                 _o
013 ~ | INITIA float32  1:16                 EEEE                 p_ | INITIA float32  2:16x16              ZAAA                 _o
014 ~ | INITIA float32  1:16                 AAAA                 p_ | INITIA float32  2:16x16              AAAA                 _o
015 - | INITIA float32  2:128x16             BAAX                 p_ |
016 ~ | INITIA float32  1:128                AAAA                 p_ | INITIA float32  2:16x16              AAZY                 _o
017 - | INITIA float32  2:16x128             AZAA                 p_ |
018 ~ | INITIA float32  1:16                 AAAA                 p_ | INITIA float32  2:30x30              KGSP                 sl
019 - | INITIA float32  2:256x256            AOCQ                 b_ |
020 - | INITIA float32  2:256x256            AOCQ                 b_ |
021 - | INITIA float32                       AAAA                 in |
022 ~ | INITIA int64    1:1                  BAAA                 in | INITIA float32  1:1                  AAAA                 _o
023 ~ | INITIA int64    1:1                  AAAA                 in | INITIA float32  2:32x16              BAAA                 _o
024 + |                                                              | INITIA float32  2:16x128             AVEA                 _o
025 + |                                                              | INITIA float32  2:128x16             BZAA                 _o
026 ~ | INITIA int64    1:1                  EAAA                 in | INITIA float32  1:16                 EEEE                 La
027 - | INITIA float32                       AAAA                 in |
028 ~ | INITIA float32  1:1                  ?AAA                 in | INITIA float32  1:16                 AAAA                 La
029 = | INITIA float32  1:16                 EEEE                 in | INITIA float32  1:16                 EEEE                 La
030 = | INITIA float32  1:16                 AAAA                 in | INITIA float32  1:16                 AAAA                 La
031 = | INPUT  int64    2:1x30               COAD                 in | INPUT  int64    2:1x30               COAD                 in
032 = | RESULT float32  3:1x30x16            FDNV Gather          em | RESULT float32  3:1x30x16            FDNV Gather          em
033 = | RESULT float32  3:1x30x16            QUDH Gather          em | RESULT float32  3:1x30x16            QUDH Gather          em
034 = | RESULT float32  3:1x30x16            WYQC Add             ad | RESULT float32  3:1x30x16            WYQC Add             ad
035 - | RESULT float32  1:16                 EEEE Mul             La |
036 - | RESULT float32  1:16                 AAAA Mul             La |
037 - | RESULT float32  1:16                 AAAA Add             La |
038 = | RESULT float32  3:1x30x16            ZBBZ LayerNormalizat _o | RESULT float32  3:1x30x16            ZBBZ LayerNormalizat _o
039 - | RESULT float32  2:16x16              CAZA Transpose       _o |
040 = | RESULT float32  3:1x30x16            MQST MatMul          li | RESULT float32  3:1x30x16            MQST MatMul          li
041 - | RESULT float32  2:16x16              AAAA Transpose       _o |
042 = | RESULT float32  3:1x30x16            VXOC MatMul          li | RESULT float32  3:1x30x16            VXOC MatMul          li
043 - | RESULT float32  2:16x16              AAAA Transpose       _o |
044 ~ | RESULT float32  3:1x30x16            VBUE MatMul          li | RESULT float32  3:1x30x30            YCFE FusedMatMul     _o
045 ~ | RESULT float32  3:1x16x30            WAUT Transpose       tr | RESULT float32  3:1x30x16            VBUE MatMul          li
046 - | RESULT float32  3:1x30x30            QLXR MatMul          ma |
047 - | RESULT float32  1:1                  AAAA Reshape         _o |
048 - | RESULT float32  3:1x30x30            YCFE Mul             _o |
049 - | RESULT float32  2:30x256             KGAH Slice           sl |
050 - | RESULT float32  2:30x30              KGSP Slice           sl |
051 - | RESULT float32  1:1                  AAAA Reshape         _o |
052 = | RESULT bool     2:30x30              HLZC Equal           eq | RESULT bool     2:30x30              HLZC Equal           eq
053 = | RESULT float32  3:1x30x30            ???? Where           _o | RESULT float32  3:1x30x30            ???? Where           _o
054 = | RESULT float32  3:1x30x30            HGHH Softmax         so | RESULT float32  3:1x30x30            HGHH Softmax         so
055 = | RESULT float32  3:1x30x16            DYYY MatMul          ma | RESULT float32  3:1x30x16            DYYY MatMul          ma
056 - | RESULT float32  2:16x16              ZAAA Transpose       _o |
057 = | RESULT float32  3:1x30x16            BLRA MatMul          li | RESULT float32  3:1x30x16            BLRA MatMul          li
058 - | RESULT float32  2:16x16              AAAA Transpose       _o |
059 = | RESULT float32  3:1x30x16            ZQZD MatMul          li | RESULT float32  3:1x30x16            ZQZD MatMul          li
060 - | RESULT float32  2:16x16              AAZY Transpose       _o |
061 - | RESULT float32  3:1x30x16            ZBFC MatMul          li |
062 - | RESULT float32  3:1x16x30            PBAC Transpose       tr |
063 ~ | RESULT float32  3:1x30x30            KHTN MatMul          ma | RESULT float32  3:1x30x30            CWZX FusedMatMul     _o
064 - | RESULT float32  1:1                  AAAA Reshape         _o |
065 ~ | RESULT float32  3:1x30x30            CWZX Mul             _o | RESULT float32  3:1x30x16            ZBFC MatMul          li
066 - | RESULT float32  2:30x256             KGAH Slice           sl |
067 - | RESULT float32  2:30x30              KGSP Slice           sl |
068 - | RESULT float32  1:1                  AAAA Reshape         _o |
069 = | RESULT bool     2:30x30              HLZC Equal           eq | RESULT bool     2:30x30              HLZC Equal           eq
070 = | RESULT float32  3:1x30x30            ???? Where           _o | RESULT float32  3:1x30x30            ???? Where           _o
071 = | RESULT float32  3:1x30x30            HHHH Softmax         so | RESULT float32  3:1x30x30            HHHH Softmax         so
072 = | RESULT float32  3:1x30x16            SCAB MatMul          ma | RESULT float32  3:1x30x16            SCAB MatMul          ma
073 = | RESULT float32  3:1x30x32            VAZA Concat          ca | RESULT float32  3:1x30x32            VAZA Concat          ca
074 - | RESULT float32  2:32x16              BAAA Transpose       _o |
075 = | RESULT float32  3:1x30x16            WAAA MatMul          _o | RESULT float32  3:1x30x16            WAAA MatMul          _o
076 = | RESULT float32  3:1x30x16            TYYY Add             li | RESULT float32  3:1x30x16            TYYY Add             li
077 = | RESULT float32  3:1x30x16            OVNA Add             ad | RESULT float32  3:1x30x16            OVNA Add             ad
078 - | RESULT float32  1:16                 EEEE Mul             La |
079 - | RESULT float32  1:16                 AAAA Mul             La |
080 - | RESULT float32  1:16                 AAAA Add             La |
081 = | RESULT float32  3:1x30x16            ZBBZ LayerNormalizat _o | RESULT float32  3:1x30x16            ZBBZ LayerNormalizat _o
082 - | RESULT float32  2:16x128             AVEA Transpose       _o |
083 = | RESULT float32  3:1x30x128           EMOF MatMul          _o | RESULT float32  3:1x30x128           EMOF MatMul          _o
084 = | RESULT float32  3:1x30x128           PXZQ Add             li | RESULT float32  3:1x30x128           PXZQ Add             li
085 = | RESULT float32  3:1x30x128           GZAU Relu            re | RESULT float32  3:1x30x128           GZAU Relu            re
086 - | RESULT float32  2:128x16             BZAA Transpose       _o |
087 = | RESULT float32  3:1x30x16            AZBC MatMul          _o | RESULT float32  3:1x30x16            AZBC MatMul          _o
088 = | RESULT float32  3:1x30x16            AZBC Add             li | RESULT float32  3:1x30x16            AZBC Add             li
089 = | RESULT float32  3:1x30x16            PUPD Add             ou | RESULT float32  3:1x30x16            PUPD Add             ou
090 = | OUTPUT float32  3:1x30x16            PUPD                 ou | OUTPUT float32  3:1x30x16            PUPD                 ou
</pre></div>
</div>
<p>The conversion should handle dynamic shapes as well as the input sequence
can be of any length. But that’s a topic for another example.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 6.291 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-recipes-plot-exporter-recipes-c-modules-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/cb5ea34f3427dcee1ab63090ed6a2e2b/plot_exporter_recipes_c_modules.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_exporter_recipes_c_modules.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/ef2c9223bf91adc2ae05ba5115daa19e/plot_exporter_recipes_c_modules.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_exporter_recipes_c_modules.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4a095e9b122b9cb32142c72045f9a49c/plot_exporter_recipes_c_modules.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_exporter_recipes_c_modules.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../command_lines.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Command Lines</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_exporter_recipes_c_custom_ops_fct.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">to_onnx and a custom operator registered with a function</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023-2024
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">to_onnx and submodules from LLMs</a><ul>
<li><a class="reference internal" href="#a-simple-llm">A simple LLM</a></li>
<li><a class="reference internal" href="#first-conversion-to-onnx">First conversion to ONNX</a></li>
<li><a class="reference internal" href="#onnx-with-submodules">ONNX with submodules</a></li>
<li><a class="reference internal" href="#inlining">Inlining</a></li>
<li><a class="reference internal" href="#optimizations">Optimizations</a></li>
<li><a class="reference internal" href="#optimizations-for-cuda">Optimizations for CUDA</a></li>
<li><a class="reference internal" href="#comparison-optimized-and-not-optimized">Comparison optimized and not optimized?</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=a1637f0b"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    </body>
</html>