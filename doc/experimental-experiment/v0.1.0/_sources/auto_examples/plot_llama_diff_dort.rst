
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_dort.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_dort.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_dort.py:


.. _l-plot-onnxrt-diff:

Compares LLAMA exporters for onnxrt backend
===========================================

The script compares exported models in :epkg:`pytorch`
using :epkg:`onnxrt backend`. It tries to do a side by side
of the execution of both models.

To run the script:

::

    python _doc/examples/plot_llama_diff_dort --help

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 20-70

.. code-block:: Python


    import copy
    import os
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import onnx
    from onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator
    import torch
    from torch._dynamo.backends.common import aot_autograd
    from experimental_experiment.ext_test_case import get_parsed_args, unit_test_going
    from experimental_experiment.convert.convert_helper import (
        optimize_model_proto,
        ort_optimize,
    )
    from experimental_experiment.torch_helper.llama_helper import (
        get_llama_model,
        get_llama_attention,
        get_llama_decoder,
    )
    from experimental_experiment.torch_helper.dump_helper import (
        assert_all_close,
        dump_onnx,
        reorder_functions_in_proto,
        inputs_from_onnx_model,
        build_matching_inputs,
    )
    from experimental_experiment.torch_helper.debug_backend import onnx_debug_backend
    from experimental_experiment.torch_helper.training_helper import (
        train_loop,
        make_aot_ort,
    )

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [2024-02-11 17:45:43,315] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)




.. GENERATED FROM PYTHON SOURCE LINES 71-73

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 73-90

.. code-block:: Python



    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("attention", "one value among attention, decoder, model"),
        ortopt=(1, "run onnxruntime optimization"),
        backward=(0, "does one operator for backward"),
        expose="part,exporter,ortopt",
    )

    print(f"part={script_args.part}")
    ortopt = script_args.ortopt in (1, "1")
    print(f"ortopt={ortopt}")
    backward = script_args.backward in (1, "1")
    print(f"backward={backward}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=attention
    ortopt=True
    backward=False




.. GENERATED FROM PYTHON SOURCE LINES 91-93

Model and data
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 93-129

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "attention":
        model, inputs = get_llama_attention(**kwargs)
    elif script_args.part == "decoder":
        model, inputs = get_llama_decoder(**kwargs)
    elif script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    print(f"simple run with {len(inputs)} inputs")
    if backward:
        expected = train_loop(copy.deepcopy(model), *inputs[0])
        print(
            f"-- eager mode worked, {len(expected)} gradients, first one is "
            f"{expected[0].shape}, {expected[0].dtype}"
        )
    else:
        expected = model(*inputs[0])
        print(f"eager mode worked {expected.shape}, {expected.dtype}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    eager mode worked torch.Size([2, 1024, 512]), torch.float32




.. GENERATED FROM PYTHON SOURCE LINES 130-132

Exporting
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 132-192

.. code-block:: Python


    folder = "dump_models"
    storage = {}

    if backward:
        # onnxrt backend
        local_aot_ort, _ = make_aot_ort(dynamic=False)

        optimized_mod = torch.compile(
            copy.deepcopy(model), backend=local_aot_ort, dynamic=False, fullgraph=True
        )

        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            expected_onnxrt = train_loop(optimized_mod, *inputs[0])
        assert_all_close(expected[0], expected_onnxrt[0])
        print(
            f"-- onnxrt backend worked, {len(expected_onnxrt)} gradients, first one is "
            f"{expected_onnxrt[0].shape}, {expected_onnxrt[0].dtype}"
        )

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=18,
                storage=storage,
                **kwargs,
            )
        )
        onnx_mod = torch.compile(copy.deepcopy(model), backend=aot_compiler, fullgraph=True)
        got = train_loop(onnx_mod, *inputs[0])
        assert_all_close(expected[0], got[0])
        print(
            f"-- debug backend worked, {len(got)} gradients, first one is "
            f"{got[0].shape}, {got[0].dtype}"
        )

    else:
        # onnxrt backend
        optimized_mod = torch.compile(model, backend="onnxrt", fullgraph=True)
        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            expected_onnxrt = optimized_mod(*inputs[0])
        assert_all_close(expected, expected_onnxrt)

        # debugging backend
        onnx_mod = torch.compile(
            model,
            backend=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=18,
                storage=storage,
                **kwargs,
            ),
            fullgraph=True,
        )
        got = onnx_mod(*inputs[0])
        assert_all_close(expected, got)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:136: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 193-196

For forward, there are two files, one onnx model and the graph module
printed in a txt file. For backward, there are two onnx models.
Then it is multiplied by the number of backends.

.. GENERATED FROM PYTHON SOURCE LINES 196-200

.. code-block:: Python


    models = os.listdir(folder)
    print(f"exported models: {models}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    exported models: ['llama_onnxrt_0.onnx', 'llama_debug_0.onnx', 'llama_debug_0.txt', 'llama_onnxrt_0.txt']




.. GENERATED FROM PYTHON SOURCE LINES 201-202

Inputs used by the debug backend

.. GENERATED FROM PYTHON SOURCE LINES 202-207

.. code-block:: Python


    feeds = storage["instance"][0]["inputs"][0]
    for k, v in feeds.items():
        print(f"-- {k} {v.dtype} {v.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- input0 float32 (2, 1024, 512)
    -- input1 int64 (1, 1024)
    -- input2 float32 (2, 1, 1024, 1024)




.. GENERATED FROM PYTHON SOURCE LINES 208-209

Let's the first line of the graph module

.. GENERATED FROM PYTHON SOURCE LINES 209-214

.. code-block:: Python


    graph_module = storage["instance"][0]["graph_module"]
    print("\n".join(str(graph_module.graph).split("\n")[:10]))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %l_hidden_states_ : torch.Tensor [num_users=3] = placeholder[target=L_hidden_states_]
        %position_ids : torch.Tensor [num_users=1] = placeholder[target=L_position_ids_]
        %l_attention_mask_ : torch.Tensor [num_users=1] = placeholder[target=L_attention_mask_]
        %query_states : [num_users=1] = call_module[target=L__self___attention_q_proj](args = (%l_hidden_states_,), kwargs = {})
        %key_states : [num_users=1] = call_module[target=L__self___attention_k_proj](args = (%l_hidden_states_,), kwargs = {})
        %value_states : [num_users=1] = call_module[target=L__self___attention_v_proj](args = (%l_hidden_states_,), kwargs = {})
        %view : [num_users=1] = call_method[target=view](args = (%query_states, 2, 1024, 8, 64), kwargs = {})
        %query_states_1 : [num_users=3] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})
        %view_1 : [num_users=1] = call_method[target=view](args = (%key_states, 2, 1024, 8, 64), kwargs = {})




.. GENERATED FROM PYTHON SOURCE LINES 215-217

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 217-243

.. code-block:: Python


    if backward:
        print(f"-- {len(storage['instance'])} onnx models were creates")
        for i, inst in enumerate(storage["instance"]):
            print(f"  model {i}: {len(inst['inputs'])} runs")

        # deal with forward
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx") and "_0" in m]))
        assert len(onnx_models) == 2, f"unexpected value {onnx_models}"
        model_onnxrt = os.path.join(folder, onnx_models[1])
        model_debug = os.path.join(folder, onnx_models[0])
    else:
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        if len(onnx_models) == 2:
            model_onnxrt = os.path.join(folder, onnx_models[1])
            model_debug = os.path.join(folder, onnx_models[0])
        else:
            model_debug = os.path.join(folder, onnx_models[0])
            # the following error may appear:
            # Node type 'Rank' from domain 'pkg.onnxscript.torch_lib.common' is unknown
            print(f"One model is missing, onnx_models={onnx_models}")
            model_onnxrt = model_debug

    print(f"model_onnxrt={model_onnxrt}")
    print(f"model_debug={model_debug}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    model_onnxrt=dump_models/llama_onnxrt_0.onnx
    model_debug=dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 244-245

The inputs of both models

.. GENERATED FROM PYTHON SOURCE LINES 245-249

.. code-block:: Python


    print("onnxrt:", inputs_from_onnx_model(model_onnxrt))
    print("debug:", inputs_from_onnx_model(model_debug))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    onnxrt: [('INPUT', 'primals_7', 7, (1, 1024)), ('INPUT', 'primals_8', 1, (2, 1, 1024, 1024)), ('INPUT', 'primals_5', 1, (32,)), ('INPUT', 'primals_6', 1, (2, 1024, 512)), ('INPUT', 'primals_3', 1, (512, 512)), ('INPUT', 'primals_1', 1, (512, 512)), ('INPUT', 'primals_2', 1, (512, 512)), ('INPUT', 'primals_4', 1, (512, 512))]
    debug: [('INPUT', 'input0', 1, (2, 1024, 512)), ('INPUT', 'input1', 7, (1, 1024)), ('INPUT', 'input2', 1, (2, 1, 1024, 1024))]




.. GENERATED FROM PYTHON SOURCE LINES 250-252

Inputs are not the same. The first model has more and some inputs were
moved into the initializer list into for `model_debug`.

.. GENERATED FROM PYTHON SOURCE LINES 252-255

.. code-block:: Python


    print("debug:", inputs_from_onnx_model(model_debug, init=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    debug: [('INPUT', 'input0', 1, (2, 1024, 512)), ('INPUT', 'input1', 7, (1, 1024)), ('INPUT', 'input2', 1, (2, 1, 1024, 1024)), ('INIT', '_sub_Linear_weight', 1, (512, 512)), ('INIT', '_sub_Linear_weight2', 1, (512, 512)), ('INIT', '_sub_Linear_weight3', 1, (512, 512)), ('INIT', '_sub_Linear_weight4', 1, (512, 512)), ('INIT', 'causal_mask_axis', 7, (2,)), ('INIT', 'causal_mask_start', 7, (2,)), ('INIT', 'causal_mask_step', 7, (2,)), ('INIT', 'getitem_axis', 7, (1,)), ('INIT', 'getitem_axis_0', 7, (1,)), ('INIT', 'getitem_start', 7, (1,)), ('INIT', 'getitem_step', 7, (1,)), ('INIT', 'init11_s_', 11, ()), ('INIT', 'init1_s_', 1, ()), ('INIT', 'init7_s1_1', 7, (1,)), ('INIT', 'init7_s1_32', 7, (1,)), ('INIT', 'init7_s1_322', 7, (1,)), ('INIT', 'init7_s2_1024_1024', 7, (2,)), ('INIT', 'init7_s2_32_1', 7, (2,)), ('INIT', 'init7_s3_2_1024_512', 7, (3,)), ('INIT', 'init9_s_', 9, ()), ('INIT', 'key_states_view_shape', 7, (4,)), ('INIT', 'l__self___attention_rotary_emb_inv_freq', 1, (32,)), ('INIT', 'query_states_view_shape', 7, (4,)), ('INIT', 'value_states_view_shape', 7, (4,)), ('INIT', 'x1_1_axis', 7, (1,)), ('INIT', 'x1_1_start', 7, (1,)), ('INIT', 'x1_1_step', 7, (1,)), ('INIT', 'x1_axis', 7, (1,)), ('INIT', 'x1_start', 7, (1,)), ('INIT', 'x1_step', 7, (1,)), ('INIT', 'x2_1_axis', 7, (1,)), ('INIT', 'x2_1_axis_-1', 7, (1,)), ('INIT', 'x2_1_start', 7, (1,)), ('INIT', 'x2_1_step', 7, (1,)), ('INIT', 'x2_axis', 7, (1,)), ('INIT', 'x2_axis_-1', 7, (1,)), ('INIT', 'x2_start', 7, (1,)), ('INIT', 'x2_step', 7, (1,))]




.. GENERATED FROM PYTHON SOURCE LINES 256-264

Optimization and Verification
+++++++++++++++++++++++++++++

Let's try the model with a python backend (reference implementation).
First step, onnx-script uses many functions. The reference evaluation expects
every function to be defined so the order of functions in the model matters.
No recursivity is allowed by this runtime. We need to reorder as function Rank is usually placed
at the end of the model.

.. GENERATED FROM PYTHON SOURCE LINES 264-267

.. code-block:: Python


    reorder_functions_in_proto(model_onnxrt)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    'dump_models/llama_onnxrt_0.onnx'



.. GENERATED FROM PYTHON SOURCE LINES 268-269

For what's following, we need to build two lists of matching inputs.

.. GENERATED FROM PYTHON SOURCE LINES 269-272

.. code-block:: Python


    feedsrt = build_matching_inputs(model_debug, feeds, model_onnxrt)








.. GENERATED FROM PYTHON SOURCE LINES 273-274

Let's load the model and optimize them.

.. GENERATED FROM PYTHON SOURCE LINES 274-282

.. code-block:: Python


    try:
        onnxrt = optimize_model_proto(onnx.load(model_onnxrt))
    except ImportError as e:
        print("missing library", e)
        onnxrt = model_debug
    debug = onnx.load(model_debug)








.. GENERATED FROM PYTHON SOURCE LINES 283-284

Let's apply onnxruntime optimization

.. GENERATED FROM PYTHON SOURCE LINES 284-297

.. code-block:: Python


    if ortopt:
        print(f"run onnxruntime optimization on {model_onnxrt}")
        optimized = model_onnxrt.replace(".onnx", ".opt.onnx")
        ort_optimize(onnxrt, output=optimized)
        onnxrt = onnx.load(optimized)

        print(f"run onnxruntime optimization on {model_debug}")
        optimized = model_debug.replace(".onnx", ".opt.onnx")
        ort_optimize(debug, output=optimized)
        debug = onnx.load(optimized)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run onnxruntime optimization on dump_models/llama_onnxrt_0.onnx
    run onnxruntime optimization on dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 298-299

We check both models are running.

.. GENERATED FROM PYTHON SOURCE LINES 299-307

.. code-block:: Python


    out_onnxrt = ExtendedReferenceEvaluator(onnxrt).run(None, feedsrt)
    out_debug = ExtendedReferenceEvaluator(debug).run(None, feeds)
    assert out_onnxrt
    assert out_debug

    # assert_all_close(out_onnxrt, out_debug)








.. GENERATED FROM PYTHON SOURCE LINES 308-309

Side by side

.. GENERATED FROM PYTHON SOURCE LINES 309-320

.. code-block:: Python



    res1, res2, align, dc = compare_onnx_execution(
        onnxrt,
        debug,
        verbose=1,
        raise_exc=True,
        inputs=(feedsrt, feeds),
    )
    text = dc.to_str(res1, res2, align, column_size=90)
    print(text)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] generate inputs
    [compare_onnx_execution] got 2 inputs
    [compare_onnx_execution] execute first model
    /home/xadupre/github/onnx-array-api/onnx_array_api/reference/evaluator_yield.py:98: RuntimeWarning: invalid value encountered in cast
      value4i = value4.astype(np.int64) % modulo
    [compare_onnx_execution] got 97 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 64 results
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 105 pairs
    [compare_onnx_execution] done
    001 ~ | INITIA int64    2               USAA            ortshared_7_1_2_1_token_160                | INITIA float32  512x512         ZRAA            _sub_Linear__onx_transpose02              
    002 + |                                                                                            | INITIA float32  512x512         DBFX            _sub_Linear__onx_transpose03               
    003 ~ | INITIA int64    1               SAAA            ortshared_7_1_1_1_token_151                | INITIA int64    1               MAAA            ortshared_7_1_1_3_token_138               
    004 ~ | INITIA int64    1               AAAA            ortshared_7_1_1_2_token_155                | INITIA float32  32x1            DAAA            expand                                    
    005 ~ | INITIA int64    3               QMKA            ortshared_7_1_3_1_token_156                | INITIA int64    2               YZAA            ortshared_7_1_2_2_token_136               
    006 ~ | INITIA int64    2               ABAA            ortshared_7_1_2_0_token_148                | INITIA int64    2               AAAA            ortshared_7_1_2_4_token_141               
    007 ~ | INITIA int64                    BAAA            ortshared_7_0_1_0_token_152                | INITIA int64    2               BBAA            ortshared_7_1_2_3_token_137               
    008 ~ | INITIA int64    3               QKKA            ortshared_7_1_3_0_token_153                | INITIA int64    1               AAAA            ortshared_7_1_1_1_token_132               
    009 ~ | INITIA int64    4               CIKK            ortshared_7_1_4_0_token_149                | INITIA int64    4               CKIM            ortshared_7_1_4_0_token_140               
    010 + |                                                                                            | INITIA float32  512x512         ECCZ            _sub_Linear__onx_transpose0                
    011 ~ | INITIA int64    3               QKMA            ortshared_7_1_3_2_token_159                | INITIA int64    1               GAAA            ortshared_7_1_1_0_token_130               
    012 - | INITIA float32                  IAAA            ortshared_1_0_1_1_token_163                |                                                                                           
    013 - | INITIA float32                  BAAA            ortshared_1_0_1_0_token_154                |                                                                                           
    014 ~ | INITIA int64    3               CKSA            ortshared_7_1_3_3_token_164                | INITIA int64    1               BAAA            ortshared_7_1_1_5_token_142               
    015 ~ | INITIA int64    4               CKIM            ortshared_7_1_4_2_token_158                | INITIA int64    2               ABAA            ortshared_7_1_2_1_token_135               
    016 ~ | INITIA int64    4               CIKM            ortshared_7_1_4_1_token_157                | INITIA int64    1               DAAA            ortshared_7_1_1_4_token_139               
    017 ~ | INITIA int64    1               GAAA            ortshared_7_1_1_3_token_161                | INITIA int64    3               CKSA            ortshared_7_1_3_0_token_133               
    018 ~ | INITIA int64    1               BAAA            ortshared_7_1_1_4_token_162                | INITIA int64    1               ZAAA            ortshared_7_1_1_2_token_134               
    019 ~ | INITIA int64    1               DAAA            ortshared_7_1_1_0_token_150                | INITIA int64    2               KKAA            ortshared_7_1_2_0_token_131               
    020 + |                                                                                            | INITIA float32  512x512         GJVY            _sub_Linear__onx_transpose04               
    021 + |                                                                                            | INPUT  float32  2x1024x512      VBKZ            input0                                     
    022 = | INPUT  int64    1x1024          KAQG            primals_7                                  | INPUT  int64    1x1024          KAQG            input1                                    
    023 = | INPUT  float32  2x1x1024x1024   AAAA            primals_8                                  | INPUT  float32  2x1x1024x1024   AAAA            input2                                    
    024 - | INPUT  float32  32              DAAA            primals_5                                  |                                                                                           
    025 - | INPUT  float32  2x1024x512      VBKZ            primals_6                                  |                                                                                           
    026 - | INPUT  float32  512x512         WEHB            primals_3                                  |                                                                                           
    027 - | INPUT  float32  512x512         NBEV            primals_1                                  |                                                                                           
    028 - | INPUT  float32  512x512         SBLC            primals_2                                  |                                                                                           
    029 - | INPUT  float32  512x512         AYYM            primals_4                                  |                                                                                           
    030 - | RESULT float32  512x512         AYYM Identity   t_7                                        |                                                                                           
    031 - | RESULT float32  2x1x1024x1024   AAAA Mul        _inlfunc_aten_add|folded_2_other_1         |                                                                                           
    032 - | RESULT float32  32              DAAA Slice      slice_1                                    |                                                                                           
    033 - | RESULT float32  32x1            DAAA Unsqueeze  unsqueeze                                  |                                                                                           
    034 = | RESULT float32  1x1024          KAQG Cast       _to_copy                                   | RESULT float32  1x1024          KAQG Cast       float_2                                   
    035 = | RESULT float32  32x1024         EFXM MatMul     mm_3                                       | RESULT float32  32x1024         EFXM MatMul     matmul                                    
    036 = | RESULT float32  64x1024         JKJK Concat     aten_cat_92_n0                             | RESULT float32  64x1024         JKJK Concat     cat                                       
    037 ~ | RESULT float32  1024x64         VFPY Transpose  cat                                        | RESULT float32  64x1024         RMRM Sin        _token_7                                  
    038 ~ | RESULT float32  1024x64         GSEC Sin        sin                                        | RESULT float32  1x1x64x1024     RMRM Unsqueeze  Unsqueeze                                 
    039 - | RESULT float32  1x1x1024x64     GSEC Unsqueeze  Unsqueeze_out0                             |                                                                                           
    040 = | RESULT float32  1x1024x1x64     GSEC Transpose  Transpose_token_6_out0                     | RESULT float32  1x1024x1x64     GSEC Transpose  Transpose_token_10_out0                   
    041 - | RESULT float32  2048x512        VBKZ Reshape    view                                       |                                                                                           
    042 - | RESULT float32  2048x512        WCMJ FusedMatMu mm_1                                       |                                                                                           
    043 ~ | RESULT float32  2x1024x512      WCMJ Reshape    view_3                                     | RESULT float32  2x1024x512      GQOK MatMul     _sub_Linear_linear2                       
    044 ~ | RESULT float32  2x1024x8x64     WCMJ Reshape    view_7                                     | RESULT float32  2x1024x8x64     GQOK Reshape    view_1                                    
    045 ~ | RESULT float32  2x1024x8x32     LQNY Slice      Slice_162                                  | RESULT float32  2x1024x8x32     YBRY Slice      getitem_slice15                           
    046 ~ | RESULT float32  2x1024x8x32     PKNC Neg        aten_neg_167_n0                            | RESULT float32  2x1024x8x32     CZJC Neg        _token_1                                  
    047 ~ | RESULT float32  2x1024x8x32     MNZK Slice      Slice_145                                  | RESULT float32  2x1024x8x32     JPXL Slice      getitem_slice11                           
    048 ~ | RESULT float32  2x1024x8x64     AWMM Concat     aten_cat_175_n0                            | RESULT float32  2x1024x8x64     LOGO Concat     cat3                                      
    049 ~ | RESULT float32  2x1024x8x64     EVIN Mul        aten_mul_178_n0                            | RESULT float32  2x1024x8x64     UKRA Mul        mul7                                      
    050 + |                                                                                            | RESULT float32  64x1024         NHNH Cos        _token_13                                  
    051 ~ | RESULT float32  1024x64         CJYF Cos        cos                                        | RESULT float32  1x1x64x1024     NHNH Unsqueeze  Unsqueeze_token_15                        
    052 - | RESULT float32  1x1x1024x64     CJYF Unsqueeze  Unsqueeze_token_9_out0                     |                                                                                           
    053 = | RESULT float32  1x1024x1x64     CJYF Transpose  Transpose_token_10_out0                    | RESULT float32  1x1024x1x64     CJYF Transpose  Transpose_token_17_out0                   
    054 ~ | RESULT float32  2x1024x8x64     CDBQ Mul        aten_mul_169_n0                            | RESULT float32  2x1024x8x64     NNNC Mul        mul5                                      
    055 ~ | RESULT float32  2x1024x8x64     GYJD Add        _inlfunc_aten_add|folded_1_n3              | RESULT float32  2x1024x8x64     GYEC Add        add3                                      
    056 ~ | RESULT float32  2x8x64x1024     QPAL Transpose  transpose_3                                | RESULT float32  2x8x64x1024     NSBE Transpose  transpose_3                               
    057 + |                                                                                            | RESULT float32  1024x64         GSEC Transpose  sin                                        
    058 - | RESULT float32  16x64x1024      QPAL Reshape    view_10                                    |                                                                                           
    059 - | RESULT float32  2048x512        GQOK FusedMatMu mm                                         |                                                                                           
    060 ~ | RESULT float32  2x1024x512      GQOK Reshape    view_1                                     | RESULT float32  2x1024x512      XDPH MatMul     _sub_Linear_linear                        
    061 ~ | RESULT float32  2x1024x8x64     GQOK Reshape    view_6                                     | RESULT float32  2x1024x8x64     XDPH Reshape    view                                      
    062 ~ | RESULT float32  2x8x1024x64     IOHP Transpose  transpose                                  | RESULT float32  2x8x1024x64     DXIP Transpose  query_states_1                            
    063 ~ | RESULT float32  2x8x1024x32     BYMC Slice      slice_3                                    | RESULT float32  2x8x1024x32     FHWK Slice      x2                                        
    064 ~ | RESULT float32  2x8x1024x32     ZCOY Neg        neg                                        | RESULT float32  2x8x1024x32     VTEQ Neg        neg                                       
    065 ~ | RESULT float32  2x8x1024x32     GRWO Slice      slice_2                                    | RESULT float32  2x8x1024x32     ZQLF Slice      x1                                        
    066 ~ | RESULT float32  2x8x1024x64     FTJL Concat     cat_1                                      | RESULT float32  2x8x1024x64     UJQU Concat     cat_1                                     
    067 ~ | RESULT float32  2x8x1024x64     OQNF Mul        mul_1                                      | RESULT float32  2x8x1024x64     HLHS Mul        _onx_mul02                                
    068 + |                                                                                            | RESULT float32  1024x64         CJYF Transpose  cos                                        
    069 ~ | RESULT float32  2x8x1024x64     YCOA Mul        mul                                        | RESULT float32  2x8x1024x64     TNEF Mul        _onx_mul0                                 
    070 ~ | RESULT float32  2x8x1024x64     NSBE Add        add                                        | RESULT float32  2x8x1024x64     ZXMY Add        _onx_add0                                 
    071 - | RESULT float32  16x1024x64      NSBE Reshape    view_9                                     |                                                                                           
    072 - | RESULT float32  16x1024x1024    LBDF MatMul     bmm                                        |                                                                                           
    073 - | RESULT float32  2x8x1024x1024   LBDF Reshape    view_11                                    |                                                                                           
    074 ~ | RESULT float32  2x8x1024x1024   SUXB Div        div                                        | RESULT float32  2x8x1024x1024   UWGI FusedMatMu attn_weights                              
    075 + |                                                                                            | RESULT float32  2x1x1024x1024   AAAA Slice      causal_mask                                
    076 ~ | RESULT float32  2x8x1024x1024   SUXB Add        add_2                                      | RESULT float32  2x8x1024x1024   UWGI Add        _onx_add03                                
    077 ~ | RESULT float32  2x8x1024x1024   ONNN Softmax    aten_softmax_no_dtype_200_result           | RESULT float32  2x8x1024x1024   OONN Softmax    softmax                                   
    078 - | RESULT float32  16x1024x1024    ONNN Reshape    view_12                                    |                                                                                           
    079 - | RESULT float32  2048x512        XDPH FusedMatMu mm_2                                       |                                                                                           
    080 ~ | RESULT float32  2x1024x512      XDPH Reshape    view_5                                     | RESULT float32  2x1024x512      WCMJ MatMul     _sub_Linear_linear3                       
    081 ~ | RESULT float32  2x1024x8x64     XDPH Reshape    view_8                                     | RESULT float32  2x1024x8x64     WCMJ Reshape    view_2                                    
    082 ~ | RESULT float32  2x8x1024x64     DXIP Transpose  transpose_2                                | RESULT float32  2x8x1024x64     JPKL Transpose  value_states_2                            
    083 - | RESULT float32  16x1024x64      DXIP Reshape    view_13                                    |                                                                                           
    084 - | RESULT float32  16x1024x64      DVEX MatMul     bmm_1                                      |                                                                                           
    085 ~ | RESULT float32  2x8x1024x64     DVEX Reshape    view_14                                    | RESULT float32  2x8x1024x64     MYNO MatMul     matmul_2                                  
    086 ~ | RESULT float32  2x1024x8x64     QHZC Transpose  transpose_4                                | RESULT float32  2x1024x8x64     JBBB Transpose  transpose_4                               
    087 ~ | RESULT float32  2x1024x512      QHZC Reshape    view_15                                    | RESULT float32  2x1024x512      JBBB Reshape    reshape                                   
    088 - | RESULT float32  2048x512        QHZC Reshape    view_16                                    |                                                                                           
    089 - | RESULT float32  2048x512        JGDJ FusedMatMu mm_4                                       |                                                                                           
    090 ~ | RESULT float32  2x1024x512      JGDJ Reshape    view_17                                    | RESULT float32  2x1024x512      YDRN MatMul     output_0                                  
    091 - | RESULT float32  16x1024x1024    ONNN Transpose  transpose_6                                |                                                                                           
    092 - | RESULT float32  2x8x1024x1024   ONNN Identity   detach_3                                   |                                                                                           
    093 - | RESULT float32  16x1024x64      QPAL Transpose  transpose_9                                |                                                                                           
    094 - | RESULT float32  16x64x1024      NSBE Transpose  transpose_8                                |                                                                                           
    095 - | RESULT float32  16x64x1024      DXIP Transpose  transpose_7                                |                                                                                           
    096 - | OUTPUT float32  2048x512        VBKZ            view                                       |                                                                                           
    097 - | OUTPUT float32  512x512         AYYM            t_7                                        |                                                                                           
    098 - | OUTPUT float32  1024x64         VFPY            cat                                        |                                                                                           
    099 - | OUTPUT float32  16x64x1024      DXIP            transpose_7                                |                                                                                           
    100 - | OUTPUT float32  16x64x1024      NSBE            transpose_8                                |                                                                                           
    101 - | OUTPUT float32  16x1024x64      QPAL            transpose_9                                |                                                                                           
    102 - | OUTPUT float32  2x8x1024x1024   ONNN            detach_3                                   |                                                                                           
    103 - | OUTPUT float32  16x1024x1024    ONNN            transpose_6                                |                                                                                           
    104 - | OUTPUT float32  2048x512        QHZC            view_16                                    |                                                                                           
    105 ~ | OUTPUT float32  2x1024x512      JGDJ            view_17                                    | OUTPUT float32  2x1024x512      YDRN            output_0                                  





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 13.431 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_dort.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_dort.ipynb <plot_llama_diff_dort.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_dort.py <plot_llama_diff_dort.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
