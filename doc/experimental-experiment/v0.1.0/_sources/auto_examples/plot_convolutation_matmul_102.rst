
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_convolutation_matmul_102.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_convolutation_matmul_102.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_convolutation_matmul_102.py:


.. _l-plot-convolution-matmul-102:

==========================================
102: Convolution and Matrix Multiplication
==========================================

The `convolution <https://en.wikipedia.org/wiki/Kernel_(image_processing)>`_
is a well known image transformation used to transform an image.
It can be used to blur, to compute the gradient in one direction and
it is widely used in deep neural networks.
Having a fast implementation is important.

numpy
=====

Image have often 4 dimensions (N, C, H, W) = (batch, channels, height, width).
Let's first start with a 2D image.

.. GENERATED FROM PYTHON SOURCE LINES 20-44

.. code-block:: Python


    from typing import Sequence
    import numpy as np
    from numpy.testing import assert_almost_equal
    from onnx.reference import ReferenceEvaluator
    from onnx_array_api.light_api import start
    from onnx_array_api.plotting.graphviz_helper import plot_dot
    from onnxruntime import InferenceSession
    from torch import from_numpy
    from torch.nn import Fold, Unfold
    from torch.nn.functional import conv_transpose2d, conv2d
    from experimental_experiment.gradient.grad_helper import (
        onnx_derivative,
        DerivativeOptions,
    )


    shape = (5, 7)
    N = np.prod(shape)
    data = np.arange(N).astype(np.float32).reshape(shape)
    # data[:, :] = 0
    # data[2, 3] = 1
    data.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (5, 7)



.. GENERATED FROM PYTHON SOURCE LINES 45-46

Let's a 2D kernel, the same one.

.. GENERATED FROM PYTHON SOURCE LINES 46-51

.. code-block:: Python


    kernel = (np.arange(9) + 1).reshape(3, 3).astype(np.float32)
    kernel






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[1., 2., 3.],
           [4., 5., 6.],
           [7., 8., 9.]], dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 52-56

raw convolution
+++++++++++++++

A raw version of a 2D convolution.

.. GENERATED FROM PYTHON SOURCE LINES 56-77

.. code-block:: Python



    def raw_convolution(data: np.ndarray, kernel: Sequence[int]) -> np.ndarray:
        rx = (kernel.shape[0] - 1) // 2
        ry = (kernel.shape[1] - 1) // 2
        res = np.zeros(data.shape, dtype=data.dtype)
        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                for x in range(kernel.shape[0]):
                    for y in range(kernel.shape[1]):
                        a = i + x - rx
                        b = j + y - ry
                        if a < 0 or b < 0 or a >= data.shape[0] or b >= data.shape[1]:
                            continue
                        res[i, j] += kernel[x, y] * data[a, b]
        return res


    res = raw_convolution(data, kernel)
    res.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (5, 7)



.. GENERATED FROM PYTHON SOURCE LINES 78-79

Full result.

.. GENERATED FROM PYTHON SOURCE LINES 79-83

.. code-block:: Python


    res






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[ 134.,  211.,  250.,  289.,  328.,  367.,  238.],
           [ 333.,  492.,  537.,  582.,  627.,  672.,  423.],
           [ 564.,  807.,  852.,  897.,  942.,  987.,  612.],
           [ 795., 1122., 1167., 1212., 1257., 1302.,  801.],
           [ 422.,  571.,  592.,  613.,  634.,  655.,  382.]], dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 84-90

With pytorch
++++++++++++

*pytorch* is optimized for deep learning and prefers 4D tenors
to represent multiple images. We add two empty dimension
to the previous example.

.. GENERATED FROM PYTHON SOURCE LINES 90-99

.. code-block:: Python



    rest = conv2d(
        from_numpy(data[np.newaxis, np.newaxis, ...]),
        from_numpy(kernel[np.newaxis, np.newaxis, ...]),
        padding=(1, 1),
    )
    rest.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    torch.Size([1, 1, 5, 7])



.. GENERATED FROM PYTHON SOURCE LINES 100-101

Full result.

.. GENERATED FROM PYTHON SOURCE LINES 101-104

.. code-block:: Python


    rest





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[[[ 134.,  211.,  250.,  289.,  328.,  367.,  238.],
              [ 333.,  492.,  537.,  582.,  627.,  672.,  423.],
              [ 564.,  807.,  852.,  897.,  942.,  987.,  612.],
              [ 795., 1122., 1167., 1212., 1257., 1302.,  801.],
              [ 422.,  571.,  592.,  613.,  634.,  655.,  382.]]]])



.. GENERATED FROM PYTHON SOURCE LINES 105-106

Everything works.

.. GENERATED FROM PYTHON SOURCE LINES 106-110

.. code-block:: Python



    assert_almost_equal(res, rest[0, 0].numpy())








.. GENERATED FROM PYTHON SOURCE LINES 111-121

using Gemm?
+++++++++++

A fast implementation could reuse whatever exists with a fast implementation
such as a matrix multiplication. The goal is to transform the tensor `data`
into a new matrix which can be mutiplied with a flatten kernel and finally
reshaped into the expected result. pytorch calls this function
`Unfold <https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html>`_.
This function is also called
`im2col <https://caffe.berkeleyvision.org/tutorial/layers/im2col.html>`_.

.. GENERATED FROM PYTHON SOURCE LINES 121-128

.. code-block:: Python



    unfold = Unfold(kernel_size=(3, 3), padding=(1, 1))(
        from_numpy(data[np.newaxis, np.newaxis, ...])
    )
    unfold.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    torch.Size([1, 9, 35])



.. GENERATED FROM PYTHON SOURCE LINES 129-130

We then multiply this matrix with the flattened kernel and reshape it.

.. GENERATED FROM PYTHON SOURCE LINES 130-136

.. code-block:: Python



    impl = kernel.flatten() @ unfold.numpy()
    impl = impl.reshape(data.shape)
    impl.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (5, 7)



.. GENERATED FROM PYTHON SOURCE LINES 137-138

Full result.

.. GENERATED FROM PYTHON SOURCE LINES 138-141

.. code-block:: Python


    impl





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[ 134.,  211.,  250.,  289.,  328.,  367.,  238.],
           [ 333.,  492.,  537.,  582.,  627.,  672.,  423.],
           [ 564.,  807.,  852.,  897.,  942.,  987.,  612.],
           [ 795., 1122., 1167., 1212., 1257., 1302.,  801.],
           [ 422.,  571.,  592.,  613.,  634.,  655.,  382.]], dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 142-143

Everything works as expected.

.. GENERATED FROM PYTHON SOURCE LINES 143-148

.. code-block:: Python



    assert_almost_equal(res, impl)









.. GENERATED FROM PYTHON SOURCE LINES 149-160

What is ConvTranspose?
++++++++++++++++++++++

Deep neural network are trained with a stochastic gradient descent.
The gradient of every layer needs to be computed including the gradient
of a convolution transpose. That seems easier with the second expression
of a convolution relying on a matrix multiplication and function `im2col`.
`im2col` is just a new matrix built from `data` where every value was
copied in 9=3x3 locations. The gradient against an input value `data[i,j]`
is the sum of 9=3x3 values from the output gradient. If `im2col` plays
with indices, the gradient requires to do the same thing in the other way.

.. GENERATED FROM PYTHON SOURCE LINES 160-166

.. code-block:: Python



    # impl[:, :] = 0
    # impl[2, 3] = 1
    impl





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[ 134.,  211.,  250.,  289.,  328.,  367.,  238.],
           [ 333.,  492.,  537.,  582.,  627.,  672.,  423.],
           [ 564.,  807.,  852.,  897.,  942.,  987.,  612.],
           [ 795., 1122., 1167., 1212., 1257., 1302.,  801.],
           [ 422.,  571.,  592.,  613.,  634.,  655.,  382.]], dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 167-168

ConvTranspose...

.. GENERATED FROM PYTHON SOURCE LINES 168-177

.. code-block:: Python



    ct = conv_transpose2d(
        from_numpy(impl.reshape(data.shape)[np.newaxis, np.newaxis, ...]),
        from_numpy(kernel[np.newaxis, np.newaxis, ...]),
        padding=(1, 1),
    ).numpy()
    ct





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[[[ 2672.,  5379.,  6804.,  7659.,  8514.,  8403.,  6254.],
             [ 8117., 15408., 18909., 20790., 22671., 21780., 15539.],
             [14868., 27315., 32400., 34425., 36450., 34191., 23922.],
             [20039., 35544., 41283., 43164., 45045., 41508., 28325.],
             [18608., 32055., 36756., 38151., 39546., 35943., 23966.]]]],
          dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 178-184

And now the version with `col2im` or
`Fold <https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold>`_
applied on the result product of the output from `Conv` and the kernel:
the output of `Conv` is multiplied by every coefficient of the kernel.
Then all these matrices are concatenated to build a matrix of the same
shape of `unfold`.

.. GENERATED FROM PYTHON SOURCE LINES 184-189

.. code-block:: Python



    p = kernel.flatten().reshape((-1, 1)) @ impl.flatten().reshape((1, -1))
    p.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (9, 35)



.. GENERATED FROM PYTHON SOURCE LINES 190-191

Fold...

.. GENERATED FROM PYTHON SOURCE LINES 191-198

.. code-block:: Python



    fold = Fold(kernel_size=(3, 3), output_size=(5, 7), padding=(1, 1))(
        from_numpy(p[np.newaxis, ...])
    )
    fold.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    torch.Size([1, 1, 5, 7])



.. GENERATED FROM PYTHON SOURCE LINES 199-200

Full result.

.. GENERATED FROM PYTHON SOURCE LINES 200-203

.. code-block:: Python


    fold





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[[[ 2672.,  5379.,  6804.,  7659.,  8514.,  8403.,  6254.],
              [ 8117., 15408., 18909., 20790., 22671., 21780., 15539.],
              [14868., 27315., 32400., 34425., 36450., 34191., 23922.],
              [20039., 35544., 41283., 43164., 45045., 41508., 28325.],
              [18608., 32055., 36756., 38151., 39546., 35943., 23966.]]]])



.. GENERATED FROM PYTHON SOURCE LINES 204-212

onnxruntime-training
====================

Following lines shows how :epkg:`onnxruntime` handles the
gradient computation. This section still needs work.

Conv
++++

.. GENERATED FROM PYTHON SOURCE LINES 212-228

.. code-block:: Python



    model = (
        start(ir_version=9, opset=18)
        .vin("X", shape=[1, 1, None, None])
        .cst(kernel[np.newaxis, np.newaxis, ...])
        .rename("W")
        .bring("X", "W")
        .Conv(pads=[1, 1, 1, 1])
        .rename("Y")
        .vout()
        .to_onnx()
    )
    plot_dot(model)





.. image-sg:: /auto_examples/images/sphx_glr_plot_convolutation_matmul_102_001.png
   :alt: plot convolutation matmul 102
   :srcset: /auto_examples/images/sphx_glr_plot_convolutation_matmul_102_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 229-230

Execution

.. GENERATED FROM PYTHON SOURCE LINES 230-236

.. code-block:: Python



    ref = ReferenceEvaluator(model)
    ref.run(None, {"X": data[np.newaxis, np.newaxis, ...]})[0]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[[[ 134.,  211.,  250.,  289.,  328.,  367.,  238.],
             [ 333.,  492.,  537.,  582.,  627.,  672.,  423.],
             [ 564.,  807.,  852.,  897.,  942.,  987.,  612.],
             [ 795., 1122., 1167., 1212., 1257., 1302.,  801.],
             [ 422.,  571.,  592.,  613.,  634.,  655.,  382.]]]],
          dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 237-238

Gradient

.. GENERATED FROM PYTHON SOURCE LINES 238-246

.. code-block:: Python



    grad = onnx_derivative(
        model, options=DerivativeOptions.FillGrad | DerivativeOptions.KeepOutputs, verbose=1
    )
    plot_dot(grad)





.. image-sg:: /auto_examples/images/sphx_glr_plot_convolutation_matmul_102_002.png
   :alt: plot convolutation matmul 102
   :srcset: /auto_examples/images/sphx_glr_plot_convolutation_matmul_102_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [_onnx_derivative_fw] weights=None inputs=None options=6
    [_onnx_derivative_fw] guessed weights=['W']
    [_onnx_derivative_fw] OrtModuleGraphBuilder
    [_onnx_derivative_fw] TrainingGraphTransformerConfiguration with inputs_name=['X']
    [_onnx_derivative_fw] builder initialize
    [_onnx_derivative_fw] build
    [_onnx_derivative_fw] final graph
    [_onnx_derivative_fw] optimize
    [_onnx_derivative_fw] done




.. GENERATED FROM PYTHON SOURCE LINES 247-248

Execution.

.. GENERATED FROM PYTHON SOURCE LINES 248-260

.. code-block:: Python



    sess = InferenceSession(grad.SerializeToString(), providers=["CPUExecutionProvider"])
    res = sess.run(
        None,
        {
            "X": data[np.newaxis, np.newaxis, ...],
            "W": kernel[np.newaxis, np.newaxis, ...],
        },
    )
    res





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [array([[[[12., 21., 21., 21., 21., 21., 16.],
             [27., 45., 45., 45., 45., 45., 33.],
             [27., 45., 45., 45., 45., 45., 33.],
             [27., 45., 45., 45., 45., 45., 33.],
             [24., 39., 39., 39., 39., 39., 28.]]]], dtype=float32), array([[[[312., 378., 336.],
             [495., 595., 525.],
             [480., 574., 504.]]]], dtype=float32), array([[[[ 134.,  211.,  250.,  289.,  328.,  367.,  238.],
             [ 333.,  492.,  537.,  582.,  627.,  672.,  423.],
             [ 564.,  807.,  852.,  897.,  942.,  987.,  612.],
             [ 795., 1122., 1167., 1212., 1257., 1302.,  801.],
             [ 422.,  571.,  592.,  613.,  634.,  655.,  382.]]]],
          dtype=float32)]



.. GENERATED FROM PYTHON SOURCE LINES 261-263

ConvTranspose
+++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 263-278

.. code-block:: Python



    model = (
        start(ir_version=9, opset=18)
        .vin("X", shape=[1, 1, None, None])
        .cst(kernel[np.newaxis, np.newaxis, ...])
        .rename("W")
        .bring("X", "W")
        .ConvTranspose(pads=[1, 1, 1, 1])
        .rename("Y")
        .vout()
        .to_onnx()
    )
    plot_dot(model)




.. image-sg:: /auto_examples/images/sphx_glr_plot_convolutation_matmul_102_003.png
   :alt: plot convolutation matmul 102
   :srcset: /auto_examples/images/sphx_glr_plot_convolutation_matmul_102_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 279-280

Execution.

.. GENERATED FROM PYTHON SOURCE LINES 280-286

.. code-block:: Python


    sess = InferenceSession(model.SerializeToString(), providers=["CPUExecutionProvider"])
    ct = sess.run(None, {"X": impl[np.newaxis, np.newaxis, ...]})[0]
    ct






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[[[ 2672.,  5379.,  6804.,  7659.,  8514.,  8403.,  6254.],
             [ 8117., 15408., 18909., 20790., 22671., 21780., 15539.],
             [14868., 27315., 32400., 34425., 36450., 34191., 23922.],
             [20039., 35544., 41283., 43164., 45045., 41508., 28325.],
             [18608., 32055., 36756., 38151., 39546., 35943., 23966.]]]],
          dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 287-292

im2col and col2im
=================

Function `im2col` transforms an image so that the convolution of this image
can be expressed as a matrix multiplication. It takes the image and the kernel shape.

.. GENERATED FROM PYTHON SOURCE LINES 292-353

.. code-block:: Python



    def _get_indices(i: int, shape: Sequence[int]) -> np.ndarray:
        res = np.empty((len(shape),), dtype=np.int64)
        k = len(shape) - 1
        while k > 0:
            m = i % shape[k]
            res[k] = m
            i -= m
            i /= shape[k]
            k -= 1
        res[0] = i
        return res


    def _is_out(ind: Sequence[int], shape: Sequence[int]) -> bool:
        for i, s in zip(ind, shape):
            if i < 0:
                return True
            if i >= s:
                return True
        return False


    def im2col_naive_implementation(
        data: np.ndarray, kernel_shape: Sequence[int], fill_value: int = 0
    ) -> np.ndarray:
        """
        Naive implementation for `im2col` or
        :func:`torch.nn.Unfold` (but with `padding=1`).

        :param image: image (float)
        :param kernel_shape: kernel shape
        :param fill_value: fill value
        :return: result
        """
        if not isinstance(kernel_shape, tuple):
            raise TypeError(f"Unexpected type {type(kernel_shape)!r} for kernel_shape.")
        if len(data.shape) != len(kernel_shape):
            raise ValueError(f"Shape mismatch {data.shape!r} and {kernel_shape!r}.")
        output_shape = data.shape + kernel_shape
        res = np.empty(output_shape, dtype=data.dtype)
        middle = np.array([-m / 2 for m in kernel_shape], dtype=np.int64)
        kernel_size = np.prod(kernel_shape)
        data_size = np.prod(data.shape)
        for i in range(data_size):
            for j in range(kernel_size):
                i_data = _get_indices(i, data.shape)
                i_kernel = _get_indices(j, kernel_shape)
                ind = i_data + i_kernel + middle
                t_data = tuple(i_data)
                t_kernel = tuple(i_kernel)
                i_out = t_data + t_kernel
                res[i_out] = fill_value if _is_out(ind, data.shape) else data[tuple(ind)]
        return res


    v = np.arange(5).astype(np.float32)
    w = im2col_naive_implementation(v, (3,))
    w





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([[0., 0., 1.],
           [0., 1., 2.],
           [1., 2., 3.],
           [2., 3., 4.],
           [3., 4., 0.]], dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 354-355

All is left is the matrix multiplication.

.. GENERATED FROM PYTHON SOURCE LINES 355-361

.. code-block:: Python



    k = np.array([1, 1, 1], dtype=np.float32)
    conv = w @ k
    conv





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([1., 3., 6., 9., 7.], dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 362-363

Let's compare with the numpy function.

.. GENERATED FROM PYTHON SOURCE LINES 363-368

.. code-block:: Python



    np.convolve(v, k, mode="same")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([1., 3., 6., 9., 7.], dtype=float32)



.. GENERATED FROM PYTHON SOURCE LINES 369-405

..math::

    conv(v, k) = im2col(v, shape(k)) \; k = w \; k` where `w = im2col(v, shape(k))

In deep neural network, the gradient is propagated from the last layer
to the first one. At some point, the backpropagation produces the gradient
:math:`\frac{d(E)}{d(conv)}`, the gradient of the error against
the outputs of the convolution layer. Then
:math:`\frac{d(E)}{d(v)} = \frac{d(E)}{d(conv(v, k))}\frac{d(conv(v, k))}{d(v)}`.

We need to compute
:math:`\frac{d(conv(v, k))}{d(v)} = \frac{d(conv(v, k))}{d(w)}\frac{d(w)}{d(v)}`.

We can say that :math:`\frac{d(conv(v, k))}{d(w)} = k`.

That leaves :math:`\frac{d(w)}{d(v)} = \frac{d(im2col(v, shape(k)))}{d(v)}`.
And this last term is equal to :math:`im2col(m, shape(k))` where :math:`m`
is a matrix identical to :math:`v` except that all not null parameter
are replaced by 1. To summarize:
:math:`\frac{d(im2col(v, shape(k)))}{d(v)} = im2col(v \neq 0, shape(k))`.

Finally:

.. math::

  \frac{d(E)}{d(v)} = \frac{d(E)}{d(conv(v, k))}\frac{d(conv(v, k))}{d(v)} =
  \frac{d(E)}{d(conv(v, k))} \; k \; im2col(v \neq 0, shape(k))

Now, :math:`im2col(v \neq 0, shape(k))` is a very simple matrix with only ones or zeros.
Is there a way we can avoid doing the matrix multiplication but simply
adding terms? That's the purpose of function ``col2im`` defined so that:

.. math::

  \frac{d(E)}{d(v)} = \frac{d(E)}{d(conv(v, k))} \; k \; i
  m2col(v \neq 0, shape(k)) = col2im\left(\frac{d(E)}{d(conv(v, k))} \; k, shape(k) \right)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.514 seconds)


.. _sphx_glr_download_auto_examples_plot_convolutation_matmul_102.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_convolutation_matmul_102.ipynb <plot_convolutation_matmul_102.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_convolutation_matmul_102.py <plot_convolutation_matmul_102.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_convolutation_matmul_102.zip <plot_convolutation_matmul_102.zip>`


.. include:: plot_convolutation_matmul_102.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
