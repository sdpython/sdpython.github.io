
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_torch_aot_201.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_torch_aot_201.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_torch_aot_201.py:


201: Evaluate DORT Training
===========================

It compares DORT to eager mode and :epkg:`onnxrt backend`.

To run the script:

::

    python _doc/examples/plot_torch_aot --help

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 16-85

.. code-block:: Python


    import warnings

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import torch._dynamo
    import contextlib
    import itertools
    import os
    import gc
    import platform

    # import pickle
    import pprint
    import multiprocessing
    import time
    import cProfile
    import pstats
    import io
    import logging
    from pstats import SortKey

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas
    import onnx
    from onnx_array_api.profiling import profile2graph
    import torch
    from torch import nn
    import torch.nn.functional as F
    import experimental_experiment
    from experimental_experiment.plotting.memory import memory_peak_plot
    from experimental_experiment.ext_test_case import measure_time, get_figure
    from experimental_experiment.args import get_parsed_args
    from experimental_experiment.memory_peak import start_spying_on
    from experimental_experiment.torch_helper.training_helper import make_aot_ort
    from tqdm import tqdm

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)


    def system_info():
        obs = {}
        obs["processor"] = platform.processor()
        obs["cores"] = multiprocessing.cpu_count()
        try:
            obs["cuda"] = 1 if torch.cuda.is_available() else 0
            obs["cuda_count"] = torch.cuda.device_count()
            obs["cuda_name"] = torch.cuda.get_device_name()
            obs["cuda_capa"] = torch.cuda.get_device_capability()
        except (RuntimeError, AssertionError):
            # no cuda
            pass
        return obs


    pprint.pprint(system_info())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'cores': 8,
     'cuda': 1,
     'cuda_capa': (6, 1),
     'cuda_count': 1,
     'cuda_name': 'NVIDIA GeForce GTX 1060',
     'processor': 'x86_64'}




.. GENERATED FROM PYTHON SOURCE LINES 86-87

Scripts arguments

.. GENERATED FROM PYTHON SOURCE LINES 87-116

.. code-block:: Python



    script_args = get_parsed_args(
        "plot_torch_aot",
        description=__doc__,
        scenarios={
            "small": "small model to test",
            "middle": "55Mb model",
            "large": "1Gb model",
        },
        warmup=5,
        repeat=5,
        repeat1=(1, "repeat for the first iteration"),
        maxtime=(
            2,
            "maximum time to run a model to measure the computation time, "
            "it is 0.1 when scenario is small",
        ),
        expose="scenarios,repeat,repeat1,warmup",
    )

    if script_args.scenario in (None, "small"):
        script_args.maxtime = 0.1
    print(f"scenario={script_args.scenario or 'small'}")
    print(f"warmup={script_args.warmup}")
    print(f"repeat={script_args.repeat}")
    print(f"repeat1={script_args.repeat1}")
    print(f"maxtime={script_args.maxtime}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    scenario=small
    warmup=5
    repeat=5
    repeat1=1
    maxtime=0.1




.. GENERATED FROM PYTHON SOURCE LINES 117-121

The model
+++++++++

A simple model to convert.

.. GENERATED FROM PYTHON SOURCE LINES 121-213

.. code-block:: Python



    class MyModelClass(nn.Module):
        def __init__(self, scenario=script_args.scenario):
            super(MyModelClass, self).__init__()
            if scenario == "middle":
                self.large = False
                self.conv1 = nn.Conv2d(1, 32, 5)
                # self.conv2 = nn.Conv2d(128, 16, 5)
                self.fc1 = nn.Linear(30752, 1024)
                self.fcs = []
                self.fc2 = nn.Linear(1024, 128)
                self.fc3 = nn.Linear(128, 10)
            elif scenario in (None, "small"):
                self.large = False
                self.conv1 = nn.Conv2d(1, 16, 5)
                # self.conv2 = nn.Conv2d(16, 16, 5)
                self.fc1 = nn.Linear(144, 512)
                self.fcs = []
                self.fc2 = nn.Linear(512, 128)
                self.fc3 = nn.Linear(128, 10)
            elif scenario in (None, "large"):
                self.large = True
                self.conv1 = nn.Conv2d(1, 32, 5)
                # self.conv2 = nn.Conv2d(128, 16, 5)
                self.fc1 = nn.Linear(30752, 4096)
                # torch script does not support loops.
                self.fca = nn.Linear(4096, 4096)
                self.fcb = nn.Linear(4096, 4096)
                self.fcc = nn.Linear(4096, 4096)
                self.fcd = nn.Linear(4096, 4096)
                self.fce = nn.Linear(4096, 4096)
                self.fcf = nn.Linear(4096, 4096)
                self.fcg = nn.Linear(4096, 4096)
                self.fch = nn.Linear(4096, 4096)
                self.fci = nn.Linear(4096, 4096)
                # end of the unfolded loop.
                self.fc2 = nn.Linear(4096, 128)
                self.fc3 = nn.Linear(128, 10)
            else:
                raise ValueError(f"Unsupported scenario={scenario!r}.")

        def forward(self, x):
            x = F.max_pool2d(F.relu(self.conv1(x)), (4, 4))
            # x = F.max_pool2d(F.relu(self.conv2(x)), 2)
            x = torch.flatten(x, 1)
            x = F.relu(self.fc1(x))
            if self.large:
                # loop
                x = F.relu(self.fca(x))
                x = F.relu(self.fcb(x))
                x = F.relu(self.fcc(x))
                x = F.relu(self.fcd(x))
                x = F.relu(self.fce(x))
                x = F.relu(self.fcf(x))
                x = F.relu(self.fcg(x))
                x = F.relu(self.fch(x))
                x = F.relu(self.fci(x))
                # end of the loop
            x = F.relu(self.fc2(x))
            y = self.fc3(x)
            return y


    def create_model_and_input(scenario=script_args.scenario):
        if scenario == "middle":
            shape = [1, 1, 128, 128]
        elif scenario in (None, "small"):
            shape = [1, 1, 16, 16]
        elif scenario == "large":
            shape = [1, 1, 128, 128]
        else:
            raise ValueError(f"Unsupported scenario={scenario!r}.")
        input_tensor = torch.rand(*shape).to(torch.float32)
        y = torch.rand((1, 10)).to(torch.float32)
        model = MyModelClass(scenario=scenario)
        assert model(input_tensor) is not None
        return model, (input_tensor, y)


    def torch_model_size(model):
        size_model = 0
        for param in model.parameters():
            size = param.numel() * torch.finfo(param.data.dtype).bits / 8
            size_model += size
        return size_model


    model, input_tensors = create_model_and_input()
    model_size = torch_model_size(model)
    print(f"model size={model_size / 2 ** 20} Mb")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    model size=0.5401992797851562 Mb




.. GENERATED FROM PYTHON SOURCE LINES 214-216

Backends
++++++++

.. GENERATED FROM PYTHON SOURCE LINES 216-269

.. code-block:: Python



    def run(model, tensor_x, tensor_y):
        tensor_x = tensor_x.detach()
        tensor_y = tensor_y.detach()
        for param in model.parameters():
            param.grad = None
        try:
            output = model(tensor_x)
        except Exception as e:
            raise AssertionError(f"issue with {type(tensor_x)}") from e
        loss = F.mse_loss(output, tensor_y)

        # return loss
        def _backward_():
            loss.backward()

        _backward_()
        return loss, (param.grad for param in model.parameters())


    def get_torch_eager(model, *args):
        def my_compiler(gm, example_inputs):
            return gm.forward

        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                optimized_mod = torch.compile(model, fullgraph=True, backend=my_compiler)
                assert run(optimized_mod, *args)
                return optimized_mod


    def get_torch_default(model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                optimized_mod = torch.compile(model, fullgraph=True, mode="reduce-overhead")
                assert run(optimized_mod, *args)
                return optimized_mod


    def get_torch_dort(model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                local_aot_ort, _ = make_aot_ort(dynamic=True, rewrite=True)
                optimized_mod = torch.compile(model, backend=local_aot_ort, fullgraph=True)
                run(optimized_mod, *args)
                assert run(optimized_mod, *args)
                return optimized_mod









.. GENERATED FROM PYTHON SOURCE LINES 270-271

Let's check they are working.

.. GENERATED FROM PYTHON SOURCE LINES 271-297

.. code-block:: Python


    export_functions = [
        get_torch_eager,
        get_torch_default,
        get_torch_dort,
    ]

    exporters = {f.__name__.replace("get_", ""): f for f in export_functions}

    supported_exporters = {}
    for k, v in exporters.items():
        print(f"run function {k}")
        filename = f"plot_torch_aot_{k}.onnx"
        torch._dynamo.reset()
        model, input_tensors = create_model_and_input()
        if 1:  # try:
            run(model, *input_tensors)
        else:  # except Exception as e:
            print(f"skipped due to {str(e)[:1000]}")  # noqa: F821
            continue
        supported_exporters[k] = v
        del model
        gc.collect()
        time.sleep(1)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run function torch_eager
    run function torch_default
    run function torch_dort




.. GENERATED FROM PYTHON SOURCE LINES 298-300

Compile and Memory
++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 300-349

.. code-block:: Python



    def flatten(ps):
        obs = ps["cpu"].to_dict(unit=2**20)
        if "gpus" in ps:
            for i, g in enumerate(ps["gpus"]):
                for k, v in g.to_dict(unit=2**20).items():
                    obs[f"gpu{i}_{k}"] = v
        return obs


    data = []

    for k, v in supported_exporters.items():
        print(f"run compile for memory {k} on cpu")
        filename = f"plot_torch_aot_{k}.onnx"
        if has_cuda:
            torch.cuda.set_device(0)
        torch._dynamo.reset()
        # CPU
        model, input_tensors = create_model_and_input()
        stat = start_spying_on(cuda=1 if has_cuda else 0)
        run(model, *input_tensors)
        obs = flatten(stat.stop())
        print("done.")
        obs.update(dict(export=k, p="cpu"))
        data.append(obs)
        del model
        gc.collect()
        time.sleep(1)

        if not has_cuda:
            continue
        torch._dynamo.reset()
        # CUDA
        model, input_tensors = create_model_and_input()
        model = model.cuda()
        input_tensors = [i.cuda() for i in input_tensors]
        print(f"run compile for memory {k} on cuda")
        stat = start_spying_on(cuda=1 if has_cuda else 0)
        run(model, *input_tensors)
        obs = flatten(stat.stop())
        print("done.")
        obs.update(dict(export=k, p="cuda"))
        data.append(obs)
        del model
        gc.collect()
        time.sleep(1)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run compile for memory torch_eager on cpu
    done.
    run compile for memory torch_eager on cuda
    done.
    run compile for memory torch_default on cpu
    done.
    run compile for memory torch_default on cuda
    done.
    run compile for memory torch_dort on cpu
    done.
    run compile for memory torch_dort on cuda
    done.




.. GENERATED FROM PYTHON SOURCE LINES 350-351

The result.

.. GENERATED FROM PYTHON SOURCE LINES 351-368

.. code-block:: Python

    df1 = pandas.DataFrame(data)
    df1.to_csv("plot_torch_aot_1_memory.csv", index=False)
    df1.to_excel("plot_torch_aot_1_memory.xlsx", index=False)
    print(df1)

    for p in ["cpu", "cuda"]:
        if not has_cuda and p == "cuda":
            continue
        ax = memory_peak_plot(
            df1[df1["p"] == p],
            key=("export",),
            bars=[model_size * i / 2**20 for i in range(1, 5)],
            suptitle=f"Memory Consumption of the Compilation on {p}\n"
            f"model size={model_size / 2**20:1.0f} Mb",
        )
        get_figure(ax).savefig(f"plot_torch_aot_1_memory_{p}.png")




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_001.png
         :alt: Memory Consumption of the Compilation on cpu model size=1 Mb, Memory peak (Mb), Memory peak - memory begin (Mb), Memory average - memory begin (Mb), GPU Memory peak (Mb), GPU Memory peak - memory begin (Mb), GPU Memory average - memory begin (Mb)
         :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_002.png
         :alt: Memory Consumption of the Compilation on cuda model size=1 Mb, Memory peak (Mb), Memory peak - memory begin (Mb), Memory average - memory begin (Mb), GPU Memory peak (Mb), GPU Memory peak - memory begin (Mb), GPU Memory average - memory begin (Mb)
         :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

              peak         mean         n        begin          end   gpu0_peak   gpu0_mean    gpu0_n  gpu0_begin    gpu0_end         export     p
    0  3165.304688  3165.187500  0.000003  3164.953125  3165.304688  670.269531  670.269531  0.000003  670.269531  670.269531    torch_eager   cpu
    1  3636.398438  3364.678657  0.000329  3165.492188  3636.398438  872.269531  755.962285  0.000329  672.269531  872.269531    torch_eager  cuda
    2  3636.398438  3636.398438  0.000005  3636.398438  3636.398438  872.269531  872.269531  0.000005  872.269531  872.269531  torch_default   cpu
    3  3636.925781  3636.662109  0.000002  3636.398438  3636.925781  872.269531  872.269531  0.000002  872.269531  872.269531  torch_default  cuda
    4  3636.925781  3636.925781  0.000002  3636.925781  3636.925781  872.269531  872.269531  0.000002  872.269531  872.269531     torch_dort   cpu
    5  3636.925781  3636.925781  0.000002  3636.925781  3636.925781  872.269531  872.269531  0.000002  872.269531  872.269531     torch_dort  cuda




.. GENERATED FROM PYTHON SOURCE LINES 369-371

dort first iteration speed
++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 371-434

.. code-block:: Python


    data = []

    for k, v in supported_exporters.items():
        print(f"run dort cpu {k}: {script_args.repeat1}")
        times = []
        for i in range(int(script_args.repeat1)):
            model, input_tensors = create_model_and_input()
            torch._dynamo.reset()
            begin = time.perf_counter()
            run(model, *input_tensors)
            duration = time.perf_counter() - begin
            times.append(duration)
            del model
            gc.collect()
            time.sleep(1)

        print(f"done: {times[-1]}")
        data.append(
            dict(
                export=k,
                time=np.mean(times),
                min=min(times),
                max=max(times),
                first=times[0],
                last=times[-1],
                std=np.std(times),
                p="cpu",
            )
        )

        if not has_cuda:
            continue

        print(f"run dort cuda {k}: {script_args.repeat1}")
        times = []
        for i in range(int(script_args.repeat1)):
            model, input_tensors = create_model_and_input()
            model = model.cuda()
            input_tensors = [i.cuda() for i in input_tensors]
            torch._dynamo.reset()
            begin = time.perf_counter()
            run(model, *input_tensors)
            duration = time.perf_counter() - begin
            times.append(duration)
            del model
            gc.collect()
            time.sleep(1)

        print(f"done: {times[-1]}")
        data.append(
            dict(
                export=k,
                time=np.mean(times),
                min=min(times),
                max=max(times),
                first=times[0],
                last=times[-1],
                std=np.std(times),
                p="cuda",
            )
        )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run dort cpu torch_eager: 1
    done: 0.0010855000000447035
    run dort cuda torch_eager: 1
    done: 0.003418199999941862
    run dort cpu torch_default: 1
    done: 0.00669220000054338
    run dort cuda torch_default: 1
    done: 0.006715999999869382
    run dort cpu torch_dort: 1
    done: 0.0014920999983587535
    run dort cuda torch_dort: 1
    done: 0.005817400000523776




.. GENERATED FROM PYTHON SOURCE LINES 435-436

The result.

.. GENERATED FROM PYTHON SOURCE LINES 436-447

.. code-block:: Python

    df1 = pandas.DataFrame(data)
    df1.to_csv("plot_torch_aot_1_time.csv", index=False)
    df1.to_excel("plot_torch_aot_1_time.xlsx", index=False)
    print(df1)

    fig, ax = plt.subplots(1, 1)
    dfi = df1[["export", "p", "time", "std"]].set_index(["export", "p"])
    dfi["time"].plot.bar(ax=ax, title="Compilation time", yerr=dfi["std"], rot=30)
    fig.tight_layout()
    fig.savefig("plot_torch_aot_1_time.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_003.png
   :alt: Compilation time
   :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

              export      time       min       max     first      last  std     p
    0    torch_eager  0.001086  0.001086  0.001086  0.001086  0.001086  0.0   cpu
    1    torch_eager  0.003418  0.003418  0.003418  0.003418  0.003418  0.0  cuda
    2  torch_default  0.006692  0.006692  0.006692  0.006692  0.006692  0.0   cpu
    3  torch_default  0.006716  0.006716  0.006716  0.006716  0.006716  0.0  cuda
    4     torch_dort  0.001492  0.001492  0.001492  0.001492  0.001492  0.0   cpu
    5     torch_dort  0.005817  0.005817  0.005817  0.005817  0.005817  0.0  cuda




.. GENERATED FROM PYTHON SOURCE LINES 448-450

Compilation Profiling
+++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 450-523

.. code-block:: Python



    def clean_text(text):
        pathes = [
            os.path.abspath(
                os.path.normpath(os.path.join(os.path.dirname(torch.__file__), ".."))
            ),
            os.path.abspath(
                os.path.normpath(os.path.join(os.path.dirname(onnx.__file__), ".."))
            ),
            os.path.abspath(
                os.path.normpath(
                    os.path.join(os.path.dirname(experimental_experiment.__file__), "..")
                )
            ),
        ]
        for p in pathes:
            text = text.replace(p, "")
        text = text.replace("experimental_experiment", "experimental_experiment".upper())
        return text


    def profile_function(
        name, export_function, with_args=True, verbose=False, suffix="export"
    ):
        if verbose:
            print(f"profile {name}: {export_function}")
        if with_args:
            model, input_tensors = create_model_and_input()
            export_function(model, input_tensors)
            pr = cProfile.Profile()
            pr.enable()
            for i in range(int(script_args.repeat1)):
                export_function(model, input_tensors)
            pr.disable()
        else:
            pr = cProfile.Profile()
            pr.enable()
            for i in range(int(script_args.repeat1)):
                export_function()
            pr.disable()
        s = io.StringIO()
        sortby = SortKey.CUMULATIVE
        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
        ps.print_stats()
        # with open(f"plot_torch_aot_profile_{name}_{suffix}.pickle", "wb") as f:
        #     pickle.dump(ps, f)

        raw = s.getvalue()
        text = "\n".join(raw.split("\n")[:200])
        if verbose:
            print(text)
        with open(f"plot_torch_aot_profile_{name}_{suffix}.txt", "w") as f:
            f.write(raw)

        root, nodes = profile2graph(ps, clean_text=clean_text)
        text = root.to_text()
        with open(f"plot_torch_aot_profile_{name}_{suffix}_h.txt", "w") as f:
            f.write(text)
        if verbose:
            print("done.")


    model, input_tensors = create_model_and_input()


    def function_to_profile(model=model, input_tensors=input_tensors):
        return get_torch_dort(model, *input_tensors)


    profile_function("dort", function_to_profile, verbose=True, suffix="1")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    profile dort: <function function_to_profile at 0x7f14e076fe20>
             1012563 function calls (989144 primitive calls) in 2.198 seconds

       Ordered by: cumulative time

       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
            1    0.000    0.000    2.319    2.319 /home/xadupre/github/experimental-experiment/_doc/examples/plot_torch_aot_201.py:516(function_to_profile)
            1    0.000    0.000    2.319    2.319 /home/xadupre/github/experimental-experiment/_doc/examples/plot_torch_aot_201.py:258(get_torch_dort)
            2    0.000    0.000    1.900    0.950 /home/xadupre/github/experimental-experiment/_doc/examples/plot_torch_aot_201.py:218(run)
          9/5    0.000    0.000    1.882    0.376 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:427(_fn)
         23/4    0.000    0.000    1.738    0.435 /home/xadupre/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1523(_wrapped_call_impl)
         23/4    0.002    0.000    1.738    0.435 /home/xadupre/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1529(_call_impl)
            3    0.000    0.000    1.063    0.354 /home/xadupre/.local/lib/python3.10/site-packages/torch/autograd/graph.py:739(_engine_run_backward)
            3    0.003    0.001    1.063    0.354 {method 'run_backward' of 'torch._C._EngineBase' objects}
          6/4    0.000    0.000    1.056    0.264 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:105(call_func_at_runtime_with_args)
         12/4    0.001    0.000    1.052    0.263 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph_module.py:736(call_wrapped)
            4    0.000    0.000    1.052    0.263 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph_module.py:299(__call__)
            8    0.001    0.000    1.044    0.131 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/onnxruntime.py:837(_ort_acclerated_call)
            2    0.000    0.000    0.916    0.458 /home/xadupre/github/experimental-experiment/_doc/examples/plot_torch_aot_201.py:230(_backward_)
            2    0.000    0.000    0.916    0.458 /home/xadupre/.local/lib/python3.10/site-packages/torch/_tensor.py:466(backward)
            2    0.000    0.000    0.916    0.458 /home/xadupre/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:165(backward)
            2    0.000    0.000    0.915    0.457 /home/xadupre/.local/lib/python3.10/site-packages/torch/autograd/function.py:286(apply)
            2    0.000    0.000    0.915    0.457 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:625(backward)
            2    0.001    0.000    0.914    0.457 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:808(call_compiled_backward)
            2    0.000    0.000    0.754    0.377 <eval_with_key>.49:4(forward)
          5/4    0.000    0.000    0.726    0.182 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:34(inner)
            1    0.000    0.000    0.680    0.680 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:871(catch_errors)
            1    0.000    0.000    0.680    0.680 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:288(_convert_frame_assert)
          2/1    0.000    0.000    0.680    0.680 /usr/lib/python3.10/contextlib.py:76(inner)
            1    0.000    0.000    0.680    0.680 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:444(_compile)
          3/1    0.000    0.000    0.679    0.679 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:255(time_wrapper)
            1    0.000    0.000    0.678    0.678 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:524(compile_inner)
            1    0.000    0.000    0.667    0.667 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1028(transform_code_object)
            1    0.000    0.000    0.663    0.663 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:150(_fn)
            1    0.000    0.000    0.663    0.663 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:477(transform)
            4    0.000    0.000    0.663    0.166 /home/xadupre/github/experimental-experiment/experimental_experiment/torch_helper/training_helper.py:77(<lambda>)
            4    0.000    0.000    0.663    0.166 /home/xadupre/github/experimental-experiment/experimental_experiment/convert/convert_helper.py:19(optimize_model_proto)
            4    0.001    0.000    0.662    0.166 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/__init__.py:22(optimize)
            1    0.000    0.000    0.659    0.659 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2148(run)
            1    0.000    0.000    0.659    0.659 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:803(run)
           44    0.001    0.000    0.659    0.015 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:701(step)
            1    0.000    0.000    0.591    0.591 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2254(RETURN_VALUE)
            1    0.000    0.000    0.591    0.591 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:894(compile_subgraph)
            1    0.000    0.000    0.589    0.589 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1147(compile_and_call_fx_graph)
            1    0.000    0.000    0.582    0.582 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1232(call_user_compiler)
          2/1    0.000    0.000    0.582    0.582 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:59(debug_wrapper)
            1    0.000    0.000    0.582    0.582 /home/xadupre/.local/lib/python3.10/site-packages/torch/__init__.py:1767(__call__)
            1    0.000    0.000    0.582    0.582 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/onnxruntime.py:1127(__call__)
            1    0.000    0.000    0.582    0.582 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:18(compiler_fn)
            1    0.000    0.000    0.581    0.581 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:804(aot_module_simplified)
            1    0.001    0.001    0.579    0.579 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:411(create_aot_dispatcher_function)
           32    0.001    0.000    0.554    0.017 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:780(visit_model)
            1    0.000    0.000    0.446    0.446 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:410(aot_wrapper_dedupe)
            1    0.000    0.000    0.446    0.446 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:629(aot_wrapper_synthetic_base)
            1    0.000    0.000    0.446    0.446 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:184(aot_dispatch_autograd)
            1    0.000    0.000    0.418    0.418 /home/xadupre/github/experimental-experiment/experimental_experiment/torch_helper/training_helper.py:5(make_aot_ort)
            1    0.000    0.000    0.417    0.417 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/onnxruntime.py:722(__init__)
            2    0.071    0.035    0.399    0.199 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/decomposition_table.py:18(_create_onnx_supports_op_overload_table)
        90/32    0.008    0.000    0.390    0.012 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:641(visit_graph)
          285    0.016    0.000    0.333    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:267(__torch_dispatch__)
           24    0.000    0.000    0.331    0.014 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/simple_function_folding.py:28(visit_model)
            2    0.000    0.000    0.301    0.150 /home/xadupre/github/experimental-experiment/_doc/examples/plot_torch_aot_201.py:163(forward)
            2    0.000    0.000    0.301    0.150 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:913(forward)
          6/2    0.000    0.000    0.301    0.150 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:88(g)
            2    0.000    0.000    0.301    0.150 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:77(runtime_wrapper)
            2    0.000    0.000    0.300    0.150 /home/xadupre/.local/lib/python3.10/site-packages/torch/autograd/function.py:582(apply)
            2    0.000    0.000    0.300    0.150 {built-in method apply}
            2    0.000    0.000    0.300    0.150 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:485(forward)
            1    0.000    0.000    0.297    0.297 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py:112(_lazy_forward)
            2    0.000    0.000    0.296    0.148 <eval_with_key>.45:4(forward)
            1    0.000    0.000    0.291    0.291 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:130(aot_dispatch_autograd_graph)
            1    0.000    0.000    0.284    0.284 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:34(_create_graph)
            1    0.000    0.000    0.284    0.284 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:978(wrapped)
            1    0.000    0.000    0.282    0.282 /home/xadupre/.local/lib/python3.10/site-packages/torch/_compile.py:20(inner)
            1    0.000    0.000    0.282    0.282 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:535(dispatch_trace)
            1    0.000    0.000    0.281    0.281 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:663(trace)
            1    0.000    0.000    0.274    0.274 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:650(flatten_fn)
            1    0.000    0.000    0.274    0.274 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:551(wrapped)
            1    0.000    0.000    0.269    0.269 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:356(__init__)
      958/569    0.003    0.000    0.261    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/utils/_stats.py:15(wrapper)
     1232/483    0.009    0.000    0.253    0.001 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:793(visit_node)
         9914    0.031    0.000    0.252    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:556(process_value_info)
            1    0.000    0.000    0.252    0.252 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:516(joint_helper)
            1    0.000    0.000    0.252    0.252 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:346(_functionalized_f_helper)
            1    0.001    0.001    0.252    0.252 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/decomposition_table.py:78(create_onnx_friendly_decomposition_table)
            1    0.000    0.000    0.230    0.230 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:247(inner_fn_with_anomaly)
            1    0.000    0.000    0.230    0.230 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:182(inner_fn)
           16    0.000    0.000    0.225    0.014 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/simple_function_folding.py:211(inline_simple_functions)
            8    0.000    0.000    0.223    0.028 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/constant_folding.py:268(fold_constants)
            8    0.000    0.000    0.223    0.028 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/constant_folding.py:262(visit_model)
            6    0.002    0.000    0.216    0.036 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/interpreter.py:106(run)
       305/12    0.012    0.000    0.211    0.018 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:71(wrapper)
        15008    0.026    0.000    0.188    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:250(is_registered_op)
      130/100    0.008    0.000    0.186    0.002 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:826(process_function_node)
          114    0.001    0.000    0.176    0.002 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/interpreter.py:184(run_node)
            2    0.000    0.000    0.169    0.085 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/onnxruntime.py:1073(compile)
            2    0.000    0.000    0.168    0.084 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py:327(partition_and_fuse)
        15067    0.040    0.000    0.163    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:227(get_op_functions)
      263/242    0.002    0.000    0.160    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:635(__torch_dispatch__)
            1    0.000    0.000    0.156    0.156 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:34(_wrapped_bw_compiler)
            2    0.000    0.000    0.155    0.078 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py:265(fuse_partitions)
            2    0.000    0.000    0.155    0.078 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py:218(fuse_by_partitions)
      263/242    0.001    0.000    0.152    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:666(inner_torch_dispatch)
      775/597    0.004    0.000    0.151    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:859(tree_map)
    5717/5621    0.014    0.000    0.149    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/node.py:699(map_arg)
            1    0.000    0.000    0.149    0.149 /home/xadupre/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:278(grad)
        12090    0.034    0.000    0.148    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:60(load_from_value_info)
           32    0.000    0.000    0.147    0.005 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:731(_gather_function_metadata)
           32    0.000    0.000    0.146    0.005 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:693(visit_model)
           32    0.001    0.000    0.146    0.005 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:333(visit_model)
           32    0.006    0.000    0.145    0.005 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:347(visit_graph)
        69/54    0.003    0.000    0.144    0.003 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:294(proxy_call)
      671/667    0.003    0.000    0.139    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:886(__torch_dispatch__)
          404    0.008    0.000    0.136    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/constant_folding.py:178(process_node)
      671/667    0.009    0.000    0.135    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1215(dispatch)
            2    0.000    0.000    0.133    0.067 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:666(functional_call)
    12120/5630    0.066    0.000    0.132    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/node.py:707(map_aggregate)
           22    0.000    0.000    0.129    0.006 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:648(run_node)
          752    0.003    0.000    0.129    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:1066(tree_map_only)
            1    0.001    0.001    0.128    0.128 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/partitioners.py:637(min_cut_rematerialization_partition)
        27/21    0.000    0.000    0.124    0.006 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/constant_folding.py:230(process_function_node)
            4    0.001    0.000    0.121    0.030 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:495(run)
          345    0.004    0.000    0.118    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:940(_cached_dispatch_impl)
            4    0.000    0.000    0.118    0.029 /home/xadupre/github/onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_inference_collection.py:358(__init__)
            4    0.118    0.029    0.118    0.029 /home/xadupre/github/onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_inference_collection.py:436(_create_inference_session)
           24    0.000    0.000    0.117    0.005 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/simple_function_folding.py:24(_gather_function_metadata)
            8    0.000    0.000    0.110    0.014 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/simple_function_folding.py:229(inline_functions_with_unused_outputs)
        12114    0.015    0.000    0.109    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:462(lookup_or_create)
          525    0.005    0.000    0.107    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph.py:894(create_node)
           89    0.001    0.000    0.107    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:413(run_node)
          340    0.003    0.000    0.106    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph.py:1236(node_copy)
        14266    0.061    0.000    0.104    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:69(process_value_info)
     3116/618    0.018    0.000    0.099    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:734(unflatten)
      775/668    0.002    0.000    0.098    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/_ops.py:597(__call__)
           74    0.000    0.000    0.097    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/interpreter.py:256(call_function)
    95680/94354    0.063    0.000    0.096    0.000 {built-in method builtins.isinstance}
           61    0.001    0.000    0.096    0.002 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:647(call_function)
        12114    0.022    0.000    0.094    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:288(lookup_or_create)
        38614    0.047    0.000    0.093    0.000 {method 'get' of 'dict' objects}
            1    0.000    0.000    0.092    0.092 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py:90(inner)
            8    0.000    0.000    0.088    0.011 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/_pass.py:240(run)
            4    0.000    0.000    0.087    0.022 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/passes/type_promotion.py:1716(_run)
    7487/7254    0.009    0.000    0.084    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/node.py:713(<genexpr>)
           12    0.000    0.000    0.083    0.007 /home/xadupre/github/onnx-rewriter/onnxrewriter/rewriter/__init__.py:12(rewrite)
       103/79    0.004    0.000    0.082    0.001 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/simple_function_folding.py:36(process_function_node)
            1    0.000    0.000    0.079    0.079 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:98(inner_fn)
          538    0.008    0.000    0.076    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/node.py:166(__init__)
            4    0.002    0.000    0.075    0.019 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py:91(fuse_as_graphmodule)
         1054    0.002    0.000    0.075    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:799(tree_flatten)
    3621/1054    0.015    0.000    0.073    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:778(_tree_flatten_helper)
            4    0.002    0.000    0.070    0.018 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/partitioners.py:59(_extract_graph_with_inputs_outputs)
           86    0.001    0.000    0.070    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/passes/type_promotion.py:1618(run_node)
         8848    0.037    0.000    0.069    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/registration.py:55(from_qualified_name)
            8    0.000    0.000    0.065    0.008 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/irbuilder.py:218(build_ir)
            8    0.000    0.000    0.065    0.008 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/irbuilder.py:34(visit_model)
        12114    0.028    0.000    0.065    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/ir/visitor.py:201(lookup_or_create)
            8    0.000    0.000    0.064    0.008 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/interpreter.py:298(call_module)
           59    0.000    0.000    0.063    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/onnxfunction_dispatcher.py:111(dispatch)
          770    0.007    0.000    0.062    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/node.py:436(__update_args_kwargs)
            1    0.000    0.000    0.056    0.056 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/partitioners.py:157(_extract_fwd_bwd_modules)
       105/97    0.003    0.000    0.055    0.001 {method 'detach' of 'torch._C.TensorBase' objects}
          345    0.010    0.000    0.055    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:975(_cache_key)
            9    0.000    0.000    0.055    0.006 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:479(wrapper)
            9    0.000    0.000    0.054    0.006 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1215(CALL_FUNCTION)
            9    0.000    0.000    0.054    0.006 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:654(call_function)
          212    0.001    0.000    0.054    0.000 /home/xadupre/github/onnx-rewriter/onnxrewriter/optimizer/evaluator.py:38(evaluate)
            9    0.000    0.000    0.053    0.006 /home/xadupre/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:115(forward)
            9    0.003    0.000    0.053    0.006 {built-in method torch._C._nn.linear}
           19    0.000    0.000    0.052    0.003 /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/functional_utils.py:21(to_fun)
           19    0.000    0.000    0.052    0.003 /home/xadupre/.local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:171(to_functional)
          212    0.001    0.000    0.050    0.000 /home/xadupre/github/onnx/onnx/reference/op_run.py:624(eval)
        26852    0.032    0.000    0.049    0.000 /usr/lib/python3.10/logging/__init__.py:1455(debug)
        15067    0.028    0.000    0.049    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/registration.py:44(from_name_parts)
           61    0.000    0.000    0.048    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:208(track_tensor_tree)
        76/61    0.000    0.000    0.048    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:209(wrap_with_proxy)
           10    0.000    0.000    0.047    0.005 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1329(wrap_fx_proxy)
           10    0.000    0.000    0.047    0.005 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1389(wrap_fx_proxy_cls)
     1886/755    0.004    0.000    0.046    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:792(<listcomp>)
        85/80    0.000    0.000    0.044    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:139(extract_val)
            6    0.000    0.000    0.044    0.007 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph_module.py:707(recompile)
           82    0.000    0.000    0.044    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:136(snapshot_fake)
            4    0.000    0.000    0.042    0.010 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:244(call_function)
           74    0.000    0.000    0.041    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:172(set_meta)
          104    0.001    0.000    0.040    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/proxy.py:173(create_proxy)
            6    0.000    0.000    0.040    0.007 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph.py:1299(python_code)
    13427/12815    0.014    0.000    0.040    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/node.py:689(__setattr__)
           59    0.001    0.000    0.039    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/onnxfunction_dispatcher.py:198(_find_the_perfect_or_nearest_match_onnxfunction)
            9    0.000    0.000    0.039    0.004 /home/xadupre/.local/lib/python3.10/site-packages/torch/nn/functional.py:1489(relu)
            9    0.001    0.000    0.039    0.004 {built-in method torch.relu}
            6    0.000    0.000    0.038    0.006 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph.py:1361(_python_code)
            6    0.003    0.000    0.038    0.006 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph.py:380(_gen_python_code)
            9    0.000    0.000    0.038    0.004 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1619(get_fake_value)
            4    0.000    0.000    0.038    0.010 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:761(module_call_wrapper)
           14    0.000    0.000    0.038    0.003 /home/xadupre/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1196(wrap_fake_exception)
            4    0.000    0.000    0.038    0.009 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:493(call_module)
            4    0.000    0.000    0.038    0.009 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:763(forward)
    5691/5247    0.009    0.000    0.037    0.000 {built-in method builtins.next}
            8    0.000    0.000    0.037    0.005 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/graph_module.py:353(__init__)
           62    0.004    0.000    0.036    0.001 /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/fx/onnxfunction_dispatcher.py:554(perfect_match_inputs)
            4    0.000    0.000    0.036    0.009 /home/xadupre/.local/lib/python3.10/site-packages/torch/fx/passes/utils/common.py:27(lift_subgraph_as_module)
       101/82    0.001    0.000    0.036    0.000 /home/xadupre/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1706(__setattr__)
    done.




.. GENERATED FROM PYTHON SOURCE LINES 524-526

Benchmark exported models with ORT
++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 526-622

.. code-block:: Python



    def benchmark(shape):
        data = []
        data_mem_first_run = []
        data_mem_run = []
        confs = list(
            itertools.product(
                export_functions,
                ["CPU", "CUDA"],
            )
        )
        loop = tqdm(confs)
        print(f"number of experiments: {len(loop)}")
        for export_fct, p in loop:
            name = export_fct.__name__.replace("get_torch_", "")
            obs = {}  # system_info()
            obs["name"] = name
            obs["compute"] = p
            obs["export"] = name

            model, input_tensors = create_model_and_input()
            if p == "CUDA":
                if not has_cuda:
                    continue
                model = model.cuda()
                input_tensors = [i.cuda() for i in input_tensors]
            try:
                exported_model = export_fct(model, *input_tensors)
            except Exception as e:
                obs["error"] = str(e)
                data.append(obs)
                continue

            def call_model(
                export_fct=export_fct,
                exported_model=exported_model,
                input_tensors=input_tensors,
            ):
                res = run(exported_model, *input_tensors)
                return res

            stat = start_spying_on(cuda=1 if has_cuda else 0)
            try:
                call_model()
            except Exception as e:
                loop.set_description(f"ERROR-run: {name} {e}")
                obs.update({"error": e, "step": "load"})
                data.append(obs)
                stat.stop()
                continue
            memobs = flatten(stat.stop())
            memobs.update(obs)
            data_mem_first_run.append(memobs)

            # memory consumption
            stat = start_spying_on(cuda=1 if has_cuda else 0)
            for i in range(0, script_args.warmup):
                call_model()
            memobs = flatten(stat.stop())
            memobs.update(obs)
            data_mem_run.append(memobs)

            obs.update(
                measure_time(
                    call_model,
                    max_time=script_args.maxtime,
                    repeat=script_args.repeat,
                    number=1,
                )
            )

            profile_function(name, call_model, with_args=False, suffix=f"run_{p}")

            loop.set_description(f"{obs['average']} {name} {p}")
            data.append(obs)
            del model
            del exported_model
            gc.collect()
            time.sleep(1)

        df = pandas.DataFrame(data)
        df.to_csv("plot_torch_aot_ort_time.csv", index=False)
        df.to_excel("plot_torch_aot_ort_time.xlsx", index=False)
        dfmemr = pandas.DataFrame(data_mem_run)
        dfmemr.to_csv("plot_torch_aot_ort_run_mem.csv", index=False)
        dfmemr.to_excel("plot_torch_aot_ort_run_mem.xlsx", index=False)
        dfmemfr = pandas.DataFrame(data_mem_first_run)
        dfmemfr.to_csv("plot_torch_aot_ort_first_run_mem.csv", index=False)
        dfmemfr.to_excel("plot_torch_aot_ort_first_run_mem.xlsx", index=False)
        return df, dfmemfr, dfmemr


    df, dfmemfr, dfmemr = benchmark(list(input_tensors[0].shape))
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/6 [00:00<?, ?it/s]number of experiments: 6
    0.0008775983193011009 eager CPU:   0%|          | 0/6 [00:00<?, ?it/s]    0.0008775983193011009 eager CPU:  17%|        | 1/6 [00:01<00:08,  1.74s/it]    0.0015864114942313132 eager CUDA:  17%|        | 1/6 [00:02<00:08,  1.74s/it]    0.0015864114942313132 eager CUDA:  33%|      | 2/6 [00:03<00:07,  1.81s/it]    0.0011892725275114632 default CPU:  33%|      | 2/6 [00:16<00:07,  1.81s/it]    0.0011892725275114632 default CPU:  50%|     | 3/6 [00:17<00:22,  7.42s/it]    0.0011892725275114632 default CPU:  67%|   | 4/6 [00:18<00:09,  4.64s/it]/home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:117: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    0.0030704030302855026 dort CPU:  67%|   | 4/6 [00:19<00:09,  4.64s/it]       0.0030704030302855026 dort CPU:  83%| | 5/6 [00:20<00:03,  3.89s/it]/home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:117: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    0.005412360869503691 dort CUDA:  83%| | 5/6 [00:21<00:03,  3.89s/it]    0.005412360869503691 dort CUDA: 100%|| 6/6 [00:23<00:00,  3.41s/it]    0.005412360869503691 dort CUDA: 100%|| 6/6 [00:23<00:00,  3.85s/it]
          name compute   export   average  deviation  min_exec  max_exec  repeat  number     ttime  context_size  warmup_time                              error
    0    eager     CPU    eager  0.000878   0.000063  0.000830  0.001157     1.0   119.0  0.104434          64.0     0.001774                                NaN
    1    eager    CUDA    eager  0.001586   0.000130  0.001521  0.002379     1.0    87.0  0.138018          64.0     0.002604                                NaN
    2  default     CPU  default  0.001189   0.000151  0.001043  0.001486     1.0    91.0  0.108224          64.0     0.002223                                NaN
    3  default    CUDA  default       NaN        NaN       NaN       NaN     NaN     NaN       NaN           NaN          NaN  issue with <class 'torch.Tensor'>
    4     dort     CPU     dort  0.003070   0.000866  0.002444  0.004367     1.0    33.0  0.101323          64.0     0.003820                                NaN
    5     dort    CUDA     dort  0.005412   0.000410  0.004496  0.005756     1.0    23.0  0.124484          64.0     0.005138                                NaN




.. GENERATED FROM PYTHON SOURCE LINES 623-624

Other view

.. GENERATED FROM PYTHON SOURCE LINES 624-660

.. code-block:: Python



    def view_time(df, title, suffix="time"):
        piv = pandas.pivot_table(df, index="export", columns=["compute"], values="average")
        print(piv)
        piv.to_csv(f"plot_torch_aot_{suffix}_compute.csv")
        piv.to_excel(f"plot_torch_aot_{suffix}_compute.xlsx")

        piv_cpu = pandas.pivot_table(
            df[df.compute == "CPU"],
            index="export",
            columns=["compute"],
            values="average",
        )

        fig, ax = plt.subplots(1, 2, figsize=(12, 4))
        fig.suptitle(title)
        piv_cpu.plot.barh(ax=ax[0], title="CPU", logx=True)

        if has_cuda:
            piv_gpu = pandas.pivot_table(
                df[df.compute == "CUDA"],
                index="export",
                columns=["compute"],
                values="average",
            )
            piv_gpu.plot.barh(ax=ax[1], title="CUDA", logx=True)

        fig.tight_layout()
        fig.savefig(f"plot_torch_aot_{suffix}.png")
        return ax


    view_time(df, "Compares processing time on backends")





.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_004.png
   :alt: Compares processing time on backends, CPU, CUDA
   :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    compute       CPU      CUDA
    export                     
    default  0.001189       NaN
    dort     0.003070  0.005412
    eager    0.000878  0.001586

    array([<Axes: title={'center': 'CPU'}, ylabel='export'>,
           <Axes: title={'center': 'CUDA'}, ylabel='export'>], dtype=object)



.. GENERATED FROM PYTHON SOURCE LINES 661-663

Memory First Running Time (ORT)
+++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 663-677

.. code-block:: Python


    for compute in ["CPU", "CUDA"]:
        if not has_cuda and compute == "CUDA":
            continue
        ax = memory_peak_plot(
            dfmemfr[dfmemfr.compute == compute],
            ("export",),
            suptitle=f"Memory Consumption of backend, first running time"
            f"\nrunning on {compute}",
            bars=[model_size * i / 2**20 for i in range(1, 3)],
            figsize=(18, 6),
        )
        get_figure(ax).savefig(f"plot_torch_aot_first_run_mem_{compute}.png")




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_005.png
         :alt: Memory Consumption of backend, first running time running on CPU, Memory peak (Mb), Memory peak - memory begin (Mb), Memory average - memory begin (Mb), GPU Memory peak (Mb), GPU Memory peak - memory begin (Mb), GPU Memory average - memory begin (Mb)
         :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_006.png
         :alt: Memory Consumption of backend, first running time running on CUDA, Memory peak (Mb), Memory peak - memory begin (Mb), Memory average - memory begin (Mb), GPU Memory peak (Mb), GPU Memory peak - memory begin (Mb), GPU Memory average - memory begin (Mb)
         :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_006.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 678-680

Memory Running Time (ORT)
+++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 680-693

.. code-block:: Python


    for compute in ["CPU", "CUDA"]:
        if not has_cuda and compute == "CUDA":
            continue
        ax = memory_peak_plot(
            dfmemr[dfmemr.compute == compute],
            ("export",),
            suptitle=f"Memory Consumption of backens, running time"
            f"\nrunning on {compute}",
            bars=[model_size * i / 2**20 for i in range(1, 3)],
            figsize=(18, 6),
        )
        get_figure(ax).savefig(f"plot_torch_aot_run_mem_{compute}.png")



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_007.png
         :alt: Memory Consumption of backens, running time running on CPU, Memory peak (Mb), Memory peak - memory begin (Mb), Memory average - memory begin (Mb), GPU Memory peak (Mb), GPU Memory peak - memory begin (Mb), GPU Memory average - memory begin (Mb)
         :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_007.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_torch_aot_201_008.png
         :alt: Memory Consumption of backens, running time running on CUDA, Memory peak (Mb), Memory peak - memory begin (Mb), Memory average - memory begin (Mb), GPU Memory peak (Mb), GPU Memory peak - memory begin (Mb), GPU Memory average - memory begin (Mb)
         :srcset: /auto_examples/images/sphx_glr_plot_torch_aot_201_008.png
         :class: sphx-glr-multi-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 0.392 seconds)


.. _sphx_glr_download_auto_examples_plot_torch_aot_201.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_torch_aot_201.ipynb <plot_torch_aot_201.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_torch_aot_201.py <plot_torch_aot_201.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
