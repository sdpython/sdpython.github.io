
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_export_301.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_export_301.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_export_301.py:


.. _l-plot-llama-diff-export:

301: Compares LLAMA exporters
=============================

The script compares the two exporters implemented in :epkg:`pytorch`
for a part of llama model. The model are compared after all optimizations
were made with :epkg:`onnx-rewriter` and :epkg:`onnxruntime`.

* `TorchScript-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter>`_,
  let's call it **script**
* `TorchDynamo-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter>`_,
  let's call it **dynamo**

To run the script:

::

    python _doc/examples/plot_llama_diff_export --help

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 27-80

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("attention", "one value among attention, decoder, model"),
        exporter=("dynamo", "one value among dynamo, custom"),
        ortopt=(1, "run onnxruntime optimization"),
        expose="part,exporter,ortopt",
    )

    import contextlib
    import os
    import io
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import numpy as np
    import onnx
    from onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator
    import torch
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.torch_interpreter import to_onnx
    from experimental_experiment.xbuilder import OptimizationOptions
    from experimental_experiment.convert.convert_helper import (
        optimize_model_proto,
        ort_optimize,
    )
    from experimental_experiment.torch_helper.llama_helper import (
        get_llama_model,
        get_llama_attention,
        get_llama_decoder,
    )
    from experimental_experiment.torch_helper.dump_helper import reorder_functions_in_proto

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 81-83

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 83-139

.. code-block:: Python



    print(f"part={script_args.part}")
    print(f"exporter={script_args.exporter}")
    ortopt = script_args.ortopt in (1, "1")
    print(f"ortopt={ortopt}")


    def opt_filename(filename: str) -> str:
        name, ext = os.path.splitext(filename)
        return f"{name}.opt{ext}"


    def export_script(filename, model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                torch.onnx.export(model, args, filename, input_names=["input"])
        if ortopt:
            onx = onnx.load(filename)
            ort_optimize(onx, opt_filename(filename), providers=provider)


    def export_dynamo(filename, model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                export_output = torch.onnx.dynamo_export(model, *args)
                model = export_output.model_proto
        try:
            new_model = optimize_model_proto(model)
        except ImportError as e:
            print("skipping optimization, missing package or failure:", e)
            new_model = model
        with open(filename, "wb") as f:
            f.write(new_model.SerializeToString())
        if ortopt:
            ort_optimize(new_model, opt_filename(filename), providers=provider)


    def export_custom(filename, model, *args):
        new_model = to_onnx(
            model,
            tuple(args),
            input_names=[f"input{i}" for i in range(len(args))],
            options=OptimizationOptions(
                remove_unused=True,
                constant_folding=False,
            ),
        )
        with open(filename, "wb") as f:
            f.write(new_model.SerializeToString())
        if ortopt:
            ort_optimize(new_model, opt_filename(filename), providers=provider)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=attention
    exporter=dynamo
    ortopt=True




.. GENERATED FROM PYTHON SOURCE LINES 140-142

Model and data
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 142-178

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "attention":
        model, inputs = get_llama_attention(**kwargs)
    elif script_args.part == "decoder":
        model, inputs = get_llama_decoder(**kwargs)
    elif script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    print(f"simple run with {len(inputs)} inputs")
    expected = model(*inputs[0])
    if isinstance(expected, tuple):
        for t in expected:
            if not isinstance(t, tuple):
                print(f"eager worked {t.shape}, {t.dtype}")
            else:
                print(f"eager worked {type(t)}")
    else:
        print(f"eager mode worked {expected.shape}, {expected.dtype}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    eager mode worked torch.Size([2, 1024, 512]), torch.float32




.. GENERATED FROM PYTHON SOURCE LINES 179-181

Exporting
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 181-198

.. code-block:: Python


    exporter = script_args.exporter
    file1 = f"llama.{script_args.part}.script.onnx"
    file2 = f"llama.{script_args.part}.{exporter}.onnx"

    print("torch script exporter")
    export_script(file1, model, *inputs[0])

    if exporter == "dynamo":
        print("torch dynamo exporter")
        export_dynamo(file2, model, *inputs[0])
    elif exporter == "custom":
        print("torch custom exporter")
        export_custom(file2, model, *inputs[0])
    else:
        raise AssertionError(f"Unexpected value for exporter={exporter!r}.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch script exporter
    torch dynamo exporter
    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 199-201

Verification
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 201-235

.. code-block:: Python


    if ortopt:
        print("Using models optimized by onnxruntime")
        file1 = f"llama.{script_args.part}.script.opt.onnx"
        file2 = f"llama.{script_args.part}.{exporter}.opt.onnx"


    providers = (
        ["CPUExecutionProvider"]
        if provider == "cpu"
        else [("CUDAExecutionProvider", {}), ("CPUExecutionProvider", {})]
    )

    model1 = onnx.load(file1)
    model2 = onnx.load(file2)

    feeds1, feeds2 = {}, {}
    for i in range(len(inputs[0])):
        x = inputs[0][i].detach().numpy()
        feeds1[model1.graph.input[i].name] = x
        feeds2[model2.graph.input[i].name] = x

    if ortopt:
        sess1 = onnxruntime.InferenceSession(file1, providers=providers)
        sess2 = onnxruntime.InferenceSession(file2, providers=providers)

        got1 = sess1.run(None, feeds1)
        got2 = sess2.run(None, feeds2)

        diff1 = np.abs(expected.detach().numpy() - got1[0]).max()
        diff2 = np.abs(expected.detach().numpy() - got2[0]).max()

        print(f"Error with the eager model and onnxruntime: {diff1}, {diff2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using models optimized by onnxruntime
    Error with the eager model and onnxruntime: 6.705522537231445e-08, 6.705522537231445e-08




.. GENERATED FROM PYTHON SOURCE LINES 236-238

Verification with the reference evaluator
+++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 238-261

.. code-block:: Python


    reorder_functions_in_proto(file1)
    reorder_functions_in_proto(file2)

    sess1 = ExtendedReferenceEvaluator(file1)
    try:
        sess2 = ExtendedReferenceEvaluator(file2)
    except NotImplementedError as e:
        print(e)
        sess2 = None

    got1 = sess1.run(None, feeds1)
    got2 = got1 if sess2 is None else sess2.run(None, feeds2)

    if isinstance(expected, tuple):
        diff1 = np.abs(expected[0].detach().numpy() - got1[0]).max()
        diff2 = np.abs(expected[0].detach().numpy() - got2[0]).max()
    else:
        diff1 = np.abs(expected.detach().numpy() - got1[0]).max()
        diff2 = np.abs(expected.detach().numpy() - got2[0]).max()

    print(f"Error with the eager model and the reference evaluator: {diff1}, {diff2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Error with the eager model and the reference evaluator: 4.470348358154297e-08, 4.6566128730773926e-08




.. GENERATED FROM PYTHON SOURCE LINES 262-264

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 264-290

.. code-block:: Python



    def clean_name(name):
        return name.replace(
            "_inlfunc_transformers_models_llama_modeling_llama_LlamaAttention", ""
        ).replace("_inlfunc_torch_nn_modules_linear_Linear", "")


    if sess2 is not None:
        try:
            np_inputs = [i.detach().numpy() for i in inputs[0]]
            res1, res2, align, dc = compare_onnx_execution(
                model1, model2, inputs=np_inputs, verbose=1, raise_exc=False
            )
            for r in res2:
                r.name = clean_name(r.name)
            text = dc.to_str(res1, res2, align, column_size=90)
            print(text)
        except AssertionError as e:
            if (
                "Unexpected type <class 'list'> for value, it must be a numpy array."
                not in str(e)
            ):
                raise
            print(e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] execute with 3 inputs
    [compare_onnx_execution] execute first model
    [compare_onnx_execution] got 53 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 88 results
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 89 pairs
    [compare_onnx_execution] done
    001 = | INITIA float32  2:512x512            THUB                 onnx::MatMul_131                 | INITIA float32  2:512x512            THUB                 t__2                            
    002 + |                                                                                            | INITIA int64    1:4                  CIKM                 ortshared_7_1_4_2_token_173      
    003 ~ | INITIA float32  2:512x512            AWUX                 onnx::MatMul_132                 | INITIA float32  2:512x512            WEME                 t_3__58                         
    004 ~ | INITIA float32  2:512x512            HZZE                 onnx::MatMul_133                 | INITIA float32  2:512x512            AWUX                 t_1__8                          
    005 + |                                                                                            | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_3_token_170      
    006 + |                                                                                            | INITIA int64    1:2                  USAA                 ortshared_7_1_2_1_token_164      
    007 ~ | INITIA float32  2:512x512            WEME                 onnx::MatMul_169                 | INITIA float32  2:512x512            HZZE                 t_2__14                         
    008 + |                                                                                            | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_0_token_162      
    009 ~ | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_76       | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_2_token_169     
    010 ~ | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_2_token_75       | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_1_token_161     
    011 + |                                                                                            | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_156      
    012 ~ | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_1_token_74       | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_2_token_166     
    013 = | INITIA float32  2:1024x64            CJYF                 /attention/rotary_emb/Constant_o | INITIA float32  2:1024x64            CJYF                 _val_22__1                      
    014 + |                                                                                            | INITIA int64                         ZAAA                 ortshared_7_0_1_0_token_157      
    015 + |                                                                                            | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_3_token_171      
    016 = | INITIA float32  2:1024x64            GSEC                 /attention/rotary_emb/Constant_1 | INITIA float32  2:1024x64            GSEC                 _val_32__1                      
    017 + |                                                                                            | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_1_token_167      
    018 + |                                                                                            | INITIA int64                         BAAA                 ortshared_7_0_1_1_token_168      
    019 ~ | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_0_token_73       | INITIA int64    1:4                  CIKK                 ortshared_7_1_4_1_token_163     
    020 ~ | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_3_token_78       | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_0_token_158     
    021 + |                                                                                            | INITIA float32                       IAAA                 ortshared_1_0_1_1_token_165      
    022 ~ | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_0_token_80       | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_4_token_172     
    023 + |                                                                                            | INITIA float32                       BAAA                 ortshared_1_0_1_0_token_159      
    024 ~ | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_4_token_79       | INITIA int64    1:2                  BKAA                 ortshared_7_1_2_0_token_160     
    025 = | INPUT  float32  3:2x1024x512         PIQH                 input                            | INPUT  float32  3:2x1024x512         PIQH                 l_hidden_states_                
    026 = | INPUT  float32  4:2x1x1024x1024      AAAA                 onnx::Add_1                      | INPUT  float32  4:2x1x1024x1024      AAAA                 l_attention_mask_               
    027 = | INPUT  int64    2:1x1024             KAQG                 position_ids                     | INPUT  int64    2:1x1024             KAQG                 l_position_ids_                 
    028 + |                                                                                            | RESULT int64    2:1x1024             KAQG Expand          _val_35__1                       
    029 + |                                                                                            | RESULT int64    3:1x1024x1           KAQG Unsqueeze       _val_37__1                       
    030 + |                                                                                            | RESULT int64    3:1x1024x1           KAQG Concat          _val_38__1                       
    031 ~ | RESULT float32  3:1x1024x64          GSEC Gather          /attention/Gather_1_output_0     | RESULT float32  3:1x1024x64          GSEC GatherND        _val_39__1                      
    032 = | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       /attention/Unsqueeze_1_output_0  | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       n2__25                          
    033 = | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_4_out0           | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_6_out0          
    034 + |                                                                                            | RESULT float32  2:2048x512           PIQH Reshape         view_4__14                       
    035 + |                                                                                            | RESULT float32  2:2048x512           EJNU MatMul          mm_1__8                          
    036 ~ | RESULT float32  3:2x1024x512         EJNU MatMul          /attention/k_proj/MatMul_output_ | RESULT float32  3:2x1024x512         EJNU Reshape         attention_k_proj_1__1           
    037 = | RESULT float32  4:2x1024x8x64        EJNU Reshape         /attention/Reshape_1_output_0    | RESULT float32  4:2x1024x8x64        EJNU Reshape         view_7__1                       
    038 = | RESULT float32  4:2x1024x8x32        ILQU Slice           /attention/Slice_3               | RESULT float32  4:2x1024x8x32        ILQU Slice           Slice_140__1                    
    039 = | RESULT float32  4:2x1024x8x32        SPKG Neg             /attention/Neg_1                 | RESULT float32  4:2x1024x8x32        SPKG Neg             n0__32                          
    040 = | RESULT float32  4:2x1024x8x32        XXWZ Slice           /attention/Slice_2               | RESULT float32  4:2x1024x8x32        XXWZ Slice           Slice_123__1                    
    041 = | RESULT float32  4:2x1024x8x64        OLFF Concat          /attention/Concat_1              | RESULT float32  4:2x1024x8x64        OLFF Concat          n0__33                          
    042 + |                                                                                            | RESULT float32  4:2x1024x8x64        KLMA Mul             n0__34                           
    043 + |                                                                                            | RESULT float32  4:2x8x1024x64        FQPW Transpose       mul_3__1                         
    044 + |                                                                                            | RESULT float32  2:2048x512           SSUC MatMul          mm_2__14                         
    045 + |                                                                                            | RESULT float32  3:2x1024x512         SSUC Reshape         attention_v_proj_1__1            
    046 + |                                                                                            | RESULT float32  4:2x1024x8x64        SSUC Reshape         view_8__1                        
    047 + |                                                                                            | RESULT float32  4:2x8x1024x64        NXUC Transpose       transpose_2__1                   
    048 + |                                                                                            | RESULT float32  3:16x1024x64         NXUC Reshape         view_13__1                       
    049 + |                                                                                            | RESULT float32  4:2x1x1024x1024      AAAA Mul             other_1__45                      
    050 + |                                                                                            | RESULT float32                       BAAA Cast            alpha_0__35                      
    051 = | RESULT float32  4:2x1024x8x64        KLMA Mul             /attention/Mul_3                 | RESULT float32  4:2x1024x8x64        KLMA Mul             n2__35                          
    052 ~ | RESULT float32  3:1x1024x64          CJYF Gather          /attention/Gather_output_0       | RESULT float32  3:1x1024x64          CJYF GatherND        _val_29__1                      
    053 = | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       /attention/Unsqueeze_output_0    | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       n2__24                          
    054 = | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_6_out0           | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_10_out0         
    055 = | RESULT float32  4:2x1024x8x64        SMUG Mul             /attention/Mul_2                 | RESULT float32  4:2x1024x8x64        SMUG Mul             n0__31                          
    056 = | RESULT float32  4:2x1024x8x64        CWFG Add             /attention/Add_1                 | RESULT float32  4:2x1024x8x64        CWFG Add             n3__35                          
    057 = | RESULT float32  4:2x8x64x1024        XDXO Transpose       /attention/Transpose_3_output_0  | RESULT float32  4:2x8x64x1024        XDXO Transpose       transpose_3__1                  
    058 + |                                                                                            | RESULT float32  3:16x64x1024         XDXO Reshape         view_10__1                       
    059 + |                                                                                            | RESULT float32                       BAAA Cast            alpha_0__30                      
    060 + |                                                                                            | RESULT float32  4:1x1x1024x64        GSEC Transpose       unsqueeze_1__1                   
    061 + |                                                                                            | RESULT float32  2:2048x512           VQGM MatMul          mm__2                            
    062 ~ | RESULT float32  3:2x1024x512         VQGM MatMul          /attention/q_proj/MatMul_output_ | RESULT float32  3:2x1024x512         VQGM Reshape         attention_q_proj_1__1           
    063 = | RESULT float32  4:2x1024x8x64        VQGM Reshape         /attention/Reshape_output_0      | RESULT float32  4:2x1024x8x64        VQGM Reshape         view_6__1                       
    064 = | RESULT float32  4:2x8x1024x64        CKOD Transpose       /attention/Transpose_output_0    | RESULT float32  4:2x8x1024x64        CKOD Transpose       transpose__1                    
    065 = | RESULT float32  4:2x8x1024x32        PQEU Slice           /attention/Slice_1_output_0      | RESULT float32  4:2x8x1024x32        PQEU Slice           slice_4__1                      
    066 = | RESULT float32  4:2x8x1024x32        LKWG Neg             /attention/Neg_output_0          | RESULT float32  4:2x8x1024x32        LKWG Neg             neg__1                          
    067 = | RESULT float32  4:2x8x1024x32        NUKI Slice           /attention/Slice_output_0        | RESULT float32  4:2x8x1024x32        NUKI Slice           slice_3__1                      
    068 = | RESULT float32  4:2x8x1024x64        YDHP Concat          /attention/Concat_output_0       | RESULT float32  4:2x8x1024x64        YDHP Concat          cat__1                          
    069 + |                                                                                            | RESULT float32  4:2x8x1024x64        WGAL Mul             mul_1__1                         
    070 = | RESULT float32  4:2x8x1024x64        WGAL Mul             /attention/Mul_1_output_0        | RESULT float32  4:2x8x1024x64        WGAL Mul             other_1__30                     
    071 + |                                                                                            | RESULT float32  4:1x1x1024x64        CJYF Transpose       unsqueeze__1                     
    072 = | RESULT float32  4:2x8x1024x64        UCBN Mul             /attention/Mul_output_0          | RESULT float32  4:2x8x1024x64        UCBN Mul             mul__1                          
    073 = | RESULT float32  4:2x8x1024x64        PHAX Add             /attention/Add_output_0          | RESULT float32  4:2x8x1024x64        PHAX Add             add__1                          
    074 + |                                                                                            | RESULT float32  3:16x1024x64         PHAX Reshape         view_9__1                        
    075 + |                                                                                            | RESULT float32  3:16x1024x1024       FTJJ MatMul          bmm__1                           
    076 + |                                                                                            | RESULT float32  4:2x8x1024x1024      FTJJ Reshape         view_11__1                       
    077 ~ | RESULT float32  4:2x8x1024x1024      XZKO FusedMatMul     /attention/Div_output_0          | RESULT float32  4:2x8x1024x1024      XZKO Div             div__1                          
    078 = | RESULT float32  4:2x8x1024x1024      XZKO Add             /attention/Add_2_output_0        | RESULT float32  4:2x8x1024x1024      XZKO Add             add_2__1                        
    079 ~ | RESULT float32  4:2x8x1024x1024      ONNO Softmax         /attention/Softmax_output_0      | RESULT float32  4:2x8x1024x1024      ONNN Softmax         result__46                      
    080 + |                                                                                            | RESULT float32  3:16x1024x1024       ONNN Reshape         view_12__1                       
    081 + |                                                                                            | RESULT float32  3:16x1024x64         REWV MatMul          bmm_1__1                         
    082 ~ | RESULT float32  3:2x1024x512         SSUC MatMul          /attention/v_proj/MatMul_output_ | RESULT float32  4:2x8x1024x64        REWV Reshape         view_14__1                      
    083 ~ | RESULT float32  4:2x1024x8x64        SSUC Reshape         /attention/Reshape_2_output_0    | RESULT float32  4:2x1024x8x64        KLSA Transpose       transpose_4__1                  
    084 ~ | RESULT float32  4:2x8x1024x64        NXUC Transpose       /attention/Transpose_2_output_0  | RESULT float32  3:2x1024x512         KLSA Reshape         view_15__1                      
    085 - | RESULT float32  4:2x8x1024x64        REWV MatMul          /attention/MatMul_1_output_0     |                                                                                           
    086 ~ | RESULT float32  4:2x1024x8x64        KLSA Transpose       /attention/Transpose_4_output_0  | RESULT float32  2:2048x512           KLSA Reshape         view_16__58                     
    087 ~ | RESULT float32  3:2x1024x512         KLSA Reshape         /attention/Reshape_3_output_0    | RESULT float32  2:2048x512           ZEPQ MatMul          mm_3__58                        
    088 ~ | RESULT float32  3:2x1024x512         ZEPQ MatMul          130                              | RESULT float32  3:2x1024x512         ZEPQ Reshape         attention_1                     
    089 = | OUTPUT float32  3:2x1024x512         ZEPQ                 130                              | OUTPUT float32  3:2x1024x512         ZEPQ                 attention_1                     




.. GENERATED FROM PYTHON SOURCE LINES 291-292

See :ref:`l-long-outputs-llama-diff-export` for a better view.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 43.386 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_export_301.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_export_301.ipynb <plot_llama_diff_export_301.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_export_301.py <plot_llama_diff_export_301.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
