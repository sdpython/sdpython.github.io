
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_torch_custom_backend.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_torch_custom_backend.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_torch_custom_backend.py:


.. _l-plot-custom-backend:

==========================
A custom backend for torch
==========================

This example leverages the examples introduced on this page
`Custom Backends <https://pytorch.org/docs/stable/torch.compiler_custom_backends.html>`_.
It uses backend :func:`experimental_experiment.torch_dynamo.onnx_custom_backend`
based on :epkg:`onnxruntime` and running on CPU or CUDA.
It could easily replaced by 
:func:`experimental_experiment.torch_dynamo.onnx_debug_backend`.
This one based on the reference implemented from onnx
can show the intermediate results if needed. It is very slow.

A model
=======

.. GENERATED FROM PYTHON SOURCE LINES 20-50

.. code-block:: Python


    import copy
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    from onnx_array_api.plotting.graphviz_helper import plot_dot
    import torch
    from torch._dynamo.backends.common import aot_autograd
    from experimental_experiment.torch_dynamo import (
        onnx_custom_backend,
        get_decomposition_table,
    )


    class MLP(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = torch.nn.Sequential(
                torch.nn.Linear(10, 32),
                torch.nn.Sigmoid(),
                torch.nn.Linear(32, 1),
            )

        def forward(self, x):
            return self.layers(x)


    x = torch.randn(3, 10, dtype=torch.float32)

    mlp = MLP()
    print(mlp(x))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[0.1157],
            [0.0475],
            [0.0669]], grad_fn=<AddmmBackward0>)




.. GENERATED FROM PYTHON SOURCE LINES 51-59

A custom backend
================

This backend leverages :epkg:`onnxruntime`.
It is available through function
:func:`experimental_experiment.torch_dynamo.onnx_custom_backend`
and implemented by class :class:`OrtBackend
<experimental_experiment.torch_dynamo.fast_backend.OrtBackend>`.

.. GENERATED FROM PYTHON SOURCE LINES 59-71

.. code-block:: Python


    compiled_model = torch.compile(
        copy.deepcopy(mlp),
        backend=lambda *args, **kwargs: onnx_custom_backend(
            *args, target_opset=18, **kwargs
        ),
        dynamic=False,
        fullgraph=True,
    )

    print(compiled_model(x))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[0.1157],
            [0.0475],
            [0.0669]])




.. GENERATED FROM PYTHON SOURCE LINES 72-84

Training
========

It can be used for training as well. The compilation may not
be working if the model is using function the converter does not know.
Maybe, there exist a way to decompose this new function into
existing functions. A recommended list is returned by
with function :func:`get_decomposition_table
<experimental_experiment.torch_dynamo.get_decomposition_table>`.
An existing list can be filtered out from some inefficient decompositions
with function :func:`filter_decomposition_table
<experimental_experiment.torch_dynamo.filter_decomposition_table>`.

.. GENERATED FROM PYTHON SOURCE LINES 84-102

.. code-block:: Python



    aot_compiler = aot_autograd(
        fw_compiler=lambda *args, **kwargs: onnx_custom_backend(
            *args, target_opset=18, **kwargs
        ),
        decompositions=get_decomposition_table(),
    )

    compiled_model = torch.compile(
        copy.deepcopy(mlp),
        backend=aot_compiler,
        fullgraph=True,
        dynamic=False,
    )

    print(compiled_model(x))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[0.1157],
            [0.0475],
            [0.0669]], grad_fn=<CompiledFunctionBackward>)




.. GENERATED FROM PYTHON SOURCE LINES 103-104

Let's see an iteration loop.

.. GENERATED FROM PYTHON SOURCE LINES 104-168

.. code-block:: Python


    from sklearn.datasets import load_diabetes


    class DiabetesDataset(torch.utils.data.Dataset):
        def __init__(self, X, y):
            self.X = torch.from_numpy(X / 10).to(torch.float32)
            self.y = torch.from_numpy(y).to(torch.float32).reshape((-1, 1))

        def __len__(self):
            return len(self.X)

        def __getitem__(self, i):
            return self.X[i], self.y[i]


    def trained_model(max_iter=5, dynamic=False, storage=None):
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_custom_backend(
                *args, target_opset=18, storage=storage, **kwargs
            ),
            decompositions=get_decomposition_table(),
        )

        compiled_model = torch.compile(
            MLP(),
            backend=aot_compiler,
            fullgraph=True,
            dynamic=dynamic,
        )

        trainloader = torch.utils.data.DataLoader(
            DiabetesDataset(*load_diabetes(return_X_y=True)),
            batch_size=5,
            shuffle=True,
            num_workers=1,
        )

        loss_function = torch.nn.L1Loss()
        optimizer = torch.optim.Adam(compiled_model.parameters(), lr=1e-1)

        for epoch in range(0, max_iter):
            current_loss = 0.0

            for i, data in enumerate(trainloader, 0):
                X, y = data

                optimizer.zero_grad()
                p = compiled_model(X)
                loss = loss_function(p, y)
                loss.backward()

                optimizer.step()

                current_loss += loss.item()

            print(f"Loss after epoch {epoch+1}: {current_loss}")

        print("Training process has finished.")
        return compiled_model


    trained_model(3)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:107: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:107: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    Loss after epoch 1: 7459.29697561264
    Loss after epoch 2: 5841.251264572144
    Loss after epoch 3: 5550.437879562378
    Training process has finished.

    OptimizedModule(
      (_orig_mod): MLP(
        (layers): Sequential(
          (0): Linear(in_features=10, out_features=32, bias=True)
          (1): Sigmoid()
          (2): Linear(in_features=32, out_features=1, bias=True)
        )
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 169-174

What about the ONNX model?
==========================

The backend converts the model into ONNX then runs it with :epkg:`onnxruntime`.
Let's see what it looks like.

.. GENERATED FROM PYTHON SOURCE LINES 174-187

.. code-block:: Python


    storage = {}

    trained_model(3, storage=storage)

    print(f"{len(storage['instance'])} were created.")

    for i, inst in enumerate(storage["instance"][:2]):
        print()
        print(f"-- model {i} running on {inst['providers']}")
        print(onnx_simple_text_plot(inst["onnx"]))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:107: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:107: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    Loss after epoch 1: 7361.110177993774
    Loss after epoch 2: 5760.222093582153
    Loss after epoch 3: 5738.282759666443
    Training process has finished.
    4 were created.

    -- model 0 running on ['CPUExecutionProvider']
    opset: domain='' version=18
    input: name='input0' type=dtype('float32') shape=[32, 10]
    input: name='input1' type=dtype('float32') shape=[32]
    input: name='input2' type=dtype('float32') shape=[1, 32]
    input: name='input3' type=dtype('float32') shape=[1]
    input: name='input4' type=dtype('float32') shape=[5, 10]
    Gemm(input4, input0, input1, transA=0, transB=1, alpha=1.00, beta=1.00) -> addmm
      Sigmoid(addmm) -> output_2
        Gemm(output_2, input2, input3, transA=0, transB=1, alpha=1.00, beta=1.00) -> output_0
    Transpose(input2, perm=[1,0]) -> output_3
    Identity(input4) -> output_1
    output: name='output_0' type=dtype('float32') shape=[5, 1]
    output: name='output_1' type=dtype('float32') shape=[5, 10]
    output: name='output_2' type=dtype('float32') shape=[5, 32]
    output: name='output_3' type=dtype('float32') shape=[32, 1]

    -- model 1 running on ['CPUExecutionProvider']
    opset: domain='' version=18
    input: name='input0' type=dtype('float32') shape=[5, 10]
    input: name='input1' type=dtype('float32') shape=[5, 32]
    input: name='input2' type=dtype('float32') shape=[32, 1]
    input: name='input3' type=dtype('float32') shape=[5, 1]
    init: name='init1_s1_' type=dtype('float32') shape=(1,) -- array([1.], dtype=float32)
    init: name='init7_s1_0' type=dtype('int64') shape=(1,) -- array([0])
    init: name='init7_s1_1' type=dtype('int64') shape=(1,) -- array([1])
    init: name='init7_s1_32' type=dtype('int64') shape=(1,) -- array([32])
    Constant(value_float=0) -> output_NONE_4
    Gemm(input3, input2, transA=0, transB=1) -> mm
    Gemm(input3, input1, transA=1, transB=0) -> output_2
    ReduceSum(input3, init7_s1_0, keepdims=1) -> sum_1
      Reshape(sum_1, init7_s1_1) -> output_3
    Sub(init1_s1_, input1) -> _onx_sub0
      Mul(input1, _onx_sub0) -> _onx_mul0
      Mul(mm, _onx_mul0) -> sigmoid_backward
        Gemm(sigmoid_backward, input0, transA=1, transB=0) -> output_0
    ReduceSum(sigmoid_backward, init7_s1_0, keepdims=1) -> sum_2
      Reshape(sum_2, init7_s1_32) -> output_1
    output: name='output_0' type=dtype('float32') shape=[32, 10]
    output: name='output_1' type=dtype('float32') shape=[32]
    output: name='output_2' type=dtype('float32') shape=[1, 32]
    output: name='output_3' type=dtype('float32') shape=[1]
    output: name='output_NONE_4' type=dtype('float32') shape=None




.. GENERATED FROM PYTHON SOURCE LINES 188-189

The forward graph.

.. GENERATED FROM PYTHON SOURCE LINES 189-193

.. code-block:: Python


    plot_dot(storage["instance"][0]["onnx"])





.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_custom_backend_001.png
   :alt: plot torch custom backend
   :srcset: /auto_examples/images/sphx_glr_plot_torch_custom_backend_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <Axes: >



.. GENERATED FROM PYTHON SOURCE LINES 194-195

The brackward graph.

.. GENERATED FROM PYTHON SOURCE LINES 195-199

.. code-block:: Python


    plot_dot(storage["instance"][1]["onnx"])





.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_custom_backend_002.png
   :alt: plot torch custom backend
   :srcset: /auto_examples/images/sphx_glr_plot_torch_custom_backend_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <Axes: >



.. GENERATED FROM PYTHON SOURCE LINES 200-206

What about dynamic shape?
=========================

Any input or output having `_dim_` in its name is a dynamic dimension.
Any output having `_NONE_` in its name is replace by None.
It is needed by pytorch.

.. GENERATED FROM PYTHON SOURCE LINES 206-219

.. code-block:: Python


    storage = {}

    trained_model(3, storage=storage, dynamic=True)

    print(f"{len(storage['instance'])} were created.")

    for i, inst in enumerate(storage["instance"]):
        print()
        print(f"-- model {i} running on {inst['providers']}")
        print()
        print(onnx_simple_text_plot(inst["onnx"]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:107: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    /home/xadupre/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:107: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.
      warnings.warn(
    Loss after epoch 1: 7359.001808166504
    Loss after epoch 2: 5790.268497467041
    Loss after epoch 3: 5699.565372467041
    Training process has finished.
    4 were created.

    -- model 0 running on ['CPUExecutionProvider']

    opset: domain='' version=18
    input: name='input0' type=dtype('float32') shape=[32, 10]
    input: name='input1' type=dtype('float32') shape=[32]
    input: name='input2' type=dtype('float32') shape=[1, 32]
    input: name='input3' type=dtype('float32') shape=[1]
    input: name='input_dim_4' type=dtype('int64') shape=[1]
    input: name='input5' type=dtype('float32') shape=[5, 10]
    Gemm(input5, input0, input1, transA=0, transB=1, alpha=1.00, beta=1.00) -> addmm
      Sigmoid(addmm) -> output_2
        Gemm(output_2, input2, input3, transA=0, transB=1, alpha=1.00, beta=1.00) -> output_0
    Transpose(input2, perm=[1,0]) -> output_3
    Identity(input5) -> output_1
    Identity(input_dim_4) -> output_dim_4
    output: name='output_0' type=dtype('float32') shape=[5, 1]
    output: name='output_1' type=dtype('float32') shape=[5, 10]
    output: name='output_2' type=dtype('float32') shape=[5, 32]
    output: name='output_3' type=dtype('float32') shape=[32, 1]
    output: name='output_dim_4' type=dtype('int64') shape=[1]

    -- model 1 running on ['CPUExecutionProvider']

    opset: domain='' version=18
    input: name='input_dim_0' type=dtype('int64') shape=[1]
    input: name='input1' type=dtype('float32') shape=[5, 10]
    input: name='input2' type=dtype('float32') shape=[5, 32]
    input: name='input3' type=dtype('float32') shape=[32, 1]
    input: name='input4' type=dtype('float32') shape=[5, 1]
    init: name='init1_s1_' type=dtype('float32') shape=(1,) -- array([1.], dtype=float32)
    init: name='init7_s1_0' type=dtype('int64') shape=(1,) -- array([0])
    init: name='init7_s1_1' type=dtype('int64') shape=(1,) -- array([1])
    init: name='init7_s1_32' type=dtype('int64') shape=(1,) -- array([32])
    Constant(value_float=0) -> output_NONE_4
    Gemm(input4, input3, transA=0, transB=1) -> mm
    Gemm(input4, input2, transA=1, transB=0) -> output_2
    ReduceSum(input4, init7_s1_0, keepdims=1) -> sum_1
      Reshape(sum_1, init7_s1_1) -> output_3
    Sub(init1_s1_, input2) -> _onx_sub0
      Mul(input2, _onx_sub0) -> _onx_mul0
      Mul(mm, _onx_mul0) -> sigmoid_backward
        Gemm(sigmoid_backward, input1, transA=1, transB=0) -> output_0
    ReduceSum(sigmoid_backward, init7_s1_0, keepdims=1) -> sum_2
      Reshape(sum_2, init7_s1_32) -> output_1
    Constant(value_float=0) -> output_NONE_5
    output: name='output_0' type=dtype('float32') shape=[32, 10]
    output: name='output_1' type=dtype('float32') shape=[32]
    output: name='output_2' type=dtype('float32') shape=[1, 32]
    output: name='output_3' type=dtype('float32') shape=[1]
    output: name='output_NONE_4' type=dtype('float32') shape=None
    output: name='output_NONE_5' type=dtype('float32') shape=None

    -- model 2 running on ['CPUExecutionProvider']

    opset: domain='' version=18
    input: name='input0' type=dtype('float32') shape=[32, 10]
    input: name='input1' type=dtype('float32') shape=[32]
    input: name='input2' type=dtype('float32') shape=[1, 32]
    input: name='input3' type=dtype('float32') shape=[1]
    input: name='input_dim_4' type=dtype('int64') shape=[1]
    input: name='input5' type=dtype('float32') shape=[2, 10]
    Gemm(input5, input0, input1, transA=0, transB=1, alpha=1.00, beta=1.00) -> addmm
      Sigmoid(addmm) -> output_2
        Gemm(output_2, input2, input3, transA=0, transB=1, alpha=1.00, beta=1.00) -> output_0
    Transpose(input2, perm=[1,0]) -> output_3
    Identity(input5) -> output_1
    Identity(input_dim_4) -> output_dim_4
    output: name='output_0' type=dtype('float32') shape=[2, 1]
    output: name='output_1' type=dtype('float32') shape=[2, 10]
    output: name='output_2' type=dtype('float32') shape=[2, 32]
    output: name='output_3' type=dtype('float32') shape=[32, 1]
    output: name='output_dim_4' type=dtype('int64') shape=[1]

    -- model 3 running on ['CPUExecutionProvider']

    opset: domain='' version=18
    input: name='input_dim_0' type=dtype('int64') shape=[1]
    input: name='input1' type=dtype('float32') shape=[2, 10]
    input: name='input2' type=dtype('float32') shape=[2, 32]
    input: name='input3' type=dtype('float32') shape=[32, 1]
    input: name='input4' type=dtype('float32') shape=[2, 1]
    init: name='init1_s1_' type=dtype('float32') shape=(1,) -- array([1.], dtype=float32)
    init: name='init7_s1_0' type=dtype('int64') shape=(1,) -- array([0])
    init: name='init7_s1_1' type=dtype('int64') shape=(1,) -- array([1])
    init: name='init7_s1_32' type=dtype('int64') shape=(1,) -- array([32])
    Constant(value_float=0) -> output_NONE_4
    Gemm(input4, input3, transA=0, transB=1) -> mm
    Gemm(input4, input2, transA=1, transB=0) -> output_2
    ReduceSum(input4, init7_s1_0, keepdims=1) -> sum_1
      Reshape(sum_1, init7_s1_1) -> output_3
    Sub(init1_s1_, input2) -> _onx_sub0
      Mul(input2, _onx_sub0) -> _onx_mul0
      Mul(mm, _onx_mul0) -> sigmoid_backward
        Gemm(sigmoid_backward, input1, transA=1, transB=0) -> output_0
    ReduceSum(sigmoid_backward, init7_s1_0, keepdims=1) -> sum_2
      Reshape(sum_2, init7_s1_32) -> output_1
    Constant(value_float=0) -> output_NONE_5
    output: name='output_0' type=dtype('float32') shape=[32, 10]
    output: name='output_1' type=dtype('float32') shape=[32]
    output: name='output_2' type=dtype('float32') shape=[1, 32]
    output: name='output_3' type=dtype('float32') shape=[1]
    output: name='output_NONE_4' type=dtype('float32') shape=None
    output: name='output_NONE_5' type=dtype('float32') shape=None




.. GENERATED FROM PYTHON SOURCE LINES 220-221

The forward graph.

.. GENERATED FROM PYTHON SOURCE LINES 221-225

.. code-block:: Python


    plot_dot(storage["instance"][0]["onnx"])





.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_custom_backend_003.png
   :alt: plot torch custom backend
   :srcset: /auto_examples/images/sphx_glr_plot_torch_custom_backend_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <Axes: >



.. GENERATED FROM PYTHON SOURCE LINES 226-227

The brackward graph.

.. GENERATED FROM PYTHON SOURCE LINES 227-231

.. code-block:: Python


    plot_dot(storage["instance"][1]["onnx"])





.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_custom_backend_004.png
   :alt: plot torch custom backend
   :srcset: /auto_examples/images/sphx_glr_plot_torch_custom_backend_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <Axes: >



.. GENERATED FROM PYTHON SOURCE LINES 232-240

Pattern Optimizations
=====================

By default, once exported into onnx, a model is optimized by
looking for patterns. Each of them locally replaces a couple of
nodes to optimize the computation
(see :ref:`l-pattern-optimization-onnx` and
# :ref:`l-pattern-optimization-ort`).


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 9.491 seconds)


.. _sphx_glr_download_auto_examples_plot_torch_custom_backend.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_torch_custom_backend.ipynb <plot_torch_custom_backend.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_torch_custom_backend.py <plot_torch_custom_backend.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
