
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_bench.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_bench.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_bench.py:


.. _l-plot-llama-bench:

Measure LLAMA speed
===================

The script is calling many times the script ``experimental_experiment.torch_bench.dort_bench.py``.

::

    python _doc/examples/plot_llama_bench.py --help
    
For exemple, to check mixed precision on multiple backend:

::

    python _doc/examples/plot_llama_bench.py --device=cuda --num_hidden_layers=1 --mixed=1


Run the following command to run one experiment and get the available options:

::

    python -m experimental_experiment.torch_bench.dort_bench --help

.. GENERATED FROM PYTHON SOURCE LINES 27-165

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args, check_cuda_availability

    parsed_args = get_parsed_args(
        "plot_llama_bench",
        description=__doc__,
        warmup=3,
        repeat=5,
        backend=("eager,inductor,ort,custom", "backend to test"),
        device=("cuda" if check_cuda_availability() else "cpu", "device to test"),
        num_hidden_layers=("2", "hidden layers to test"),
        mixed=("0", "boolean value to test (mixed precision or not)"),
        dynamic=("0", "boolean value to test dynamic shapes or not"),
        script_name=("experimental_experiment.torch_bench.dort_bench", "script to run"),
        dump=(0, "dump the models with env ONNXRT_DUMP_PATH"),
        check=(0, "just check the script is working, ignores all other parameters"),
        config=("medium", "configuration to use, default or medium"),
        patterns=("none,default,onnxruntime", "optimization patterns to use"),
        expose="backend,device,num_hidden_layers,mixed,scipt_name,repeat,"
        "warmup,dump,check,config,patterns,dynamic",
    )

    import onnxruntime  # noqa: F401
    import numpy as np
    import pandas
    import matplotlib.pyplot as plt
    import itertools
    import torch
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.bench_run import run_benchmark, get_machine, BenchmarkError

    script_name = "experimental_experiment.torch_bench.dort_bench"
    machine = {} if unit_test_going() else get_machine()


    repeat = parsed_args.repeat
    warmup = parsed_args.warmup


    def make_config(
        backend,
        device,
        num_hidden_layers,
        repeat,
        mixed,
        dynamic,
        config,
        warmup,
        pattern,
        existing=None,
    ):
        cf = dict(
            backend=backend,
            device=device,
            num_hidden_layers=num_hidden_layers,
            repeat=repeat,
            mixed=mixed,
            dynamic=dynamic,
            config=config,
            warmup=warmup,
        )

        if existing and backend != "custom":
            for ex in existing:
                if not ex:
                    continue
                equal = True
                for k in cf:
                    if cf[k] != ex[k]:
                        equal = False
                        break
                if equal:
                    return None

        if pattern == "none":
            opt = dict(disable_pattern="default")
        elif pattern == "default":
            opt = dict(enable_pattern="default")
        elif pattern == "onnxruntime":
            opt = dict(enable_pattern="onnxruntime")
        else:
            raise AssertionError(f"unexpected value for pattern={pattern!r}")
        cf.update(opt)
        return cf


    if parsed_args.check not in (1, "1"):
        verbose = 1
        configs = []
        for (
            backend,
            device,
            num_hidden_layers,
            mixed,
            dynamic,
            pattern,
        ) in itertools.product(
            parsed_args.backend.split(","),
            parsed_args.device.split(","),
            list(map(int, parsed_args.num_hidden_layers.split(","))),
            list(map(int, parsed_args.mixed.split(","))),
            list(map(int, parsed_args.dynamic.split(","))),
            parsed_args.patterns.split(","),
        ):
            if mixed == 1 and device == "cpu":
                continue
            if machine.get("capability", (0, 0)) < (7, 0) and backend == "inductor":
                continue
            configs.append(
                make_config(
                    backend=backend,
                    device=device,
                    num_hidden_layers=num_hidden_layers,
                    repeat=repeat,
                    mixed=mixed,
                    dynamic=dynamic,
                    config=parsed_args.config,
                    warmup=warmup,
                    pattern=pattern,
                    existing=configs,
                )
            )
    else:
        verbose = 5
        device = "cuda" if torch.cuda.is_available() else "cpu"
        configs = [
            dict(
                backend="ort",
                device=device,
                num_hidden_layers=1,
                repeat=1,
                mixed=0,
                dynamic=0,
                warmup=1,
                config="small",
            ),
        ]








.. GENERATED FROM PYTHON SOURCE LINES 166-167

All configurations to consider.

.. GENERATED FROM PYTHON SOURCE LINES 167-172

.. code-block:: Python


    configs = [cf for cf in configs if cf]
    for i, cf in enumerate(configs):
        print(f"config {i+1}: {cf}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    config 1: {'backend': 'eager', 'device': 'cuda', 'num_hidden_layers': 2, 'repeat': 5, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 3, 'disable_pattern': 'default'}
    config 2: {'backend': 'ort', 'device': 'cuda', 'num_hidden_layers': 2, 'repeat': 5, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 3, 'disable_pattern': 'default'}
    config 3: {'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 2, 'repeat': 5, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 3, 'disable_pattern': 'default'}
    config 4: {'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 2, 'repeat': 5, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 3, 'enable_pattern': 'default'}
    config 5: {'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 2, 'repeat': 5, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 3, 'enable_pattern': 'onnxruntime'}




.. GENERATED FROM PYTHON SOURCE LINES 173-174

Running configuration.

.. GENERATED FROM PYTHON SOURCE LINES 174-189

.. code-block:: Python



    try:
        data = run_benchmark(
            parsed_args.script_name,
            configs,
            verbose=verbose,
            stop_if_exception=False,
            dump=parsed_args.dump in ("1", 1),
        )
        data_collected = True
    except BenchmarkError as e:
        print(e)
        data_collected = False





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|██        | 1/5 [00:07<00:31,  7.96s/it]     40%|████      | 2/5 [00:31<00:50, 16.95s/it]     60%|██████    | 3/5 [00:40<00:26, 13.43s/it]     80%|████████  | 4/5 [00:49<00:11, 11.55s/it]    100%|██████████| 5/5 [01:09<00:00, 14.63s/it]    100%|██████████| 5/5 [01:09<00:00, 13.84s/it]




.. GENERATED FROM PYTHON SOURCE LINES 190-191

Let's process the data.

.. GENERATED FROM PYTHON SOURCE LINES 191-223

.. code-block:: Python


    if data_collected:

        def make_legend(row):
            row = row.to_dict()
            val = [row["device"], row["backend"], f"h{row['num_hidden_layers']}"]
            if row["mixed"]:
                val.append("mixed")
            if row["dynamic"]:
                val.append("dyn")
            if "patterns" in row and row["patterns"] and "nan" not in str(row["patterns"]):
                val.append(f"({row['patterns']})")
            s = "-".join(map(str, val))
            assert "nan" not in s, f"Legend {s!r} is wrong, row={row}"
            return s

        df = pandas.DataFrame(data)
        df = df.drop(["OUTPUT", "ERROR"], axis=1)
        df["legend"] = df.apply(make_legend, axis=1)
        df["time"] = df["time"].astype(float)
        min_eager = df[df.legend.str.contains("eager")]["time"].dropna().min()
        df["increase"] = df["time"] / min_eager - 1
        # df["ERROR"] = df["ERROR"].apply(lambda s: s.replace("\n", " "))
        filename = "plot_llama_bench_with_cmd.csv"
        df.to_csv(filename, index=False)

        df = df.drop(["CMD"], axis=1)
        filename = "plot_llama_bench.csv"
        df.to_csv(filename, index=False)
        df = pandas.read_csv(filename)  # to cast type
        print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                      llama  config  mixed  dynamic backend  repeat  ...  num_hidden_layers disable_pattern          patterns  enable_pattern                             legend  increase
    0  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0   eager       5  ...                  2         default               NaN             NaN                      cuda-eager-h2  0.000000
    1  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0     ort       5  ...                  2         default               NaN             NaN                        cuda-ort-h2  6.060062
    2  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0  custom       5  ...                  2         default  +default-default             NaN  cuda-custom-h2-(+default-default)  0.053286
    3  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0  custom       5  ...                  2             NaN         +default-         default         cuda-custom-h2-(+default-)  0.061584
    4  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0  custom       5  ...                  2             NaN     +onnxruntime-     onnxruntime     cuda-custom-h2-(+onnxruntime-)  0.089563

    [5 rows x 18 columns]




.. GENERATED FROM PYTHON SOURCE LINES 224-225

First lines.

.. GENERATED FROM PYTHON SOURCE LINES 225-228

.. code-block:: Python


    print(df.head(2).T)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                                          0                                     1
    llama              2x1024-1024-2-1024-1024-1024-2-eager  2x1024-1024-2-1024-1024-1024-2-eager
    config                                           medium                                medium
    mixed                                                 0                                     0
    dynamic                                               0                                     0
    backend                                           eager                                   ort
    repeat                                                5                                     5
    warmup                                                3                                     3
    torch                           2.3.0.dev20240222+cu118               2.3.0.dev20240222+cu118
    transformers                                     4.37.2                                4.37.2
    warmup_time                                    0.677067                              8.133532
    time                                            0.11233                              0.793056
    device                                             cuda                                  cuda
    num_hidden_layers                                     2                                     2
    disable_pattern                                 default                               default
    patterns                                            NaN                                   NaN
    enable_pattern                                      NaN                                   NaN
    legend                                    cuda-eager-h2                           cuda-ort-h2
    increase                                            0.0                              6.060062




.. GENERATED FROM PYTHON SOURCE LINES 229-230

More simple

.. GENERATED FROM PYTHON SOURCE LINES 230-235

.. code-block:: Python


    for c in ["time", "warmup_time"]:
        if c not in df.columns:
            df[c] = np.nan








.. GENERATED FROM PYTHON SOURCE LINES 236-237

Simplified data

.. GENERATED FROM PYTHON SOURCE LINES 237-240

.. code-block:: Python


    print(df.sort_values("legend"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                      llama  config  mixed  dynamic backend  repeat  ...  num_hidden_layers disable_pattern          patterns  enable_pattern                             legend  increase
    3  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0  custom       5  ...                  2             NaN         +default-         default         cuda-custom-h2-(+default-)  0.061584
    2  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0  custom       5  ...                  2         default  +default-default             NaN  cuda-custom-h2-(+default-default)  0.053286
    4  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0  custom       5  ...                  2             NaN     +onnxruntime-     onnxruntime     cuda-custom-h2-(+onnxruntime-)  0.089563
    0  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0   eager       5  ...                  2         default               NaN             NaN                      cuda-eager-h2  0.000000
    1  2x1024-1024-2-1024-1024-1024-2-eager  medium      0        0     ort       5  ...                  2         default               NaN             NaN                        cuda-ort-h2  6.060062

    [5 rows x 18 columns]




.. GENERATED FROM PYTHON SOURCE LINES 241-242

Plot warmup time.

.. GENERATED FROM PYTHON SOURCE LINES 242-260

.. code-block:: Python


    torch_version = list(set(df["torch"].dropna()))
    transformers_version = list(set(df["transformers"].dropna()))
    ver = f"{torch_version[0]} - {transformers_version[0]}"
    llama = list(set(df["llama"].dropna()))[0]

    if data_collected:
        fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))

        df = df.sort_values("time").set_index("legend")
        df[["warmup_time"]].plot.barh(
            ax=ax, title=f"lower better\n{llama}\nwarmup time\n{ver}"
        )
        ax.grid(True)

        fig.tight_layout()
        fig.savefig("plot_llama_bench_warmup_time.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_llama_bench_001.png
   :alt: lower better 2x1024-1024-2-1024-1024-1024-2-eager warmup time 2.3.0.dev20240222+cu118 - 4.37.2
   :srcset: /auto_examples/images/sphx_glr_plot_llama_bench_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 261-262

Plot time.

.. GENERATED FROM PYTHON SOURCE LINES 262-275

.. code-block:: Python


    if data_collected:
        fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))

        df[["time"]].plot.barh(ax=ax, title=f"lower better\n{llama}\niteration time\n{ver}")
        mi, ma = df["time"].min(), df["time"].max()
        mi = mi - (ma - mi) / 10
        ax.set_xlim(left=mi)
        ax.grid(True)

        fig.tight_layout()
        fig.savefig("plot_llama_bench_time.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_llama_bench_002.png
   :alt: lower better 2x1024-1024-2-1024-1024-1024-2-eager iteration time 2.3.0.dev20240222+cu118 - 4.37.2
   :srcset: /auto_examples/images/sphx_glr_plot_llama_bench_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 276-277

Plot increase.

.. GENERATED FROM PYTHON SOURCE LINES 277-288

.. code-block:: Python


    if data_collected:
        fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))

        df[["increase"]].plot.barh(
            ax=ax, title=f"lower better\n{llama}\ncomparison to eager %"
        )
        ax.grid(True)

        fig.tight_layout()
        fig.savefig("plot_llama_bench_relative.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_llama_bench_003.png
   :alt: lower better 2x1024-1024-2-1024-1024-1024-2-eager comparison to eager %
   :srcset: /auto_examples/images/sphx_glr_plot_llama_bench_003.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 11.645 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_bench.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_bench.ipynb <plot_llama_bench.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_bench.py <plot_llama_bench.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
