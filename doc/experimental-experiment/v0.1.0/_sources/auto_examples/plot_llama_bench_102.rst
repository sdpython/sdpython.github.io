
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_bench_102.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_bench_102.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_bench_102.py:


.. _l-plot-llama-bench-102:

102: Measure LLAMA speed
========================

The script is calling many times the script
``experimental_experiment.torch_bench.dort_bench.py``.

::

    python _doc/examples/plot_llama_bench_102.py --help

For exemple, to check mixed precision on multiple backend:

::

    python _doc/examples/plot_llama_bench_102.py \
           --device=cuda --num_hidden_layers=2 --mixed=1

::

    python _doc/examples/plot_llama_bench_102.py --device=cuda --num_hidden_layers=2 \
           --mixed=1 --backend=eager,dynger,ortmodule,inductor,ort+,custom --config=large

With 32Gb GPU memory, the script runs with 6 layers.

::

    python _doc/examples/plot_llama_bench_102.py --device=cuda \
           --num_hidden_layers=6 --mixed=1 \
           --backend=eager,dynger,ortmodule,inductor,trt,ort+,custom --config=large

    python _doc/examples/plot_llama_bench_102.py --device=cuda \
           --num_hidden_layers=2 --mixed=1 \
           --backend=eager,ort+,custom --config=large

Run the following command to run one experiment and get the available options:

::

    python -m experimental_experiment.torch_bench.dort_bench --help

.. GENERATED FROM PYTHON SOURCE LINES 45-261

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args, check_cuda_availability

    parsed_args = get_parsed_args(
        "plot_llama_bench",
        description=__doc__,
        warmup=5,
        repeat=10,
        model=("llama", "model to benchmark"),
        backend=(
            "eager,dynger,inductor,ort,ort+,custom,ortmodule",
            "backend to test, among eager,dynger,inductor,"
            "ort,ort+,custom,plug,ortmodule,backort",
        ),
        device=("cuda" if check_cuda_availability() else "cpu", "device to test"),
        num_hidden_layers=("1", "hidden layers to test"),
        mixed=("0", "boolean value to test (mixed precision or not)"),
        dynamic=("0", "boolean value to test dynamic shapes or not"),
        script_name=("experimental_experiment.torch_bench.dort_bench", "script to run"),
        dump=(0, "dump the models with env ONNXRT_DUMP_PATH"),
        check=(0, "just check the script is working, ignores all other parameters"),
        config=("medium", "configuration to use, default or medium"),
        patterns=(
            "none,default,default+onnxruntime,default+onnxruntime+experimental",
            "optimization patterns to use",
        ),
        implementation=("eager", "eager or sdpa or both values comma separated value"),
        with_mask=(1, "with or without a second input (mask"),
        disable_pattern=("none", "pattern or patterns to disable"),
        ort_optimize=(
            "0,1",
            "enable or disable onnxruntime optimization, by default, tries both",
        ),
        order=("none", "optimization order see class OrderAlgorithm, none by default"),
        shape_scenario=(
            "",
            "shapes to use, 2x1024 by default, 'batch' to get "
            "shapes with different batch dimensions, 'length' to get "
            "different length sizes",
        ),
        verbose=(1, "verbosity"),
        expose="backend,device,num_hidden_layers,mixed,scipt_name,repeat,"
        "warmup,dump,check,config,patterns,dynamic,disable_pattern,model"
        "implementation,with_mask,ort_optimize,verbose,order,shape_scenario",
    )

    import onnxruntime  # noqa: F401
    import numpy as np
    import pandas
    import matplotlib.pyplot as plt
    import itertools
    import torch
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.bench_run import run_benchmark, get_machine, BenchmarkError

    script_name = "experimental_experiment.torch_bench.dort_bench"
    machine = {} if unit_test_going() else get_machine(False)


    repeat = parsed_args.repeat
    warmup = parsed_args.warmup


    def make_config(
        model,
        backend,
        device,
        num_hidden_layers,
        repeat,
        mixed,
        dynamic,
        config,
        warmup,
        pattern,
        disable_pattern,
        implementation,
        with_mask,
        ort_optimize,
        order,
        shape_scenario,
        verbose,
        existing=None,
    ):
        if backend not in ("custom", "ort+"):
            ort_optimize = None
            pattern = None
            disable_pattern = None
        cf = dict(
            model=model,
            backend=backend,
            device=device,
            num_hidden_layers=num_hidden_layers,
            repeat=repeat,
            mixed=mixed,
            dynamic=dynamic,
            config=config,
            warmup=warmup,
            implementation=implementation,
            with_mask=with_mask,
            ort_optimize=ort_optimize,
            order=order,
            shape_scenario=shape_scenario,
            verbose=verbose,
        )
        cf = {k: v for k, v in cf.items() if v is not None}

        if existing and backend not in ("custom", "ort+"):
            for ex in existing:
                if not ex:
                    continue
                equal = True
                for k in cf:
                    if cf[k] != ex[k]:
                        equal = False
                        break
                if equal:
                    return None

        if pattern is None:
            opt = {}
        elif pattern == "none":
            opt = dict(enable_pattern="default", disable_pattern="default")
        elif pattern in "default" or "+" in pattern or "-" in pattern:
            opt = dict(enable_pattern=pattern)
        else:
            raise AssertionError(f"unexpected value for pattern={pattern!r}")
        cf.update(opt)
        if disable_pattern not in ("none", None):
            if "disable_pattern" in cf:
                cf["disable_pattern"] += f",{disable_pattern}"
            else:
                cf["disable_pattern"] = disable_pattern
        if "enable_pattern" in cf and "+experimental" in cf["enable_pattern"]:
            try:
                import onnx_extended  # noqa: F401
            except ImportError:
                return None
        elif not ort_optimize and backend in ("custom", "ort+"):
            return None
        assert (
            cf["backend"] != "eager" or cf.get("ort_optimize", None) is None
        ), f"Wrong configuration {cf}"
        return cf


    if parsed_args.check not in (1, "1") and not unit_test_going():

        def _split(s):
            if isinstance(s, int):
                return [s]
            return [int(i) for i in s.split(",")]

        verbose = parsed_args.verbose
        configs = []
        for (
            backend,
            device,
            num_hidden_layers,
            mixed,
            dynamic,
            pattern,
            impl,
            ort_optimize,
        ) in itertools.product(
            parsed_args.backend.split(","),
            parsed_args.device.split(","),
            _split(parsed_args.num_hidden_layers),
            _split(parsed_args.mixed),
            _split(parsed_args.dynamic),
            parsed_args.patterns.split(","),
            parsed_args.implementation.split(","),
            _split(parsed_args.ort_optimize),
        ):
            if mixed == 1 and device == "cpu":
                continue
            if machine.get("capability", (0, 0)) < (7, 0) and backend == "inductor":
                continue
            configs.append(
                make_config(
                    model=parsed_args.model,
                    backend=backend,
                    device=device,
                    num_hidden_layers=num_hidden_layers,
                    repeat=repeat,
                    mixed=mixed,
                    dynamic=dynamic,
                    config=parsed_args.config,
                    warmup=warmup,
                    pattern=pattern,
                    disable_pattern=parsed_args.disable_pattern,
                    existing=configs,
                    implementation=impl,
                    with_mask=parsed_args.with_mask,
                    ort_optimize=ort_optimize,
                    order=parsed_args.order,
                    shape_scenario=parsed_args.shape_scenario,
                    verbose=verbose,
                )
            )
    else:
        verbose = 5
        device = "cuda" if torch.cuda.is_available() else "cpu"
        configs = [
            dict(
                model=parsed_args.model,
                backend="custom",
                device=device,
                num_hidden_layers=1,
                repeat=1,
                mixed=0,
                dynamic=0,
                warmup=1,
                config="small",
            ),
        ]








.. GENERATED FROM PYTHON SOURCE LINES 262-263

All configurations to consider.

.. GENERATED FROM PYTHON SOURCE LINES 263-269

.. code-block:: Python


    configs = [cf for cf in configs if cf]
    if verbose:
        for i, cf in enumerate(configs):
            print(f"config {i+1}: {cf}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    config 1: {'model': 'llama', 'backend': 'eager', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1}
    config 2: {'model': 'llama', 'backend': 'dynger', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1}
    config 3: {'model': 'llama', 'backend': 'inductor', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1}
    config 4: {'model': 'llama', 'backend': 'ort', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1}
    config 5: {'model': 'llama', 'backend': 'ort+', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default', 'disable_pattern': 'default'}
    config 6: {'model': 'llama', 'backend': 'ort+', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default'}
    config 7: {'model': 'llama', 'backend': 'ort+', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default+onnxruntime'}
    config 8: {'model': 'llama', 'backend': 'ort+', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 0, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default+onnxruntime+experimental'}
    config 9: {'model': 'llama', 'backend': 'ort+', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default+onnxruntime+experimental'}
    config 10: {'model': 'llama', 'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default', 'disable_pattern': 'default'}
    config 11: {'model': 'llama', 'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default'}
    config 12: {'model': 'llama', 'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default+onnxruntime'}
    config 13: {'model': 'llama', 'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 0, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default+onnxruntime+experimental'}
    config 14: {'model': 'llama', 'backend': 'custom', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'ort_optimize': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1, 'enable_pattern': 'default+onnxruntime+experimental'}
    config 15: {'model': 'llama', 'backend': 'ortmodule', 'device': 'cuda', 'num_hidden_layers': 1, 'repeat': 10, 'mixed': 0, 'dynamic': 0, 'config': 'medium', 'warmup': 5, 'implementation': 'eager', 'with_mask': 1, 'order': 'none', 'shape_scenario': '', 'verbose': 1}




.. GENERATED FROM PYTHON SOURCE LINES 270-271

Running configuration.

.. GENERATED FROM PYTHON SOURCE LINES 271-287

.. code-block:: Python



    try:
        data = run_benchmark(
            parsed_args.script_name,
            configs,
            verbose=verbose,
            stop_if_exception=False,
            dump=parsed_args.dump in ("1", 1),
        )
        data_collected = True
    except BenchmarkError as e:
        if verbose:
            print(e)
        data_collected = False





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/15 [00:00<?, ?it/s]    [llama]:   0%|          | 0/15 [00:00<?, ?it/s]    [llama]:   7%|▋         | 1/15 [00:12<02:59, 12.80s/it]    [llama]:   7%|▋         | 1/15 [00:12<02:59, 12.80s/it]    [llama]:  13%|█▎        | 2/15 [00:29<03:16, 15.08s/it]    [llama]:  13%|█▎        | 2/15 [00:29<03:16, 15.08s/it]    [llama]:  20%|██        | 3/15 [00:43<02:56, 14.74s/it]    [llama]:  20%|██        | 3/15 [00:43<02:56, 14.74s/it]    [llama]:  27%|██▋       | 4/15 [01:13<03:48, 20.79s/it]    [llama]:  27%|██▋       | 4/15 [01:13<03:48, 20.79s/it]    [llama]:  33%|███▎      | 5/15 [01:45<04:07, 24.78s/it]    [llama]:  33%|███▎      | 5/15 [01:45<04:07, 24.78s/it]    [llama]:  40%|████      | 6/15 [02:20<04:13, 28.14s/it]    [llama]:  40%|████      | 6/15 [02:20<04:13, 28.14s/it]    [llama]:  47%|████▋     | 7/15 [02:52<03:54, 29.37s/it]    [llama]:  47%|████▋     | 7/15 [02:52<03:54, 29.37s/it]    [llama]:  53%|█████▎    | 8/15 [03:22<03:27, 29.69s/it]    [llama]:  53%|█████▎    | 8/15 [03:22<03:27, 29.69s/it]    [llama]:  60%|██████    | 9/15 [03:51<02:57, 29.58s/it]    [llama]:  60%|██████    | 9/15 [03:51<02:57, 29.58s/it]    [llama]:  67%|██████▋   | 10/15 [04:04<02:01, 24.33s/it]    [llama]:  67%|██████▋   | 10/15 [04:04<02:01, 24.33s/it]    [llama]:  73%|███████▎  | 11/15 [04:16<01:22, 20.63s/it]    [llama]:  73%|███████▎  | 11/15 [04:16<01:22, 20.63s/it]    [llama]:  80%|████████  | 12/15 [04:29<00:54, 18.20s/it]    [llama]:  80%|████████  | 12/15 [04:29<00:54, 18.20s/it]    [llama]:  87%|████████▋ | 13/15 [04:41<00:32, 16.32s/it]    [llama]:  87%|████████▋ | 13/15 [04:41<00:32, 16.32s/it]    [llama]:  93%|█████████▎| 14/15 [04:54<00:15, 15.33s/it]    [llama]:  93%|█████████▎| 14/15 [04:54<00:15, 15.33s/it]    [llama]: 100%|██████████| 15/15 [05:00<00:00, 12.44s/it]    [llama]: 100%|██████████| 15/15 [05:00<00:00, 20.02s/it]




.. GENERATED FROM PYTHON SOURCE LINES 288-289

Let's process the data.

.. GENERATED FROM PYTHON SOURCE LINES 289-356

.. code-block:: Python


    prefix = (
        f"plot_{parsed_args.model}-{parsed_args.with_mask}-"
        f"m{parsed_args.mixed}d{parsed_args.dynamic}h{parsed_args.num_hidden_layers}-"
        f"{parsed_args.implementation}"
    )

    if data_collected:

        def clean_pattern(s):
            s = s.replace("+default-default", "")
            return s

        def make_legend(row):
            row = row.to_dict()
            val = [
                row["device"],
                f"h{row['num_hidden_layers']}",
                row["implementation"],
                row["backend"],
            ]
            if row["mixed"]:
                val.append("mix")
            if row["dynamic"]:
                val.append("dyn")
            if "patterns" in row and row["patterns"] and "nan" not in str(row["patterns"]):
                val.append(f"({clean_pattern(row['patterns'])})")
            s = "-".join(map(str, val))
            assert "nan" not in s, f"Legend {s!r} is wrong, row={row}"
            return s

        df = pandas.DataFrame(data)
        df = df.drop(["OUTPUT", "ERROR"], axis=1)
        if "implementation" in df.columns:
            df["legend"] = df.apply(make_legend, axis=1)
            df["time"] = df["time"].astype(float)
            df_eager = df[(df["implementation"] == "eager") & (df["backend"] == "eager")][
                "time"
            ].dropna()
            if df_eager.shape[0] > 0:
                min_eager = df_eager.min()
                df["increase"] = df["time"] / min_eager - 1
                # df["ERROR"] = df["ERROR"].apply(lambda s: s.replace("\n", " "))
        filename = f"plot_{prefix}_bench_with_cmd.csv"
        df.to_csv(filename, index=False)
        filename = f"plot_{prefix}_bench_with_cmd.xlsx"
        df.to_excel(filename, index=False)

        df = df.drop(["CMD"], axis=1)
        filename = f"plot_{prefix}_bench.csv"
        df.to_csv(filename, index=False)
        df = pandas.read_csv(filename)  # to cast type
        print(df)

        # summary
        cs = [
            c
            for c in ["backend", "patterns", "warmup_time", "time", "increase"]
            if c in df.columns
        ]
        dfs = df[cs]
        filename = f"plot_{prefix}_summary.xlsx"
        dfs.to_excel(filename, index=False)
        filename = f"plot_{prefix}_summary.csv"
        dfs.to_csv(filename, index=False)
        print(dfs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                                  llama  config  mixed  dynamic  ...             config_enable_pattern config_disable_pattern                                             legend  increase
    0   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                                cuda-h1-eager-eager  0.000000
    1   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                               cuda-h1-eager-dynger  0.024138
    2   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                             cuda-h1-eager-inductor -0.278051
    3   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                                  cuda-h1-eager-ort  3.144991
    4   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                default                           cuda-h1-eager-ort+-(+oo)  3.142859
    5   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                    NaN                  cuda-h1-eager-ort+-(+default-+oo)  2.890707
    6   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...               default+onnxruntime                    NaN      cuda-h1-eager-ort+-(+default+onnxruntime-+oo)  2.830773
    7   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-ort+-(+default+onnxruntime+exper...  2.802292
    8   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-ort+-(+default+onnxruntime+exper...  2.849034
    9   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                default                         cuda-h1-eager-custom-(+oo) -0.132404
    10  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                    NaN                cuda-h1-eager-custom-(+default-+oo) -0.139556
    11  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...               default+onnxruntime                    NaN    cuda-h1-eager-custom-(+default+onnxruntime-+oo) -0.170234
    12  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-custom-(+default+onnxruntime+exp... -0.222833
    13  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-custom-(+default+onnxruntime+exp... -0.203548
    14                                              NaN  medium      0        0  ...                               NaN                    NaN                            cuda-h1-eager-ortmodule       NaN

    [15 rows x 58 columns]
          backend                               patterns  warmup_time      time  increase
    0       eager                                    NaN     2.051938  0.312793  0.000000
    1      dynger                                    NaN     4.847931  0.320343  0.024138
    2    inductor                                    NaN     5.089327  0.225821 -0.278051
    3         ort                                    NaN     9.817106  1.296525  3.144991
    4        ort+                    +default-default+oo    10.381171  1.295858  3.142859
    5        ort+                           +default-+oo    12.110737  1.216987  2.890707
    6        ort+               +default+onnxruntime-+oo    10.985888  1.198240  2.830773
    7        ort+     +default+onnxruntime+experimental-    10.689280  1.189331  2.802292
    8        ort+  +default+onnxruntime+experimental-+oo    10.202735  1.203952  2.849034
    9      custom                    +default-default+oo     3.691045  0.271378 -0.132404
    10     custom                           +default-+oo     3.514600  0.269141 -0.139556
    11     custom               +default+onnxruntime-+oo     3.615589  0.259545 -0.170234
    12     custom     +default+onnxruntime+experimental-     3.671849  0.243093 -0.222833
    13     custom  +default+onnxruntime+experimental-+oo     3.596110  0.249125 -0.203548
    14  ortmodule                                    NaN          NaN       NaN       NaN




.. GENERATED FROM PYTHON SOURCE LINES 357-358

First lines.

.. GENERATED FROM PYTHON SOURCE LINES 358-361

.. code-block:: Python


    print(df.head(2).T)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                                                              0                                                  1
    llama                       (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False    (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False
    config                                                               medium                                             medium
    mixed                                                                     0                                                  0
    dynamic                                                                   0                                                  0
    optimize                                                               True                                               True
    order                                                                  none                                               none
    ort_optimize                                                           True                                               True
    backend                                                               eager                                             dynger
    repeat                                                                   10                                                 10
    warmup                                                                    5                                                  5
    with_mask                                                                 1                                                  1
    implementation                                                        eager                                              eager
    torch                                               2.6.0.dev20241015+cu121                            2.6.0.dev20241015+cu121
    transformers                                                         4.45.2                                             4.45.2
    memory_peak                                                      990.453125                                        1161.511719
    memory_mean                                                      976.169395                                        1001.546068
    memory_n                                                              460.0                                              731.0
    memory_begin                                                     682.355469                                         682.859375
    memory_end                                                       990.453125                                        1161.511719
    memory_gpu0_peak                                                2571.234375                                        2631.234375
    memory_gpu0_mean                                                2545.138723                                        2502.799354
    memory_gpu0_n                                                         460.0                                              731.0
    memory_gpu0_begin                                               2277.234375                                        2277.234375
    memory_gpu0_end                                                 2571.234375                                        2631.234375
    warmup_time                                                        2.051938                                           4.847931
    time                                                               0.312793                                           0.320343
    model                                                                 llama                                              llama
    device                                                                 cuda                                               cuda
    num_hidden_layers                                                         1                                                  1
    shape_scenario                                                          NaN                                                NaN
    verbose                                                                   1                                                  1
    DATE                                                             2024-10-16                                         2024-10-16
    ITER                                                                      0                                                  1
    TIME_ITER                                                         12.796956                                          16.670818
    ERR_stdout                model=llamamodel config={'input_dims': [(2 102...  model=llamamodel config={'input_dims': [(2 102...
    config_model                                                          llama                                              llama
    config_backend                                                        eager                                             dynger
    config_device                                                          cuda                                               cuda
    config_num_hidden_layers                                                  1                                                  1
    config_repeat                                                            10                                                 10
    config_mixed                                                              0                                                  0
    config_dynamic                                                            0                                                  0
    config_config                                                        medium                                             medium
    config_warmup                                                             5                                                  5
    config_implementation                                                 eager                                              eager
    config_with_mask                                                          1                                                  1
    config_order                                                           none                                               none
    config_shape_scenario                                                   NaN                                                NaN
    config_verbose                                                            1                                                  1
    ERR_std                                                                 NaN  /home/xadupre/vv/this/lib/python3.10/site-pack...
    patterns                                                                NaN                                                NaN
    enable_pattern                                                          NaN                                                NaN
    disable_pattern                                                         NaN                                                NaN
    config_ort_optimize                                                     NaN                                                NaN
    config_enable_pattern                                                   NaN                                                NaN
    config_disable_pattern                                                  NaN                                                NaN
    legend                                                  cuda-h1-eager-eager                               cuda-h1-eager-dynger
    increase                                                                0.0                                           0.024138




.. GENERATED FROM PYTHON SOURCE LINES 362-363

More simple

.. GENERATED FROM PYTHON SOURCE LINES 363-368

.. code-block:: Python


    for c in ["time", "warmup_time"]:
        if c not in df.columns:
            df[c] = np.nan








.. GENERATED FROM PYTHON SOURCE LINES 369-370

Simplified data

.. GENERATED FROM PYTHON SOURCE LINES 370-373

.. code-block:: Python


    print(df.sort_values("legend") if "legend" in df.columns else df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                                  llama  config  mixed  dynamic  ...             config_enable_pattern config_disable_pattern                                             legend  increase
    12  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-custom-(+default+onnxruntime+exp... -0.222833
    13  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-custom-(+default+onnxruntime+exp... -0.203548
    11  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...               default+onnxruntime                    NaN    cuda-h1-eager-custom-(+default+onnxruntime-+oo) -0.170234
    10  (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                    NaN                cuda-h1-eager-custom-(+default-+oo) -0.139556
    9   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                default                         cuda-h1-eager-custom-(+oo) -0.132404
    1   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                               cuda-h1-eager-dynger  0.024138
    0   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                                cuda-h1-eager-eager  0.000000
    2   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                             cuda-h1-eager-inductor -0.278051
    3   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                               NaN                    NaN                                  cuda-h1-eager-ort  3.144991
    7   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-ort+-(+default+onnxruntime+exper...  2.802292
    8   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...  default+onnxruntime+experimental                    NaN  cuda-h1-eager-ort+-(+default+onnxruntime+exper...  2.849034
    6   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...               default+onnxruntime                    NaN      cuda-h1-eager-ort+-(+default+onnxruntime-+oo)  2.830773
    5   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                    NaN                  cuda-h1-eager-ort+-(+default-+oo)  2.890707
    4   (2, 1024)-1024-1-1024-1024-1024-2-eager-1-False  medium      0        0  ...                           default                default                           cuda-h1-eager-ort+-(+oo)  3.142859
    14                                              NaN  medium      0        0  ...                               NaN                    NaN                            cuda-h1-eager-ortmodule       NaN

    [15 rows x 58 columns]




.. GENERATED FROM PYTHON SOURCE LINES 374-375

Plot warmup time.

.. GENERATED FROM PYTHON SOURCE LINES 375-400

.. code-block:: Python


    torch_version = list(set(df["torch"].dropna())) if "torch" in df.columns else (0, 0)
    transformers_version = (
        list(set(df["transformers"].dropna())) if "transformers" in df.columns else (0, 0)
    )
    ver = f"{torch_version[0]} - {transformers_version[0]}"
    model = parsed_args.model
    modeldf = list(set(df[model].dropna()))[0] if model in df.columns else "?"  # noqa: RUF015
    title_prefix = (
        f"lower better\n"
        f"{parsed_args.model} - {ver} - mask{parsed_args.with_mask}"
        f"\n<device>-h<hidden-layers>-<implementation>-<backend>-(optimization)"
    )


    if data_collected and "legend" in df.columns:
        fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))

        df = df.sort_values("time").set_index("legend")
        df[["warmup_time"]].plot.barh(ax=ax, title=f"warmup time\n{title_prefix}")
        ax.grid(True)

        fig.tight_layout()
        fig.savefig(f"plot_{prefix}_bench_warmup_time.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_llama_bench_102_001.png
   :alt: warmup time lower better llama - 2.6.0.dev20241015+cu121 - 4.45.2 - mask1 <device>-h<hidden-layers>-<implementation>-<backend>-(optimization)
   :srcset: /auto_examples/images/sphx_glr_plot_llama_bench_102_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 401-402

Plot time.

.. GENERATED FROM PYTHON SOURCE LINES 402-416

.. code-block:: Python


    if data_collected and "time" in df.columns:
        fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))

        df[["time"]].plot.barh(ax=ax, title=f"computation time\n{title_prefix}")
        mi, ma = df["time"].min(), df["time"].max()
        mi = mi - (ma - mi) / 10
        if not np.isnan(mi):
            ax.set_xlim(left=mi)
        ax.grid(True)

        fig.tight_layout()
        fig.savefig(f"plot_{prefix}_bench_time.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_llama_bench_102_002.png
   :alt: computation time lower better llama - 2.6.0.dev20241015+cu121 - 4.45.2 - mask1 <device>-h<hidden-layers>-<implementation>-<backend>-(optimization)
   :srcset: /auto_examples/images/sphx_glr_plot_llama_bench_102_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 417-418

Plot increase.

.. GENERATED FROM PYTHON SOURCE LINES 418-427

.. code-block:: Python


    if data_collected and "increase" in df.columns:
        fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))

        df[["increase"]].plot.barh(ax=ax, title=f"comparison to eager %\n{title_prefix}")
        ax.grid(True)

        fig.tight_layout()
        fig.savefig(f"plot_{prefix}_bench_relative.png")



.. image-sg:: /auto_examples/images/sphx_glr_plot_llama_bench_102_003.png
   :alt: comparison to eager % lower better llama - 2.6.0.dev20241015+cu121 - 4.45.2 - mask1 <device>-h<hidden-layers>-<implementation>-<backend>-(optimization)
   :srcset: /auto_examples/images/sphx_glr_plot_llama_bench_102_003.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (5 minutes 1.222 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_bench_102.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_bench_102.ipynb <plot_llama_bench_102.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_bench_102.py <plot_llama_bench_102.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_llama_bench_102.zip <plot_llama_bench_102.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
