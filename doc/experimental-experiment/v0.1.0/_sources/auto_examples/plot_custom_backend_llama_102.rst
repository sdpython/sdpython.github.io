
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_custom_backend_llama_102.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_custom_backend_llama_102.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_custom_backend_llama_102.py:


.. _l-plot-custom-backend-llama-102:

========================================
102: Fuse kernels in a small Llama Model
========================================

This example leverages the function :epkg:`torch.compile` and the ability
to use a custom backend (see :epkg:`Custom Backends`)
to test the optimization of a model by fusing simple element-wise kernels.

It takes a small Llama model and uses a backend based on :epkg:`onnxruntime`.
The model is converted into ONNX and then optimized by fusing element-wise
kernels.

::

    python plot_custom_backend_llama --config large

The script requires the following packages beside pytorch,
:epkg:`onnxruntime-training` (for GPU), :epkg:`onnx-extended`
(compiled for GPU) and :epkg:`transformers`.

.. GENERATED FROM PYTHON SOURCE LINES 24-42

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_custom_backend_llama",
        config=("medium", "large or medium depending, large means closer to the real model"),
        num_hidden_layers=(1, "number of hidden layers"),
        with_mask=(0, "tries with a mask as a secondary input"),
        optim=("", "Optimization to apply, empty string for all"),
        description=__doc__,
        expose="config,num_hidden_layers,with_mask,optim",
    )

    print(f"config={script_args.config!r}")
    print(f"num_hidden_layers={script_args.num_hidden_layers!r}")
    print(f"with_mask={script_args.with_mask!r}")
    print(f"optim={script_args.optim!r}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    config='medium'
    num_hidden_layers=1
    with_mask=0
    optim=''




.. GENERATED FROM PYTHON SOURCE LINES 43-44

Imports.

.. GENERATED FROM PYTHON SOURCE LINES 44-63

.. code-block:: Python


    import time
    import numpy as np
    import pandas
    from tqdm import tqdm
    import torch
    from transformers import LlamaConfig
    from transformers.models.llama.modeling_llama import LlamaModel
    from experimental_experiment.xbuilder import OptimizationOptions
    from experimental_experiment.torch_dynamo import onnx_custom_backend
    from experimental_experiment.bench_run import get_machine
    from experimental_experiment.ext_test_case import unit_test_going

    has_cuda = torch.cuda.is_available()
    machine = get_machine()
    print(f"has_cuda={has_cuda}")
    print(f"processor: {machine['processor_name']}")
    print(f"device: {machine.get('device_name', '?')}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    has_cuda=True
    processor: 13th Gen Intel(R) Core(TM) i7-13800H
    device: NVIDIA GeForce RTX 4060 Laptop GPU




.. GENERATED FROM PYTHON SOURCE LINES 64-66

The dummy model
===============

.. GENERATED FROM PYTHON SOURCE LINES 66-80

.. code-block:: Python



    def ids_tensor(shape, vocab_size):
        total_dims = 1
        for dim in shape:
            total_dims *= dim

        values = []
        for _ in range(total_dims):
            values.append(np.random.randint(0, vocab_size - 1))

        return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()









.. GENERATED FROM PYTHON SOURCE LINES 81-82

The size of the input.

.. GENERATED FROM PYTHON SOURCE LINES 82-93

.. code-block:: Python

    if script_args.config == "large":
        batch, seq, vocab_size = 2, 1024, 32000
        intermediate_size = 11008
        hidden_size = 4096
        num_attention_heads = 32
    else:
        batch, seq, vocab_size = 2, 1024, 1024
        intermediate_size = 1024
        hidden_size = 512
        num_attention_heads = 8








.. GENERATED FROM PYTHON SOURCE LINES 94-95

The configuration of the model.

.. GENERATED FROM PYTHON SOURCE LINES 95-106

.. code-block:: Python


    config = LlamaConfig(
        hidden_size=hidden_size,
        num_hidden_layers=int(script_args.num_hidden_layers),
        vocab_size=vocab_size,
        intermediate_size=intermediate_size,
        max_position_embeddings=2048,
        num_attention_heads=num_attention_heads,
    )
    config._attn_implementation = "eager"








.. GENERATED FROM PYTHON SOURCE LINES 107-109

The number of time we run the model to measure
the inference.

.. GENERATED FROM PYTHON SOURCE LINES 109-112

.. code-block:: Python

    warmup = 10 if script_args.config == "medium" else 5
    N = 50 if script_args.config == "medium" else 25








.. GENERATED FROM PYTHON SOURCE LINES 113-114

Let's create the model with dummy inputs.

.. GENERATED FROM PYTHON SOURCE LINES 114-128

.. code-block:: Python

    print("creates the model")
    model = LlamaModel(config)

    inputs = (ids_tensor([batch, seq], vocab_size),)
    if script_args.with_mask in (1, "1"):
        input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))
        inputs = (*inputs, input_mask)

    processor = "cuda" if has_cuda else "cpu"
    print(f"moving model and inputs to processor={processor!r}")
    model = model.to(processor)
    inputs = tuple(i.to(processor) for i in inputs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    creates the model
    moving model and inputs to processor='cuda'




.. GENERATED FROM PYTHON SOURCE LINES 129-131

Measure of eager mode
=====================

.. GENERATED FROM PYTHON SOURCE LINES 131-156

.. code-block:: Python


    times = []

    with torch.no_grad():

        # warmup
        print("warmup eager")
        for _ in tqdm(range(warmup)):
            # model(input_ids, input_mask)
            model(*inputs)
            if has_cuda:
                torch.cuda.synchronize()

        # repeat
        print("repeat eager")
        begin = time.perf_counter()
        for _ in tqdm(range(N)):
            model(*inputs)
            if has_cuda:
                torch.cuda.synchronize()
        d = (time.perf_counter() - begin) / N
        baseline = d
        times.append(dict(optim="eager", processor=processor, avg_time=d, warmup=warmup, N=N))
        print("avg time eager", d)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    warmup eager
      0%|          | 0/10 [00:00<?, ?it/s]     10%|█         | 1/10 [00:00<00:06,  1.42it/s]    100%|██████████| 10/10 [00:00<00:00, 12.92it/s]
    repeat eager
      0%|          | 0/50 [00:00<?, ?it/s]     26%|██▌       | 13/50 [00:00<00:00, 125.57it/s]     54%|█████▍    | 27/50 [00:00<00:00, 129.76it/s]     82%|████████▏ | 41/50 [00:00<00:00, 130.41it/s]    100%|██████████| 50/50 [00:00<00:00, 128.47it/s]
    avg time eager 0.007795672159991227




.. GENERATED FROM PYTHON SOURCE LINES 157-188

Measure with the custom backend
===============================

Three kind of optimization:

- **default**: the onnx model is optimized with less onnx operators
- **default+onnxruntime**: the onnx model is optimized with fused kernels
  implemented by onnxruntime
- **default+onnxruntime+experimental**: the onnx model is optimized with fused kernels
  implemented by onnxruntime and also custom kernels, this does not work on
  CPU.

Some links:

* :class:`experimental_experiment.xbuilder.OptimizationOptions`:
  that class defines the optimizations to apply after the model
  is converted to onnx,
* :func:`experimental_experiment.torch_dynamo.onnx_custom_backend`:
  that function implements the custom backend based on :epkg:`onnxruntime`,
  it converts the model into ONNX, optimizes and runs it,
  it does not support :epkg:`graph break`,
  it does not work well with dynamic shapes yet.
* The CUDA kernels are implemented at
  `onnx_extended/ortops/optim/cuda
  <https://github.com/sdpython/onnx-extended/tree/main/onnx_extended/ortops/optim/cuda>`_
* See :ref:`l-design-pattern-optimizer` to understand how
  these are applied to modify an onnx model.

The GPU memory is not fully freed before two iterations. Only one scenario
should be handled in the same process.
Results may be very different with a different chip.

.. GENERATED FROM PYTHON SOURCE LINES 188-272

.. code-block:: Python


    optimization = (
        [script_args.optim]
        if script_args.optim
        else ["default", "default+onnxruntime", "default+onnxruntime+experimental"]
    )

    if unit_test_going():
        # It is too long.
        optimization = []
        times = []


    with torch.no_grad():

        for optim in optimization:
            print("----------------------")
            print(f"optim={optim}")

            # This variable is used to retrieve the onnx models created by the backend.
            # It can be set to None if it is not needed.
            # Graph are usually small as they do not contain weights.
            storage = None  # {}

            options = OptimizationOptions(
                constant_folding=True,
                patterns=None if optim == "" else optim,
                verbose=0,
                processor=processor.upper(),
            )

            # The backend used here overwrite some of the parameters provided by
            # function onnx_custom_backend.
            custom_custom_backend = lambda *args, optim=optim, options=options, storage=storage, **kwargs: onnx_custom_backend(  # noqa: E731, E501
                *args,
                target_opset=18,
                verbose=0,
                options=options,
                optimize=optim != "",
                storage=storage,
                dump_prefix=f"dump_onx_llama_{optim.replace('+', '_')}",
                **kwargs,
            )

            # The function setting the backend.
            compiled_model = torch.compile(
                model, backend=custom_custom_backend, fullgraph=True, dynamic=False
            )

            # warmup
            print("warmup compiled model")
            for _ in tqdm(range(warmup)):
                compiled_model(*inputs)
                if has_cuda:
                    torch.cuda.synchronize()

            # repeat
            print("repeat compiled_model")
            begin = time.perf_counter()
            for _ in tqdm(range(N)):
                compiled_model(*inputs)
                if has_cuda:
                    torch.cuda.synchronize()
            d = (time.perf_counter() - begin) / N

            # let's measure the number of custom ops
            n_custom_ops = None
            if storage is not None:
                onnx_model = storage["instance"][0]["onnx"]
                n_custom_ops = len([node for node in onnx_model.graph.node if node.domain != ""])

            times.append(
                dict(
                    optim=optim,
                    processor=processor,
                    avg_time=d,
                    warmup=warmup,
                    N=N,
                    n_custom_ops=n_custom_ops,
                    speedup=baseline / d,
                )
            )
            print(f"avg time custom backend with optimization={optim!r}", d)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ----------------------
    optim=default
    warmup compiled model
      0%|          | 0/10 [00:00<?, ?it/s]     10%|█         | 1/10 [00:01<00:09,  1.10s/it]    100%|██████████| 10/10 [00:01<00:00,  8.80it/s]
    repeat compiled_model
      0%|          | 0/50 [00:00<?, ?it/s]     46%|████▌     | 23/50 [00:00<00:00, 228.78it/s]     92%|█████████▏| 46/50 [00:00<00:00, 227.45it/s]    100%|██████████| 50/50 [00:00<00:00, 227.79it/s]
    avg time custom backend with optimization='default' 0.004404186760002631
    ----------------------
    optim=default+onnxruntime
    warmup compiled model
      0%|          | 0/10 [00:00<?, ?it/s]     10%|█         | 1/10 [00:00<00:06,  1.36it/s]    100%|██████████| 10/10 [00:00<00:00, 12.97it/s]
    repeat compiled_model
      0%|          | 0/50 [00:00<?, ?it/s]     48%|████▊     | 24/50 [00:00<00:00, 236.87it/s]     96%|█████████▌| 48/50 [00:00<00:00, 237.02it/s]    100%|██████████| 50/50 [00:00<00:00, 237.04it/s]
    avg time custom backend with optimization='default+onnxruntime' 0.004235392539994791
    ----------------------
    optim=default+onnxruntime+experimental
    warmup compiled model
      0%|          | 0/10 [00:00<?, ?it/s]     10%|█         | 1/10 [00:00<00:08,  1.12it/s]    100%|██████████| 10/10 [00:00<00:00, 10.75it/s]
    repeat compiled_model
      0%|          | 0/50 [00:00<?, ?it/s]     50%|█████     | 25/50 [00:00<00:00, 243.78it/s]    100%|██████████| 50/50 [00:00<00:00, 237.27it/s]    100%|██████████| 50/50 [00:00<00:00, 237.73it/s]
    avg time custom backend with optimization='default+onnxruntime+experimental' 0.004214652699956787




.. GENERATED FROM PYTHON SOURCE LINES 273-278

Final results
=============

avg_time, lower is better,
speedup compare to eager mode, higher is better.

.. GENERATED FROM PYTHON SOURCE LINES 278-283

.. code-block:: Python


    if times:
        df = pandas.DataFrame(times)
        print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                  optim processor  avg_time  warmup   N  n_custom_ops   speedup
    0                             eager      cuda  0.007796      10  50           NaN       NaN
    1                           default      cuda  0.004404      10  50           NaN  1.770059
    2               default+onnxruntime      cuda  0.004235      10  50           NaN  1.840602
    3  default+onnxruntime+experimental      cuda  0.004215      10  50           NaN  1.849659




.. GENERATED FROM PYTHON SOURCE LINES 284-285

Plot

.. GENERATED FROM PYTHON SOURCE LINES 285-290

.. code-block:: Python


    if times:
        df.set_index("optim")[["speedup"]].plot.bar(
            title="Speedup for different optimization scenario"
        )



.. image-sg:: /auto_examples/images/sphx_glr_plot_custom_backend_llama_102_001.png
   :alt: Speedup for different optimization scenario
   :srcset: /auto_examples/images/sphx_glr_plot_custom_backend_llama_102_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 11.941 seconds)


.. _sphx_glr_download_auto_examples_plot_custom_backend_llama_102.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_custom_backend_llama_102.ipynb <plot_custom_backend_llama_102.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_custom_backend_llama_102.py <plot_custom_backend_llama_102.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_custom_backend_llama_102.zip <plot_custom_backend_llama_102.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
