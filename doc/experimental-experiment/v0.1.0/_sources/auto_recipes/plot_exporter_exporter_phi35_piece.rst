
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_recipes/plot_exporter_exporter_phi35_piece.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_recipes_plot_exporter_exporter_phi35_piece.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_recipes_plot_exporter_exporter_phi35_piece.py:


.. _l-plot-exporter-exporter-phi35-piece:

Export Phi-3.5-mini-instruct piece by piece
===========================================

:func:`torch.export.export` often breaks on big models because there
are control flows or instructions breaking the propagation of
dynamic shapes (see ...). The function usually gives an indication where
the model implementation can be fixed but in case, that is not possible,
we can try to export the model piece by piece: every module
is converted separately from its submodule. A model can be exported even
if one of its submodules cannot.

Model
+++++

.. GENERATED FROM PYTHON SOURCE LINES 18-212

.. code-block:: Python


    import pprint
    from typing import Any, Dict
    import torch
    import torch._export.tools
    import transformers
    from experimental_experiment.helpers import string_type
    from experimental_experiment.torch_interpreter.piece_by_piece import (
        trace_execution_piece_by_piece,
    )


    def get_phi35_untrained(batch_size: int = 2, **kwargs) -> Dict[str, Any]:
        """
        Gets a non initialized model with two sets of inputs and different shapes.

        :param batch_size: batch size
        :param kwargs: to overwrite the configuration, example ``num_hidden_layers=1``
        :return: dictionary

        See `Phi-3.5-mini-instruct/config.json
        <https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/config.json>`_.
        """
        config = {
            "_name_or_path": "Phi-3.5-mini-instruct",
            "architectures": ["Phi3ForCausalLM"],
            "attention_dropout": 0.0,
            "auto_map": {
                "AutoConfig": "configuration_phi3.Phi3Config",
                "AutoModelForCausalLM": "modeling_phi3.Phi3ForCausalLM",
            },
            "bos_token_id": 1,
            "embd_pdrop": 0.0,
            "eos_token_id": 32000,
            "hidden_act": "silu",
            "hidden_size": 3072,
            "initializer_range": 0.02,
            "intermediate_size": 8192,
            "max_position_embeddings": 131072,
            "model_type": "phi3",
            "num_attention_heads": 32,
            "num_hidden_layers": 32,
            "num_key_value_heads": 32,
            "original_max_position_embeddings": 4096,
            "pad_token_id": 32000,
            "resid_pdrop": 0.0,
            "rms_norm_eps": 1e-05,
            "rope_scaling": {
                "long_factor": [
                    1.0800000429153442,
                    1.1100000143051147,
                    1.1399999856948853,
                    1.340000033378601,
                    1.5899999141693115,
                    1.600000023841858,
                    1.6200000047683716,
                    2.620000123977661,
                    3.2300000190734863,
                    3.2300000190734863,
                    4.789999961853027,
                    7.400000095367432,
                    7.700000286102295,
                    9.09000015258789,
                    12.199999809265137,
                    17.670000076293945,
                    24.46000099182129,
                    28.57000160217285,
                    30.420001983642578,
                    30.840002059936523,
                    32.590003967285156,
                    32.93000411987305,
                    42.320003509521484,
                    44.96000289916992,
                    50.340003967285156,
                    50.45000457763672,
                    57.55000305175781,
                    57.93000411987305,
                    58.21000289916992,
                    60.1400032043457,
                    62.61000442504883,
                    62.62000274658203,
                    62.71000289916992,
                    63.1400032043457,
                    63.1400032043457,
                    63.77000427246094,
                    63.93000411987305,
                    63.96000289916992,
                    63.970001220703125,
                    64.02999877929688,
                    64.06999969482422,
                    64.08000183105469,
                    64.12000274658203,
                    64.41000366210938,
                    64.4800033569336,
                    64.51000213623047,
                    64.52999877929688,
                    64.83999633789062,
                ],
                "short_factor": [
                    1.0,
                    1.0199999809265137,
                    1.0299999713897705,
                    1.0299999713897705,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0699999332427979,
                    1.0999999046325684,
                    1.1099998950958252,
                    1.1599998474121094,
                    1.1599998474121094,
                    1.1699998378753662,
                    1.2899998426437378,
                    1.339999794960022,
                    1.679999828338623,
                    1.7899998426437378,
                    1.8199998140335083,
                    1.8499997854232788,
                    1.8799997568130493,
                    1.9099997282028198,
                    1.9399996995925903,
                    1.9899996519088745,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0799996852874756,
                    2.0899996757507324,
                    2.189999580383301,
                    2.2199995517730713,
                    2.5899994373321533,
                    2.729999542236328,
                    2.749999523162842,
                    2.8399994373321533,
                ],
                "type": "longrope",
            },
            "rope_theta": 10000.0,
            "sliding_window": 262144,
            "tie_word_embeddings": False,
            "torch_dtype": "bfloat16",
            "use_cache": True,
            "attention_bias": False,
            "vocab_size": 32064,
        }
        config.update(**kwargs)
        conf = transformers.Phi3Config(**config)
        model = transformers.Phi3ForCausalLM(conf)
        model.eval()

        cache = transformers.cache_utils.DynamicCache(config["num_hidden_layers"])
        for i in range(config["num_hidden_layers"]):
            cache.update(
                torch.randn(batch_size, 32, 30, 96), torch.randn(batch_size, 32, 30, 96), i
            )
        cache2 = transformers.cache_utils.DynamicCache(config["num_hidden_layers"])
        for i in range(config["num_hidden_layers"]):
            cache2.update(
                torch.randn(batch_size + 1, 32, 31, 96),
                torch.randn(batch_size + 1, 32, 31, 96),
                i,
            )

        inputs = dict(
            input_ids=torch.randint(0, 32064, (batch_size, 3)).to(torch.int64),
            attention_mask=torch.ones((batch_size, 33)).to(torch.int64),
            past_key_values=cache,
        )
        inputs2 = dict(
            input_ids=torch.randint(0, 32064, (batch_size + 1, 4)).to(torch.int64),
            attention_mask=torch.ones((batch_size + 1, 35)).to(torch.int64),
            past_key_values=cache2,
        )
        return dict(inputs=inputs, model=model, inputs2=inputs2)


    data = get_phi35_untrained(num_hidden_layers=2)
    model, inputs, inputs2 = data["model"], data["inputs"], data["inputs2"]

    print(string_type(inputs, with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dict(input_ids:T7s2x3,attention_mask:T7s2x33,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x96,T1s2x32x30x96], value_cache=#2[T1s2x32x30x96,T1s2x32x30x96]))




.. GENERATED FROM PYTHON SOURCE LINES 213-220

Dynamic Shapes
++++++++++++++

We want to infer the dynamic shapes from the two sets of inputs we gave.
For that, we use a function to trace the execution of the model
including its submodules. It is going to execute the model twice
with the two sets of inputs and stores every intermediate input and output.

.. GENERATED FROM PYTHON SOURCE LINES 220-223

.. code-block:: Python


    diag = trace_execution_piece_by_piece(model, [inputs, inputs2], verbose=2)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [_trace_forward_execution]  __main__-Phi3ForCausalLM.forward
    [_trace_forward_execution] .. model-Phi3Model.forward
    [_trace_forward_execution] .... embed_tokens-Embedding.forward
    [_trace_forward_execution] .... layers[0]-Phi3DecoderLayer.forward
    [_trace_forward_execution] ...... self_attn-Phi3Attention.forward
    [_trace_forward_execution] ........ o_proj-Linear.forward
    [_trace_forward_execution] ........ qkv_proj-Linear.forward
    [_trace_forward_execution] ...... mlp-Phi3MLP.forward
    [_trace_forward_execution] ........ gate_up_proj-Linear.forward
    [_trace_forward_execution] ........ down_proj-Linear.forward
    [_trace_forward_execution] ........ activation_fn-SiLU.forward
    [_trace_forward_execution] ...... input_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... resid_attn_dropout-Dropout.forward
    [_trace_forward_execution] ...... resid_mlp_dropout-Dropout.forward
    [_trace_forward_execution] .... layers[1]-Phi3DecoderLayer.forward
    [_trace_forward_execution] ...... self_attn-Phi3Attention.forward
    [_trace_forward_execution] ........ o_proj-Linear.forward
    [_trace_forward_execution] ........ qkv_proj-Linear.forward
    [_trace_forward_execution] ...... mlp-Phi3MLP.forward
    [_trace_forward_execution] ........ gate_up_proj-Linear.forward
    [_trace_forward_execution] ........ down_proj-Linear.forward
    [_trace_forward_execution] ........ activation_fn-SiLU.forward
    [_trace_forward_execution] ...... input_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... resid_attn_dropout-Dropout.forward
    [_trace_forward_execution] ...... resid_mlp_dropout-Dropout.forward
    [_trace_forward_execution] .... norm-Phi3RMSNorm.forward
    [_trace_forward_execution] .... rotary_emb-Phi3RotaryEmbedding.forward
    [_trace_forward_execution] .. lm_head-Linear.forward
    [trace_execution_piece_by_piece] run with dict(args:(),kwargs:dict(input_ids:T7s2x3,attention_mask:T7s2x33,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x96,T1s2x32x30x96], value_cache=#2[T1s2x32x30x96,T1s2x32x30x96])))
    [__main__:Phi3ForCausalLM] > **dict(input_ids:T7r2,attention_mask:T7r2,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [model:Phi3Model]   > **dict(input_ids:T7r2,attention_mask:T7r2,position_ids:None,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),inputs_embeds:None,use_cache:None,output_attentions:bool,output_hidden_states:bool,return_dict:bool,cache_position:None)
    [embed_tokens:Embedding]     > T7r2
    [embed_tokens:Embedding]     < T1r3
    [rotary_emb:Phi3RotaryEmbedding]     > *(T1r3,T7r2)
    [rotary_emb:Phi3RotaryEmbedding]     < *(T1r3,T1r3)
    [layers[0]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[0]:Phi3DecoderLayer]     < *(T1r3,)
    [layers[1]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[1]:Phi3DecoderLayer]     < *(T1r3,)
    [norm:Phi3RMSNorm]     > T1r3
    [norm:Phi3RMSNorm]     < T1r3
    [model:Phi3Model]   < *dict(last_hidden_state:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [lm_head:Linear]   > T1r3
    [lm_head:Linear]   < T1r3
    [__main__:Phi3ForCausalLM] < *dict(logits:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [trace_execution_piece_by_piece] run with dict(args:(),kwargs:dict(input_ids:T7s3x4,attention_mask:T7s3x35,past_key_values:DynamicCache(key_cache=#2[T1s3x32x31x96,T1s3x32x31x96], value_cache=#2[T1s3x32x31x96,T1s3x32x31x96])))
    [__main__:Phi3ForCausalLM] > **dict(input_ids:T7r2,attention_mask:T7r2,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [model:Phi3Model]   > **dict(input_ids:T7r2,attention_mask:T7r2,position_ids:None,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),inputs_embeds:None,use_cache:None,output_attentions:bool,output_hidden_states:bool,return_dict:bool,cache_position:None)
    [embed_tokens:Embedding]     > T7r2
    [embed_tokens:Embedding]     < T1r3
    [rotary_emb:Phi3RotaryEmbedding]     > *(T1r3,T7r2)
    [rotary_emb:Phi3RotaryEmbedding]     < *(T1r3,T1r3)
    [layers[0]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[0]:Phi3DecoderLayer]     < *(T1r3,)
    [layers[1]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[1]:Phi3DecoderLayer]     < *(T1r3,)
    [norm:Phi3RMSNorm]     > T1r3
    [norm:Phi3RMSNorm]     < T1r3
    [model:Phi3Model]   < *dict(last_hidden_state:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [lm_head:Linear]   > T1r3
    [lm_head:Linear]   < T1r3
    [__main__:Phi3ForCausalLM] < *dict(logits:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [trace_forward_execution] traced execution of model Phi3ForCausalLM
    >>> __main__: Phi3ForCausalLM
      > ((),dict(input_ids:CT7s2x3[5574,27483:A18297.5],attention_mask:CT7s2x33[1,1:A1.0],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.161431312561035,4.224785804748535:A0.004634807757155357],CT1s2x32x30x96[-4.246689319610596,4.243753910064697:A0.00023453347523725792]], value_cache=#2[CT1s2x32x30x96[-4.095401763916016,4.3370513916015625:A-0.001788098125419081],CT1s2x32x30x96[-4.393380641937256,4.321014404296875:A0.00103943639713003]])))
      > ((),dict(input_ids:CT7s3x4[2803,31581:A17770.25],attention_mask:CT7s3x35[1,1:A1.0],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.16038703918457,4.514561176300049:A0.00222829958913058],CT1s3x32x31x96[-4.354647159576416,4.509848117828369:A0.0017182797955132081]], value_cache=#2[CT1s3x32x31x96[-4.775425910949707,4.700263977050781:A-0.0023650375673462984],CT1s3x32x31x96[-4.5358781814575195,4.645477771759033:A0.0002931489946303579]])))
        >>> model: Phi3Model
          > ((),dict(input_ids:CT7s2x3[5574,27483:A18297.5],attention_mask:CT7s2x33[1,1:A1.0],position_ids:None,past_key_values:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.161431312561035,4.224785804748535:A0.004634807757155357],CT1s2x32x30x96[-4.246689319610596,4.243753910064697:A0.00023453347523725792]], value_cache=#2[CT1s2x32x30x96[-4.095401763916016,4.3370513916015625:A-0.001788098125419081],CT1s2x32x30x96[-4.393380641937256,4.321014404296875:A0.00103943639713003]]),inputs_embeds:None,use_cache:None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,cache_position:None))
          > ((),dict(input_ids:CT7s3x4[2803,31581:A17770.25],attention_mask:CT7s3x35[1,1:A1.0],position_ids:None,past_key_values:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.16038703918457,4.514561176300049:A0.00222829958913058],CT1s3x32x31x96[-4.354647159576416,4.509848117828369:A0.0017182797955132081]], value_cache=#2[CT1s3x32x31x96[-4.775425910949707,4.700263977050781:A-0.0023650375673462984],CT1s3x32x31x96[-4.5358781814575195,4.645477771759033:A0.0002931489946303579]]),inputs_embeds:None,use_cache:None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,cache_position:None))
            >>> embed_tokens: Embedding
              > ((CT7s2x3[5574,27483:A18297.5],),{})
              > ((CT7s3x4[2803,31581:A17770.25],),{})
              < (CT1s2x3x3072[-0.07480277121067047,0.08109849691390991:A0.00012241863119738516],)
              < (CT1s3x4x3072[-0.08730854839086533,0.07834544777870178:A-1.2272093172389456e-05],)
            <<<
            >>> layers[0]: Phi3DecoderLayer
              > ((CT1s2x3x3072[-0.07480277121067047,0.08109849691390991:A0.00012241863119738516],),dict(attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.161431312561035,4.224785804748535:A0.004634807757155357],CT1s2x32x30x96[-4.246689319610596,4.243753910064697:A0.00023453347523725792]], value_cache=#2[CT1s2x32x30x96[-4.095401763916016,4.3370513916015625:A-0.001788098125419081],CT1s2x32x30x96[-4.393380641937256,4.321014404296875:A0.00103943639713003]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
              > ((CT1s3x4x3072[-0.08730854839086533,0.07834544777870178:A-1.2272093172389456e-05],),dict(attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.16038703918457,4.514561176300049:A0.00222829958913058],CT1s3x32x31x96[-4.354647159576416,4.509848117828369:A0.0017182797955132081]], value_cache=#2[CT1s3x32x31x96[-4.775425910949707,4.700263977050781:A-0.0023650375673462984],CT1s3x32x31x96[-4.5358781814575195,4.645477771759033:A0.0002931489946303579]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                >>> self_attn: Phi3Attention
                  > ((),dict(hidden_states:CT1s2x3x3072[-3.707216262817383,4.050014019012451:A0.006040942115907555],attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.161431312561035,4.224785804748535:A0.004634807757155357],CT1s2x32x30x96[-4.246689319610596,4.243753910064697:A0.00023453347523725792]], value_cache=#2[CT1s2x32x30x96[-4.095401763916016,4.3370513916015625:A-0.001788098125419081],CT1s2x32x30x96[-4.393380641937256,4.321014404296875:A0.00103943639713003]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
                  > ((),dict(hidden_states:CT1s3x4x3072[-4.355161190032959,3.934891700744629:A-0.0006150025812186537],attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.16038703918457,4.514561176300049:A0.00222829958913058],CT1s3x32x31x96[-4.354647159576416,4.509848117828369:A0.0017182797955132081]], value_cache=#2[CT1s3x32x31x96[-4.775425910949707,4.700263977050781:A-0.0023650375673462984],CT1s3x32x31x96[-4.5358781814575195,4.645477771759033:A0.0002931489946303579]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                    >>> o_proj: Linear
                      > ((CT1s2x3x3072[-2.029766321182251,2.047563314437866:A-0.003584892175442494],),{})
                      > ((CT1s3x4x3072[-2.7823708057403564,2.8556041717529297:A-0.0040341293934005075],),{})
                      < (CT1s2x3x3072[-1.439682960510254,1.4214481115341187:A0.0023619559601936796],)
                      < (CT1s3x4x3072[-1.900197148323059,1.596337914466858:A-0.003698362308515218],)
                    <<<
                    >>> qkv_proj: Linear
                      > ((CT1s2x3x3072[-3.707216262817383,4.050014019012451:A0.006040942115907555],),{})
                      > ((CT1s3x4x3072[-4.355161190032959,3.934891700744629:A-0.0006150025812186537],),{})
                      < (CT1s2x3x9216[-4.790672302246094,4.445572376251221:A-0.011796673397445492],)
                      < (CT1s3x4x9216[-4.585371017456055,5.187196254730225:A0.0034512365934283983],)
                    <<<
                  < (CT1s2x3x3072[-1.439682960510254,1.4214481115341187:A0.0023619559601936796],None)
                  < (CT1s3x4x3072[-1.900197148323059,1.596337914466858:A-0.003698362308515218],None)
                <<<
                >>> mlp: Phi3MLP
                  > ((CT1s2x3x3072[-3.9449074268341064,3.7173683643341064:A0.00634386778754011],),{})
                  > ((CT1s3x4x3072[-4.277142524719238,4.299569606781006:A-0.00904952836464987],),{})
                    >>> gate_up_proj: Linear
                      > ((CT1s2x3x3072[-3.9449074268341064,3.7173683643341064:A0.00634386778754011],),{})
                      > ((CT1s3x4x3072[-4.277142524719238,4.299569606781006:A-0.00904952836464987],),{})
                      < (CT1s2x3x16384[-4.643964767456055,4.755533695220947:A0.007217531379671034],)
                      < (CT1s3x4x16384[-5.158224105834961,4.845681190490723:A0.0021104452119801533],)
                    <<<
                    >>> down_proj: Linear
                      > ((CT1s2x3x8192[-9.84500503540039,9.552478790283203:A-9.334995957142936e-05],),{})
                      > ((CT1s3x4x8192[-8.45986557006836,11.592777252197266:A-0.0008772642037185573],),{})
                      < (CT1s2x3x3072[-5.717556953430176,5.6448187828063965:A0.0008606511478824865],)
                      < (CT1s3x4x3072[-5.688261985778809,5.993067264556885:A-0.00022524263773170282],)
                    <<<
                    >>> activation_fn: SiLU
                      > ((CT1s2x3x8192[-4.494906902313232,4.47076940536499:A0.011820390218379847],),{})
                      > ((CT1s3x4x8192[-5.158224105834961,4.845681190490723:A0.005403731546317865],),{})
                      < (CT1s2x3x8192[-0.27846455574035645,4.420208930969238:A0.251418728424879],)
                      < (CT1s3x4x8192[-0.27846455574035645,4.807880401611328:A0.24772069877169414],)
                    <<<
                  < (CT1s2x3x3072[-5.717556953430176,5.6448187828063965:A0.0008606511478824865],)
                  < (CT1s3x4x3072[-5.688261985778809,5.993067264556885:A-0.00022524263773170282],)
                <<<
                >>> input_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-0.07480277121067047,0.08109849691390991:A0.00012241863119738516],),{})
                  > ((CT1s3x4x3072[-0.08730854839086533,0.07834544777870178:A-1.2272093172389456e-05],),{})
                  < (CT1s2x3x3072[-3.707216262817383,4.050014019012451:A0.006040942115907555],)
                  < (CT1s3x4x3072[-4.355161190032959,3.934891700744629:A-0.0006150025812186537],)
                <<<
                >>> post_attention_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-1.4535324573516846,1.4494394063949585:A0.0024843746973301803],),{})
                  > ((CT1s3x4x3072[-1.9223039150238037,1.603024959564209:A-0.003710634354795338],),{})
                  < (CT1s2x3x3072[-3.9449074268341064,3.7173683643341064:A0.00634386778754011],)
                  < (CT1s3x4x3072[-4.277142524719238,4.299569606781006:A-0.00904952836464987],)
                <<<
                >>> resid_attn_dropout: Dropout
                  > ((CT1s2x3x3072[-1.439682960510254,1.4214481115341187:A0.0023619559601936796],),{})
                  > ((CT1s3x4x3072[-1.900197148323059,1.596337914466858:A-0.003698362308515218],),{})
                  < (CT1s2x3x3072[-1.439682960510254,1.4214481115341187:A0.0023619559601936796],)
                  < (CT1s3x4x3072[-1.900197148323059,1.596337914466858:A-0.003698362308515218],)
                <<<
                >>> resid_mlp_dropout: Dropout
                  > ((CT1s2x3x3072[-5.717556953430176,5.6448187828063965:A0.0008606511478824865],),{})
                  > ((CT1s3x4x3072[-5.688261985778809,5.993067264556885:A-0.00022524263773170282],),{})
                  < (CT1s2x3x3072[-5.717556953430176,5.6448187828063965:A0.0008606511478824865],)
                  < (CT1s3x4x3072[-5.688261985778809,5.993067264556885:A-0.00022524263773170282],)
                <<<
              < (CT1s2x3x3072[-6.139637470245361,5.455146789550781:A0.0033450258186146836],)
              < (CT1s3x4x3072[-5.890384197235107,6.185659885406494:A-0.003935877036800169],)
            <<<
            >>> layers[1]: Phi3DecoderLayer
              > ((CT1s2x3x3072[-6.139637470245361,5.455146789550781:A0.0033450258186146836],),dict(attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.150918483734131,5.777583122253418:A0.0038630417958863414],CT1s2x32x30x96[-4.246689319610596,4.243753910064697:A0.00023453347523725792]], value_cache=#2[CT1s2x32x33x96[-4.361945152282715,4.445572376251221:A-0.0022890200105243207],CT1s2x32x30x96[-4.393380641937256,4.321014404296875:A0.00103943639713003]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
              > ((CT1s3x4x3072[-5.890384197235107,6.185659885406494:A-0.003935877036800169],),dict(attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.5246782302856445,5.150930404663086:A0.0022855420174682675],CT1s3x32x31x96[-4.354647159576416,4.509848117828369:A0.0017182797955132081]], value_cache=#2[CT1s3x32x35x96[-4.775425910949707,5.187196254730225:A-0.0022569701417174113],CT1s3x32x31x96[-4.5358781814575195,4.645477771759033:A0.0002931489946303579]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                >>> self_attn: Phi3Attention
                  > ((),dict(hidden_states:CT1s2x3x3072[-4.284478187561035,4.008422374725342:A0.002219776057119205],attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.150918483734131,5.777583122253418:A0.0038630417958863414],CT1s2x32x30x96[-4.246689319610596,4.243753910064697:A0.00023453347523725792]], value_cache=#2[CT1s2x32x33x96[-4.361945152282715,4.445572376251221:A-0.0022890200105243207],CT1s2x32x30x96[-4.393380641937256,4.321014404296875:A0.00103943639713003]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
                  > ((),dict(hidden_states:CT1s3x4x3072[-4.319344997406006,4.540497779846191:A-0.0026530663741191804],attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.5246782302856445,5.150930404663086:A0.0022855420174682675],CT1s3x32x31x96[-4.354647159576416,4.509848117828369:A0.0017182797955132081]], value_cache=#2[CT1s3x32x35x96[-4.775425910949707,5.187196254730225:A-0.0022569701417174113],CT1s3x32x31x96[-4.5358781814575195,4.645477771759033:A0.0002931489946303579]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                    >>> o_proj: Linear
                      > ((CT1s2x3x3072[-1.8964695930480957,1.8733530044555664:A-0.0017044281418954076],),{})
                      > ((CT1s3x4x3072[-2.5581154823303223,2.5130867958068848:A-0.001177042879221367],),{})
                      < (CT1s2x3x3072[-1.404975414276123,1.4752793312072754:A0.006144630074636047],)
                      < (CT1s3x4x3072[-1.5518382787704468,1.6641747951507568:A0.0013710559266857875],)
                    <<<
                    >>> qkv_proj: Linear
                      > ((CT1s2x3x3072[-4.284478187561035,4.008422374725342:A0.002219776057119205],),{})
                      > ((CT1s3x4x3072[-4.319344997406006,4.540497779846191:A-0.0026530663741191804],),{})
                      < (CT1s2x3x9216[-4.8605546951293945,4.830465316772461:A0.0008736905102182391],)
                      < (CT1s3x4x9216[-4.883401393890381,5.118810653686523:A-0.0036122056137523266],)
                    <<<
                  < (CT1s2x3x3072[-1.404975414276123,1.4752793312072754:A0.006144630074636047],None)
                  < (CT1s3x4x3072[-1.5518382787704468,1.6641747951507568:A0.0013710559266857875],None)
                <<<
                >>> mlp: Phi3MLP
                  > ((CT1s2x3x3072[-4.009374141693115,3.8299498558044434:A0.006381719192020007],),{})
                  > ((CT1s3x4x3072[-4.196354389190674,4.562172889709473:A-0.0016308635125987792],),{})
                    >>> gate_up_proj: Linear
                      > ((CT1s2x3x3072[-4.009374141693115,3.8299498558044434:A0.006381719192020007],),{})
                      > ((CT1s3x4x3072[-4.196354389190674,4.562172889709473:A-0.0016308635125987792],),{})
                      < (CT1s2x3x16384[-4.773173809051514,4.801597595214844:A-0.00832319508059006],)
                      < (CT1s3x4x16384[-5.1458210945129395,5.003308296203613:A-0.004895441993359857],)
                    <<<
                    >>> down_proj: Linear
                      > ((CT1s2x3x8192[-9.461995124816895,11.584807395935059:A-0.0008163911991713556],),{})
                      > ((CT1s3x4x8192[-10.857090950012207,11.003339767456055:A-0.0006822407298075782],),{})
                      < (CT1s2x3x3072[-6.310835361480713,5.906414985656738:A-0.00034612437222373654],)
                      < (CT1s3x4x3072[-5.354459762573242,6.061708927154541:A-0.001775945299812065],)
                    <<<
                    >>> activation_fn: SiLU
                      > ((CT1s2x3x8192[-4.307795524597168,4.801597595214844:A-0.006617166295408576],),{})
                      > ((CT1s3x4x8192[-4.9219865798950195,5.003308296203613:A-0.0034617676823576935],),{})
                      < (CT1s2x3x8192[-0.27846455574035645,4.7624664306640625:A0.24302491726109807],)
                      < (CT1s3x4x8192[-0.27846455574035645,4.9699320793151855:A0.24251884100132393],)
                    <<<
                  < (CT1s2x3x3072[-6.310835361480713,5.906414985656738:A-0.00034612437222373654],)
                  < (CT1s3x4x3072[-5.354459762573242,6.061708927154541:A-0.001775945299812065],)
                <<<
                >>> input_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-6.139637470245361,5.455146789550781:A0.0033450258186146836],),{})
                  > ((CT1s3x4x3072[-5.890384197235107,6.185659885406494:A-0.003935877036800169],),{})
                  < (CT1s2x3x3072[-4.284478187561035,4.008422374725342:A0.002219776057119205],)
                  < (CT1s3x4x3072[-4.319344997406006,4.540497779846191:A-0.0026530663741191804],)
                <<<
                >>> post_attention_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-5.7926554679870605,5.533426761627197:A0.009489655712261284],),{})
                  > ((CT1s3x4x3072[-5.987976551055908,6.473928451538086:A-0.0025648210108657724],),{})
                  < (CT1s2x3x3072[-4.009374141693115,3.8299498558044434:A0.006381719192020007],)
                  < (CT1s3x4x3072[-4.196354389190674,4.562172889709473:A-0.0016308635125987792],)
                <<<
                >>> resid_attn_dropout: Dropout
                  > ((CT1s2x3x3072[-1.404975414276123,1.4752793312072754:A0.006144630074636047],),{})
                  > ((CT1s3x4x3072[-1.5518382787704468,1.6641747951507568:A0.0013710559266857875],),{})
                  < (CT1s2x3x3072[-1.404975414276123,1.4752793312072754:A0.006144630074636047],)
                  < (CT1s3x4x3072[-1.5518382787704468,1.6641747951507568:A0.0013710559266857875],)
                <<<
                >>> resid_mlp_dropout: Dropout
                  > ((CT1s2x3x3072[-6.310835361480713,5.906414985656738:A-0.00034612437222373654],),{})
                  > ((CT1s3x4x3072[-5.354459762573242,6.061708927154541:A-0.001775945299812065],),{})
                  < (CT1s2x3x3072[-6.310835361480713,5.906414985656738:A-0.00034612437222373654],)
                  < (CT1s3x4x3072[-5.354459762573242,6.061708927154541:A-0.001775945299812065],)
                <<<
              < (CT1s2x3x3072[-7.087924480438232,7.711885452270508:A0.009143531553389848],)
              < (CT1s3x4x3072[-7.98233699798584,9.591446876525879:A-0.004340766643609741],)
            <<<
            >>> norm: Phi3RMSNorm
              > ((CT1s2x3x3072[-7.087924480438232,7.711885452270508:A0.009143531553389848],),{})
              > ((CT1s3x4x3072[-7.98233699798584,9.591446876525879:A-0.004340766643609741],),{})
              < (CT1s2x3x3072[-3.595163345336914,3.899627685546875:A0.004595888253353763],)
              < (CT1s3x4x3072[-4.0565032958984375,4.702434062957764:A-0.002125624942503216],)
            <<<
            >>> rotary_emb: Phi3RotaryEmbedding
              > ((CT1s2x3x3072[-0.07480277121067047,0.08109849691390991:A0.00012241863119738516],CT7s1x3[30,32:A31.0]),{})
              > ((CT1s3x4x3072[-0.08730854839086533,0.07834544777870178:A-1.2272093172389456e-05],CT7s1x4[31,34:A32.5]),{})
              < (CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])
              < (CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])
            <<<
          < (dict(last_hidden_state:CT1s2x3x3072[-3.595163345336914,3.899627685546875:A0.004595888253353763],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.150918483734131,5.777583122253418:A0.0038630417958863414],CT1s2x32x33x96[-5.765370845794678,5.285614967346191:A0.000589027329453154]], value_cache=#2[CT1s2x32x33x96[-4.361945152282715,4.445572376251221:A-0.0022890200105243207],CT1s2x32x33x96[-4.393380641937256,4.830465316772461:A0.0009392725729309316]])),)
          < (dict(last_hidden_state:CT1s3x4x3072[-4.0565032958984375,4.702434062957764:A-0.002125624942503216],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.5246782302856445,5.150930404663086:A0.0022855420174682675],CT1s3x32x35x96[-5.0672993659973145,5.234931945800781:A0.0026916378520111324]], value_cache=#2[CT1s3x32x35x96[-4.775425910949707,5.187196254730225:A-0.0022569701417174113],CT1s3x32x35x96[-4.883401393890381,5.118810653686523:A-0.00035775732637225214]])),)
        <<<
        >>> lm_head: Linear
          > ((CT1s2x3x3072[-3.595163345336914,3.899627685546875:A0.004595888253353763],),{})
          > ((CT1s3x4x3072[-4.0565032958984375,4.702434062957764:A-0.002125624942503216],),{})
          < (CT1s2x3x32064[-4.803446292877197,4.688033580780029:A0.00056727891147537],)
          < (CT1s3x4x32064[-5.301267147064209,4.953386306762695:A-0.0008260947111444434],)
        <<<
      < (dict(logits:CT1s2x3x32064[-4.803446292877197,4.688033580780029:A0.00056727891147537],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.150918483734131,5.777583122253418:A0.0038630417958863414],CT1s2x32x33x96[-5.765370845794678,5.285614967346191:A0.000589027329453154]], value_cache=#2[CT1s2x32x33x96[-4.361945152282715,4.445572376251221:A-0.0022890200105243207],CT1s2x32x33x96[-4.393380641937256,4.830465316772461:A0.0009392725729309316]])),)
      < (dict(logits:CT1s3x4x32064[-5.301267147064209,4.953386306762695:A-0.0008260947111444434],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.5246782302856445,5.150930404663086:A0.0022855420174682675],CT1s3x32x35x96[-5.0672993659973145,5.234931945800781:A0.0026916378520111324]], value_cache=#2[CT1s3x32x35x96[-4.775425910949707,5.187196254730225:A-0.0022569701417174113],CT1s3x32x35x96[-4.883401393890381,5.118810653686523:A-0.00035775732637225214]])),)
    <<<
    [_untrace_forward_execution]  __main__-Phi3ForCausalLM
    [_untrace_forward_execution] .. model-Phi3Model
    [_untrace_forward_execution] .... embed_tokens-Embedding
    [_untrace_forward_execution] .... layers[0]-Phi3DecoderLayer
    [_untrace_forward_execution] ...... self_attn-Phi3Attention
    [_untrace_forward_execution] ........ o_proj-Linear
    [_untrace_forward_execution] ........ qkv_proj-Linear
    [_untrace_forward_execution] ...... mlp-Phi3MLP
    [_untrace_forward_execution] ........ gate_up_proj-Linear
    [_untrace_forward_execution] ........ down_proj-Linear
    [_untrace_forward_execution] ........ activation_fn-SiLU
    [_untrace_forward_execution] ...... input_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... resid_attn_dropout-Dropout
    [_untrace_forward_execution] ...... resid_mlp_dropout-Dropout
    [_untrace_forward_execution] .... layers[1]-Phi3DecoderLayer
    [_untrace_forward_execution] ...... self_attn-Phi3Attention
    [_untrace_forward_execution] ........ o_proj-Linear
    [_untrace_forward_execution] ........ qkv_proj-Linear
    [_untrace_forward_execution] ...... mlp-Phi3MLP
    [_untrace_forward_execution] ........ gate_up_proj-Linear
    [_untrace_forward_execution] ........ down_proj-Linear
    [_untrace_forward_execution] ........ activation_fn-SiLU
    [_untrace_forward_execution] ...... input_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... resid_attn_dropout-Dropout
    [_untrace_forward_execution] ...... resid_mlp_dropout-Dropout
    [_untrace_forward_execution] .... norm-Phi3RMSNorm
    [_untrace_forward_execution] .... rotary_emb-Phi3RotaryEmbedding
    [_untrace_forward_execution] .. lm_head-Linear




.. GENERATED FROM PYTHON SOURCE LINES 224-227

Now we keep in memory every input/output for the submodules,
we can guess the dynamic shapes for every of them.
The final ones:

.. GENERATED FROM PYTHON SOURCE LINES 227-231

.. code-block:: Python

    dynamic_shapes = diag.guess_dynamic_shapes()
    print("The dynamic shapes are:")
    pprint.pprint(dynamic_shapes)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The dynamic shapes are:
    ((),
     {'attention_mask': {0: <_DimHint.DYNAMIC: 3>, 1: <_DimHint.DYNAMIC: 3>},
      'input_ids': {0: <_DimHint.DYNAMIC: 3>, 1: <_DimHint.DYNAMIC: 3>},
      'past_key_values': [[{0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>},
                           {0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>}],
                          [{0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>},
                           {0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>}]]})




.. GENERATED FROM PYTHON SOURCE LINES 232-233

And all the dynamic shapes all along the traced submodules.

.. GENERATED FROM PYTHON SOURCE LINES 233-243

.. code-block:: Python

    print(
        diag.pretty_text(
            with_dynamic_shape=True,
            with_shape=False,
            with_min_max=False,
            with_device=False,
            with_inputs=False,
        ).replace("<_DimHint.DYNAMIC: 3>", "DYN")
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    >>> __main__: Phi3ForCausalLM
      DS=((), {'attention_mask': {0: DYN, 1: DYN}, 'input_ids': {0: DYN, 1: DYN}, 'past_key_values': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]]})
        >>> model: Phi3Model
          DS=((), {'attention_mask': {0: DYN, 1: DYN}, 'cache_position': None, 'input_ids': {0: DYN, 1: DYN}, 'inputs_embeds': None, 'output_attentions': None, 'output_hidden_states': None, 'past_key_values': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_ids': None, 'return_dict': None, 'use_cache': None})
            >>> embed_tokens: Embedding: DS=(({0: DYN, 1: DYN},), {}) <<<
            >>> layers[0]: Phi3DecoderLayer
              DS=(({0: DYN, 1: DYN},), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                >>> self_attn: Phi3Attention
                  DS=((), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'hidden_states': {0: DYN, 1: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                    >>> o_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> qkv_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> mlp: Phi3MLP
                  DS=(({0: DYN, 1: DYN},), {})
                    >>> gate_up_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> down_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> activation_fn: SiLU: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> post_attention_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_attn_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_mlp_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
            <<<
            >>> layers[1]: Phi3DecoderLayer
              DS=(({0: DYN, 1: DYN},), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                >>> self_attn: Phi3Attention
                  DS=((), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'hidden_states': {0: DYN, 1: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                    >>> o_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> qkv_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> mlp: Phi3MLP
                  DS=(({0: DYN, 1: DYN},), {})
                    >>> gate_up_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> down_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> activation_fn: SiLU: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> post_attention_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_attn_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_mlp_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
            <<<
            >>> norm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
            >>> rotary_emb: Phi3RotaryEmbedding: DS=(({0: DYN, 1: DYN}, {1: DYN}), {}) <<<
        <<<
        >>> lm_head: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
    <<<




.. GENERATED FROM PYTHON SOURCE LINES 244-254

Evaluate the export
+++++++++++++++++++

In many cases, the export (to :class:`torch.fx.Graph`, to ONNX)
does not work on the first try. We need a way to understand
how much the model can be exported. It can be used to evaluate
the how much code needs to be rewritten or patched to be exportable.
The verbosity can be increase to show dynamic shapes, results
of the discrepancies.
Let's display the module and its submodule first.

.. GENERATED FROM PYTHON SOURCE LINES 254-265

.. code-block:: Python


    print(
        diag.pretty_text(
            with_dynamic_shape=False,
            with_shape=False,
            with_min_max=False,
            with_device=False,
            with_inputs=False,
        )
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    >>> __main__: Phi3ForCausalLM
        >>> model: Phi3Model
            >>> embed_tokens: Embedding <<<
            >>> layers[0]: Phi3DecoderLayer
                >>> self_attn: Phi3Attention
                    >>> o_proj: Linear <<<
                    >>> qkv_proj: Linear <<<
                <<<
                >>> mlp: Phi3MLP
                    >>> gate_up_proj: Linear <<<
                    >>> down_proj: Linear <<<
                    >>> activation_fn: SiLU <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm <<<
                >>> post_attention_layernorm: Phi3RMSNorm <<<
                >>> resid_attn_dropout: Dropout <<<
                >>> resid_mlp_dropout: Dropout <<<
            <<<
            >>> layers[1]: Phi3DecoderLayer
                >>> self_attn: Phi3Attention
                    >>> o_proj: Linear <<<
                    >>> qkv_proj: Linear <<<
                <<<
                >>> mlp: Phi3MLP
                    >>> gate_up_proj: Linear <<<
                    >>> down_proj: Linear <<<
                    >>> activation_fn: SiLU <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm <<<
                >>> post_attention_layernorm: Phi3RMSNorm <<<
                >>> resid_attn_dropout: Dropout <<<
                >>> resid_mlp_dropout: Dropout <<<
            <<<
            >>> norm: Phi3RMSNorm <<<
            >>> rotary_emb: Phi3RotaryEmbedding <<<
        <<<
        >>> lm_head: Linear <<<
    <<<




.. GENERATED FROM PYTHON SOURCE LINES 266-269

The we try to export to see the submodule failing the whole model.
We can pickle the failing model and restore it to speedup
the refactoring to make it work.

.. GENERATED FROM PYTHON SOURCE LINES 269-279

.. code-block:: Python

    print("----------------------")
    ep = diag.try_export(
        exporter="fx",
        use_dynamic_shapes=True,
        exporter_kwargs=dict(strict=False),
        verbose=1,
    )
    print(f"success: {ep.status}")
    print(diag.get_export_report())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ----------------------

    [try_export-FX]  __main__-Phi3ForCausalLM --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_values']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_values']` (expected None)
    [try_export-FX] .. model-Phi3Model --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_values']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_values']` (expected None)
    [try_export-FX] .... embed_tokens-Embedding --- OK: 
    [try_export-FX] .... layers[0]-Phi3DecoderLayer --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ...... self_attn-Phi3Attention --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ........ o_proj-Linear --- OK: 
    [try_export-FX] ........ qkv_proj-Linear --- OK: 
    [try_export-FX] ...... mlp-Phi3MLP --- OK: 
    [try_export-FX] ...... input_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... post_attention_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... resid_attn_dropout-Dropout --- OK: 
    [try_export-FX] ...... resid_mlp_dropout-Dropout --- OK: 
    [try_export-FX] .... layers[1]-Phi3DecoderLayer --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ...... self_attn-Phi3Attention --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ........ o_proj-Linear --- OK: 
    [try_export-FX] ........ qkv_proj-Linear --- OK: 
    [try_export-FX] ...... mlp-Phi3MLP --- OK: 
    [try_export-FX] ...... input_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... post_attention_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... resid_attn_dropout-Dropout --- OK: 
    [try_export-FX] ...... resid_mlp_dropout-Dropout --- OK: 
    [try_export-FX] .... norm-Phi3RMSNorm --- OK: 
    [try_export-FX] .... rotary_emb-Phi3RotaryEmbedding --- FAIL, step=EXPORT, reason=Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)
    [try_export-FX] .... rotary_emb-Phi3RotaryEmbedding --- FAIL: Could not guard on data-depend...
    [try_export-FX] .. lm_head-Linear --- OK: 
    success: 2
    __main__                         Phi3ForCausalLM       FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ..model                          Phi3Model             FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ....embed_tokens                 Embedding             OK -- ExportedProgram
    ....layers[0]                    Phi3DecoderLayer      FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ......self_attn                  Phi3Attention         FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ........o_proj                   Linear                OK -- ExportedProgram
    ........qkv_proj                 Linear                OK -- ExportedProgram
    ......mlp                        Phi3MLP               OK -- ExportedProgram
    ........gate_up_proj             Linear                OK as part of its owner
    ........down_proj                Linear                OK as part of its owner
    ........activation_fn            SiLU                  OK as part of its owner
    ......input_layernorm            Phi3RMSNorm           OK -- ExportedProgram
    ......post_attention_layernorm   Phi3RMSNorm           OK -- ExportedProgram
    ......resid_attn_dropout         Dropout               OK -- ExportedProgram
    ......resid_mlp_dropout          Dropout               OK -- ExportedProgram
    ....layers[1]                    Phi3DecoderLayer      FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ......self_attn                  Phi3Attention         FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ........o_proj                   Linear                OK -- ExportedProgram
    ........qkv_proj                 Linear                OK -- ExportedProgram
    ......mlp                        Phi3MLP               OK -- ExportedProgram
    ........gate_up_proj             Linear                OK as part of its owner
    ........down_proj                Linear                OK as part of its owner
    ........activation_fn            SiLU                  OK as part of its owner
    ......input_layernorm            Phi3RMSNorm           OK -- ExportedProgram
    ......post_attention_layernorm   Phi3RMSNorm           OK -- ExportedProgram
    ......resid_attn_dropout         Dropout               OK -- ExportedProgram
    ......resid_mlp_dropout          Dropout               OK -- ExportedProgram
    ....norm                         Phi3RMSNorm           OK -- ExportedProgram
    ....rotary_emb                   Phi3RotaryEmbedding   FAIL -- step=EXPORT, reason='Could not guard on data-dependent expres...'
    ..lm_head                        Linear                OK -- ExportedProgram




.. GENERATED FROM PYTHON SOURCE LINES 280-287

Replace the failing module by a custom op
+++++++++++++++++++++++++++++++++++++++++

The main module is not exportable because one piece cannot be exported.
But maybe if we assume it works, maybe everything else is working.
So let's try to replace this class by a custom op.
This will be something for another example.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 10.281 seconds)


.. _sphx_glr_download_auto_recipes_plot_exporter_exporter_phi35_piece.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_exporter_exporter_phi35_piece.ipynb <plot_exporter_exporter_phi35_piece.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_exporter_exporter_phi35_piece.py <plot_exporter_exporter_phi35_piece.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_exporter_exporter_phi35_piece.zip <plot_exporter_exporter_phi35_piece.zip>`


.. include:: plot_exporter_exporter_phi35_piece.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
