
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_recipes/plot_exporter_recipes_oe_lr.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_recipes_plot_exporter_recipes_oe_lr.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_recipes_plot_exporter_recipes_oe_lr.py:


.. _l-plot-torch-linreg-101-oe:

====================================
Linear Regression and export to ONNX
====================================

:epkg:`scikit-learn` and :epkg:`torch` to train a linear regression.

data
====

.. GENERATED FROM PYTHON SOURCE LINES 13-30

.. code-block:: Python


    import numpy as np
    from sklearn.datasets import make_regression
    from sklearn.linear_model import LinearRegression, SGDRegressor
    from sklearn.metrics import mean_squared_error, r2_score
    from sklearn.model_selection import train_test_split
    import torch
    from onnxruntime import InferenceSession
    from experimental_experiment.helpers import pretty_onnx
    from onnx_array_api.plotting.graphviz_helper import plot_dot


    X, y = make_regression(1000, n_features=5, noise=10.0, n_informative=2)
    print(X.shape, y.shape)

    X_train, X_test, y_train, y_test = train_test_split(X, y)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (1000, 5) (1000,)




.. GENERATED FROM PYTHON SOURCE LINES 31-37

scikit-learn: the simple regression
===================================

.. math::

      A^* = (X'X)^{-1}X'Y

.. GENERATED FROM PYTHON SOURCE LINES 37-44

.. code-block:: Python



    clr = LinearRegression()
    clr.fit(X_train, y_train)

    print(f"coefficients: {clr.coef_}, {clr.intercept_}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    coefficients: [ 1.52427816e+01 -2.54252527e-01 -3.82964908e-01  8.20315174e+01
      5.67765398e-02], -0.08683603222894343




.. GENERATED FROM PYTHON SOURCE LINES 45-47

Evaluation
==========

.. GENERATED FROM PYTHON SOURCE LINES 47-53

.. code-block:: Python


    y_pred = clr.predict(X_test)
    l2 = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"LinearRegression: l2={l2}, r2={r2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    LinearRegression: l2=103.35968339727624, r2=0.9866212174736183




.. GENERATED FROM PYTHON SOURCE LINES 54-58

scikit-learn: SGD algorithm
===================================

SGD = Stochastic Gradient Descent

.. GENERATED FROM PYTHON SOURCE LINES 58-64

.. code-block:: Python


    clr = SGDRegressor(max_iter=5, verbose=1)
    clr.fit(X_train, y_train)

    print(f"coefficients: {clr.coef_}, {clr.intercept_}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- Epoch 1
    Norm: 70.01, NNZs: 5, Bias: -0.596756, T: 750, Avg. loss: 762.940298
    Total training time: 0.00 seconds.
    -- Epoch 2
    Norm: 79.68, NNZs: 5, Bias: -0.372727, T: 1500, Avg. loss: 75.006016
    Total training time: 0.00 seconds.
    -- Epoch 3
    Norm: 82.11, NNZs: 5, Bias: -0.302945, T: 2250, Avg. loss: 45.922539
    Total training time: 0.00 seconds.
    -- Epoch 4
    Norm: 82.98, NNZs: 5, Bias: -0.086139, T: 3000, Avg. loss: 43.265952
    Total training time: 0.00 seconds.
    -- Epoch 5
    Norm: 83.28, NNZs: 5, Bias: -0.133008, T: 3750, Avg. loss: 42.890983
    Total training time: 0.00 seconds.
    /home/xadupre/vv/this312/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    coefficients: [15.32148304 -0.29461492 -0.41068574 81.85480767  0.12316004], [-0.13300797]




.. GENERATED FROM PYTHON SOURCE LINES 65-66

Evaluation

.. GENERATED FROM PYTHON SOURCE LINES 66-73

.. code-block:: Python


    y_pred = clr.predict(X_test)
    sl2 = mean_squared_error(y_test, y_pred)
    sr2 = r2_score(y_test, y_pred)
    print(f"SGDRegressor: sl2={sl2}, sr2={sr2}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SGDRegressor: sl2=103.54971212325994, sr2=0.9865966203297887




.. GENERATED FROM PYTHON SOURCE LINES 74-76

Linrar Regression with pytorch
==============================

.. GENERATED FROM PYTHON SOURCE LINES 76-125

.. code-block:: Python



    class TorchLinearRegression(torch.nn.Module):
        def __init__(self, n_dims: int, n_targets: int):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x):
            return self.linear(x)


    def train_loop(dataloader, model, loss_fn, optimizer):
        total_loss = 0.0

        # Set the model to training mode - important for batch normalization and dropout layers
        # Unnecessary in this situation but added for best practices
        model.train()
        for X, y in dataloader:
            # Compute prediction and loss
            pred = model(X)
            loss = loss_fn(pred.ravel(), y)

            # Backpropagation
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            # training loss
            total_loss += loss

        return total_loss


    model = TorchLinearRegression(X_train.shape[1], 1)
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
    loss_fn = torch.nn.MSELoss()

    device = "cpu"
    model = model.to(device)
    dataset = torch.utils.data.TensorDataset(
        torch.Tensor(X_train).to(device), torch.Tensor(y_train).to(device)
    )
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)


    for i in range(5):
        loss = train_loop(dataloader, model, loss_fn, optimizer)
        print(f"iteration {i}, loss={loss}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    iteration 0, loss=1716694.25
    iteration 1, loss=150246.78125
    iteration 2, loss=68995.21875
    iteration 3, loss=64695.87890625
    iteration 4, loss=64509.41796875




.. GENERATED FROM PYTHON SOURCE LINES 126-127

Let's check the error

.. GENERATED FROM PYTHON SOURCE LINES 127-133

.. code-block:: Python


    y_pred = model(torch.Tensor(X_test)).detach().numpy()
    tl2 = mean_squared_error(y_test, y_pred)
    tr2 = r2_score(y_test, y_pred)
    print(f"TorchLinearRegression: tl2={tl2}, tr2={tr2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TorchLinearRegression: tl2=104.26683223947525, tr2=0.9865037969603202




.. GENERATED FROM PYTHON SOURCE LINES 134-135

And the coefficients.

.. GENERATED FROM PYTHON SOURCE LINES 135-141

.. code-block:: Python


    print("coefficients:")
    for p in model.parameters():
        print(p)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    coefficients:
    Parameter containing:
    tensor([[ 1.5391e+01, -3.3788e-01, -1.4247e-01,  8.2086e+01, -1.5974e-02]],
           requires_grad=True)
    Parameter containing:
    tensor([-0.1146], requires_grad=True)




.. GENERATED FROM PYTHON SOURCE LINES 142-146

Conversion to ONNX
==================

Let's convert it to ONNX.

.. GENERATED FROM PYTHON SOURCE LINES 146-150

.. code-block:: Python


    ep = torch.onnx.export(model, (torch.Tensor(X_test[:2]),), dynamo=True)
    onx = ep.model_proto





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/github/onnxscript/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.
      param_schemas = callee.param_schemas()
    /home/xadupre/github/onnxscript/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.
      param_schemas = callee.param_schemas()
    [torch.onnx] Obtain model graph for `TorchLinearRegression([...]` with `torch.export.export(..., strict=False)`...
    [torch.onnx] Obtain model graph for `TorchLinearRegression([...]` with `torch.export.export(..., strict=False)`... ✅
    [torch.onnx] Run decomposition...
    [torch.onnx] Run decomposition... ✅
    [torch.onnx] Translate the graph into ONNX...
    [torch.onnx] Translate the graph into ONNX... ✅




.. GENERATED FROM PYTHON SOURCE LINES 151-152

Let's check it is work.

.. GENERATED FROM PYTHON SOURCE LINES 152-157

.. code-block:: Python


    sess = InferenceSession(onx.SerializeToString(), providers=["CPUExecutionProvider"])
    res = sess.run(None, {"x": X_test.astype(np.float32)[:2]})
    print(res)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [array([[ 126.04387],
           [-101.80242]], dtype=float32)]




.. GENERATED FROM PYTHON SOURCE LINES 158-159

And the model.

.. GENERATED FROM PYTHON SOURCE LINES 159-163

.. code-block:: Python


    plot_dot(onx)





.. image-sg:: /auto_recipes/images/sphx_glr_plot_exporter_recipes_oe_lr_001.png
   :alt: plot exporter recipes oe lr
   :srcset: /auto_recipes/images/sphx_glr_plot_exporter_recipes_oe_lr_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 164-169

Optimization
============

By default, the exported model is not optimized and leaves many local functions.
They can be inlined and the model optimized with method `optimize`.

.. GENERATED FROM PYTHON SOURCE LINES 169-176

.. code-block:: Python


    ep.optimize()
    onx = ep.model_proto

    plot_dot(onx)





.. image-sg:: /auto_recipes/images/sphx_glr_plot_exporter_recipes_oe_lr_002.png
   :alt: plot exporter recipes oe lr
   :srcset: /auto_recipes/images/sphx_glr_plot_exporter_recipes_oe_lr_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 177-182

With dynamic shapes
===================

The dynamic shapes are used by :func:`torch.export.export` and must
follow the convention described there.

.. GENERATED FROM PYTHON SOURCE LINES 182-194

.. code-block:: Python


    ep = torch.onnx.export(
        model,
        (torch.Tensor(X_test[:2]),),
        dynamic_shapes={"x": {0: torch.export.Dim("batch")}},
        dynamo=True,
    )
    ep.optimize()
    onx = ep.model_proto

    print(pretty_onnx(onx))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch.onnx] Obtain model graph for `TorchLinearRegression([...]` with `torch.export.export(..., strict=False)`...
    [torch.onnx] Obtain model graph for `TorchLinearRegression([...]` with `torch.export.export(..., strict=False)`... ✅
    [torch.onnx] Run decomposition...
    [torch.onnx] Run decomposition... ✅
    [torch.onnx] Translate the graph into ONNX...
    [torch.onnx] Translate the graph into ONNX... ✅
    opset: domain='pkg.onnxscript.torch_lib.common' version=1
    opset: domain='' version=18
    input: name='x' type=dtype('float32') shape=['s0', 5]
    init: name='linear.weight' type=float32 shape=(1, 5)
    init: name='linear.bias' type=float32 shape=(1,) -- array([-0.11463732], dtype=float32)
    Gemm(x, linear.weight, linear.bias, beta=1.00, transB=1, alpha=1.00, transA=0) -> linear
    output: name='linear' type=dtype('float32') shape=['s0', 1]




.. GENERATED FROM PYTHON SOURCE LINES 195-197

For simplicity, it is possible to use ``torch.export.Dim.DYNAMIC``
or ``torch.export.Dim.AUTO``.

.. GENERATED FROM PYTHON SOURCE LINES 197-208

.. code-block:: Python


    ep = torch.onnx.export(
        model,
        (torch.Tensor(X_test[:2]),),
        dynamic_shapes={"x": {0: torch.export.Dim.AUTO}},
        dynamo=True,
    )
    ep.optimize()
    onx = ep.model_proto

    print(pretty_onnx(onx))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [torch.onnx] Obtain model graph for `TorchLinearRegression([...]` with `torch.export.export(..., strict=False)`...
    [torch.onnx] Obtain model graph for `TorchLinearRegression([...]` with `torch.export.export(..., strict=False)`... ✅
    [torch.onnx] Run decomposition...
    [torch.onnx] Run decomposition... ✅
    [torch.onnx] Translate the graph into ONNX...
    [torch.onnx] Translate the graph into ONNX... ✅
    opset: domain='pkg.onnxscript.torch_lib.common' version=1
    opset: domain='' version=18
    input: name='x' type=dtype('float32') shape=['s0', 5]
    init: name='linear.weight' type=float32 shape=(1, 5)
    init: name='linear.bias' type=float32 shape=(1,) -- array([-0.11463732], dtype=float32)
    Gemm(x, linear.weight, linear.bias, beta=1.00, transB=1, alpha=1.00, transA=0) -> linear
    output: name='linear' type=dtype('float32') shape=['s0', 1]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 7.282 seconds)


.. _sphx_glr_download_auto_recipes_plot_exporter_recipes_oe_lr.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_exporter_recipes_oe_lr.ipynb <plot_exporter_recipes_oe_lr.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_exporter_recipes_oe_lr.py <plot_exporter_recipes_oe_lr.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_exporter_recipes_oe_lr.zip <plot_exporter_recipes_oe_lr.zip>`


.. include:: plot_exporter_recipes_oe_lr.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
