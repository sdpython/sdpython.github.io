<!doctype html>
<html class="no-js" lang="en" data-content_root="">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Change Logs" href="../CHANGELOGS.html" /><link rel="prev" title="Example gallery" href="index.html" />

    <!-- Generated with Sphinx 7.1.2 and Furo 2023.09.10 -->
        <title>Evaluate different ways to export a torch model to ONNX - experimental-experiment 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">experimental-experiment 0.1.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">experimental-experiment 0.1.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../tutorial/index.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API</a></li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Example gallery</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Example gallery</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Evaluate different ways to export a torch model to ONNX</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-torch-export-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="evaluate-different-ways-to-export-a-torch-model-to-onnx">
<span id="sphx-glr-auto-examples-plot-torch-export-py"></span><h1>Evaluate different ways to export a torch model to ONNX<a class="headerlink" href="#evaluate-different-ways-to-export-a-torch-model-to-onnx" title="Permalink to this heading">#</a></h1>
<p>The example evaluates the performance of onnxruntime of a simple
torch model after it was converted into ONNX through different processes:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter">TorchScript-based ONNX Exporter</a>,
let’s call it <strong>script</strong></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter">TorchDynamo-based ONNX Exporter</a>,
let’s call it <strong>dynamo</strong></p></li>
<li><p>if available, the previous model but optimized, <strong>dynopt</strong></p></li>
<li><p>a custom exporter <strong>cus_p0</strong>, this exporter supports a very limited
set of models, as <strong>dynamo</strong>, it relies on
<a class="reference external" href="https://pytorch.org/docs/stable/fx.html">torch.fx</a> but the design is closer to
what tensorflow-onnx does.</p></li>
<li><p>the same exporter but unused nodes were removed, <strong>cus_p1</strong></p></li>
<li><p>the same exporter but constant where folded, <strong>cus_p2</strong></p></li>
</ul>
<section id="some-helpers">
<h2>Some helpers<a class="headerlink" href="#some-helpers" title="Permalink to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">cProfile</span>
<span class="kn">import</span> <span class="nn">pstats</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">from</span> <span class="nn">pstats</span> <span class="kn">import</span> <span class="n">SortKey</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx_extended.ext_test_case</span> <span class="kn">import</span> <span class="n">measure_time</span>
<span class="kn">from</span> <span class="nn">onnx_array_api.plotting.text_plot</span> <span class="kn">import</span> <span class="n">onnx_simple_text_plot</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">experimental_experiment</span>
<span class="kn">from</span> <span class="nn">experimental_experiment.torch_exp.onnx_export</span> <span class="kn">import</span> <span class="n">to_onnx</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>


<span class="k">def</span> <span class="nf">system_info</span><span class="p">():</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;processor&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/platform.html#platform.processor" title="platform.processor" class="sphx-glr-backref-module-platform sphx-glr-backref-type-py-function"><span class="n">platform</span><span class="o">.</span><span class="n">processor</span></a><span class="p">()</span>
    <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;cores&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.cpu_count" title="multiprocessing.cpu_count" class="sphx-glr-backref-module-multiprocessing sphx-glr-backref-type-py-function"><span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span></a><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;cuda&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <a href="https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available" class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span></a><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;cuda_count&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count" title="torch.cuda.device_count" class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span></a><span class="p">()</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;cuda_name&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name" title="torch.cuda.get_device_name" class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span></a><span class="p">()</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;cuda_capa&quot;</span><span class="p">]</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability" title="torch.cuda.get_device_capability" class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span></a><span class="p">()</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="ne">AssertionError</span><span class="p">):</span>
        <span class="c1"># no cuda</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="n">obs</span>


<a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span></a><span class="p">(</span><span class="n">system_info</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;cores&#39;: 8,
 &#39;cuda&#39;: 1,
 &#39;cuda_capa&#39;: (6, 1),
 &#39;cuda_count&#39;: 1,
 &#39;cuda_name&#39;: &#39;NVIDIA GeForce GTX 1060&#39;,
 &#39;processor&#39;: &#39;x86_64&#39;}
</pre></div>
</div>
</section>
<section id="the-model">
<h2>The model<a class="headerlink" href="#the-model" title="Permalink to this heading">#</a></h2>
<p>A simple model to convert.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Module</span></a><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">MyModel</span></a><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span></a><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">13456</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span></a><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d" title="torch.nn.functional.max_pool2d" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d" title="torch.nn.functional.max_pool2d" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten" title="torch.flatten" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">flatten</span></a><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu" class="sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"><span class="n">F</span><span class="o">.</span><span class="n">relu</span></a><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="the-exporters">
<h2>The exporters<a class="headerlink" href="#the-exporters" title="Permalink to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">export_script</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <a href="https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export" title="torch.onnx.export" class="sphx-glr-backref-module-torch-onnx sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">export_dynamo</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">export_output</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.dynamo_export" title="torch.onnx.dynamo_export" class="sphx-glr-backref-module-torch-onnx sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">dynamo_export</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="n">export_output</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">)</span>


<span class="k">def</span> <span class="nf">export_dynopt</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">export_output</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.dynamo_export" title="torch.onnx.dynamo_export" class="sphx-glr-backref-module-torch-onnx sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">dynamo_export</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="n">export_output</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">)</span>
    <span class="n">model_onnx</span> <span class="o">=</span> <a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">)</span>

    <span class="kn">from</span> <span class="nn">onnxrewriter.optimizer</span> <span class="kn">import</span> <span class="n">optimize</span>

    <span class="n">optimized_model</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">model_onnx</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">optimized_model</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">export_cus_p0</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a> <span class="o">=</span> <span class="n">to_onnx</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">])</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">export_cus_p1</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a> <span class="o">=</span> <span class="n">to_onnx</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="n">remove_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">export_cus_p2</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a> <span class="o">=</span> <span class="n">to_onnx</span><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="n">args</span><span class="p">),</span>
        <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>
        <span class="n">remove_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
</pre></div>
</div>
<p>Let’s check they are working.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">export_functions</span></a> <span class="o">=</span> <span class="p">[</span>
    <span class="n">export_script</span><span class="p">,</span>
    <span class="n">export_dynamo</span><span class="p">,</span>
    <span class="n">export_dynopt</span><span class="p">,</span>
    <span class="n">export_cus_p0</span><span class="p">,</span>
    <span class="n">export_cus_p1</span><span class="p">,</span>
    <span class="n">export_cus_p2</span><span class="p">,</span>
<span class="p">]</span>

<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">exporters</span></a> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;export_&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">export_functions</span></a><span class="p">}</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">shape</span></a> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_tensor</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand" title="torch.rand" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">rand</span></a><span class="p">(</span><span class="o">*</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">shape</span></a><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)</span>
<a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">MyModel</span></a><span class="p">()</span>

<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">supported_exporters</span></a> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">exporters</span></a><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;run exporter </span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;plot_torch_export_</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="si">}</span><span class="s2">.onnx&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">v</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_tensor</span></a><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;skipped due to </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">continue</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">supported_exporters</span></a><span class="p">[</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;done.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>run exporter script
[2023-12-05 17:12:57,997] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
done.
run exporter dynamo
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
done.
run exporter dynopt
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
done.
run exporter cus_p0
done.
run exporter cus_p1
done.
run exporter cus_p2
done.
</pre></div>
</div>
</section>
<section id="exporter-speed">
<h2>Exporter speed<a class="headerlink" href="#exporter-speed" title="Permalink to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">supported_exporters</span></a><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;run exporter </span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;plot_torch_export_</span><span class="si">{</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="si">}</span><span class="s2">.onnx&quot;</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">i</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">begin</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/time.html#time.perf_counter" title="time.perf_counter" class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function"><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span></a><span class="p">()</span>
        <span class="n">v</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_tensor</span></a><span class="p">)</span>
        <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">duration</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/time.html#time.perf_counter" title="time.perf_counter" class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function"><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span></a><span class="p">()</span> <span class="o">-</span> <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">begin</span></a>
        <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">duration</span></a><span class="p">)</span>
    <a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a> <span class="o">=</span> <a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">filename</span></a><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;done.&quot;</span><span class="p">)</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="nb">dict</span><span class="p">(</span>
            <span class="n">export</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">k</span></a><span class="p">,</span>
            <span class="n">time</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.mean.html#numpy.mean" title="numpy.mean" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">mean</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
            <span class="nb">min</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
            <span class="nb">max</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
            <span class="n">first</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">last</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">std</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.std.html#numpy.std" title="numpy.std" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">std</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
            <span class="n">nodes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>run exporter script
done.
run exporter dynamo
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
done.
run exporter dynopt
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
/home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
  warnings.warn(
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
done.
run exporter cus_p0
done.
run exporter cus_p1
done.
run exporter cus_p2
done.
</pre></div>
</div>
<p>The last export to measure time torch spends in export the model
before any other export can begin the translation
except the first one.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">i</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">begin</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/time.html#time.perf_counter" title="time.perf_counter" class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function"><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span></a><span class="p">()</span>
    <a href="https://pytorch.org/docs/stable/export.html#torch.export.ExportedProgram" title="torch.export.ExportedProgram" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">exported_mod</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_tensor</span></a><span class="p">,))</span>
    <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">duration</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/time.html#time.perf_counter" title="time.perf_counter" class="sphx-glr-backref-module-time sphx-glr-backref-type-py-function"><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span></a><span class="p">()</span> <span class="o">-</span> <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">begin</span></a>
    <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">duration</span></a><span class="p">)</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="nb">dict</span><span class="p">(</span>
        <span class="n">export</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span>
        <span class="n">time</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.mean.html#numpy.mean" title="numpy.mean" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">mean</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
        <span class="nb">min</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
        <span class="nb">max</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
        <span class="n">first</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">last</span><span class="o">=</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">std</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.std.html#numpy.std" title="numpy.std" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">std</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">times</span></a><span class="p">),</span>
        <span class="n">nodes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The result.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df1</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df1</span></a><span class="p">)</span>

<a href="https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure" title="matplotlib.figure.Figure" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fig</span></a><span class="p">,</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html#matplotlib.axes.Axes" title="matplotlib.axes.Axes" class="sphx-glr-backref-module-matplotlib-axes sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ax</span></a> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dfi</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df1</span></a><span class="p">[[</span><span class="s2">&quot;export&quot;</span><span class="p">,</span> <span class="s2">&quot;time&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;export&quot;</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dfi</span></a><span class="p">[</span><span class="s2">&quot;time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html#matplotlib.axes.Axes" title="matplotlib.axes.Axes" class="sphx-glr-backref-module-matplotlib-axes sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ax</span></a><span class="o">=</span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html#matplotlib.axes.Axes" title="matplotlib.axes.Axes" class="sphx-glr-backref-module-matplotlib-axes sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ax</span></a><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Export time&quot;</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dfi</span></a><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">],</span> <span class="n">rot</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.tight_layout" title="matplotlib.figure.Figure.tight_layout" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-method"><span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span></a><span class="p">()</span>
<a href="https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.savefig" title="matplotlib.figure.Figure.savefig" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-method"><span class="n">fig</span><span class="o">.</span><span class="n">savefig</span></a><span class="p">(</span><span class="s2">&quot;plot_torch_export.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_torch_export_001.png" srcset="../_images/sphx_glr_plot_torch_export_001.png" alt="Export time" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>   export      time       min       max     first      last       std  nodes
0  script  0.350621  0.165329  0.440233  0.165329  0.440233  0.098664     12
1  dynamo  0.530251  0.430999  0.602355  0.585678  0.602355  0.063828     13
2  dynopt  1.262143  1.082738  1.504692  1.504692  1.115929  0.179166     13
3  cus_p0  0.523215  0.469518  0.585807  0.469518  0.585807  0.042538     27
4  cus_p1  0.606241  0.497163  0.761185  0.761185  0.557719  0.089791     15
5  cus_p2  1.636196  0.779323  2.048278  0.779323  2.048278  0.451984     12
6   torch  1.090138  0.406689  1.636102  1.636102  0.406689  0.411706     12
</pre></div>
</div>
</section>
<section id="profiling">
<h2>Profiling<a class="headerlink" href="#profiling" title="Permalink to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">pr</span> <span class="o">=</span> <span class="n">cProfile</span><span class="o">.</span><span class="n">Profile</span><span class="p">()</span>
<span class="n">pr</span><span class="o">.</span><span class="n">enable</span><span class="p">()</span>
<span class="k">for</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">i</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">export_cus_p0</span><span class="p">(</span><span class="s2">&quot;dummy.onnx&quot;</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_tensor</span></a><span class="p">)</span>
<span class="n">pr</span><span class="o">.</span><span class="n">disable</span><span class="p">()</span>
<a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">s</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class"><span class="n">io</span><span class="o">.</span><span class="n">StringIO</span></a><span class="p">()</span>
<span class="n">sortby</span> <span class="o">=</span> <span class="n">SortKey</span><span class="o">.</span><span class="n">CUMULATIVE</span>
<a href="https://docs.python.org/3/library/profile.html#pstats.Stats" title="pstats.Stats" class="sphx-glr-backref-module-pstats sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ps</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/profile.html#pstats.Stats" title="pstats.Stats" class="sphx-glr-backref-module-pstats sphx-glr-backref-type-py-class"><span class="n">pstats</span><span class="o">.</span><span class="n">Stats</span></a><span class="p">(</span><span class="n">pr</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">s</span></a><span class="p">)</span><span class="o">.</span><span class="n">sort_stats</span><span class="p">(</span><span class="n">sortby</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/profile.html#pstats.Stats.print_stats" title="pstats.Stats.print_stats" class="sphx-glr-backref-module-pstats sphx-glr-backref-type-py-method"><span class="n">ps</span><span class="o">.</span><span class="n">print_stats</span></a><span class="p">()</span>


<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a><span class="p">):</span>
    <span class="n">pathes</span> <span class="o">=</span> <span class="p">[</span>
        <a href="https://docs.python.org/3/library/os.path.html#os.path.abspath" title="os.path.abspath" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span></a><span class="p">(</span>
            <a href="https://docs.python.org/3/library/os.path.html#os.path.normpath" title="os.path.normpath" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">normpath</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/os.path.html#os.path.dirname" title="os.path.dirname" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span></a><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="vm">__file__</span><span class="p">),</span> <span class="s2">&quot;..&quot;</span><span class="p">))</span>
        <span class="p">),</span>
        <a href="https://docs.python.org/3/library/os.path.html#os.path.abspath" title="os.path.abspath" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span></a><span class="p">(</span>
            <a href="https://docs.python.org/3/library/os.path.html#os.path.normpath" title="os.path.normpath" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">normpath</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/os.path.html#os.path.dirname" title="os.path.dirname" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span></a><span class="p">(</span><span class="n">onnx</span><span class="o">.</span><span class="vm">__file__</span><span class="p">),</span> <span class="s2">&quot;..&quot;</span><span class="p">))</span>
        <span class="p">),</span>
        <a href="https://docs.python.org/3/library/os.path.html#os.path.abspath" title="os.path.abspath" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span></a><span class="p">(</span>
            <a href="https://docs.python.org/3/library/os.path.html#os.path.normpath" title="os.path.normpath" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">normpath</span></a><span class="p">(</span>
                <a href="https://docs.python.org/3/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/os.path.html#os.path.dirname" title="os.path.dirname" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span></a><span class="p">(</span><span class="n">experimental_experiment</span><span class="o">.</span><span class="vm">__file__</span><span class="p">),</span> <span class="s2">&quot;..&quot;</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">),</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pathes</span><span class="p">:</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;experimental_experiment&quot;</span><span class="p">,</span> <span class="s2">&quot;experimental_experiment&quot;</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
    <span class="k">return</span> <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a>


<a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><a href="https://docs.python.org/3/library/io.html#io.StringIO" title="io.StringIO" class="sphx-glr-backref-module-io sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">s</span></a><span class="o">.</span><span class="n">getvalue</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)[:</span><span class="mi">200</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">clean_text</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">text</span></a><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>         1827575 function calls (1727235 primitive calls) in 9.696 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        5    0.002    0.000    9.900    1.980 /_doc/examples/plot_torch_export.py:119(export_cus_p0)
        5    0.004    0.001    9.263    1.853 /EXPERIMENTAL_EXPERIMENT/torch_exp/onnx_export.py:8(to_onnx)
        5    0.000    0.000    7.932    1.586 /torch/export/__init__.py:930(export)
        5    0.014    0.003    7.931    1.586 /torch/_export/__init__.py:212(export)
    15/10    0.001    0.000    5.512    0.551 /torch/_dynamo/utils.py:182(time_wrapper)
    20/10    0.001    0.000    5.382    0.538 /torch/_dynamo/eval_frame.py:307(_fn)
3520/1655    0.028    0.000    5.069    0.003 /torch/utils/_stats.py:15(wrapper)
   115/55    0.001    0.000    4.773    0.087 /torch/nn/modules/module.py:1514(_wrapped_call_impl)
   115/55    0.002    0.000    4.773    0.087 /torch/nn/modules/module.py:1520(_call_impl)
2910/1810    0.039    0.000    4.487    0.002 /torch/_subclasses/fake_tensor.py:1246(__torch_dispatch__)
2910/1810    0.307    0.000    4.417    0.002 /torch/_subclasses/fake_tensor.py:1270(dispatch)
        5    0.001    0.000    3.521    0.704 /torch/_dynamo/eval_frame.py:1028(inner)
       20    0.019    0.001    3.177    0.159 /torch/fx/interpreter.py:99(run)
      355    0.007    0.000    3.127    0.009 /torch/fx/interpreter.py:177(run_node)
        5    0.001    0.000    3.055    0.611 /torch/_functorch/aot_autograd.py:3914(aot_export_module)
        5    0.000    0.000    3.037    0.607 /torch/_functorch/aot_autograd.py:4164(_aot_export_function)
        5    0.002    0.000    3.032    0.606 /torch/_functorch/aot_autograd.py:3277(create_aot_dispatcher_function)
    15/10    0.000    0.000    2.895    0.289 /torch/_dynamo/external_utils.py:15(inner)
        5    0.000    0.000    2.486    0.497 /torch/_dynamo/eval_frame.py:456(catch_errors)
        5    0.000    0.000    2.485    0.497 /torch/_dynamo/convert_frame.py:122(_fn)
        5    0.000    0.000    2.483    0.497 /torch/_dynamo/convert_frame.py:249(_convert_frame_assert)
        5    0.000    0.000    2.481    0.496 /torch/_dynamo/convert_frame.py:414(_compile)
        5    0.001    0.000    2.478    0.496 /torch/_dynamo/convert_frame.py:481(compile_inner)
2590/1295    0.015    0.000    2.409    0.002 /torch/_ops.py:447(__call__)
       10    0.000    0.000    2.262    0.226 /torch/_functorch/aot_autograd.py:3519(flat_fn)
       10    0.001    0.000    2.259    0.226 /torch/_functorch/aot_autograd.py:3486(functional_call)
        5    0.000    0.000    2.139    0.428 /torch/_functorch/aot_autograd.py:2182(aot_wrapper_dedupe)
        5    0.000    0.000    2.138    0.428 /torch/_functorch/aot_autograd.py:2375(aot_wrapper_synthetic_base)
        5    0.000    0.000    2.136    0.427 /torch/_functorch/aot_autograd.py:1518(aot_dispatch_base_graph)
        5    0.000    0.000    2.044    0.409 /torch/_dynamo/bytecode_transformation.py:1020(transform_code_object)
        5    0.001    0.000    2.037    0.407 /torch/_functorch/aot_autograd.py:1349(create_functionalized_graph)
        5    0.001    0.000    2.036    0.407 /torch/fx/experimental/proxy_tensor.py:721(wrapped)
        5    0.000    0.000    2.026    0.405 /torch/_compile.py:20(inner)
        5    0.000    0.000    2.024    0.405 /torch/fx/experimental/proxy_tensor.py:462(dispatch_trace)
        5    0.000    0.000    2.018    0.404 /torch/_dynamo/convert_frame.py:439(transform)
        5    0.001    0.000    1.964    0.393 /torch/fx/_symbolic_trace.py:695(trace)
       60    0.001    0.000    1.926    0.032 /torch/nn/modules/linear.py:113(forward)
       60    0.034    0.001    1.924    0.032 {built-in method torch._C._nn.linear}
        5    0.000    0.000    1.923    0.385 /torch/fx/experimental/proxy_tensor.py:477(wrapped)
        5    0.000    0.000    1.916    0.383 /torch/_dynamo/symbolic_convert.py:2068(run)
        5    0.001    0.000    1.916    0.383 /torch/_dynamo/symbolic_convert.py:712(run)
  600/175    0.006    0.000    1.915    0.011 /torch/_prims_common/wrappers.py:221(_fn)
      265    0.010    0.000    1.914    0.007 /torch/_dynamo/symbolic_convert.py:617(step)
5660/3350    0.051    0.000    1.802    0.001 /torch/utils/_pytree.py:281(tree_map)
45710/11325    0.462    0.000    1.696    0.000 /torch/utils/_pytree.py:230(tree_flatten)
       75    0.004    0.000    1.629    0.022 /torch/_decomp/decompositions.py:48(inner)
        5    0.000    0.000    1.623    0.325 /torch/_functorch/aot_autograd.py:1411(fwd_helper)
        5    0.000    0.000    1.622    0.324 /torch/_functorch/aot_autograd.py:1357(functionalized_f_helper)
        5    0.000    0.000    1.596    0.319 /torch/_functorch/aot_autograd.py:1164(inner_fn)
       60    0.001    0.000    1.582    0.026 /torch/_dynamo/symbolic_convert.py:384(wrapper)
       60    0.001    0.000    1.572    0.026 /torch/_dynamo/symbolic_convert.py:1106(CALL_FUNCTION)
       60    0.002    0.000    1.570    0.026 /torch/_dynamo/symbolic_convert.py:537(call_function)
  410/380    0.006    0.000    1.532    0.004 /torch/fx/experimental/proxy_tensor.py:552(__torch_dispatch__)
  410/380    0.003    0.000    1.508    0.004 /torch/fx/experimental/proxy_tensor.py:573(inner_torch_dispatch)
       65    0.001    0.000    1.496    0.023 /torch/_dynamo/variables/builder.py:1190(wrap_fx_proxy)
       65    0.008    0.000    1.495    0.023 /torch/_dynamo/variables/builder.py:1240(wrap_fx_proxy_cls)
   105/75    0.017    0.000    1.485    0.020 /torch/fx/experimental/proxy_tensor.py:243(proxy_call)
       50    0.008    0.000    1.428    0.029 /torch/fx/interpreter.py:291(call_module)
       60    0.006    0.000    1.341    0.022 /torch/_dynamo/utils.py:1291(get_fake_value)
      155    0.001    0.000    1.326    0.009 /torch/fx/interpreter.py:249(call_function)
       90    0.000    0.000    1.313    0.015 /torch/_dynamo/utils.py:914(wrap_fake_exception)
        5    0.000    0.000    1.262    0.252 /EXPERIMENTAL_EXPERIMENT/torch_exp/graph_builder.py:325(to_onnx)
       10    0.001    0.000    1.209    0.121 /torch/export/__init__.py:395(_transform)
       25    0.002    0.000    1.184    0.047 /torch/_dynamo/variables/nn_module.py:243(call_function)
5660/3350    0.039    0.000    1.140    0.000 /torch/utils/_pytree.py:283(&lt;listcomp&gt;)
       75    0.019    0.000    1.066    0.014 /torch/_decomp/decompositions.py:1222(addmm)
       10    0.001    0.000    1.064    0.106 /torch/fx/passes/infra/pass_manager.py:242(__call__)
       10    0.000    0.000    0.980    0.098 /torch/fx/passes/infra/pass_base.py:34(__call__)
        5    0.001    0.000    0.973    0.195 /torch/_export/passes/add_runtime_assertions_for_constraints_pass.py:138(call)
       25    0.000    0.000    0.970    0.039 /torch/fx/_symbolic_trace.py:785(module_call_wrapper)
       25    0.000    0.000    0.968    0.039 /torch/fx/experimental/proxy_tensor.py:422(call_module)
       25    0.000    0.000    0.968    0.039 /torch/fx/_symbolic_trace.py:787(forward)
        5    0.001    0.000    0.963    0.193 /torch/_export/pass_base.py:400(call)
        5    0.001    0.000    0.961    0.192 /torch/_export/pass_base.py:376(call_submodule)
  310/260    0.025    0.000    0.956    0.004 {method &#39;detach&#39; of &#39;torch._C._TensorBase&#39; objects}
     4410    0.023    0.000    0.925    0.000 /torch/_subclasses/fake_tensor.py:207(tree_flatten_only)
        5    0.001    0.000    0.871    0.174 /torch/_dynamo/eval_frame.py:1075(result_capturing_wrapper)
      145    0.001    0.000    0.867    0.006 /torch/_export/pass_base.py:230(run_node)
     1920    0.017    0.000    0.849    0.000 /torch/utils/_pytree.py:352(tree_map_only)
        5    0.000    0.000    0.780    0.156 /torch/_functorch/functional_call.py:10(functional_call)
        5    0.000    0.000    0.780    0.156 /torch/nn/utils/stateless.py:230(_functional_call)
        5    0.000    0.000    0.776    0.155 /torch/fx/graph_module.py:677(call_wrapped)
        5    0.000    0.000    0.776    0.155 /torch/fx/graph_module.py:269(__call__)
       90    0.005    0.000    0.771    0.009 /torch/_export/pass_base.py:244(_fx)
       85    0.002    0.000    0.770    0.009 /torch/_export/pass_base.py:173(call_function)
       75    0.001    0.000    0.752    0.010 /torch/_export/passes/add_runtime_assertions_for_constraints_pass.py:84(call_operator)
       75    0.000    0.000    0.747    0.010 /torch/_export/pass_base.py:312(call_operator)
       80    0.001    0.000    0.710    0.009 /torch/nn/functional.py:1460(relu)
       80    0.016    0.000    0.709    0.009 {built-in method torch.relu}
        5    0.002    0.000    0.681    0.136 /torch/_functorch/aot_autograd.py:742(inner)
       60    0.000    0.000    0.668    0.011 /torch/_dynamo/utils.py:1338(&lt;lambda&gt;)
       60    0.001    0.000    0.668    0.011 /torch/_dynamo/utils.py:1379(run_node)
  655/250    0.006    0.000    0.650    0.003 /usr/lib/python3.10/copy.py:259(_reconstruct)
  2405/70    0.020    0.000    0.648    0.009 /usr/lib/python3.10/copy.py:128(deepcopy)
   105/45    0.003    0.000    0.640    0.014 /usr/lib/python3.10/copy.py:227(_deepcopy_dict)
       25    0.001    0.000    0.634    0.025 /torch/_dynamo/utils.py:925(deepcopy_to_fake_tensor)
       25    0.000    0.000    0.633    0.025 /torch/_dynamo/utils.py:927(&lt;lambda&gt;)
     1005    0.013    0.000    0.627    0.001 /torch/_subclasses/fake_tensor.py:1604(wrap_meta_outputs_with_default_device_logic)
       80    0.001    0.000    0.626    0.008 /torch/fx/experimental/proxy_tensor.py:182(track_tensor_tree)
   155/80    0.002    0.000    0.626    0.008 /torch/fx/experimental/proxy_tensor.py:183(wrap_with_proxy)
  525/325    0.022    0.000    0.602    0.002 /torch/_prims_common/wrappers.py:110(_fn)
       50    0.004    0.000    0.600    0.012 /torch/nn/parameter.py:54(__deepcopy__)
      250    0.004    0.000    0.589    0.002 /torch/_subclasses/fake_tensor.py:1799(__torch_function__)
      150    0.002    0.000    0.589    0.004 /torch/fx/experimental/proxy_tensor.py:144(set_meta)
     1430    0.008    0.000    0.583    0.000 /torch/_subclasses/fake_tensor.py:1569(validate_and_convert_non_fake_tensors)
  170/150    0.001    0.000    0.579    0.004 /torch/fx/experimental/proxy_tensor.py:117(extract_val)
      160    0.001    0.000    0.578    0.004 /torch/fx/experimental/proxy_tensor.py:114(snapshot_fake)
     1055    0.022    0.000    0.539    0.001 /torch/_subclasses/fake_tensor.py:1620(wrap)
        5    0.004    0.001    0.525    0.105 /EXPERIMENTAL_EXPERIMENT/torch_exp/graph_builder.py:305(_build_initializers)
  200/150    0.008    0.000    0.523    0.003 /torch/_subclasses/fake_tensor.py:1059(__torch_dispatch__)
       50    0.228    0.005    0.517    0.010 /EXPERIMENTAL_EXPERIMENT/torch_exp/graph_builder.py:270(from_array)
       70    0.003    0.000    0.514    0.007 /torch/fx/graph_module.py:649(recompile)
      400    0.023    0.000    0.486    0.001 {method &#39;to&#39; of &#39;torch._C._TensorBase&#39; objects}
       40    0.001    0.000    0.468    0.012 /torch/nn/modules/conv.py:459(forward)
       40    0.000    0.000    0.467    0.012 /torch/nn/modules/conv.py:451(_conv_forward)
       40    0.025    0.001    0.466    0.012 {built-in method torch.conv2d}
    45715    0.142    0.000    0.461    0.000 &lt;string&gt;:2(__init__)
       70    0.002    0.000    0.460    0.007 /torch/fx/graph.py:1208(python_code)
       70    0.001    0.000    0.435    0.006 /torch/fx/graph.py:1270(_python_code)
       70    0.028    0.000    0.433    0.006 /torch/fx/graph.py:326(_gen_python_code)
    70900    0.151    0.000    0.430    0.000 /torch/utils/_pytree.py:186(_get_node_type)
265050/258585    0.342    0.000    0.428    0.000 {built-in method builtins.isinstance}
        5    0.001    0.000    0.422    0.084 /torch/_dynamo/guards.py:879(__init__)
    45710    0.117    0.000    0.415    0.000 /torch/utils/_pytree.py:192(_is_leaf)
        5    0.000    0.000    0.406    0.081 /onnx/helper.py:278(make_model)
       15    0.000    0.000    0.406    0.027 /google/protobuf/message.py:118(CopyFrom)
       15    0.406    0.027    0.406    0.027 {method &#39;MergeFrom&#39; of &#39;google._upb._message.Message&#39; objects}
       35    0.010    0.000    0.382    0.011 /torch/_dynamo/variables/torch.py:208(call_function)
       50    0.001    0.000    0.381    0.008 /torch/nn/parameter.py:33(__new__)
       40    0.000    0.000    0.362    0.009 /torch/_jit_internal.py:478(fn)
       40    0.001    0.000    0.362    0.009 /torch/nn/functional.py:769(_max_pool2d)
       40    0.010    0.000    0.360    0.009 {built-in method torch.max_pool2d}
      225    0.002    0.000    0.359    0.002 /torch/_decomp/decompositions.py:58(increase_prec)
        1    0.000    0.000    0.359    0.359 /_doc/examples/plot_torch_export.py:83(forward)
       45    0.004    0.000    0.354    0.008 /torch/fx/graph_module.py:318(__init__)
     1430    0.016    0.000    0.348    0.000 /torch/_subclasses/fake_tensor.py:1557(check_for_subclass)
      155    0.333    0.002    0.347    0.002 {method &#39;extend&#39; of &#39;google._upb._message.RepeatedCompositeContainer&#39; objects}
  490/400    0.007    0.000    0.345    0.001 /torch/nn/modules/module.py:1697(__setattr__)
        5    0.001    0.000    0.345    0.069 /torch/_dynamo/guards.py:943(compile_check_fn)
      420    0.006    0.000    0.344    0.001 /torch/fx/proxy.py:170(create_proxy)
        5    0.004    0.001    0.336    0.067 /torch/_dynamo/guards.py:1162(build_guard_function)
       45    0.001    0.000    0.333    0.007 /torch/fx/graph_module.py:416(graph)
        5    0.000    0.000    0.331    0.066 /onnx/helper.py:191(make_graph)
  290/240    0.121    0.000    0.328    0.001 {method &#39;clone&#39; of &#39;torch._C._TensorBase&#39; objects}
     1005    0.012    0.000    0.323    0.000 /torch/_subclasses/fake_tensor.py:1115(_find_common_device)
    45715    0.198    0.000    0.319    0.000 /torch/utils/_pytree.py:207(__post_init__)
        5    0.315    0.063    0.315    0.063 {method &#39;write&#39; of &#39;_io.BufferedWriter&#39; objects}
        1    0.000    0.000    0.310    0.310 &lt;eval_with_key&gt;.504:4(forward)
    20520    0.094    0.000    0.294    0.000 /torch/utils/_pytree.py:223(__init__)
      100    0.003    0.000    0.288    0.003 /torch/_refs/nn/functional/__init__.py:134(_fn)
17415/5695    0.194    0.000    0.283    0.000 /torch/utils/_pytree.py:252(tree_unflatten)
    70900    0.214    0.000    0.279    0.000 /torch/utils/_pytree.py:176(_is_namedtuple_instance)
     6670    0.036    0.000    0.279    0.000 /torch/fx/node.py:632(map_arg)
 5390/330    0.035    0.000    0.267    0.001 /usr/lib/python3.10/ast.py:414(visit)
      225    0.001    0.000    0.260    0.001 /torch/_subclasses/fake_tensor.py:358(__call__)
      225    0.003    0.000    0.258    0.001 /torch/_subclasses/fake_tensor.py:283(from_real_tensor)
      100    0.001    0.000    0.251    0.003 /torch/_refs/nn/functional/__init__.py:246(relu)
      165    0.011    0.000    0.243    0.001 /torch/_subclasses/meta_utils.py:494(__call__)
14150/6675    0.119    0.000    0.236    0.000 /torch/fx/node.py:640(map_aggregate)
      165    0.029    0.000    0.230    0.001 /torch/_subclasses/meta_utils.py:177(meta_tensor)
      430    0.014    0.000    0.211    0.000 /torch/fx/proxy.py:114(create_node)
      250    0.006    0.000    0.202    0.001 /torch/_refs/__init__.py:957(_ref)
       60    0.001    0.000    0.201    0.003 /torch/_refs/__init__.py:4265(t)
       60    0.002    0.000    0.201    0.003 {built-in method torch.transpose}
       10    0.001    0.000    0.199    0.020 /torch/_decomp/decompositions_for_rng.py:129(reset)
       35    0.196    0.006    0.196    0.006 {method &#39;SerializeToString&#39; of &#39;google._upb._message.Message&#39; objects}
15445/14455    0.032    0.000    0.196    0.000 {built-in method builtins.next}
       30    0.000    0.000    0.195    0.007 /torch/_decomp/decompositions_for_rng.py:71(__init__)
       30    0.000    0.000    0.195    0.006 /torch/_decomp/decompositions_for_rng.py:74(reset)
       60    0.012    0.000    0.194    0.003 {built-in method torch.tensor}
       60    0.001    0.000    0.192    0.003 /torch/_refs/__init__.py:4301(transpose)
       60    0.003    0.000    0.189    0.003 {built-in method torch.permute}
     1450    0.028    0.000    0.188    0.000 /torch/fx/graph.py:482(emit_node)
     1055    0.023    0.000    0.185    0.000 /torch/_subclasses/fake_tensor.py:341(from_meta_and_device)
       55    0.185    0.003    0.185    0.003 {method &#39;tobytes&#39; of &#39;numpy.ndarray&#39; objects}
       60    0.001    0.000    0.180    0.003 /torch/_dynamo/symbolic_convert.py:1184(LOAD_ATTR)
       75    0.005    0.000    0.179    0.002 {built-in method torch.mm}
      440    0.009    0.000    0.175    0.000 /torch/fx/graph.py:805(create_node)
       15    0.001    0.000    0.175    0.012 /torch/export/__init__.py:232(__init__)
       15    0.000    0.000    0.174    0.012 /torch/_export/exported_program.py:230(_create_graph_module_for_export)
    70/50    0.020    0.000    0.167    0.003 {built-in method torch._ops.aten.}
      425    0.017    0.000    0.164    0.000 /torch/_prims/__init__.py:331(_elementwise_meta)
       50    0.006    0.000    0.163    0.003 /torch/_subclasses/fake_tensor.py:653(conv)
      110    0.001    0.000    0.163    0.001 /torch/_dynamo/guards.py:1169(replace)
      110    0.002    0.000    0.162    0.001 /torch/_dynamo/guards.py:862(replace)
8925/8365    0.017    0.000    0.155    0.000 /torch/fx/node.py:646(&lt;genexpr&gt;)
      670    0.006    0.000    0.154    0.000 /torch/_dynamo/guards.py:128(_ast_unparse)
      165    0.001    0.000    0.154    0.001 /torch/_subclasses/fake_tensor.py:1702(from_tensor)
      670    0.003    0.000    0.148    0.000 /usr/lib/python3.10/ast.py:1679(unparse)
        5    0.000    0.000    0.147    0.029 /torch/_dynamo/eval_frame.py:806(rewrite_signature)
      670    0.008    0.000    0.143    0.000 /usr/lib/python3.10/ast.py:811(visit)
      425    0.008    0.000    0.143    0.000 /torch/_refs/__init__.py:402(_maybe_broadcast)
        1    0.000    0.000    0.141    0.141 &lt;eval_with_key&gt;.518:4(forward)
       70    0.001    0.000    0.141    0.002 /torch/_dynamo/variables/builder.py:216(__call__)
 1080/480    0.012    0.000    0.140    0.000 /torch/_dynamo/variables/base.py:95(__call__)
</pre></div>
</div>
<p>The following display helps to understand.
Most of the tiume added by the custom converter is used to
converter the initializer and build the onnx model once the conversion
is complete.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># from onnx_array_api.profiling import profile2graph</span>
<span class="c1"># root, nodes = profile2graph(ps, clean_text=clean_text)</span>
<span class="c1"># text = root.to_text()</span>
<span class="c1"># print(text)</span>
</pre></div>
</div>
</section>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">benchmark</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">onnxruntime</span> <span class="kn">import</span> <span class="n">InferenceSession</span><span class="p">,</span> <span class="n">SessionOptions</span><span class="p">,</span> <span class="n">GraphOptimizationLevel</span>

    <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">shape</span></a> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
    <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">confs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <a href="https://docs.python.org/3/library/itertools.html#itertools.product" title="itertools.product" class="sphx-glr-backref-module-itertools sphx-glr-backref-type-py-function"><span class="n">itertools</span><span class="o">.</span><span class="n">product</span></a><span class="p">(</span>
            <span class="p">[</span><span class="n">_</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/os.html#os.listdir" title="os.listdir" class="sphx-glr-backref-module-os sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">listdir</span></a><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;.onnx&quot;</span> <span class="ow">in</span> <span class="n">_</span> <span class="ow">and</span> <span class="n">_</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;plot_torch&quot;</span><span class="p">)],</span>
            <span class="p">[</span>
                <span class="p">[</span><span class="s2">&quot;CPUExecutionProvider&quot;</span><span class="p">],</span>
                <span class="p">[</span><span class="s2">&quot;CUDAExecutionProvider&quot;</span><span class="p">,</span> <span class="s2">&quot;CPUExecutionProvider&quot;</span><span class="p">],</span>
            <span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">],</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">loop</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">confs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;number of experiments: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loop</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <a href="https://docs.python.org/3/library/profile.html#pstats.Stats" title="pstats.Stats" class="sphx-glr-backref-module-pstats sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ps</span></a><span class="p">,</span> <span class="n">aot</span> <span class="ow">in</span> <span class="n">loop</span><span class="p">:</span>
        <span class="n">root</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/os.path.html#os.path.split" title="os.path.split" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span></a><span class="p">(</span><span class="n">name</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ext</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/os.path.html#os.path.splitext" title="os.path.splitext" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span></a><span class="p">(</span><span class="n">root</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ext</span> <span class="o">!=</span> <span class="s2">&quot;.onnx&quot;</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">obs</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># system_info()</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;providers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><a href="https://docs.python.org/3/library/profile.html#pstats.Stats" title="pstats.Stats" class="sphx-glr-backref-module-pstats sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ps</span></a><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="s2">&quot;CUDA&quot;</span> <span class="k">if</span> <span class="s2">&quot;CUDA&quot;</span> <span class="ow">in</span> <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;providers&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;CPU&quot;</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;compute&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;aot&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">aot</span> <span class="o">==</span> <span class="s2">&quot;0&quot;</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;export&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;plot_torch_export_&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.onnx&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

        <a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a> <span class="o">=</span> <a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;n_nodes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">)</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;n_function&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">functions</span> <span class="ow">or</span> <span class="p">[])</span>
        <span class="n">obs</span><span class="p">[</span><span class="s2">&quot;n_sub&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <a href="https://onnx.ai/onnx/api/serialization.html#onnx.ModelProto" title="onnx.ModelProto" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">onx</span></a><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op_type</span> <span class="o">==</span> <span class="s2">&quot;Sub&quot;</span><span class="p">])</span>

        <span class="n">opts</span> <span class="o">=</span> <span class="n">SessionOptions</span><span class="p">()</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">add_session_config_entry</span><span class="p">(</span><span class="s2">&quot;session.disable_aot_function_inlining&quot;</span><span class="p">,</span> <span class="n">aot</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">graph_optimization_level</span> <span class="o">=</span> <span class="n">GraphOptimizationLevel</span><span class="o">.</span><span class="n">ORT_ENABLE_ALL</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">optimized_model_filepath</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;ort-</span><span class="si">{</span><span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">-aot</span><span class="si">{</span><span class="n">aot</span><span class="si">}</span><span class="s2">.onnx&quot;</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">sess</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><a href="https://docs.python.org/3/library/profile.html#pstats.Stats" title="pstats.Stats" class="sphx-glr-backref-module-pstats sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ps</span></a><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR-load: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">obs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="n">e</span><span class="p">,</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="s2">&quot;run&quot;</span><span class="p">})</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="n">input_name</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
        <span class="n">feeds</span> <span class="o">=</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html#numpy.random.rand" title="numpy.random.rand" class="sphx-glr-backref-module-numpy-random sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span></a><span class="p">(</span><span class="o">*</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">shape</span></a><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)}</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">i</span></a> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
                <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">feeds</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR-run: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">obs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="n">e</span><span class="p">,</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="s2">&quot;load&quot;</span><span class="p">})</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="n">obs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">measure_time</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">feeds</span><span class="p">),</span> <span class="n">max_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">obs</span><span class="p">[</span><span class="s1">&#39;average&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><a href="https://docs.python.org/3/library/profile.html#pstats.Stats" title="pstats.Stats" class="sphx-glr-backref-module-pstats sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ps</span></a><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

    <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class"><span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data</span></a><span class="p">)</span>
    <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv" title="pandas.DataFrame.to_csv" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">df</span><span class="o">.</span><span class="n">to_csv</span></a><span class="p">(</span><span class="s2">&quot;benchmark.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel" title="pandas.DataFrame.to_excel" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">df</span><span class="o">.</span><span class="n">to_excel</span></a><span class="p">(</span><span class="s2">&quot;benchmark.xlsx&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a>


<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a> <span class="o">=</span> <span class="n">benchmark</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/24 [00:00&lt;?, ?it/s]number of experiments: 24

0.01435662799999894 plot_torch_export_cus_p1.onnx [&#39;CPUExecutionProvider&#39;]:   0%|          | 0/24 [00:01&lt;?, ?it/s]
0.01435662799999894 plot_torch_export_cus_p1.onnx [&#39;CPUExecutionProvider&#39;]:   4%|▍         | 1/24 [00:01&lt;00:35,  1.57s/it]
0.01708664126982469 plot_torch_export_cus_p1.onnx [&#39;CPUExecutionProvider&#39;]:   4%|▍         | 1/24 [00:03&lt;00:35,  1.57s/it]
0.01708664126982469 plot_torch_export_cus_p1.onnx [&#39;CPUExecutionProvider&#39;]:   8%|▊         | 2/24 [00:03&lt;00:33,  1.51s/it]
0.0015930594377455158 plot_torch_export_cus_p1.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:   8%|▊         | 2/24 [00:15&lt;00:33,  1.51s/it]
0.0015930594377455158 plot_torch_export_cus_p1.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  12%|█▎        | 3/24 [00:15&lt;02:20,  6.70s/it]
0.0015328173027950372 plot_torch_export_cus_p1.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  12%|█▎        | 3/24 [00:17&lt;02:20,  6.70s/it]
0.0015328173027950372 plot_torch_export_cus_p1.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  17%|█▋        | 4/24 [00:17&lt;01:33,  4.67s/it]
0.013948288505695206 plot_torch_export_cus_p2.onnx [&#39;CPUExecutionProvider&#39;]:  17%|█▋        | 4/24 [00:19&lt;01:33,  4.67s/it]
0.013948288505695206 plot_torch_export_cus_p2.onnx [&#39;CPUExecutionProvider&#39;]:  21%|██        | 5/24 [00:19&lt;01:07,  3.57s/it]
0.013323481609111842 plot_torch_export_cus_p2.onnx [&#39;CPUExecutionProvider&#39;]:  21%|██        | 5/24 [00:20&lt;01:07,  3.57s/it]
0.013323481609111842 plot_torch_export_cus_p2.onnx [&#39;CPUExecutionProvider&#39;]:  25%|██▌       | 6/24 [00:20&lt;00:51,  2.89s/it]
0.0017884948914470302 plot_torch_export_cus_p2.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  25%|██▌       | 6/24 [00:22&lt;00:51,  2.89s/it]
0.0017884948914470302 plot_torch_export_cus_p2.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  29%|██▉       | 7/24 [00:22&lt;00:42,  2.50s/it]
0.0022198849673217636 plot_torch_export_cus_p2.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  29%|██▉       | 7/24 [00:24&lt;00:42,  2.50s/it]
0.0022198849673217636 plot_torch_export_cus_p2.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  33%|███▎      | 8/24 [00:24&lt;00:36,  2.26s/it]
0.013479507017556196 plot_torch_export_script.onnx [&#39;CPUExecutionProvider&#39;]:  33%|███▎      | 8/24 [00:26&lt;00:36,  2.26s/it]
0.013479507017556196 plot_torch_export_script.onnx [&#39;CPUExecutionProvider&#39;]:  38%|███▊      | 9/24 [00:26&lt;00:34,  2.31s/it]
0.009387667938926477 plot_torch_export_script.onnx [&#39;CPUExecutionProvider&#39;]:  38%|███▊      | 9/24 [00:28&lt;00:34,  2.31s/it]
0.009387667938926477 plot_torch_export_script.onnx [&#39;CPUExecutionProvider&#39;]:  42%|████▏     | 10/24 [00:28&lt;00:30,  2.16s/it]
0.001605140254237779 plot_torch_export_script.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  42%|████▏     | 10/24 [00:29&lt;00:30,  2.16s/it]
0.001605140254237779 plot_torch_export_script.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  46%|████▌     | 11/24 [00:29&lt;00:25,  1.94s/it]
0.0015587610644205614 plot_torch_export_script.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  46%|████▌     | 11/24 [00:31&lt;00:25,  1.94s/it]
0.0015587610644205614 plot_torch_export_script.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  50%|█████     | 12/24 [00:31&lt;00:21,  1.81s/it]
0.027748971794864748 plot_torch_export_cus_p0.onnx [&#39;CPUExecutionProvider&#39;]:  50%|█████     | 12/24 [00:34&lt;00:21,  1.81s/it]
0.027748971794864748 plot_torch_export_cus_p0.onnx [&#39;CPUExecutionProvider&#39;]:  54%|█████▍    | 13/24 [00:34&lt;00:22,  2.08s/it]
0.023434440000115006 plot_torch_export_cus_p0.onnx [&#39;CPUExecutionProvider&#39;]:  54%|█████▍    | 13/24 [00:36&lt;00:22,  2.08s/it]
0.023434440000115006 plot_torch_export_cus_p0.onnx [&#39;CPUExecutionProvider&#39;]:  58%|█████▊    | 14/24 [00:36&lt;00:22,  2.25s/it]
0.002633516778530393 plot_torch_export_cus_p0.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  58%|█████▊    | 14/24 [00:38&lt;00:22,  2.25s/it]
0.002633516778530393 plot_torch_export_cus_p0.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  62%|██████▎   | 15/24 [00:38&lt;00:18,  2.05s/it]
0.002382198126443533 plot_torch_export_cus_p0.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  62%|██████▎   | 15/24 [00:39&lt;00:18,  2.05s/it]
0.002382198126443533 plot_torch_export_cus_p0.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  67%|██████▋   | 16/24 [00:39&lt;00:15,  1.92s/it]
0.023885112280703124 plot_torch_export_dynopt.onnx [&#39;CPUExecutionProvider&#39;]:  67%|██████▋   | 16/24 [00:42&lt;00:15,  1.92s/it]
0.023885112280703124 plot_torch_export_dynopt.onnx [&#39;CPUExecutionProvider&#39;]:  71%|███████   | 17/24 [00:42&lt;00:14,  2.03s/it]
0.04249741481490438 plot_torch_export_dynopt.onnx [&#39;CPUExecutionProvider&#39;]:  71%|███████   | 17/24 [00:43&lt;00:14,  2.03s/it]
0.04249741481490438 plot_torch_export_dynopt.onnx [&#39;CPUExecutionProvider&#39;]:  75%|███████▌  | 18/24 [00:43&lt;00:11,  1.92s/it]
0.002214792735055828 plot_torch_export_dynopt.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  75%|███████▌  | 18/24 [00:45&lt;00:11,  1.92s/it]
0.002214792735055828 plot_torch_export_dynopt.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  79%|███████▉  | 19/24 [00:45&lt;00:08,  1.79s/it]
0.00425435637859024 plot_torch_export_dynopt.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  79%|███████▉  | 19/24 [00:46&lt;00:08,  1.79s/it]
0.00425435637859024 plot_torch_export_dynopt.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  83%|████████▎ | 20/24 [00:46&lt;00:06,  1.67s/it]
0.051433961904660375 plot_torch_export_dynamo.onnx [&#39;CPUExecutionProvider&#39;]:  83%|████████▎ | 20/24 [00:49&lt;00:06,  1.67s/it]
0.051433961904660375 plot_torch_export_dynamo.onnx [&#39;CPUExecutionProvider&#39;]:  88%|████████▊ | 21/24 [00:49&lt;00:06,  2.11s/it]
0.04570852173903562 plot_torch_export_dynamo.onnx [&#39;CPUExecutionProvider&#39;]:  88%|████████▊ | 21/24 [00:51&lt;00:06,  2.11s/it]
0.04570852173903562 plot_torch_export_dynamo.onnx [&#39;CPUExecutionProvider&#39;]:  92%|█████████▏| 22/24 [00:51&lt;00:04,  2.09s/it]
0.0026424060975708615 plot_torch_export_dynamo.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  92%|█████████▏| 22/24 [00:54&lt;00:04,  2.09s/it]
0.0026424060975708615 plot_torch_export_dynamo.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  96%|█████████▌| 23/24 [00:54&lt;00:02,  2.14s/it]
0.005336299999994697 plot_torch_export_dynamo.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]:  96%|█████████▌| 23/24 [00:55&lt;00:02,  2.14s/it]
0.005336299999994697 plot_torch_export_dynamo.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]: 100%|██████████| 24/24 [00:55&lt;00:00,  2.01s/it]
0.005336299999994697 plot_torch_export_dynamo.onnx [&#39;CUDAExecutionProvider&#39;, &#39;CPUExecutionProvider&#39;]: 100%|██████████| 24/24 [00:55&lt;00:00,  2.33s/it]
                             name  ... warmup_time
0   plot_torch_export_cus_p1.onnx  ...    0.019359
1   plot_torch_export_cus_p1.onnx  ...    0.014290
2   plot_torch_export_cus_p1.onnx  ...    0.002311
3   plot_torch_export_cus_p1.onnx  ...    0.001310
4   plot_torch_export_cus_p2.onnx  ...    0.014692
5   plot_torch_export_cus_p2.onnx  ...    0.014002
6   plot_torch_export_cus_p2.onnx  ...    0.002229
7   plot_torch_export_cus_p2.onnx  ...    0.001721
8   plot_torch_export_script.onnx  ...    0.008847
9   plot_torch_export_script.onnx  ...    0.008325
10  plot_torch_export_script.onnx  ...    0.002055
11  plot_torch_export_script.onnx  ...    0.001376
12  plot_torch_export_cus_p0.onnx  ...    0.027252
13  plot_torch_export_cus_p0.onnx  ...    0.020315
14  plot_torch_export_cus_p0.onnx  ...    0.002522
15  plot_torch_export_cus_p0.onnx  ...    0.002964
16  plot_torch_export_dynopt.onnx  ...    0.034636
17  plot_torch_export_dynopt.onnx  ...    0.049269
18  plot_torch_export_dynopt.onnx  ...    0.002528
19  plot_torch_export_dynopt.onnx  ...    0.005325
20  plot_torch_export_dynamo.onnx  ...    0.085985
21  plot_torch_export_dynamo.onnx  ...    0.063649
22  plot_torch_export_dynamo.onnx  ...    0.001979
23  plot_torch_export_dynamo.onnx  ...    0.004548

[24 rows x 17 columns]
</pre></div>
</div>
<p>Other view</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">piv</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html#pandas.pivot_table" title="pandas.pivot_table" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-function"><span class="n">pandas</span><span class="o">.</span><span class="n">pivot_table</span></a><span class="p">(</span>
    <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">df</span></a><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="s2">&quot;export&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;compute&quot;</span><span class="p">,</span> <span class="s2">&quot;aot&quot;</span><span class="p">],</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;average&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">piv</span></a><span class="p">)</span>

<a href="https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure" title="matplotlib.figure.Figure" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fig</span></a><span class="p">,</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html#matplotlib.axes.Axes" title="matplotlib.axes.Axes" class="sphx-glr-backref-module-matplotlib-axes sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ax</span></a> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">()</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">piv</span></a><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html#matplotlib.axes.Axes" title="matplotlib.axes.Axes" class="sphx-glr-backref-module-matplotlib-axes sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ax</span></a><span class="o">=</span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html#matplotlib.axes.Axes" title="matplotlib.axes.Axes" class="sphx-glr-backref-module-matplotlib-axes sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ax</span></a><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Compares onnxruntime time on exported models&quot;</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.tight_layout" title="matplotlib.figure.Figure.tight_layout" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-method"><span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span></a><span class="p">()</span>
<a href="https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.savefig" title="matplotlib.figure.Figure.savefig" class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-method"><span class="n">fig</span><span class="o">.</span><span class="n">savefig</span></a><span class="p">(</span><span class="s2">&quot;plot_torch_export_ort.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_torch_export_002.png" srcset="../_images/sphx_glr_plot_torch_export_002.png" alt="Compares onnxruntime time on exported models" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>compute       CPU                CUDA
aot             0         1         0         1
export
cus_p0   0.023434  0.027749  0.002382  0.002634
cus_p1   0.017087  0.014357  0.001533  0.001593
cus_p2   0.013323  0.013948  0.002220  0.001788
dynamo   0.045709  0.051434  0.005336  0.002642
dynopt   0.042497  0.023885  0.004254  0.002215
script   0.009388  0.013480  0.001559  0.001605
</pre></div>
</div>
</section>
<section id="show-the-interesting-models-for-cpu">
<h2>Show the interesting models for CPU<a class="headerlink" href="#show-the-interesting-models-for-cpu" title="Permalink to this heading">#</a></h2>
<section id="script">
<h3>script<a class="headerlink" href="#script" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_cus_p2-cpu-aot0.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;&#39; version=18
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;input&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;reorder&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;arg1_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;reorder_token_11&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;arg3_1&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;arg5_1&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;arg7_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;arg9_1&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;ortshared_7_1_2_0_token_8&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;permute&#39; type=dtype(&#39;float32&#39;) shape=(13456, 1024)
init: name=&#39;permute_1&#39; type=dtype(&#39;float32&#39;) shape=(1024, 128)
init: name=&#39;permute_2&#39; type=dtype(&#39;float32&#39;) shape=(128, 10)
Conv[com.microsoft.nchwc](input, reorder, arg1_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; reorder_token_10
  ReorderOutput[com.microsoft.nchwc](reorder_token_10, channels_last=0, channels=128) -&gt; relu
    MaxPool(relu, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool0, _onx_maxpool1
      ReorderInput[com.microsoft.nchwc](_onx_maxpool0, channels_last=0) -&gt; reorder_token_12
        Conv[com.microsoft.nchwc](reorder_token_12, reorder_token_11, arg3_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; reorder_token_13
          ReorderOutput[com.microsoft.nchwc](reorder_token_13, channels_last=0, channels=16) -&gt; relu_1
            MaxPool(relu_1, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool03, _onx_maxpool13
              Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -&gt; view
                FusedGemm[com.microsoft](view, permute, arg5_1, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_2
                  FusedGemm[com.microsoft](relu_2, permute_1, arg7_1, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_3
                    Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; output
output: name=&#39;output&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
</pre></div>
</div>
</section>
<section id="cus-p2">
<h3>cus_p2<a class="headerlink" href="#cus-p2" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_cus_p2-cpu-aot0.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;&#39; version=18
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;input&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;reorder&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;arg1_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;reorder_token_11&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;arg3_1&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;arg5_1&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;arg7_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;arg9_1&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;ortshared_7_1_2_0_token_8&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;permute&#39; type=dtype(&#39;float32&#39;) shape=(13456, 1024)
init: name=&#39;permute_1&#39; type=dtype(&#39;float32&#39;) shape=(1024, 128)
init: name=&#39;permute_2&#39; type=dtype(&#39;float32&#39;) shape=(128, 10)
Conv[com.microsoft.nchwc](input, reorder, arg1_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; reorder_token_10
  ReorderOutput[com.microsoft.nchwc](reorder_token_10, channels_last=0, channels=128) -&gt; relu
    MaxPool(relu, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool0, _onx_maxpool1
      ReorderInput[com.microsoft.nchwc](_onx_maxpool0, channels_last=0) -&gt; reorder_token_12
        Conv[com.microsoft.nchwc](reorder_token_12, reorder_token_11, arg3_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; reorder_token_13
          ReorderOutput[com.microsoft.nchwc](reorder_token_13, channels_last=0, channels=16) -&gt; relu_1
            MaxPool(relu_1, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool03, _onx_maxpool13
              Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -&gt; view
                FusedGemm[com.microsoft](view, permute, arg5_1, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_2
                  FusedGemm[com.microsoft](relu_2, permute_1, arg7_1, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_3
                    Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; output
output: name=&#39;output&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
</pre></div>
</div>
</section>
<section id="dynopt">
<h3>dynopt<a class="headerlink" href="#dynopt" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_dynopt-cpu-aot1.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
opset: domain=&#39;pkg.torch.2.1.1+cu118&#39; version=1
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;arg0&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;reorder_token_11&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;conv1.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;reorder&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;conv2.bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;fc1.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 13456)
init: name=&#39;fc1.bias&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;_inlfunc_torch_nn_modules_linear_Linear_fc3_1|folded_0|folded_0_t_2&#39; type=dtype(&#39;float32&#39;) shape=(128, 10)
init: name=&#39;fc2.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;_inlfunc_torch_nn_modules_linear_Linear_fc2_1|folded_0|folded_0_t_1&#39; type=dtype(&#39;float32&#39;) shape=(1024, 128)
init: name=&#39;fc3.bias&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;_inlfunc_aten_view|folded_0|folded_0_size_0&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_0&#39; type=dtype(&#39;int64&#39;) shape=() -- array([4])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
Cast(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank_cast
Transpose(fc1.weight, perm=[1,0]) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc1_1|folded_0|folded_0_t
Cast(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank_cast
Conv[com.microsoft.nchwc](arg0, reorder, conv1.bias, auto_pad=b&#39;NOTSET&#39;, pads=[0,0,0,0], strides=[1,1], group=1, dilations=[1,1]) -&gt; reorder_token_10
  ReorderOutput[com.microsoft.nchwc](reorder_token_10, channels_last=0, channels=128) -&gt; _inlfunc__aten_convolution_onnx|folded_0|folded_0_result_7
    Identity(_inlfunc__aten_convolution_onnx|folded_0|folded_0_result_7) -&gt; conv1_1
      Relu(conv1_1) -&gt; relu
        Shape(relu, start=0) -&gt; _inlfunc_Rank|folded_2|folded_0_tmp
          Size(_inlfunc_Rank|folded_2|folded_0_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_tmp
  Equal(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_tmp, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank
    If(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank, else_branch=G1, then_branch=G2) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_2
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0__, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_flatten_indices
        Slice(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_starts, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_ends, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_delta
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_pool_result, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_indices
        Sub(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_indices_3
    If(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank, else_branch=G3, then_branch=G4) -&gt; max_pool2d_with_indices_1.1, max_pool2d_with_indices
      Shape(max_pool2d_with_indices, start=0) -&gt; _inlfunc_Rank|folded_3|folded_0_tmp
        Size(_inlfunc_Rank|folded_3|folded_0_tmp) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp
          Equal(_inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp, _inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_0) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_1
            Not(_inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_1) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch
              If(_inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch, else_branch=G5, then_branch=G6) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_input_5
                ReorderInput[com.microsoft.nchwc](_inlfunc__aten_convolution_onnx|folded_1|folded_0_input_5, channels_last=0) -&gt; reorder_token_12
                  Conv[com.microsoft.nchwc](reorder_token_12, reorder_token_11, conv2.bias, auto_pad=b&#39;NOTSET&#39;, pads=[0,0,0,0], strides=[1,1], group=1, dilations=[1,1]) -&gt; reorder_token_13
                    ReorderOutput[com.microsoft.nchwc](reorder_token_13, channels_last=0, channels=16) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_result_7
              If(_inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch, else_branch=G7, then_branch=G8) -&gt; conv2_1
                Relu(conv2_1) -&gt; relu_1
                  Shape(relu_1, start=0) -&gt; _inlfunc_Rank|folded_5|folded_0_tmp
                    Size(_inlfunc_Rank|folded_5|folded_0_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_tmp
  Equal(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_tmp, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank
    If(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank, else_branch=G9, then_branch=G10) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_2
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0__, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_flatten_indices
        Slice(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_starts, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_ends, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_delta
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_pool_result, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_indices
        Sub(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_indices_3
    If(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank, else_branch=G11, then_branch=G12) -&gt; max_pool2d_with_indices_1_1, max_pool2d_with_indices_1
      Reshape(max_pool2d_with_indices_1, _inlfunc_aten_view|folded_0|folded_0_size_0, allowzero=0) -&gt; view
  FusedGemm[com.microsoft](view, _inlfunc_torch_nn_modules_linear_Linear_fc1_1|folded_0|folded_0_t, fc1.bias, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_2
    FusedGemm[com.microsoft](relu_2, _inlfunc_torch_nn_modules_linear_Linear_fc2_1|folded_0|folded_0_t_1, fc2.bias, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_3
      Gemm(relu_3, _inlfunc_torch_nn_modules_linear_Linear_fc3_1|folded_0|folded_0_t_2, fc3.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc3_1
output: name=&#39;fc3_1&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
----- function name=_aten_convolution_onnx|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: ConvXd with attributes pre-computed to fit the ONNX spec.
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
input: &#39;transposed&#39;
attribute: &#39;strides&#39;
attribute: &#39;pads&#39;
attribute: &#39;dilations&#39;
Conv(input, weight, bias, dilations=$dilations, group=$groups, pads=$pads, strides=$strides) -&gt; result_7
  Identity(result_7) -&gt; result_11
output: name=&#39;result_11&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv1_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;arg0&#39;
input: &#39;conv1.weight&#39;
input: &#39;conv1.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx|folded_0|folded_0[pkg.onnxscript.torch_lib](arg0, conv1.weight, conv1.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution
output: name=&#39;convolution&#39; type=? shape=?
----- function name=aten_relu|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=Rank|folded_2|folded_0 domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_max_pool_with_indices_onnx|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
attribute: &#39;kernel_size&#39;
attribute: &#39;stride&#39;
attribute: &#39;padding&#39;
attribute: &#39;dilation&#39;
attribute: &#39;ceil_mode&#39;
attribute: &#39;unbatched_rank&#39;
attribute: &#39;n_dims_one&#39;
attribute: &#39;n_dims_zero&#39;
attribute: &#39;n_dims_axes&#39;
Constant(value_int=$unbatched_rank) -&gt; unbatched_rank
Rank|folded_2|folded_0[pkg.onnxscript.torch_lib.common](self) -&gt; tmp
  CastLike(unbatched_rank, tmp) -&gt; unbatched_rank_cast
  Equal(tmp, unbatched_rank_cast) -&gt; self_rank_is_unbatched_rank
    If(self_rank_is_unbatched_rank, then_branch=G13, else_branch=G14) -&gt; self_2
      MaxPool(self_2, ceil_mode=$ceil_mode, dilations=$dilation, kernel_shape=$kernel_size, pads=$padding, strides=$stride) -&gt; pool_result, indices
      MaxPool(self_2, dilations=$dilation, kernel_shape=$n_dims_one, strides=$n_dims_one) -&gt; _, flatten_indices
Constant(value_ints=$n_dims_one) -&gt; ends
Constant(value_ints=$n_dims_zero) -&gt; starts
Constant(value_ints=$n_dims_axes) -&gt; axes
  Slice(flatten_indices, starts, ends, axes) -&gt; delta
    Sub(indices, delta) -&gt; indices_3
If(self_rank_is_unbatched_rank, then_branch=G15, else_branch=G16) -&gt; indices_10, pool_result_11
output: name=&#39;pool_result_11&#39; type=? shape=?
output: name=&#39;indices_10&#39; type=? shape=?
----- function name=Rank|folded_3|folded_0 domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_convolution_onnx|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: ConvXd with attributes pre-computed to fit the ONNX spec.
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
input: &#39;transposed&#39;
attribute: &#39;strides&#39;
attribute: &#39;pads&#39;
attribute: &#39;dilations&#39;
Constant(value=4) -&gt; tmp_0
Rank|folded_3|folded_0[pkg.onnxscript.torch_lib.common](input) -&gt; tmp
  Equal(tmp, tmp_0) -&gt; tmp_1
    Not(tmp_1) -&gt; no_batch
      If(no_batch, then_branch=G17, else_branch=G18) -&gt; input_5
        Conv(input_5, weight, bias, dilations=$dilations, group=$groups, pads=$pads, strides=$strides) -&gt; result_7
      If(no_batch, then_branch=G19, else_branch=G20) -&gt; result_11
output: name=&#39;result_11&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv2_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;getitem&#39;
input: &#39;conv2.weight&#39;
input: &#39;conv2.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx|folded_1|folded_0[pkg.onnxscript.torch_lib](getitem, conv2.weight, conv2.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution_1
output: name=&#39;convolution_1&#39; type=? shape=?
----- function name=aten_relu|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=Rank|folded_5|folded_0 domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_max_pool_with_indices_onnx|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
attribute: &#39;kernel_size&#39;
attribute: &#39;stride&#39;
attribute: &#39;padding&#39;
attribute: &#39;dilation&#39;
attribute: &#39;ceil_mode&#39;
attribute: &#39;unbatched_rank&#39;
attribute: &#39;n_dims_one&#39;
attribute: &#39;n_dims_zero&#39;
attribute: &#39;n_dims_axes&#39;
Constant(value_int=$unbatched_rank) -&gt; unbatched_rank
Rank|folded_5|folded_0[pkg.onnxscript.torch_lib.common](self) -&gt; tmp
  CastLike(unbatched_rank, tmp) -&gt; unbatched_rank_cast
  Equal(tmp, unbatched_rank_cast) -&gt; self_rank_is_unbatched_rank
    If(self_rank_is_unbatched_rank, then_branch=G21, else_branch=G22) -&gt; self_2
      MaxPool(self_2, ceil_mode=$ceil_mode, dilations=$dilation, kernel_shape=$kernel_size, pads=$padding, strides=$stride) -&gt; pool_result, indices
      MaxPool(self_2, dilations=$dilation, kernel_shape=$n_dims_one, strides=$n_dims_one) -&gt; _, flatten_indices
Constant(value_ints=$n_dims_one) -&gt; ends
Constant(value_ints=$n_dims_zero) -&gt; starts
Constant(value_ints=$n_dims_axes) -&gt; axes
  Slice(flatten_indices, starts, ends, axes) -&gt; delta
    Sub(indices, delta) -&gt; indices_3
If(self_rank_is_unbatched_rank, then_branch=G23, else_branch=G24) -&gt; indices_10, pool_result_11
output: name=&#39;pool_result_11&#39; type=? shape=?
output: name=&#39;indices_10&#39; type=? shape=?
----- function name=aten_view|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;size&#39;
Constant(value=[1, 13456]) -&gt; size_0
  Reshape(self, size_0) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=aten_t|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: t(Tensor(a) self) -&gt; Tensor(a)
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Transpose(self, perm=[1,0]) -&gt; result_1
output: name=&#39;result_1&#39; type=? shape=?
----- function name=aten_addmm|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc1_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;view&#39;
input: &#39;fc1.weight&#39;
input: &#39;fc1.bias&#39;
aten_t|folded_0|folded_0[pkg.onnxscript.torch_lib](fc1.weight) -&gt; t
  aten_addmm|folded_0|folded_0[pkg.onnxscript.torch_lib](fc1.bias, view, t, alpha=1.00, beta=1.00) -&gt; addmm
output: name=&#39;addmm&#39; type=? shape=?
----- function name=aten_relu|folded_2|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=aten_addmm|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc2_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_2&#39;
input: &#39;fc2.weight&#39;
input: &#39;fc2.bias&#39;
Constant(value=[[0.021504...) -&gt; t_1
  aten_addmm|folded_1|folded_0[pkg.onnxscript.torch_lib](fc2.bias, relu_2, t_1, alpha=1.00, beta=1.00) -&gt; addmm_1
output: name=&#39;addmm_1&#39; type=? shape=?
----- function name=aten_relu|folded_3|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=aten_addmm|folded_2|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc3_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_3&#39;
input: &#39;fc3.weight&#39;
input: &#39;fc3.bias&#39;
Constant(value=[[0.000341...) -&gt; t_2
  aten_addmm|folded_2|folded_0[pkg.onnxscript.torch_lib](fc3.bias, relu_3, t_2, alpha=1.00, beta=1.00) -&gt; addmm_2
output: name=&#39;addmm_2&#39; type=? shape=?
</pre></div>
</div>
</section>
<section id="dynamo">
<h3>dynamo<a class="headerlink" href="#dynamo" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_dynamo-cpu-aot1.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
opset: domain=&#39;pkg.torch.2.1.1+cu118&#39; version=1
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;arg0&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;conv1.weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;conv1.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;conv2.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;conv2.bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;fc1.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 13456)
init: name=&#39;fc1.bias&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;fc2.weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 1024)
init: name=&#39;fc2.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;fc3.weight&#39; type=dtype(&#39;float32&#39;) shape=(10, 128)
init: name=&#39;fc3.bias&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;ortshared_7_1_2_0_token_8&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;_inlfunc_torch_nn_modules_conv_Conv2d_conv1_1__val_3&#39; type=dtype(&#39;bool&#39;) shape=() -- array([False])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc_torch_nn_modules_conv_Conv2d_conv2_1__val_3&#39; type=dtype(&#39;bool&#39;) shape=() -- array([False])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc_aten_t_int64_2&#39; type=dtype(&#39;int64&#39;) shape=() -- array([2])
init: name=&#39;_inlfunc_aten_t_token_16_int64_2&#39; type=dtype(&#39;int64&#39;) shape=() -- array([2])
init: name=&#39;_inlfunc_aten_t_token_18_int64_2&#39; type=dtype(&#39;int64&#39;) shape=() -- array([2])
Cast(_inlfunc_aten_t_int64_2, to=7) -&gt; _inlfunc_aten_t_int64_2_cast
Shape(fc1.weight, start=0) -&gt; _inlfunc_Rank_token_24_tmp
  Size(_inlfunc_Rank_token_24_tmp) -&gt; _inlfunc_aten_t_rank
  Equal(_inlfunc_aten_t_rank, _inlfunc_aten_t_int64_2_cast) -&gt; _inlfunc_aten_t_cond
    If(_inlfunc_aten_t_cond, else_branch=G1, then_branch=G2) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc1_1_t
Cast(ortshared_7_1_2_0_token_8, to=7) -&gt; _inlfunc_aten_view_size_0
Cast(_inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank_cast
Shape(conv2.weight, start=0) -&gt; _inlfunc_Rank_token_23_tmp
  Size(_inlfunc_Rank_token_23_tmp) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp_0
Cast(_inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank_cast
Shape(conv1.weight, start=0) -&gt; _inlfunc_Rank_token_21_tmp
  Size(_inlfunc_Rank_token_21_tmp) -&gt; _inlfunc__aten_convolution_onnx_tmp_0
Shape(arg0, start=0) -&gt; _inlfunc_Rank_token_20_tmp
  Size(_inlfunc_Rank_token_20_tmp) -&gt; _inlfunc__aten_convolution_onnx_tmp
    Equal(_inlfunc__aten_convolution_onnx_tmp, _inlfunc__aten_convolution_onnx_tmp_0) -&gt; _inlfunc__aten_convolution_onnx_tmp_1
      Not(_inlfunc__aten_convolution_onnx_tmp_1) -&gt; _inlfunc__aten_convolution_onnx_no_batch
        If(_inlfunc__aten_convolution_onnx_no_batch, else_branch=G3, then_branch=G4) -&gt; _inlfunc__aten_convolution_onnx_input_5
If(_inlfunc_torch_nn_modules_conv_Conv2d_conv1_1__val_3, else_branch=G5, then_branch=G6) -&gt; _inlfunc__aten_convolution_onnx_result_7
If(_inlfunc__aten_convolution_onnx_no_batch, else_branch=G7, then_branch=G8) -&gt; conv1_1
  Relu(conv1_1) -&gt; relu
    Shape(relu, start=0) -&gt; _inlfunc_Rank_tmp
      Size(_inlfunc_Rank_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_tmp
  Equal(_inlfunc__aten_max_pool_with_indices_onnx_tmp, _inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank
    If(_inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank, else_branch=G9, then_branch=G10) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_self_2
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx__, _inlfunc__aten_max_pool_with_indices_onnx_flatten_indices
        Slice(_inlfunc__aten_max_pool_with_indices_onnx_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx_starts, _inlfunc__aten_max_pool_with_indices_onnx_ends, _inlfunc__aten_max_pool_with_indices_onnx_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_delta
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_pool_result, _inlfunc__aten_max_pool_with_indices_onnx_indices
        Sub(_inlfunc__aten_max_pool_with_indices_onnx_indices, _inlfunc__aten_max_pool_with_indices_onnx_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_indices_3
    If(_inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank, else_branch=G11, then_branch=G12) -&gt; max_pool2d_with_indices_1.1, max_pool2d_with_indices
      Shape(max_pool2d_with_indices, start=0) -&gt; _inlfunc_Rank_token_22_tmp
        Size(_inlfunc_Rank_token_22_tmp) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp
    Equal(_inlfunc__aten_convolution_onnx_token_13_tmp, _inlfunc__aten_convolution_onnx_token_13_tmp_0) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp_1
      Not(_inlfunc__aten_convolution_onnx_token_13_tmp_1) -&gt; _inlfunc__aten_convolution_onnx_token_13_no_batch
        If(_inlfunc__aten_convolution_onnx_token_13_no_batch, else_branch=G13, then_branch=G14) -&gt; _inlfunc__aten_convolution_onnx_token_13_input_5
If(_inlfunc_torch_nn_modules_conv_Conv2d_conv2_1__val_3, else_branch=G15, then_branch=G16) -&gt; _inlfunc__aten_convolution_onnx_token_13_result_7
If(_inlfunc__aten_convolution_onnx_token_13_no_batch, else_branch=G17, then_branch=G18) -&gt; conv2_1
  Relu(conv2_1) -&gt; relu_1
    Shape(relu_1, start=0) -&gt; _inlfunc_Rank_token_14_tmp
      Size(_inlfunc_Rank_token_14_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_tmp
  Equal(_inlfunc__aten_max_pool_with_indices_onnx_token_10_tmp, _inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank
    If(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank, else_branch=G19, then_branch=G20) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_self_2
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10__, _inlfunc__aten_max_pool_with_indices_onnx_token_10_flatten_indices
        Slice(_inlfunc__aten_max_pool_with_indices_onnx_token_10_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx_token_10_starts, _inlfunc__aten_max_pool_with_indices_onnx_token_10_ends, _inlfunc__aten_max_pool_with_indices_onnx_token_10_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_delta
      MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_pool_result, _inlfunc__aten_max_pool_with_indices_onnx_token_10_indices
        Sub(_inlfunc__aten_max_pool_with_indices_onnx_token_10_indices, _inlfunc__aten_max_pool_with_indices_onnx_token_10_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_indices_3
    If(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank, else_branch=G21, then_branch=G22) -&gt; max_pool2d_with_indices_1_1, max_pool2d_with_indices_1
  Reshape(max_pool2d_with_indices_1, _inlfunc_aten_view_size_0, allowzero=0) -&gt; view
    FusedGemm[com.microsoft](view, _inlfunc_torch_nn_modules_linear_Linear_fc1_1_t, fc1.bias, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_2
Cast(_inlfunc_aten_t_token_16_int64_2, to=7) -&gt; _inlfunc_aten_t_token_16_int64_2_cast
Shape(fc2.weight, start=0) -&gt; _inlfunc_Rank_token_26_tmp
  Size(_inlfunc_Rank_token_26_tmp) -&gt; _inlfunc_aten_t_token_16_rank
  Equal(_inlfunc_aten_t_token_16_rank, _inlfunc_aten_t_token_16_int64_2_cast) -&gt; _inlfunc_aten_t_token_16_cond
    If(_inlfunc_aten_t_token_16_cond, else_branch=G23, then_branch=G24) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc2_1_t_1
      FusedGemm[com.microsoft](relu_2, _inlfunc_torch_nn_modules_linear_Linear_fc2_1_t_1, fc2.bias, activation=b&#39;Relu&#39;, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; relu_3
Cast(_inlfunc_aten_t_token_18_int64_2, to=7) -&gt; _inlfunc_aten_t_token_18_int64_2_cast
Shape(fc3.weight, start=0) -&gt; _inlfunc_Rank_token_28_tmp
  Size(_inlfunc_Rank_token_28_tmp) -&gt; _inlfunc_aten_t_token_18_rank
  Equal(_inlfunc_aten_t_token_18_rank, _inlfunc_aten_t_token_18_int64_2_cast) -&gt; _inlfunc_aten_t_token_18_cond
    If(_inlfunc_aten_t_token_18_cond, else_branch=G25, then_branch=G26) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc3_1_t_2
      Gemm(relu_3, _inlfunc_torch_nn_modules_linear_Linear_fc3_1_t_2, fc3.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc3_1
output: name=&#39;fc3_1&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
----- function name=_aten_convolution_onnx domain=pkg.onnxscript.torch_lib
----- doc_string: ConvXd with attributes pre-computed to fit the ONNX spec.
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
input: &#39;transposed&#39;
attribute: &#39;strides&#39;
attribute: &#39;pads&#39;
attribute: &#39;dilations&#39;
Rank[pkg.onnxscript.torch_lib.common](input) -&gt; tmp
Rank[pkg.onnxscript.torch_lib.common](weight) -&gt; tmp_0
  Equal(tmp, tmp_0) -&gt; tmp_1
    Not(tmp_1) -&gt; no_batch
      If(no_batch, then_branch=G27, else_branch=G28) -&gt; input_5
If(transposed, then_branch=G29, else_branch=G30) -&gt; result_7
If(no_batch, then_branch=G31, else_branch=G32) -&gt; result_11
output: name=&#39;result_11&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv1_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;arg0&#39;
input: &#39;conv1.weight&#39;
input: &#39;conv1.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx[pkg.onnxscript.torch_lib](arg0, conv1.weight, conv1.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution
output: name=&#39;convolution&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv2_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;getitem&#39;
input: &#39;conv2.weight&#39;
input: &#39;conv2.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx[pkg.onnxscript.torch_lib](getitem, conv2.weight, conv2.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution_1
output: name=&#39;convolution_1&#39; type=? shape=?
----- function name=aten_t domain=pkg.onnxscript.torch_lib
----- doc_string: t(Tensor(a) self) -&gt; Tensor(a)
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Constant(value=2) -&gt; int64_2
Rank[pkg.onnxscript.torch_lib.common](self) -&gt; rank
  CastLike(int64_2, rank) -&gt; int64_2_cast
  Equal(rank, int64_2_cast) -&gt; cond
    If(cond, then_branch=G28, else_branch=G33) -&gt; result_1
output: name=&#39;result_1&#39; type=? shape=?
----- function name=aten_addmm domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc1_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;view&#39;
input: &#39;fc1.weight&#39;
input: &#39;fc1.bias&#39;
aten_t[pkg.onnxscript.torch_lib](fc1.weight) -&gt; t
  aten_addmm[pkg.onnxscript.torch_lib](fc1.bias, view, t, alpha=1.00, beta=1.00) -&gt; addmm
output: name=&#39;addmm&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc2_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_2&#39;
input: &#39;fc2.weight&#39;
input: &#39;fc2.bias&#39;
aten_t[pkg.onnxscript.torch_lib](fc2.weight) -&gt; t_1
  aten_addmm[pkg.onnxscript.torch_lib](fc2.bias, relu_2, t_1, alpha=1.00, beta=1.00) -&gt; addmm_1
output: name=&#39;addmm_1&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc3_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_3&#39;
input: &#39;fc3.weight&#39;
input: &#39;fc3.bias&#39;
aten_t[pkg.onnxscript.torch_lib](fc3.weight) -&gt; t_2
  aten_addmm[pkg.onnxscript.torch_lib](fc3.bias, relu_3, t_2, alpha=1.00, beta=1.00) -&gt; addmm_2
output: name=&#39;addmm_2&#39; type=? shape=?
----- function name=aten_relu domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_max_pool_with_indices_onnx domain=pkg.onnxscript.torch_lib
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
attribute: &#39;kernel_size&#39;
attribute: &#39;stride&#39;
attribute: &#39;padding&#39;
attribute: &#39;dilation&#39;
attribute: &#39;ceil_mode&#39;
attribute: &#39;unbatched_rank&#39;
attribute: &#39;n_dims_one&#39;
attribute: &#39;n_dims_zero&#39;
attribute: &#39;n_dims_axes&#39;
Constant(value_int=$unbatched_rank) -&gt; unbatched_rank
Rank[pkg.onnxscript.torch_lib.common](self) -&gt; tmp
  CastLike(unbatched_rank, tmp) -&gt; unbatched_rank_cast
  Equal(tmp, unbatched_rank_cast) -&gt; self_rank_is_unbatched_rank
    If(self_rank_is_unbatched_rank, then_branch=G34, else_branch=G35) -&gt; self_2
      MaxPool(self_2, ceil_mode=$ceil_mode, dilations=$dilation, kernel_shape=$kernel_size, pads=$padding, strides=$stride) -&gt; pool_result, indices
      MaxPool(self_2, dilations=$dilation, kernel_shape=$n_dims_one, strides=$n_dims_one) -&gt; _, flatten_indices
Constant(value_ints=$n_dims_one) -&gt; ends
Constant(value_ints=$n_dims_zero) -&gt; starts
Constant(value_ints=$n_dims_axes) -&gt; axes
  Slice(flatten_indices, starts, ends, axes) -&gt; delta
    Sub(indices, delta) -&gt; indices_3
If(self_rank_is_unbatched_rank, then_branch=G36, else_branch=G37) -&gt; indices_10, pool_result_11
output: name=&#39;pool_result_11&#39; type=? shape=?
output: name=&#39;indices_10&#39; type=? shape=?
----- function name=aten_view domain=pkg.onnxscript.torch_lib
----- doc_string: view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;size&#39;
Cast(size, to=7) -&gt; size_0
  Reshape(self, size_0) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=Rank domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=IsScalar domain=pkg.onnxscript.torch_lib.common
----- doc_string: Return whether the input has rank 0, or is a scalar.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Constant(value_int=0) -&gt; tmp_1
Shape(input) -&gt; tmp
  Size(tmp) -&gt; tmp_0
  Equal(tmp_0, tmp_1) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
</pre></div>
</div>
</section>
</section>
<section id="show-the-interesting-models-for-cuda">
<h2>Show the interesting models for CUDA<a class="headerlink" href="#show-the-interesting-models-for-cuda" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3>script<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_cus_p2-cuda-aot0.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;&#39; version=18
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;input&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;arg0_1&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;arg1_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;arg2_1&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;arg3_1&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;arg5_1&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;arg7_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;arg9_1&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;ortshared_7_1_2_0_token_8&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;permute&#39; type=dtype(&#39;float32&#39;) shape=(13456, 1024)
init: name=&#39;permute_1&#39; type=dtype(&#39;float32&#39;) shape=(1024, 128)
init: name=&#39;permute_2&#39; type=dtype(&#39;float32&#39;) shape=(128, 10)
FusedConv[com.microsoft](input, arg0_1, arg1_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; relu
  MaxPool(relu, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool0, _onx_maxpool1
    FusedConv[com.microsoft](_onx_maxpool0, arg2_1, arg3_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; relu_1
      MaxPool(relu_1, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool03, _onx_maxpool13
        Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -&gt; view
          Gemm(view, permute, arg5_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; addmm
            Relu(addmm) -&gt; relu_2
              Gemm(relu_2, permute_1, arg7_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; addmm_1
                Relu(addmm_1) -&gt; relu_3
                  Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; output
output: name=&#39;output&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
</pre></div>
</div>
</section>
<section id="id2">
<h3>cus_p2<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_cus_p2-cuda-aot0.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;&#39; version=18
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;input&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;arg0_1&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;arg1_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;arg2_1&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;arg3_1&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;arg5_1&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;arg7_1&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;arg9_1&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;ortshared_7_1_2_0_token_8&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;permute&#39; type=dtype(&#39;float32&#39;) shape=(13456, 1024)
init: name=&#39;permute_1&#39; type=dtype(&#39;float32&#39;) shape=(1024, 128)
init: name=&#39;permute_2&#39; type=dtype(&#39;float32&#39;) shape=(128, 10)
FusedConv[com.microsoft](input, arg0_1, arg1_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; relu
  MaxPool(relu, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool0, _onx_maxpool1
    FusedConv[com.microsoft](_onx_maxpool0, arg2_1, arg3_1, activation=b&#39;Relu&#39;, dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b&#39;NOTSET&#39;) -&gt; relu_1
      MaxPool(relu_1, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _onx_maxpool03, _onx_maxpool13
        Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -&gt; view
          Gemm(view, permute, arg5_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; addmm
            Relu(addmm) -&gt; relu_2
              Gemm(relu_2, permute_1, arg7_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; addmm_1
                Relu(addmm_1) -&gt; relu_3
                  Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; output
output: name=&#39;output&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
</pre></div>
</div>
</section>
<section id="id3">
<h3>dynopt<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_dynopt-cuda-aot1.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
opset: domain=&#39;pkg.torch.2.1.1+cu118&#39; version=1
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;arg0&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;conv1.weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;conv1.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;conv2.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;conv2.bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;fc1.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 13456)
init: name=&#39;fc1.bias&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;_inlfunc_torch_nn_modules_linear_Linear_fc3_1|folded_0|folded_0_t_2&#39; type=dtype(&#39;float32&#39;) shape=(128, 10)
init: name=&#39;fc2.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;_inlfunc_torch_nn_modules_linear_Linear_fc2_1|folded_0|folded_0_t_1&#39; type=dtype(&#39;float32&#39;) shape=(1024, 128)
init: name=&#39;fc3.bias&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;_inlfunc_aten_view|folded_0|folded_0_size_0&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_0&#39; type=dtype(&#39;int64&#39;) shape=() -- array([4])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
Cast(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank_cast
Transpose(fc1.weight, perm=[1,0]) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc1_1|folded_0|folded_0_t
Conv(arg0, conv1.weight, conv1.bias, auto_pad=b&#39;NOTSET&#39;, pads=[0,0,0,0], strides=[1,1], group=1, dilations=[1,1]) -&gt; _inlfunc__aten_convolution_onnx|folded_0|folded_0_result_7
  Identity(_inlfunc__aten_convolution_onnx|folded_0|folded_0_result_7) -&gt; conv1_1
    Relu(conv1_1) -&gt; relu
      Shape(relu, start=0) -&gt; _inlfunc_Rank|folded_2|folded_0_tmp
        Size(_inlfunc_Rank|folded_2|folded_0_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_tmp
          MemcpyFromHost(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_tmp_CUDAExecutionProvider
  Equal(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_tmp_CUDAExecutionProvider, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank_CUDAExecutionProvider
    MemcpyToHost(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank_CUDAExecutionProvider) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank
      If(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank, else_branch=G1, then_branch=G2) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_2
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0__, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_flatten_indices
          Slice(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_starts, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_ends, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_delta
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_pool_result, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_indices
          Sub(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_indices_3
      If(_inlfunc__aten_max_pool_with_indices_onnx|folded_0|folded_0_self_rank_is_unbatched_rank, else_branch=G3, then_branch=G4) -&gt; max_pool2d_with_indices_1.1, max_pool2d_with_indices
        Shape(max_pool2d_with_indices, start=0) -&gt; _inlfunc_Rank|folded_3|folded_0_tmp
          Size(_inlfunc_Rank|folded_3|folded_0_tmp) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp
            MemcpyFromHost(_inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_CUDAExecutionProvider
              Equal(_inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_CUDAExecutionProvider, _inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_0) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_1
                Not(_inlfunc__aten_convolution_onnx|folded_1|folded_0_tmp_1) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch_CUDAExecutionProvider
                  MemcpyToHost(_inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch_CUDAExecutionProvider) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch
                    If(_inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch, else_branch=G5, then_branch=G6) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_input_5
                      Conv(_inlfunc__aten_convolution_onnx|folded_1|folded_0_input_5, conv2.weight, conv2.bias, auto_pad=b&#39;NOTSET&#39;, pads=[0,0,0,0], strides=[1,1], group=1, dilations=[1,1]) -&gt; _inlfunc__aten_convolution_onnx|folded_1|folded_0_result_7
                    If(_inlfunc__aten_convolution_onnx|folded_1|folded_0_no_batch, else_branch=G7, then_branch=G8) -&gt; conv2_1
                      Relu(conv2_1) -&gt; relu_1
                        Shape(relu_1, start=0) -&gt; _inlfunc_Rank|folded_5|folded_0_tmp
                          Size(_inlfunc_Rank|folded_5|folded_0_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_tmp
                            MemcpyFromHost(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_tmp_CUDAExecutionProvider
Cast(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank_cast
  Equal(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_tmp_CUDAExecutionProvider, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank_CUDAExecutionProvider
    MemcpyToHost(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank_CUDAExecutionProvider) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank
      If(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank, else_branch=G9, then_branch=G10) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_2
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0__, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_flatten_indices
          Slice(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_starts, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_ends, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_delta
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_pool_result, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_indices
          Sub(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_indices, _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_indices_3
      If(_inlfunc__aten_max_pool_with_indices_onnx|folded_1|folded_0_self_rank_is_unbatched_rank, else_branch=G11, then_branch=G12) -&gt; max_pool2d_with_indices_1_1, max_pool2d_with_indices_1
        Reshape(max_pool2d_with_indices_1, _inlfunc_aten_view|folded_0|folded_0_size_0, allowzero=0) -&gt; view
  Gemm(view, _inlfunc_torch_nn_modules_linear_Linear_fc1_1|folded_0|folded_0_t, fc1.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc1_1
    Relu(fc1_1) -&gt; relu_2
      Gemm(relu_2, _inlfunc_torch_nn_modules_linear_Linear_fc2_1|folded_0|folded_0_t_1, fc2.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc2_1
        Relu(fc2_1) -&gt; relu_3
          Gemm(relu_3, _inlfunc_torch_nn_modules_linear_Linear_fc3_1|folded_0|folded_0_t_2, fc3.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc3_1
output: name=&#39;fc3_1&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
----- function name=_aten_convolution_onnx|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: ConvXd with attributes pre-computed to fit the ONNX spec.
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
input: &#39;transposed&#39;
attribute: &#39;strides&#39;
attribute: &#39;pads&#39;
attribute: &#39;dilations&#39;
Conv(input, weight, bias, dilations=$dilations, group=$groups, pads=$pads, strides=$strides) -&gt; result_7
  Identity(result_7) -&gt; result_11
output: name=&#39;result_11&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv1_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;arg0&#39;
input: &#39;conv1.weight&#39;
input: &#39;conv1.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx|folded_0|folded_0[pkg.onnxscript.torch_lib](arg0, conv1.weight, conv1.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution
output: name=&#39;convolution&#39; type=? shape=?
----- function name=aten_relu|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=Rank|folded_2|folded_0 domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_max_pool_with_indices_onnx|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
attribute: &#39;kernel_size&#39;
attribute: &#39;stride&#39;
attribute: &#39;padding&#39;
attribute: &#39;dilation&#39;
attribute: &#39;ceil_mode&#39;
attribute: &#39;unbatched_rank&#39;
attribute: &#39;n_dims_one&#39;
attribute: &#39;n_dims_zero&#39;
attribute: &#39;n_dims_axes&#39;
Constant(value_int=$unbatched_rank) -&gt; unbatched_rank
Rank|folded_2|folded_0[pkg.onnxscript.torch_lib.common](self) -&gt; tmp
  CastLike(unbatched_rank, tmp) -&gt; unbatched_rank_cast
  Equal(tmp, unbatched_rank_cast) -&gt; self_rank_is_unbatched_rank
    If(self_rank_is_unbatched_rank, then_branch=G13, else_branch=G14) -&gt; self_2
      MaxPool(self_2, ceil_mode=$ceil_mode, dilations=$dilation, kernel_shape=$kernel_size, pads=$padding, strides=$stride) -&gt; pool_result, indices
      MaxPool(self_2, dilations=$dilation, kernel_shape=$n_dims_one, strides=$n_dims_one) -&gt; _, flatten_indices
Constant(value_ints=$n_dims_one) -&gt; ends
Constant(value_ints=$n_dims_zero) -&gt; starts
Constant(value_ints=$n_dims_axes) -&gt; axes
  Slice(flatten_indices, starts, ends, axes) -&gt; delta
    Sub(indices, delta) -&gt; indices_3
If(self_rank_is_unbatched_rank, then_branch=G15, else_branch=G16) -&gt; indices_10, pool_result_11
output: name=&#39;pool_result_11&#39; type=? shape=?
output: name=&#39;indices_10&#39; type=? shape=?
----- function name=Rank|folded_3|folded_0 domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_convolution_onnx|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: ConvXd with attributes pre-computed to fit the ONNX spec.
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
input: &#39;transposed&#39;
attribute: &#39;strides&#39;
attribute: &#39;pads&#39;
attribute: &#39;dilations&#39;
Constant(value=4) -&gt; tmp_0
Rank|folded_3|folded_0[pkg.onnxscript.torch_lib.common](input) -&gt; tmp
  Equal(tmp, tmp_0) -&gt; tmp_1
    Not(tmp_1) -&gt; no_batch
      If(no_batch, then_branch=G17, else_branch=G18) -&gt; input_5
        Conv(input_5, weight, bias, dilations=$dilations, group=$groups, pads=$pads, strides=$strides) -&gt; result_7
      If(no_batch, then_branch=G19, else_branch=G20) -&gt; result_11
output: name=&#39;result_11&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv2_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;getitem&#39;
input: &#39;conv2.weight&#39;
input: &#39;conv2.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx|folded_1|folded_0[pkg.onnxscript.torch_lib](getitem, conv2.weight, conv2.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution_1
output: name=&#39;convolution_1&#39; type=? shape=?
----- function name=aten_relu|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=Rank|folded_5|folded_0 domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_max_pool_with_indices_onnx|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
attribute: &#39;kernel_size&#39;
attribute: &#39;stride&#39;
attribute: &#39;padding&#39;
attribute: &#39;dilation&#39;
attribute: &#39;ceil_mode&#39;
attribute: &#39;unbatched_rank&#39;
attribute: &#39;n_dims_one&#39;
attribute: &#39;n_dims_zero&#39;
attribute: &#39;n_dims_axes&#39;
Constant(value_int=$unbatched_rank) -&gt; unbatched_rank
Rank|folded_5|folded_0[pkg.onnxscript.torch_lib.common](self) -&gt; tmp
  CastLike(unbatched_rank, tmp) -&gt; unbatched_rank_cast
  Equal(tmp, unbatched_rank_cast) -&gt; self_rank_is_unbatched_rank
    If(self_rank_is_unbatched_rank, then_branch=G21, else_branch=G22) -&gt; self_2
      MaxPool(self_2, ceil_mode=$ceil_mode, dilations=$dilation, kernel_shape=$kernel_size, pads=$padding, strides=$stride) -&gt; pool_result, indices
      MaxPool(self_2, dilations=$dilation, kernel_shape=$n_dims_one, strides=$n_dims_one) -&gt; _, flatten_indices
Constant(value_ints=$n_dims_one) -&gt; ends
Constant(value_ints=$n_dims_zero) -&gt; starts
Constant(value_ints=$n_dims_axes) -&gt; axes
  Slice(flatten_indices, starts, ends, axes) -&gt; delta
    Sub(indices, delta) -&gt; indices_3
If(self_rank_is_unbatched_rank, then_branch=G23, else_branch=G24) -&gt; indices_10, pool_result_11
output: name=&#39;pool_result_11&#39; type=? shape=?
output: name=&#39;indices_10&#39; type=? shape=?
----- function name=aten_view|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;size&#39;
Constant(value=[1, 13456]) -&gt; size_0
  Reshape(self, size_0) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=aten_t|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: t(Tensor(a) self) -&gt; Tensor(a)
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Transpose(self, perm=[1,0]) -&gt; result_1
output: name=&#39;result_1&#39; type=? shape=?
----- function name=aten_addmm|folded_0|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc1_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;view&#39;
input: &#39;fc1.weight&#39;
input: &#39;fc1.bias&#39;
aten_t|folded_0|folded_0[pkg.onnxscript.torch_lib](fc1.weight) -&gt; t
  aten_addmm|folded_0|folded_0[pkg.onnxscript.torch_lib](fc1.bias, view, t, alpha=1.00, beta=1.00) -&gt; addmm
output: name=&#39;addmm&#39; type=? shape=?
----- function name=aten_relu|folded_2|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=aten_addmm|folded_1|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc2_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_2&#39;
input: &#39;fc2.weight&#39;
input: &#39;fc2.bias&#39;
Constant(value=[[0.021504...) -&gt; t_1
  aten_addmm|folded_1|folded_0[pkg.onnxscript.torch_lib](fc2.bias, relu_2, t_1, alpha=1.00, beta=1.00) -&gt; addmm_1
output: name=&#39;addmm_1&#39; type=? shape=?
----- function name=aten_relu|folded_3|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=aten_addmm|folded_2|folded_0 domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc3_1|folded_0|folded_0 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_3&#39;
input: &#39;fc3.weight&#39;
input: &#39;fc3.bias&#39;
Constant(value=[[0.000341...) -&gt; t_2
  aten_addmm|folded_2|folded_0[pkg.onnxscript.torch_lib](fc3.bias, relu_3, t_2, alpha=1.00, beta=1.00) -&gt; addmm_2
output: name=&#39;addmm_2&#39; type=? shape=?
</pre></div>
</div>
</section>
<section id="id4">
<h3>dynamo<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a> <span class="o">=</span> <span class="s2">&quot;ort-plot_torch_export_dynamo-cuda-aot1.onnx&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx_simple_text_plot</span><span class="p">(</span><a href="https://onnx.ai/onnx/api/serialization.html#onnx.load" title="onnx.load" class="sphx-glr-backref-module-onnx sphx-glr-backref-type-py-function"><span class="n">onnx</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model</span></a><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
opset: domain=&#39;pkg.torch.2.1.1+cu118&#39; version=1
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;ai.onnx.ml&#39; version=4
opset: domain=&#39;ai.onnx.training&#39; version=1
opset: domain=&#39;ai.onnx.preview.training&#39; version=1
opset: domain=&#39;com.microsoft&#39; version=1
opset: domain=&#39;com.microsoft.experimental&#39; version=1
opset: domain=&#39;com.microsoft.nchwc&#39; version=1
opset: domain=&#39;org.pytorch.aten&#39; version=1
input: name=&#39;arg0&#39; type=dtype(&#39;float32&#39;) shape=[1, 1, 128, 128]
init: name=&#39;conv1.weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 1, 5, 5)
init: name=&#39;conv1.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;conv2.weight&#39; type=dtype(&#39;float32&#39;) shape=(16, 128, 5, 5)
init: name=&#39;conv2.bias&#39; type=dtype(&#39;float32&#39;) shape=(16,)
init: name=&#39;fc1.weight&#39; type=dtype(&#39;float32&#39;) shape=(1024, 13456)
init: name=&#39;fc1.bias&#39; type=dtype(&#39;float32&#39;) shape=(1024,)
init: name=&#39;fc2.weight&#39; type=dtype(&#39;float32&#39;) shape=(128, 1024)
init: name=&#39;fc2.bias&#39; type=dtype(&#39;float32&#39;) shape=(128,)
init: name=&#39;fc3.weight&#39; type=dtype(&#39;float32&#39;) shape=(10, 128)
init: name=&#39;fc3.bias&#39; type=dtype(&#39;float32&#39;) shape=(10,)
init: name=&#39;ortshared_7_1_2_0_token_8&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([    1, 13456])
init: name=&#39;_inlfunc_torch_nn_modules_conv_Conv2d_conv1_1__val_3&#39; type=dtype(&#39;bool&#39;) shape=() -- array([False])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc_torch_nn_modules_conv_Conv2d_conv2_1__val_3&#39; type=dtype(&#39;bool&#39;) shape=() -- array([False])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank&#39; type=dtype(&#39;int64&#39;) shape=() -- array([3])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_ends&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([1, 1])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_starts&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([0, 0])
init: name=&#39;_inlfunc__aten_max_pool_with_indices_onnx_token_10_axes&#39; type=dtype(&#39;int64&#39;) shape=(2,) -- array([2, 3])
init: name=&#39;_inlfunc_aten_t_int64_2&#39; type=dtype(&#39;int64&#39;) shape=() -- array([2])
init: name=&#39;_inlfunc_aten_t_token_16_int64_2&#39; type=dtype(&#39;int64&#39;) shape=() -- array([2])
init: name=&#39;_inlfunc_aten_t_token_18_int64_2&#39; type=dtype(&#39;int64&#39;) shape=() -- array([2])
Cast(_inlfunc_aten_t_token_18_int64_2, to=7) -&gt; _inlfunc_aten_t_token_18_int64_2_cast
Shape(fc3.weight, start=0) -&gt; _inlfunc_Rank_token_28_tmp
  Size(_inlfunc_Rank_token_28_tmp) -&gt; _inlfunc_aten_t_token_18_rank
    MemcpyFromHost(_inlfunc_aten_t_token_18_rank) -&gt; _inlfunc_aten_t_token_18_rank_CUDAExecutionProvider
  Equal(_inlfunc_aten_t_token_18_rank_CUDAExecutionProvider, _inlfunc_aten_t_token_18_int64_2_cast) -&gt; _inlfunc_aten_t_token_18_cond_CUDAExecutionProvider
    MemcpyToHost(_inlfunc_aten_t_token_18_cond_CUDAExecutionProvider) -&gt; _inlfunc_aten_t_token_18_cond
      If(_inlfunc_aten_t_token_18_cond, else_branch=G1, then_branch=G2) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc3_1_t_2
Shape(fc2.weight, start=0) -&gt; _inlfunc_Rank_token_26_tmp
  Size(_inlfunc_Rank_token_26_tmp) -&gt; _inlfunc_aten_t_token_16_rank
    MemcpyFromHost(_inlfunc_aten_t_token_16_rank) -&gt; _inlfunc_aten_t_token_16_rank_CUDAExecutionProvider
Cast(_inlfunc_aten_t_token_16_int64_2, to=7) -&gt; _inlfunc_aten_t_token_16_int64_2_cast
  Equal(_inlfunc_aten_t_token_16_rank_CUDAExecutionProvider, _inlfunc_aten_t_token_16_int64_2_cast) -&gt; _inlfunc_aten_t_token_16_cond_CUDAExecutionProvider
    MemcpyToHost(_inlfunc_aten_t_token_16_cond_CUDAExecutionProvider) -&gt; _inlfunc_aten_t_token_16_cond
      If(_inlfunc_aten_t_token_16_cond, else_branch=G3, then_branch=G4) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc2_1_t_1
Shape(fc1.weight, start=0) -&gt; _inlfunc_Rank_token_24_tmp
  Size(_inlfunc_Rank_token_24_tmp) -&gt; _inlfunc_aten_t_rank
    MemcpyFromHost(_inlfunc_aten_t_rank) -&gt; _inlfunc_aten_t_rank_CUDAExecutionProvider
Cast(_inlfunc_aten_t_int64_2, to=7) -&gt; _inlfunc_aten_t_int64_2_cast
  Equal(_inlfunc_aten_t_rank_CUDAExecutionProvider, _inlfunc_aten_t_int64_2_cast) -&gt; _inlfunc_aten_t_cond_CUDAExecutionProvider
    MemcpyToHost(_inlfunc_aten_t_cond_CUDAExecutionProvider) -&gt; _inlfunc_aten_t_cond
      If(_inlfunc_aten_t_cond, else_branch=G5, then_branch=G6) -&gt; _inlfunc_torch_nn_modules_linear_Linear_fc1_1_t
Cast(ortshared_7_1_2_0_token_8, to=7) -&gt; _inlfunc_aten_view_size_0_CUDAExecutionProvider
  MemcpyToHost(_inlfunc_aten_view_size_0_CUDAExecutionProvider) -&gt; _inlfunc_aten_view_size_0
Shape(conv2.weight, start=0) -&gt; _inlfunc_Rank_token_23_tmp
  Size(_inlfunc_Rank_token_23_tmp) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp_0
    MemcpyFromHost(_inlfunc__aten_convolution_onnx_token_13_tmp_0) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp_0_CUDAExecutionProvider
Shape(conv1.weight, start=0) -&gt; _inlfunc_Rank_token_21_tmp
  Size(_inlfunc_Rank_token_21_tmp) -&gt; _inlfunc__aten_convolution_onnx_tmp_0
    MemcpyFromHost(_inlfunc__aten_convolution_onnx_tmp_0) -&gt; _inlfunc__aten_convolution_onnx_tmp_0_CUDAExecutionProvider
Shape(arg0, start=0) -&gt; _inlfunc_Rank_token_20_tmp
  Size(_inlfunc_Rank_token_20_tmp) -&gt; _inlfunc__aten_convolution_onnx_tmp
    MemcpyFromHost(_inlfunc__aten_convolution_onnx_tmp) -&gt; _inlfunc__aten_convolution_onnx_tmp_CUDAExecutionProvider
      Equal(_inlfunc__aten_convolution_onnx_tmp_CUDAExecutionProvider, _inlfunc__aten_convolution_onnx_tmp_0_CUDAExecutionProvider) -&gt; _inlfunc__aten_convolution_onnx_tmp_1
        Not(_inlfunc__aten_convolution_onnx_tmp_1) -&gt; _inlfunc__aten_convolution_onnx_no_batch_CUDAExecutionProvider
          MemcpyToHost(_inlfunc__aten_convolution_onnx_no_batch_CUDAExecutionProvider) -&gt; _inlfunc__aten_convolution_onnx_no_batch
            If(_inlfunc__aten_convolution_onnx_no_batch, else_branch=G7, then_branch=G8) -&gt; _inlfunc__aten_convolution_onnx_input_5
If(_inlfunc_torch_nn_modules_conv_Conv2d_conv1_1__val_3, else_branch=G9, then_branch=G10) -&gt; _inlfunc__aten_convolution_onnx_result_7
If(_inlfunc__aten_convolution_onnx_no_batch, else_branch=G11, then_branch=G12) -&gt; conv1_1
  Relu(conv1_1) -&gt; relu
    Shape(relu, start=0) -&gt; _inlfunc_Rank_tmp
      Size(_inlfunc_Rank_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_tmp
        MemcpyFromHost(_inlfunc__aten_max_pool_with_indices_onnx_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_tmp_CUDAExecutionProvider
Cast(_inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank_cast
  Equal(_inlfunc__aten_max_pool_with_indices_onnx_tmp_CUDAExecutionProvider, _inlfunc__aten_max_pool_with_indices_onnx_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank_CUDAExecutionProvider
    MemcpyToHost(_inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank_CUDAExecutionProvider) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank
      If(_inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank, else_branch=G13, then_branch=G14) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_self_2
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx__, _inlfunc__aten_max_pool_with_indices_onnx_flatten_indices
          Slice(_inlfunc__aten_max_pool_with_indices_onnx_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx_starts, _inlfunc__aten_max_pool_with_indices_onnx_ends, _inlfunc__aten_max_pool_with_indices_onnx_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_delta
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_pool_result, _inlfunc__aten_max_pool_with_indices_onnx_indices
          Sub(_inlfunc__aten_max_pool_with_indices_onnx_indices, _inlfunc__aten_max_pool_with_indices_onnx_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_indices_3
      If(_inlfunc__aten_max_pool_with_indices_onnx_self_rank_is_unbatched_rank, else_branch=G15, then_branch=G16) -&gt; max_pool2d_with_indices_1.1, max_pool2d_with_indices
        Shape(max_pool2d_with_indices, start=0) -&gt; _inlfunc_Rank_token_22_tmp
          Size(_inlfunc_Rank_token_22_tmp) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp
            MemcpyFromHost(_inlfunc__aten_convolution_onnx_token_13_tmp) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp_CUDAExecutionProvider
      Equal(_inlfunc__aten_convolution_onnx_token_13_tmp_CUDAExecutionProvider, _inlfunc__aten_convolution_onnx_token_13_tmp_0_CUDAExecutionProvider) -&gt; _inlfunc__aten_convolution_onnx_token_13_tmp_1
        Not(_inlfunc__aten_convolution_onnx_token_13_tmp_1) -&gt; _inlfunc__aten_convolution_onnx_token_13_no_batch_CUDAExecutionProvider
          MemcpyToHost(_inlfunc__aten_convolution_onnx_token_13_no_batch_CUDAExecutionProvider) -&gt; _inlfunc__aten_convolution_onnx_token_13_no_batch
            If(_inlfunc__aten_convolution_onnx_token_13_no_batch, else_branch=G17, then_branch=G18) -&gt; _inlfunc__aten_convolution_onnx_token_13_input_5
If(_inlfunc_torch_nn_modules_conv_Conv2d_conv2_1__val_3, else_branch=G19, then_branch=G20) -&gt; _inlfunc__aten_convolution_onnx_token_13_result_7
If(_inlfunc__aten_convolution_onnx_token_13_no_batch, else_branch=G21, then_branch=G22) -&gt; conv2_1
  Relu(conv2_1) -&gt; relu_1
    Shape(relu_1, start=0) -&gt; _inlfunc_Rank_token_14_tmp
      Size(_inlfunc_Rank_token_14_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_tmp
        MemcpyFromHost(_inlfunc__aten_max_pool_with_indices_onnx_token_10_tmp) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_tmp_CUDAExecutionProvider
Cast(_inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank, to=7) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank_cast
  Equal(_inlfunc__aten_max_pool_with_indices_onnx_token_10_tmp_CUDAExecutionProvider, _inlfunc__aten_max_pool_with_indices_onnx_token_10_unbatched_rank_cast) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank_CUDAExecutionProvider
    MemcpyToHost(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank_CUDAExecutionProvider) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank
      If(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank, else_branch=G23, then_branch=G24) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_self_2
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_2, auto_pad=b&#39;NOTSET&#39;, dilations=[1,1], ceil_mode=0, kernel_shape=[1,1], storage_order=0, strides=[1,1]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10__, _inlfunc__aten_max_pool_with_indices_onnx_token_10_flatten_indices
          Slice(_inlfunc__aten_max_pool_with_indices_onnx_token_10_flatten_indices, _inlfunc__aten_max_pool_with_indices_onnx_token_10_starts, _inlfunc__aten_max_pool_with_indices_onnx_token_10_ends, _inlfunc__aten_max_pool_with_indices_onnx_token_10_axes) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_delta
        MaxPool(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_2, storage_order=0, auto_pad=b&#39;NOTSET&#39;, ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_pool_result, _inlfunc__aten_max_pool_with_indices_onnx_token_10_indices
          Sub(_inlfunc__aten_max_pool_with_indices_onnx_token_10_indices, _inlfunc__aten_max_pool_with_indices_onnx_token_10_delta) -&gt; _inlfunc__aten_max_pool_with_indices_onnx_token_10_indices_3
      If(_inlfunc__aten_max_pool_with_indices_onnx_token_10_self_rank_is_unbatched_rank, else_branch=G25, then_branch=G26) -&gt; max_pool2d_with_indices_1_1, max_pool2d_with_indices_1
    Reshape(max_pool2d_with_indices_1, _inlfunc_aten_view_size_0, allowzero=0) -&gt; view
      Gemm(view, _inlfunc_torch_nn_modules_linear_Linear_fc1_1_t, fc1.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc1_1
        Relu(fc1_1) -&gt; relu_2
        Gemm(relu_2, _inlfunc_torch_nn_modules_linear_Linear_fc2_1_t_1, fc2.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc2_1
          Relu(fc2_1) -&gt; relu_3
        Gemm(relu_3, _inlfunc_torch_nn_modules_linear_Linear_fc3_1_t_2, fc3.bias, transB=0, transA=0, alpha=1.00, beta=1.00) -&gt; fc3_1
output: name=&#39;fc3_1&#39; type=dtype(&#39;float32&#39;) shape=[1, 10]
----- function name=_aten_convolution_onnx domain=pkg.onnxscript.torch_lib
----- doc_string: ConvXd with attributes pre-computed to fit the ONNX spec.
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
input: &#39;weight&#39;
input: &#39;bias&#39;
input: &#39;transposed&#39;
attribute: &#39;strides&#39;
attribute: &#39;pads&#39;
attribute: &#39;dilations&#39;
Rank[pkg.onnxscript.torch_lib.common](input) -&gt; tmp
Rank[pkg.onnxscript.torch_lib.common](weight) -&gt; tmp_0
  Equal(tmp, tmp_0) -&gt; tmp_1
    Not(tmp_1) -&gt; no_batch
      If(no_batch, then_branch=G27, else_branch=G28) -&gt; input_5
If(transposed, then_branch=G29, else_branch=G30) -&gt; result_7
If(no_batch, then_branch=G31, else_branch=G32) -&gt; result_11
output: name=&#39;result_11&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv1_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;arg0&#39;
input: &#39;conv1.weight&#39;
input: &#39;conv1.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx[pkg.onnxscript.torch_lib](arg0, conv1.weight, conv1.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution
output: name=&#39;convolution&#39; type=? shape=?
----- function name=torch_nn_modules_conv_Conv2d_conv2_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;getitem&#39;
input: &#39;conv2.weight&#39;
input: &#39;conv2.bias&#39;
Constant(value=False) -&gt; _val_3
  _aten_convolution_onnx[pkg.onnxscript.torch_lib](getitem, conv2.weight, conv2.bias, _val_3, dilations=[1,1], groups=1, output_padding=[0,0], pads=[0,0,0,0], strides=[1,1]) -&gt; convolution_1
output: name=&#39;convolution_1&#39; type=? shape=?
----- function name=aten_t domain=pkg.onnxscript.torch_lib
----- doc_string: t(Tensor(a) self) -&gt; Tensor(a)
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Constant(value=2) -&gt; int64_2
Rank[pkg.onnxscript.torch_lib.common](self) -&gt; rank
  CastLike(int64_2, rank) -&gt; int64_2_cast
  Equal(rank, int64_2_cast) -&gt; cond
    If(cond, then_branch=G28, else_branch=G33) -&gt; result_1
output: name=&#39;result_1&#39; type=? shape=?
----- function name=aten_addmm domain=pkg.onnxscript.torch_lib
----- doc_string: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;mat1&#39;
input: &#39;mat2&#39;
Gemm(mat1, mat2, self, alpha=$alpha, beta=$beta) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc1_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;view&#39;
input: &#39;fc1.weight&#39;
input: &#39;fc1.bias&#39;
aten_t[pkg.onnxscript.torch_lib](fc1.weight) -&gt; t
  aten_addmm[pkg.onnxscript.torch_lib](fc1.bias, view, t, alpha=1.00, beta=1.00) -&gt; addmm
output: name=&#39;addmm&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc2_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_2&#39;
input: &#39;fc2.weight&#39;
input: &#39;fc2.bias&#39;
aten_t[pkg.onnxscript.torch_lib](fc2.weight) -&gt; t_1
  aten_addmm[pkg.onnxscript.torch_lib](fc2.bias, relu_2, t_1, alpha=1.00, beta=1.00) -&gt; addmm_1
output: name=&#39;addmm_1&#39; type=? shape=?
----- function name=torch_nn_modules_linear_Linear_fc3_1 domain=pkg.torch.2.1.1+cu118
opset: domain=&#39;&#39; version=18
opset: domain=&#39;pkg.onnxscript.torch_lib&#39; version=1
input: &#39;relu_3&#39;
input: &#39;fc3.weight&#39;
input: &#39;fc3.bias&#39;
aten_t[pkg.onnxscript.torch_lib](fc3.weight) -&gt; t_2
  aten_addmm[pkg.onnxscript.torch_lib](fc3.bias, relu_3, t_2, alpha=1.00, beta=1.00) -&gt; addmm_2
output: name=&#39;addmm_2&#39; type=? shape=?
----- function name=aten_relu domain=pkg.onnxscript.torch_lib
----- doc_string: relu(Tensor self) -&gt; Tensor
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
Relu(self) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=_aten_max_pool_with_indices_onnx domain=pkg.onnxscript.torch_lib
opset: domain=&#39;pkg.onnxscript.torch_lib.common&#39; version=1
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
attribute: &#39;kernel_size&#39;
attribute: &#39;stride&#39;
attribute: &#39;padding&#39;
attribute: &#39;dilation&#39;
attribute: &#39;ceil_mode&#39;
attribute: &#39;unbatched_rank&#39;
attribute: &#39;n_dims_one&#39;
attribute: &#39;n_dims_zero&#39;
attribute: &#39;n_dims_axes&#39;
Constant(value_int=$unbatched_rank) -&gt; unbatched_rank
Rank[pkg.onnxscript.torch_lib.common](self) -&gt; tmp
  CastLike(unbatched_rank, tmp) -&gt; unbatched_rank_cast
  Equal(tmp, unbatched_rank_cast) -&gt; self_rank_is_unbatched_rank
    If(self_rank_is_unbatched_rank, then_branch=G34, else_branch=G35) -&gt; self_2
      MaxPool(self_2, ceil_mode=$ceil_mode, dilations=$dilation, kernel_shape=$kernel_size, pads=$padding, strides=$stride) -&gt; pool_result, indices
      MaxPool(self_2, dilations=$dilation, kernel_shape=$n_dims_one, strides=$n_dims_one) -&gt; _, flatten_indices
Constant(value_ints=$n_dims_one) -&gt; ends
Constant(value_ints=$n_dims_zero) -&gt; starts
Constant(value_ints=$n_dims_axes) -&gt; axes
  Slice(flatten_indices, starts, ends, axes) -&gt; delta
    Sub(indices, delta) -&gt; indices_3
If(self_rank_is_unbatched_rank, then_branch=G36, else_branch=G37) -&gt; indices_10, pool_result_11
output: name=&#39;pool_result_11&#39; type=? shape=?
output: name=&#39;indices_10&#39; type=? shape=?
----- function name=aten_view domain=pkg.onnxscript.torch_lib
----- doc_string: view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)
opset: domain=&#39;&#39; version=18
input: &#39;self&#39;
input: &#39;size&#39;
Cast(size, to=7) -&gt; size_0
  Reshape(self, size_0) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=Rank domain=pkg.onnxscript.torch_lib.common
----- doc_string: Take the rank of the input tensor.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Shape(input) -&gt; tmp
  Size(tmp) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
----- function name=IsScalar domain=pkg.onnxscript.torch_lib.common
----- doc_string: Return whether the input has rank 0, or is a scalar.
opset: domain=&#39;&#39; version=18
input: &#39;input&#39;
Constant(value_int=0) -&gt; tmp_1
Shape(input) -&gt; tmp
  Size(tmp) -&gt; tmp_0
  Equal(tmp_0, tmp_1) -&gt; return_val
output: name=&#39;return_val&#39; type=? shape=?
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (1 minutes 56.713 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-torch-export-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/0534a82190ecce5d98ccd3b019bd1c7a/plot_torch_export.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_torch_export.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/5ea159170a8b6e6d090b561d7986469d/plot_torch_export.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_torch_export.py</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../CHANGELOGS.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Change Logs</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="index.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Example gallery</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Xavier Dupré
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Evaluate different ways to export a torch model to ONNX</a><ul>
<li><a class="reference internal" href="#some-helpers">Some helpers</a></li>
<li><a class="reference internal" href="#the-model">The model</a></li>
<li><a class="reference internal" href="#the-exporters">The exporters</a></li>
<li><a class="reference internal" href="#exporter-speed">Exporter speed</a></li>
<li><a class="reference internal" href="#profiling">Profiling</a></li>
<li><a class="reference internal" href="#benchmark">Benchmark</a></li>
<li><a class="reference internal" href="#show-the-interesting-models-for-cpu">Show the interesting models for CPU</a><ul>
<li><a class="reference internal" href="#script">script</a></li>
<li><a class="reference internal" href="#cus-p2">cus_p2</a></li>
<li><a class="reference internal" href="#dynopt">dynopt</a></li>
<li><a class="reference internal" href="#dynamo">dynamo</a></li>
</ul>
</li>
<li><a class="reference internal" href="#show-the-interesting-models-for-cuda">Show the interesting models for CUDA</a><ul>
<li><a class="reference internal" href="#id1">script</a></li>
<li><a class="reference internal" href="#id2">cus_p2</a></li>
<li><a class="reference internal" href="#id3">dynopt</a></li>
<li><a class="reference internal" href="#id4">dynamo</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=c88f2e48"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    </body>
</html>