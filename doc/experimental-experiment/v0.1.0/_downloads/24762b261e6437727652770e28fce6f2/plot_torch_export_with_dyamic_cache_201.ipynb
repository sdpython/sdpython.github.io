{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 201: Export a model using a custom type as input\n\nWe will a class used in many model: :class:`transformers.cache_utils.DynamicCache`.\n\n## First try: it fails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Tuple\nimport torch\nimport transformers\n\n\nclass ModelTakingDynamicCacheAsInput(torch.nn.Module):\n    def forward(self, x, dc):\n        kc = torch.cat(dc.key_cache, axis=1)\n        vc = torch.cat(dc.value_cache, axis=1)\n        y = (kc + vc).sum(axis=2, keepdim=True)\n        return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the model runs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3, 8, 7, 1)\ncache = transformers.cache_utils.DynamicCache(1)\ncache.update(torch.ones((3, 8, 5, 6)), (torch.ones((3, 8, 5, 6)) * 2), 0)\n\nmodel = ModelTakingDynamicCacheAsInput()\nexpected = model(x, cache)\n\nprint(expected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check it works with others shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 8, 7, 1)\ncache = transformers.cache_utils.DynamicCache(1)\ncache.update(torch.ones((4, 8, 11, 6)), (torch.ones((4, 8, 11, 6)) * 2), 0)\n\nmodel = ModelTakingDynamicCacheAsInput()\nexpected = model(x, cache)\n\nprint(expected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's export.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    torch.export.export(model, (x, cache))\nexcept Exception as e:\n    print(\"export failed with\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register serialization of DynamicCache\n\nThat's what needs to be done.\nFeel free to adapt it to your own class.\nThe important informatin is we want to serialize\ntwo attributes ``key_cache`` and ``value_cache``.\nBoth are list of tensors of the same size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def flatten_dynamic_cache(\n    dynamic_cache: transformers.cache_utils.DynamicCache,\n) -> Tuple[List[Any], torch.utils._pytree.Context]:\n    flat = [\n        (k, getattr(dynamic_cache, k))\n        for k in [\"key_cache\", \"value_cache\"]\n        if hasattr(dynamic_cache, k)\n    ]\n    return [f[1] for f in flat], [f[0] for f in flat]\n\n\ndef unflatten_dynamic_cache(\n    values: List[Any],\n    context: torch.utils._pytree.Context,\n    output_type=None,\n) -> transformers.cache_utils.DynamicCache:\n    cache = transformers.cache_utils.DynamicCache()\n    values = dict(zip(context, values))\n    for k, v in values.items():\n        setattr(cache, k, v)\n    return cache\n\n\ndef flatten_with_keys_dynamic_cache(d: Dict[Any, Any]) -> Tuple[\n    List[Tuple[torch.utils._pytree.KeyEntry, Any]],\n    torch.utils._pytree.Context,\n]:\n    values, context = flatten_dynamic_cache(d)\n    return [(torch.utils._pytree.MappingKey(k), v) for k, v in zip(context, values)], context\n\n\ntorch.utils._pytree.register_pytree_node(\n    transformers.cache_utils.DynamicCache,\n    flatten_dynamic_cache,\n    unflatten_dynamic_cache,\n    serialized_type_name=f\"{transformers.cache_utils.DynamicCache.__module__}.{transformers.cache_utils.DynamicCache.__name__}\",\n    flatten_with_keys_fn=flatten_with_keys_dynamic_cache,\n)\ntorch.fx._pytree.register_pytree_flatten_spec(\n    transformers.cache_utils.DynamicCache, lambda x, _: [x.key_cache, x.value_cache]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try to export again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = torch.export.export(model, (x, cache))\nprint(ep.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With dynamic shapes now.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = torch.export.Dim(\"batch\", min=1, max=1024)\nclength = torch.export.Dim(\"clength\", min=1, max=1024)\n\ntry:\n    ep = torch.export.export(\n        model,\n        (x, cache),\n        dynamic_shapes=({0: batch}, [[{0: batch, 2: clength}], [{0: batch, 2: clength}]]),\n    )\n    print(ep.graph)\n    failed = False\nexcept Exception as e:\n    print(\"FAILS:\", e)\n    failed = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If it failed, let's understand why.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if failed:\n\n    class Model(torch.nn.Module):\n        def forward(self, dc):\n            kc = dc.key_cache[0]\n            vc = dc.value_cache[0]\n            return kc + vc\n\n    ep = torch.export.export(\n        Model(),\n        (cache,),\n        dynamic_shapes={\"dc\": [[{0: batch, 2: clength}], [{0: batch, 2: clength}]]},\n    )\n    for node in ep.graph.nodes:\n        print(f\"{node.name} -> {node.meta.get('val', '-')}\")\n        # it prints out ``dc_key_cache_0 -> FakeTensor(..., size=(4, 8, 11, 6))``\n        # but it should be ``dc_key_cache_0 -> FakeTensor(..., size=(s0, 8, s1, 6))``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's undo the registration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.utils._pytree.SUPPORTED_NODES.pop(transformers.cache_utils.DynamicCache)\ntorch.fx._pytree.SUPPORTED_NODES.pop(transformers.cache_utils.DynamicCache)\ntorch.fx._pytree.SUPPORTED_NODES_EXACT_MATCH.pop(transformers.cache_utils.DynamicCache)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}