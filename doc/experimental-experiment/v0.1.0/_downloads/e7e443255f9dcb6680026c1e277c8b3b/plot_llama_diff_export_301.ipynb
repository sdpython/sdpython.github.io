{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 301: Compares LLAMA exporters\n\nThe script compares the two exporters implemented in :epkg:`pytorch`\nfor a part of llama model. The model are compared after all optimizations\nwere made with and :epkg:`onnxruntime`.\n\n* [TorchScript-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter),\n  let's call it **script**\n* [TorchDynamo-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter),\n  let's call it **dynamo**\n\nTo run the script:\n\n::\n\n    python _doc/examples/plot_llama_diff_export --help\n\n## Some helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from experimental_experiment.args import get_parsed_args\n\nscript_args = get_parsed_args(\n    \"plot_llama_diff_export\",\n    description=__doc__,\n    part=(\"model\", \"one value among model, ...\"),\n    exporter=(\"dynamo\", \"one value among dynamo, custom\"),\n    ortopt=(1, \"run onnxruntime optimization\"),\n    opset=(18, \"onnx opset\"),\n    expose=\"part,exporter,ortopt,opset\",\n)\n\nimport contextlib\nimport os\nimport io\nimport warnings\nimport logging\n\ntry:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        import onnxruntime\n\n        has_cuda = \"CUDAExecutionProvider\" in onnxruntime.get_available_providers()\nexcept ImportError:\n    print(\"onnxruntime not available.\")\n    import sys\n\n    sys.exit(0)\n\nimport numpy as np\nimport onnx\nfrom onnx_array_api.reference import compare_onnx_execution\nimport torch\nfrom experimental_experiment.ext_test_case import unit_test_going\nfrom experimental_experiment.reference import ExtendedReferenceEvaluator\nfrom experimental_experiment.torch_interpreter import to_onnx\nfrom experimental_experiment.helpers import string_type\nfrom experimental_experiment.xbuilder import OptimizationOptions\nfrom experimental_experiment.convert.convert_helper import ort_optimize\nfrom experimental_experiment.torch_models.llama_helper import get_llama_model\nfrom experimental_experiment.torch_models.dump_helper import reorder_functions_in_proto\n\nhas_cuda = has_cuda and torch.cuda.device_count() > 0\nlogging.disable(logging.ERROR)\nprovider = \"cuda\" if has_cuda else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The exporting functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"part={script_args.part}\")\nprint(f\"exporter={script_args.exporter}\")\nortopt = script_args.ortopt in (1, \"1\")\nprint(f\"ortopt={ortopt}\")\nopset = int(script_args.opset)\nprint(f\"opset={opset}\")\n\n\ndef opt_filename(filename: str) -> str:\n    name, ext = os.path.splitext(filename)\n    return f\"{name}.opt{ext}\"\n\n\ndef export_script(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            torch.onnx.export(\n                model, args, filename, input_names=[\"input\"], opset_version=opset\n            )\n    if ortopt:\n        onx = onnx.load(filename)\n        ort_optimize(onx, opt_filename(filename), providers=provider)\n\n\ndef export_dynamo(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            export_output = torch.onnx.export(model, args, dynamo=True)\n            export_output.optimize()\n            model = export_output.model_proto\n    with open(filename, \"wb\") as f:\n        f.write(model.SerializeToString())\n    if ortopt:\n        ort_optimize(model, opt_filename(filename), providers=provider)\n\n\ndef export_custom(filename, model, *args):\n    new_model = to_onnx(\n        model,\n        tuple(args),\n        input_names=[f\"input{i}\" for i in range(len(args))],\n        options=OptimizationOptions(\n            remove_unused=True,\n            constant_folding=False,\n        ),\n        target_opset=opset,\n    )\n    with open(filename, \"wb\") as f:\n        f.write(new_model.SerializeToString())\n    if ortopt:\n        ort_optimize(new_model, opt_filename(filename), providers=provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if unit_test_going():\n    kwargs = dict(input_dims=[(2, 1024)] * 2)\nelse:\n    kwargs = dict(\n        input_dims=[(2, 1024)] * 2,\n        _attn_implementation=\"eager\",\n        num_hidden_layers=1,\n        hidden_size=512,\n        vocab_size=4000,\n        intermediate_size=2000,\n        max_position_embeddings=2048,\n        num_attention_heads=8,\n    )\n\nif script_args.part == \"model\":\n    model, inputs = get_llama_model(**kwargs)\nelse:\n    raise RuntimeError(f\"Unexpected value for part={script_args.part!r}\")\n\nprint(f\"simple run with {len(inputs)} inputs\")\nexpected = model(*inputs[0])\nprint(f\"eager worked: {string_type(expected, with_shape=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exporter = script_args.exporter\nfile1 = f\"llama.{script_args.part}.script.onnx\"\nfile2 = f\"llama.{script_args.part}.{exporter}.onnx\"\n\nprint(\"torch script exporter\")\nexport_script(file1, model, *inputs[0])\n\nif exporter == \"dynamo\":\n    print(\"torch dynamo exporter\")\n    export_dynamo(file2, model, *inputs[0])\nelif exporter == \"custom\":\n    print(\"torch custom exporter\")\n    export_custom(file2, model, *inputs[0])\nelse:\n    raise AssertionError(f\"Unexpected value for exporter={exporter!r}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if ortopt:\n    print(\"Using models optimized by onnxruntime\")\n    file1 = f\"llama.{script_args.part}.script.opt.onnx\"\n    file2 = f\"llama.{script_args.part}.{exporter}.opt.onnx\"\n\n\nproviders = (\n    [\"CPUExecutionProvider\"]\n    if provider == \"cpu\"\n    else [(\"CUDAExecutionProvider\", {}), (\"CPUExecutionProvider\", {})]\n)\n\nmodel1 = onnx.load(file1)\nmodel2 = onnx.load(file2)\n\nfeeds1, feeds2 = {}, {}\nfor i in range(len(inputs[0])):\n    x = inputs[0][i].detach().numpy()\n    feeds1[model1.graph.input[i].name] = x\n    feeds2[model2.graph.input[i].name] = x\n\nif ortopt:\n    sess1 = onnxruntime.InferenceSession(file1, providers=providers)\n    sess2 = onnxruntime.InferenceSession(file2, providers=providers)\n\n    got1 = sess1.run(None, feeds1)\n    got2 = sess2.run(None, feeds2)\n\n    if isinstance(expected, tuple) and len(expected) == 1:\n        expected = expected[0]\n    diff1 = np.abs(expected.detach().numpy() - got1[0]).max()\n    diff2 = np.abs(expected.detach().numpy() - got2[0]).max()\n\n    print(f\"Error with the eager model and onnxruntime: {diff1}, {diff2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification with the reference evaluator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reorder_functions_in_proto(file1)\nreorder_functions_in_proto(file2)\n\nsess1 = ExtendedReferenceEvaluator(file1)\ntry:\n    sess2 = ExtendedReferenceEvaluator(file2)\nexcept NotImplementedError as e:\n    print(e)\n    sess2 = None\n\ngot1 = sess1.run(None, feeds1)\ngot2 = got1 if sess2 is None else sess2.run(None, feeds2)\n\nif isinstance(expected, tuple):\n    diff1 = np.abs(expected[0].detach().numpy() - got1[0]).max()\n    diff2 = np.abs(expected[0].detach().numpy() - got2[0]).max()\nelse:\n    diff1 = np.abs(expected.detach().numpy() - got1[0]).max()\n    diff2 = np.abs(expected.detach().numpy() - got2[0]).max()\n\nprint(f\"Error with the eager model and the reference evaluator: {diff1}, {diff2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison and execution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def clean_name(name):\n    return name.replace(\n        \"_inlfunc_transformers_models_llama_modeling_llama_LlamaAttention\", \"\"\n    ).replace(\"_inlfunc_torch_nn_modules_linear_Linear\", \"\")\n\n\nif sess2 is not None:\n    try:\n        np_inputs = [i.detach().numpy() for i in inputs[0]]\n        res1, res2, align, dc = compare_onnx_execution(\n            model1,\n            model2,\n            inputs=np_inputs,\n            verbose=1,\n            raise_exc=False,\n            cls=ExtendedReferenceEvaluator,\n        )\n        for r in res2:\n            r.name = clean_name(r.name)\n        text = dc.to_str(res1, res2, align, column_size=90)\n        print(text)\n    except AssertionError as e:\n        if \"Unexpected type <class 'list'> for value, it must be a numpy array.\" not in str(e):\n            raise\n        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See `l-long-outputs-llama-diff-export` for a better view.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}