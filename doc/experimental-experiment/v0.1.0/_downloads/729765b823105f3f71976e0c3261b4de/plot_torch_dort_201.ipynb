{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 201: Evaluate DORT\n\nIt compares DORT to eager mode and :epkg:`onnxrt backend`.\n\nTo run the script:\n\n::\n\n    python _doc/examples/plot_torch_dort --help\n\n## Some helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\ntry:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        import onnxruntime\n\n        has_cuda = \"CUDAExecutionProvider\" in onnxruntime.get_available_providers()\nexcept ImportError:\n    print(\"onnxruntime not available.\")\n    import sys\n\n    sys.exit(0)\n\nimport torch._dynamo\nimport contextlib\nimport itertools\nimport os\nimport gc\nimport platform\n\n# import pickle\nimport pprint\nimport multiprocessing\nimport time\nimport cProfile\nimport pstats\nimport io\nimport logging\nfrom pstats import SortKey\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nimport onnx\nfrom onnx_array_api.profiling import profile2graph\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport experimental_experiment\nfrom experimental_experiment.plotting.memory import memory_peak_plot\nfrom experimental_experiment.ext_test_case import measure_time, get_figure\nfrom experimental_experiment.args import get_parsed_args\nfrom experimental_experiment.memory_peak import start_spying_on\nfrom experimental_experiment.torch_models.training_helper import make_aot_ort\nfrom tqdm import tqdm\n\nhas_cuda = has_cuda and torch.cuda.is_available()\nlogging.disable(logging.ERROR)\n\n\ndef system_info():\n    obs = {}\n    obs[\"processor\"] = platform.processor()\n    obs[\"cores\"] = multiprocessing.cpu_count()\n    try:\n        obs[\"cuda\"] = 1 if torch.cuda.is_available() else 0\n        obs[\"cuda_count\"] = torch.cuda.device_count()\n        obs[\"cuda_name\"] = torch.cuda.get_device_name()\n        obs[\"cuda_capa\"] = torch.cuda.get_device_capability()\n    except (RuntimeError, AssertionError):\n        # no cuda\n        pass\n    return obs\n\n\npprint.pprint(system_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scripts arguments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "script_args = get_parsed_args(\n    \"plot_torch_dort\",\n    description=__doc__,\n    scenarios={\n        \"small\": \"small model to test\",\n        \"middle\": \"55Mb model\",\n        \"large\": \"1Gb model\",\n    },\n    warmup=5,\n    repeat=5,\n    repeat1=(1, \"repeat for the first iteration\"),\n    maxtime=(\n        2,\n        \"maximum time to run a model to measure the computation time, \"\n        \"it is 0.1 when scenario is small\",\n    ),\n    expose=\"scenarios,repeat,repeat1,warmup\",\n)\n\nif script_args.scenario in (None, \"small\"):\n    script_args.maxtime = 0.1\nprint(f\"scenario={script_args.scenario or 'small'}\")\nprint(f\"warmup={script_args.warmup}\")\nprint(f\"repeat={script_args.repeat}\")\nprint(f\"repeat1={script_args.repeat1}\")\nprint(f\"maxtime={script_args.maxtime}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The model\n\nA simple model to convert.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyModelClass(nn.Module):\n    def __init__(self, scenario=script_args.scenario):\n        super().__init__()\n        if scenario == \"middle\":\n            self.large = False\n            self.conv1 = nn.Conv2d(1, 32, 5)\n            # self.conv2 = nn.Conv2d(128, 16, 5)\n            self.fc1 = nn.Linear(30752, 1024)\n            self.fcs = []\n            self.fc2 = nn.Linear(1024, 128)\n            self.fc3 = nn.Linear(128, 10)\n        elif scenario in (None, \"small\"):\n            self.large = False\n            self.conv1 = nn.Conv2d(1, 16, 5)\n            # self.conv2 = nn.Conv2d(16, 16, 5)\n            self.fc1 = nn.Linear(144, 512)\n            self.fcs = []\n            self.fc2 = nn.Linear(512, 128)\n            self.fc3 = nn.Linear(128, 10)\n        elif scenario in (None, \"large\"):\n            self.large = True\n            self.conv1 = nn.Conv2d(1, 32, 5)\n            # self.conv2 = nn.Conv2d(128, 16, 5)\n            self.fc1 = nn.Linear(30752, 4096)\n            # torch script does not support loops.\n            self.fca = nn.Linear(4096, 4096)\n            self.fcb = nn.Linear(4096, 4096)\n            self.fcc = nn.Linear(4096, 4096)\n            self.fcd = nn.Linear(4096, 4096)\n            self.fce = nn.Linear(4096, 4096)\n            self.fcf = nn.Linear(4096, 4096)\n            self.fcg = nn.Linear(4096, 4096)\n            self.fch = nn.Linear(4096, 4096)\n            self.fci = nn.Linear(4096, 4096)\n            # end of the unfolded loop.\n            self.fc2 = nn.Linear(4096, 128)\n            self.fc3 = nn.Linear(128, 10)\n        else:\n            raise ValueError(f\"Unsupported scenario={scenario!r}.\")\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), (4, 4))\n        # x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        if self.large:\n            # loop\n            x = F.relu(self.fca(x))\n            x = F.relu(self.fcb(x))\n            x = F.relu(self.fcc(x))\n            x = F.relu(self.fcd(x))\n            x = F.relu(self.fce(x))\n            x = F.relu(self.fcf(x))\n            x = F.relu(self.fcg(x))\n            x = F.relu(self.fch(x))\n            x = F.relu(self.fci(x))\n            # end of the loop\n        x = F.relu(self.fc2(x))\n        y = self.fc3(x)\n        return y\n\n\ndef create_model_and_input(scenario=script_args.scenario):\n    if scenario == \"middle\":\n        shape = [1, 1, 128, 128]\n    elif scenario in (None, \"small\"):\n        shape = [1, 1, 16, 16]\n    elif scenario == \"large\":\n        shape = [1, 1, 128, 128]\n    else:\n        raise ValueError(f\"Unsupported scenario={scenario!r}.\")\n    input_tensor = torch.rand(*shape).to(torch.float32)\n    model = MyModelClass(scenario=scenario)\n    assert model(input_tensor) is not None\n    return model, input_tensor\n\n\ndef torch_model_size(model):\n    size_model = 0\n    for param in model.parameters():\n        size = param.numel() * torch.finfo(param.data.dtype).bits / 8\n        size_model += size\n    return size_model\n\n\nmodel, input_tensor = create_model_and_input()\nmodel_size = torch_model_size(model)\nprint(f\"model size={model_size / 2 ** 20} Mb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backends\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_torch_eager(model, *args):\n    def my_compiler(gm, example_inputs):\n        return gm.forward\n\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            optimized_mod = torch.compile(model, fullgraph=True, backend=my_compiler)\n            optimized_mod(*args)\n            return optimized_mod\n\n\ndef get_torch_default(model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            optimized_mod = torch.compile(model, fullgraph=True, mode=\"reduce-overhead\")\n            optimized_mod(*args)\n            return optimized_mod\n\n\ndef get_torch_dort(model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            local_aot_ort, _ = make_aot_ort(dynamic=True, rewrite=True)\n            optimized_mod = torch.compile(model, backend=local_aot_ort, fullgraph=True)\n            optimized_mod(*args)\n            return optimized_mod\n\n\ndef get_torch_opti(model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            local_aot_ort, _ = make_aot_ort(dynamic=True, rewrite=True)\n            optimized_mod = torch.compile(model, backend=local_aot_ort, fullgraph=True)\n            optimized_mod(*args)\n            return optimized_mod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check they are working.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "export_functions = [\n    get_torch_eager,\n    get_torch_default,\n    get_torch_dort,\n    # get_torch_opti,\n]\n\nexporters = {f.__name__.replace(\"get_\", \"\"): f for f in export_functions}\n\nsupported_exporters = {}\nfor k, v in exporters.items():\n    print(f\"run function {k}\")\n    filename = f\"plot_torch_dort_{k}.onnx\"\n    torch._dynamo.reset()\n    model, input_tensor = create_model_and_input()\n    try:\n        v(model, input_tensor)\n    except Exception as e:\n        print(f\"skipped due to {str(e)[:1000]}\")\n        continue\n    supported_exporters[k] = v\n    del model\n    gc.collect()\n    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile and Memory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def flatten(ps):\n    obs = ps[\"cpu\"].to_dict(unit=2**20)\n    if \"gpus\" in ps:\n        for i, g in enumerate(ps[\"gpus\"]):\n            for k, v in g.to_dict(unit=2**20).items():\n                obs[f\"gpu{i}_{k}\"] = v\n    return obs\n\n\ndata = []\n\nfor k, v in supported_exporters.items():\n    print(f\"run compile for memory {k} on cpu\")\n    filename = f\"plot_torch_dort_{k}.onnx\"\n    if has_cuda:\n        torch.cuda.set_device(0)\n    torch._dynamo.reset()\n    # CPU\n    model, input_tensor = create_model_and_input()\n    stat = start_spying_on(cuda=1 if has_cuda else 0)\n    v(model, input_tensor)\n    obs = flatten(stat.stop())\n    print(\"done.\")\n    obs.update(dict(export=k, p=\"cpu\"))\n    data.append(obs)\n    del model\n    gc.collect()\n    time.sleep(1)\n\n    if not has_cuda:\n        continue\n    if k in {\"torch_default\"}:\n        print(f\"skip compile for memory {k} on cuda\")\n        continue\n    torch._dynamo.reset()\n    # CUDA\n    model, input_tensor = create_model_and_input()\n    model = model.cuda()\n    input_tensor = input_tensor.cuda()\n    print(f\"run compile for memory {k} on cuda\")\n    stat = start_spying_on(cuda=1 if has_cuda else 0)\n    v(model, input_tensor)\n    obs = flatten(stat.stop())\n    print(\"done.\")\n    obs.update(dict(export=k, p=\"cuda\"))\n    data.append(obs)\n    del model\n    gc.collect()\n    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df1 = pandas.DataFrame(data)\ndf1.to_csv(\"plot_torch_dort_1_memory.csv\", index=False)\ndf1.to_excel(\"plot_torch_dort_1_memory.xlsx\", index=False)\nprint(df1)\n\nfor p in [\"cpu\", \"cuda\"]:\n    if not has_cuda and p == \"cuda\":\n        continue\n    ax = memory_peak_plot(\n        df1[df1[\"p\"] == p],\n        key=(\"export\",),\n        bars=[model_size * i / 2**20 for i in range(1, 5)],\n        suptitle=f\"Memory Consumption of the Compilation on {p}\\n\"\n        f\"model size={model_size / 2**20:1.0f} Mb\",\n    )\n    get_figure(ax).savefig(f\"plot_torch_dort_1_memory_{p}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## dort first iteration speed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\n\nfor k, v in supported_exporters.items():\n    print(f\"run dort cpu {k}: {script_args.repeat1}\")\n    times = []\n    for _ in range(int(script_args.repeat1)):\n        model, input_tensor = create_model_and_input()\n        torch._dynamo.reset()\n        begin = time.perf_counter()\n        v(model, input_tensor)\n        duration = time.perf_counter() - begin\n        times.append(duration)\n        del model\n        gc.collect()\n        time.sleep(1)\n\n    print(f\"done: {times[-1]}\")\n    data.append(\n        dict(\n            export=k,\n            time=np.mean(times),\n            min=min(times),\n            max=max(times),\n            first=times[0],\n            last=times[-1],\n            std=np.std(times),\n            p=\"cpu\",\n        )\n    )\n\n    if not has_cuda:\n        continue\n    if k in {\"torch_dort\", \"torch_default\"}:\n        print(f\"skip dort cuda {k}: {script_args.repeat1}\")\n        continue\n    print(f\"run dort cuda {k}: {script_args.repeat1}\")\n    times = []\n    for _ in range(int(script_args.repeat1)):\n        model, input_tensor = create_model_and_input()\n        model = model.cuda()\n        input_tensor = input_tensor.cuda()\n        torch._dynamo.reset()\n        begin = time.perf_counter()\n        v(model, input_tensor)\n        duration = time.perf_counter() - begin\n        times.append(duration)\n        del model\n        gc.collect()\n        time.sleep(1)\n\n    print(f\"done: {times[-1]}\")\n    data.append(\n        dict(\n            export=k,\n            time=np.mean(times),\n            min=min(times),\n            max=max(times),\n            first=times[0],\n            last=times[-1],\n            std=np.std(times),\n            p=\"cuda\",\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df1 = pandas.DataFrame(data)\ndf1.to_csv(\"plot_torch_dort_1_time.csv\", index=False)\ndf1.to_excel(\"plot_torch_dort_1_time.xlsx\", index=False)\nprint(df1)\n\nfig, ax = plt.subplots(1, 1)\ndfi = df1[[\"export\", \"p\", \"time\", \"std\"]].set_index([\"export\", \"p\"])\ndfi[\"time\"].plot.bar(ax=ax, title=\"Compilation time\", yerr=dfi[\"std\"], rot=30)\nfig.tight_layout()\nfig.savefig(\"plot_torch_dort_1_time.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compilation Profiling\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n    pathes = [\n        os.path.abspath(os.path.normpath(os.path.join(os.path.dirname(torch.__file__), \"..\"))),\n        os.path.abspath(os.path.normpath(os.path.join(os.path.dirname(onnx.__file__), \"..\"))),\n        os.path.abspath(\n            os.path.normpath(\n                os.path.join(os.path.dirname(experimental_experiment.__file__), \"..\")\n            )\n        ),\n    ]\n    for p in pathes:\n        text = text.replace(p, \"\")\n    text = text.replace(\"experimental_experiment\", \"experimental_experiment\".upper())\n    return text\n\n\ndef profile_function(name, export_function, with_args=True, verbose=False, suffix=\"export\"):\n    if verbose:\n        print(f\"profile {name}: {export_function}\")\n    if with_args:\n        model, input_tensor = create_model_and_input()\n        pr = cProfile.Profile()\n        pr.enable()\n        for _ in range(int(script_args.repeat1)):\n            export_function(model, input_tensor)\n        pr.disable()\n    else:\n        pr = cProfile.Profile()\n        pr.enable()\n        for _ in range(int(script_args.repeat1)):\n            export_function()\n        pr.disable()\n    s = io.StringIO()\n    sortby = SortKey.CUMULATIVE\n    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n    ps.print_stats()\n    # with open(f\"plot_torch_dort_profile_{name}_{suffix}.pickle\", \"wb\") as f:\n    #     pickle.dump(ps, f)\n\n    raw = s.getvalue()\n    text = \"\\n\".join(raw.split(\"\\n\")[:200])\n    if verbose:\n        print(text)\n    with open(f\"plot_torch_dort_profile_{name}_{suffix}.txt\", \"w\") as f:\n        f.write(raw)\n\n    root, nodes = profile2graph(ps, clean_text=clean_text)\n    text = root.to_text()\n    with open(f\"plot_torch_dort_profile_{name}_{suffix}_h.txt\", \"w\") as f:\n        f.write(text)\n    if verbose:\n        print(\"done.\")\n\n\nmodel, input_tensor = create_model_and_input()\n\n\ndef function_to_profile(model=model, input_tensor=input_tensor):\n    return get_torch_dort(model, input_tensor)\n\n\nprofile_function(\"dort\", function_to_profile, verbose=True, suffix=\"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark exported models with ORT\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark(shape):\n    data = []\n    data_mem_first_run = []\n    data_mem_run = []\n    confs = list(\n        itertools.product(\n            export_functions,\n            [\"CPU\", \"CUDA\"],\n        )\n    )\n    loop = tqdm(confs)\n    print(f\"number of experiments: {len(loop)}\")\n    for export_fct, p in loop:\n        name = export_fct.__name__.replace(\"get_torch_\", \"\")\n        obs = {}  # system_info()\n        obs[\"name\"] = name\n        obs[\"compute\"] = p\n        obs[\"export\"] = name\n\n        model, input_tensor = create_model_and_input()\n        if p == \"CUDA\":\n            if not has_cuda:\n                continue\n            model = model.cuda()\n            input_tensor = input_tensor.cuda()\n        try:\n            exported_model = export_fct(model, input_tensor)\n        except torch._dynamo.exc.BackendCompilerFailed as e:\n            # Triton only supports devices of CUDA Capability >= 7.0,\n            # but your device is of CUDA capability 6.1\n            obs[\"error\"] = str(e)\n            data.append(obs)\n            continue\n\n        def call_model(\n            export_fct=export_fct,\n            exported_model=exported_model,\n            input_tensor=input_tensor,\n        ):\n            res = exported_model(input_tensor).sum()\n            return res\n\n        stat = start_spying_on(cuda=1 if has_cuda else 0)\n        try:\n            call_model()\n        except Exception as e:\n            loop.set_description(f\"ERROR-run: {name} {e}\")\n            obs.update({\"error\": e, \"step\": \"load\"})\n            data.append(obs)\n            stat.stop()\n            continue\n        memobs = flatten(stat.stop())\n        memobs.update(obs)\n        data_mem_first_run.append(memobs)\n\n        # memory consumption\n        stat = start_spying_on(cuda=1 if has_cuda else 0)\n        for _ in range(0, script_args.warmup):\n            call_model()\n        memobs = flatten(stat.stop())\n        memobs.update(obs)\n        data_mem_run.append(memobs)\n\n        obs.update(\n            measure_time(\n                call_model,\n                max_time=script_args.maxtime,\n                repeat=script_args.repeat,\n                number=1,\n            )\n        )\n\n        profile_function(name, call_model, with_args=False, suffix=f\"run_{p}\")\n\n        loop.set_description(f\"{obs['average']} {name} {p}\")\n        data.append(obs)\n        del model\n        del exported_model\n        gc.collect()\n        time.sleep(1)\n\n    df = pandas.DataFrame(data)\n    df.to_csv(\"plot_torch_dort_ort_time.csv\", index=False)\n    df.to_excel(\"plot_torch_dort_ort_time.xlsx\", index=False)\n    dfmemr = pandas.DataFrame(data_mem_run)\n    dfmemr.to_csv(\"plot_torch_dort_ort_run_mem.csv\", index=False)\n    dfmemr.to_excel(\"plot_torch_dort_ort_run_mem.xlsx\", index=False)\n    dfmemfr = pandas.DataFrame(data_mem_first_run)\n    dfmemfr.to_csv(\"plot_torch_dort_ort_first_run_mem.csv\", index=False)\n    dfmemfr.to_excel(\"plot_torch_dort_ort_first_run_mem.xlsx\", index=False)\n    return df, dfmemfr, dfmemr\n\n\ndf, dfmemfr, dfmemr = benchmark(list(input_tensor.shape))\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other view\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def view_time(df, title, suffix=\"time\"):\n    piv = pandas.pivot_table(df, index=\"export\", columns=[\"compute\"], values=\"average\")\n    print(piv)\n    piv.to_csv(f\"plot_torch_dort_{suffix}_compute.csv\")\n    piv.to_excel(f\"plot_torch_dort_{suffix}_compute.xlsx\")\n\n    piv_cpu = pandas.pivot_table(\n        df[df.compute == \"CPU\"],\n        index=\"export\",\n        columns=[\"compute\"],\n        values=\"average\",\n    )\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    fig.suptitle(title)\n    piv_cpu.plot.barh(ax=ax[0], title=\"CPU\", logx=True)\n\n    if has_cuda:\n        piv_gpu = pandas.pivot_table(\n            df[df.compute == \"CUDA\"],\n            index=\"export\",\n            columns=[\"compute\"],\n            values=\"average\",\n        )\n        piv_gpu.plot.barh(ax=ax[1], title=\"CUDA\", logx=True)\n\n    fig.tight_layout()\n    fig.savefig(f\"plot_torch_dort_{suffix}.png\")\n    return ax\n\n\nview_time(df, \"Compares processing time on backends\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory First Running Time (ORT)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for compute in [\"CPU\", \"CUDA\"]:\n    if not has_cuda and compute == \"CUDA\":\n        continue\n    ax = memory_peak_plot(\n        dfmemfr[dfmemfr.compute == compute],\n        (\"export\",),\n        suptitle=f\"Memory Consumption of backend, first running time\"\n        f\"\\nrunning on {compute}\",\n        bars=[model_size * i / 2**20 for i in range(1, 3)],\n        figsize=(18, 6),\n    )\n    get_figure(ax).savefig(f\"plot_torch_dort_first_run_mem_{compute}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Running Time (ORT)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for compute in [\"CPU\", \"CUDA\"]:\n    if not has_cuda and compute == \"CUDA\":\n        continue\n    ax = memory_peak_plot(\n        dfmemr[dfmemr.compute == compute],\n        (\"export\",),\n        suptitle=f\"Memory Consumption of backens, running time\\nrunning on {compute}\",\n        bars=[model_size * i / 2**20 for i in range(1, 3)],\n        figsize=(18, 6),\n    )\n    get_figure(ax).savefig(f\"plot_torch_dort_run_mem_{compute}.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}