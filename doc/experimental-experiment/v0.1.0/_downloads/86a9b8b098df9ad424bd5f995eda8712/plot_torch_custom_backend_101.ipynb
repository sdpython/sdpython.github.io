{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 101: A custom backend for torch\n\nThis example leverages the examples introduced on this page\n[Custom Backends](https://pytorch.org/docs/stable/torch.compiler_custom_backends.html).\nIt uses backend :func:`experimental_experiment.torch_dynamo.onnx_custom_backend`\nbased on :epkg:`onnxruntime` and running on CPU or CUDA.\nIt could easily replaced by \n:func:`experimental_experiment.torch_dynamo.onnx_debug_backend`.\nThis one based on the reference implemented from onnx\ncan show the intermediate results if needed. It is very slow.\n\n## A model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nfrom onnx_array_api.plotting.text_plot import onnx_simple_text_plot\nfrom onnx_array_api.plotting.graphviz_helper import plot_dot\nimport torch\nfrom torch._dynamo.backends.common import aot_autograd\n\n# from torch._functorch._aot_autograd.utils import make_boxed_func\nfrom experimental_experiment.torch_dynamo import (\n    onnx_custom_backend,\n    get_decomposition_table,\n)\n\n\nclass MLP(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(10, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 1),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nx = torch.randn(3, 10, dtype=torch.float32)\n\nmlp = MLP()\nprint(mlp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A custom backend\n\nThis backend leverages :epkg:`onnxruntime`.\nIt is available through function\n:func:`experimental_experiment.torch_dynamo.onnx_custom_backend`\nand implemented by class :class:`OrtBackend\n<experimental_experiment.torch_dynamo.fast_backend.OrtBackend>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compiled_model = torch.compile(\n    copy.deepcopy(mlp),\n    backend=lambda *args, **kwargs: onnx_custom_backend(\n        *args, target_opset=18, **kwargs\n    ),\n    dynamic=False,\n    fullgraph=True,\n)\n\nprint(compiled_model(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\nIt can be used for training as well. The compilation may not\nbe working if the model is using function the converter does not know.\nMaybe, there exist a way to decompose this new function into\nexisting functions. A recommended list is returned by\nwith function :func:`get_decomposition_table\n<experimental_experiment.torch_dynamo.get_decomposition_table>`.\nAn existing list can be filtered out from some inefficient decompositions\nwith function :func:`filter_decomposition_table\n<experimental_experiment.torch_dynamo.filter_decomposition_table>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aot_compiler = aot_autograd(\n    fw_compiler=lambda *args, **kwargs: onnx_custom_backend(\n        *args, target_opset=18, **kwargs\n    ),\n    decompositions=get_decomposition_table(),\n)\n\ncompiled_model = torch.compile(\n    copy.deepcopy(mlp),\n    backend=aot_compiler,\n    fullgraph=True,\n    dynamic=False,\n)\n\nprint(compiled_model(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see an iteration loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n\n\nclass DiabetesDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X / 10).to(torch.float32)\n        self.y = torch.from_numpy(y).to(torch.float32).reshape((-1, 1))\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n\n\ndef trained_model(max_iter=5, dynamic=False, storage=None):\n    aot_compiler = aot_autograd(\n        fw_compiler=lambda *args, **kwargs: onnx_custom_backend(\n            *args, target_opset=18, storage=storage, **kwargs\n        ),\n        decompositions=get_decomposition_table(),\n    )\n\n    compiled_model = torch.compile(\n        MLP(),\n        backend=aot_compiler,\n        fullgraph=True,\n        dynamic=dynamic,\n    )\n\n    trainloader = torch.utils.data.DataLoader(\n        DiabetesDataset(*load_diabetes(return_X_y=True)),\n        batch_size=5,\n        shuffle=True,\n        num_workers=0,\n    )\n\n    loss_function = torch.nn.L1Loss()\n    optimizer = torch.optim.Adam(compiled_model.parameters(), lr=1e-1)\n\n    for epoch in range(0, max_iter):\n        current_loss = 0.0\n\n        for i, data in enumerate(trainloader, 0):\n            X, y = data\n\n            optimizer.zero_grad()\n            p = compiled_model(X)\n            loss = loss_function(p, y)\n            loss.backward()\n\n            optimizer.step()\n\n            current_loss += loss.item()\n\n        print(f\"Loss after epoch {epoch+1}: {current_loss}\")\n\n    print(\"Training process has finished.\")\n    return compiled_model\n\n\ntrained_model(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What about the ONNX model?\n\nThe backend converts the model into ONNX then runs it with :epkg:`onnxruntime`.\nLet's see what it looks like.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "storage = {}\n\ntrained_model(3, storage=storage)\n\nprint(f\"{len(storage['instance'])} were created.\")\n\nfor i, inst in enumerate(storage[\"instance\"][:2]):\n    print()\n    print(f\"-- model {i} running on {inst['providers']}\")\n    print(onnx_simple_text_plot(inst[\"onnx\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The forward graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_dot(storage[\"instance\"][0][\"onnx\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The backward graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_dot(storage[\"instance\"][1][\"onnx\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What about dynamic shapes?\n\nAny input or output having `_dim_` in its name is a dynamic dimension.\nAny output having `_NONE_` in its name is replace by None.\nIt is needed by pytorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "storage = {}\n\ntrained_model(3, storage=storage, dynamic=True)\n\nprint(f\"{len(storage['instance'])} were created.\")\n\nfor i, inst in enumerate(storage[\"instance\"]):\n    print()\n    print(f\"-- model {i} running on {inst['providers']}\")\n    print()\n    print(onnx_simple_text_plot(inst[\"onnx\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The forward graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_dot(storage[\"instance\"][0][\"onnx\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The backward graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_dot(storage[\"instance\"][1][\"onnx\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern Optimizations\n\nBy default, once exported into onnx, a model is optimized by\nlooking for patterns. Each of them locally replaces a couple of\nnodes to optimize the computation\n(see `l-pattern-optimization-onnx` and\n# `l-pattern-optimization-ort`).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}