{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 102: Measure LLAMA speed\n\nThe script is calling many times the script ``experimental_experiment.torch_bench.dort_bench.py``.\n\n::\n\n    python _doc/examples/plot_llama_bench.py --help\n    \nFor exemple, to check mixed precision on multiple backend:\n\n::\n\n    python _doc/examples/plot_llama_bench.py --device=cuda --num_hidden_layers=1 --mixed=1\n\n\nRun the following command to run one experiment and get the available options:\n\n::\n\n    python -m experimental_experiment.torch_bench.dort_bench --help\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from experimental_experiment.args import get_parsed_args, check_cuda_availability\n\nparsed_args = get_parsed_args(\n    \"plot_llama_bench\",\n    description=__doc__,\n    warmup=3,\n    repeat=5,\n    model=(\"llama\", \"model to benchmark\"),\n    backend=(\"eager,inductor,ort,custom,plug\", \"backend to test\"),\n    device=(\"cuda\" if check_cuda_availability() else \"cpu\", \"device to test\"),\n    num_hidden_layers=(\"2\", \"hidden layers to test\"),\n    mixed=(\"0\", \"boolean value to test (mixed precision or not)\"),\n    dynamic=(\"0\", \"boolean value to test dynamic shapes or not\"),\n    script_name=(\"experimental_experiment.torch_bench.dort_bench\", \"script to run\"),\n    dump=(0, \"dump the models with env ONNXRT_DUMP_PATH\"),\n    check=(0, \"just check the script is working, ignores all other parameters\"),\n    config=(\"medium\", \"configuration to use, default or medium\"),\n    patterns=(\"none,default,default+onnxruntime\", \"optimization patterns to use\"),\n    disable_pattern=(\"none\", \"pattern or patterns to disable\"),\n    expose=\"backend,device,num_hidden_layers,mixed,scipt_name,repeat,\"\n    \"warmup,dump,check,config,patterns,dynamic,disable_pattern,model\",\n)\n\nimport onnxruntime  # noqa: F401\nimport numpy as np\nimport pandas\nimport matplotlib.pyplot as plt\nimport itertools\nimport torch\nfrom experimental_experiment.ext_test_case import unit_test_going\nfrom experimental_experiment.bench_run import run_benchmark, get_machine, BenchmarkError\n\nscript_name = \"experimental_experiment.torch_bench.dort_bench\"\nmachine = {} if unit_test_going() else get_machine()\n\n\nrepeat = parsed_args.repeat\nwarmup = parsed_args.warmup\n\n\ndef make_config(\n    model,\n    backend,\n    device,\n    num_hidden_layers,\n    repeat,\n    mixed,\n    dynamic,\n    config,\n    warmup,\n    pattern,\n    disable_pattern,\n    existing=None,\n):\n    cf = dict(\n        model=model,\n        backend=backend,\n        device=device,\n        num_hidden_layers=num_hidden_layers,\n        repeat=repeat,\n        mixed=mixed,\n        dynamic=dynamic,\n        config=config,\n        warmup=warmup,\n    )\n\n    if existing and backend != \"custom\":\n        for ex in existing:\n            if not ex:\n                continue\n            equal = True\n            for k in cf:\n                if cf[k] != ex[k]:\n                    equal = False\n                    break\n            if equal:\n                return None\n\n    if pattern == \"none\":\n        opt = dict(disable_pattern=\"default\")\n    elif pattern in (\"default\", \"default+onnxruntime\"):\n        opt = dict(enable_pattern=pattern)\n    else:\n        raise AssertionError(f\"unexpected value for pattern={pattern!r}\")\n    cf.update(opt)\n    if disable_pattern != \"none\":\n        if \"disable_pattern\" in cf:\n            cf[\"disable_pattern\"] += f\",{disable_pattern}\"\n        else:\n            cf[\"disable_pattern\"] = disable_pattern\n    return cf\n\n\nif parsed_args.check not in (1, \"1\"):\n    verbose = 1\n    configs = []\n    for (\n        backend,\n        device,\n        num_hidden_layers,\n        mixed,\n        dynamic,\n        pattern,\n    ) in itertools.product(\n        parsed_args.backend.split(\",\"),\n        parsed_args.device.split(\",\"),\n        list(map(int, parsed_args.num_hidden_layers.split(\",\"))),\n        list(map(int, parsed_args.mixed.split(\",\"))),\n        list(map(int, parsed_args.dynamic.split(\",\"))),\n        parsed_args.patterns.split(\",\"),\n    ):\n        if mixed == 1 and device == \"cpu\":\n            continue\n        if machine.get(\"capability\", (0, 0)) < (7, 0) and backend == \"inductor\":\n            continue\n        configs.append(\n            make_config(\n                model=parsed_args.model,\n                backend=backend,\n                device=device,\n                num_hidden_layers=num_hidden_layers,\n                repeat=repeat,\n                mixed=mixed,\n                dynamic=dynamic,\n                config=parsed_args.config,\n                warmup=warmup,\n                pattern=pattern,\n                disable_pattern=parsed_args.disable_pattern,\n                existing=configs,\n            )\n        )\nelse:\n    verbose = 5\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    configs = [\n        dict(\n            model=parsed_args.model,\n            backend=\"ort\",\n            device=device,\n            num_hidden_layers=1,\n            repeat=1,\n            mixed=0,\n            dynamic=0,\n            warmup=1,\n            config=\"small\",\n        ),\n    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All configurations to consider.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configs = [cf for cf in configs if cf]\nfor i, cf in enumerate(configs):\n    print(f\"config {i+1}: {cf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running configuration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    data = run_benchmark(\n        parsed_args.script_name,\n        configs,\n        verbose=verbose,\n        stop_if_exception=False,\n        dump=parsed_args.dump in (\"1\", 1),\n    )\n    data_collected = True\nexcept BenchmarkError as e:\n    print(e)\n    data_collected = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's process the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data_collected:\n\n    def clean_pattern(s):\n        if \"+default\" in s:\n            s = s.replace(\"ConstantOfShapeScatterND\", \"\")\n        s = s.replace(\"+default-default\", \"\")\n        return s\n\n    def make_legend(row):\n        row = row.to_dict()\n        val = [row[\"device\"], row[\"backend\"], f\"h{row['num_hidden_layers']}\"]\n        if row[\"mixed\"]:\n            val.append(\"mixed\")\n        if row[\"dynamic\"]:\n            val.append(\"dyn\")\n        if \"patterns\" in row and row[\"patterns\"] and \"nan\" not in str(row[\"patterns\"]):\n            val.append(f\"({clean_pattern(row['patterns'])})\")\n        s = \"-\".join(map(str, val))\n        assert \"nan\" not in s, f\"Legend {s!r} is wrong, row={row}\"\n        return s\n\n    df = pandas.DataFrame(data)\n    df = df.drop([\"OUTPUT\", \"ERROR\"], axis=1)\n    df[\"legend\"] = df.apply(make_legend, axis=1)\n    df[\"time\"] = df[\"time\"].astype(float)\n    min_eager = df[df.legend.str.contains(\"eager\")][\"time\"].dropna().min()\n    df[\"increase\"] = df[\"time\"] / min_eager - 1\n    # df[\"ERROR\"] = df[\"ERROR\"].apply(lambda s: s.replace(\"\\n\", \" \"))\n    filename = f\"plot_{parsed_args.model}_bench_with_cmd.csv\"\n    df.to_csv(filename, index=False)\n\n    df = df.drop([\"CMD\"], axis=1)\n    filename = f\"plot_{parsed_args.model}_bench.csv\"\n    df.to_csv(filename, index=False)\n    df = pandas.read_csv(filename)  # to cast type\n    print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First lines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df.head(2).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More simple\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for c in [\"time\", \"warmup_time\"]:\n    if c not in df.columns:\n        df[c] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simplified data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df.sort_values(\"legend\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot warmup time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch_version = list(set(df[\"torch\"].dropna()))\ntransformers_version = list(set(df[\"transformers\"].dropna()))\nver = f\"{torch_version[0]} - {transformers_version[0]}\"\nmodel = parsed_args.model\nmodeldf = list(set(df[model].dropna()))[0]\n\nif data_collected:\n    fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))\n\n    df = df.sort_values(\"time\").set_index(\"legend\")\n    df[[\"warmup_time\"]].plot.barh(\n        ax=ax, title=f\"lower better\\n{parsed_args.model}\\nwarmup time\\n{ver}\"\n    )\n    ax.grid(True)\n\n    fig.tight_layout()\n    fig.savefig(f\"plot_{parsed_args.model}_bench_warmup_time.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data_collected:\n    fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))\n\n    df[[\"time\"]].plot.barh(\n        ax=ax, title=f\"lower better\\n{parsed_args.model}\\niteration time\\n{ver}\"\n    )\n    mi, ma = df[\"time\"].min(), df[\"time\"].max()\n    mi = mi - (ma - mi) / 10\n    ax.set_xlim(left=mi)\n    ax.grid(True)\n\n    fig.tight_layout()\n    fig.savefig(f\"plot_{parsed_args.model}_bench_time.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot increase.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data_collected:\n    fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))\n\n    df[[\"increase\"]].plot.barh(\n        ax=ax, title=f\"lower better\\n{parsed_args.model}\\ncomparison to eager %\"\n    )\n    ax.grid(True)\n\n    fig.tight_layout()\n    fig.savefig(f\"plot_{parsed_args.model}_bench_relative.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}