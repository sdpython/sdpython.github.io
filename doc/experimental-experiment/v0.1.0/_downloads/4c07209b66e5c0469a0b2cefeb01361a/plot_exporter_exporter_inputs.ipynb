{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Do no use Module as inputs!\n\nThis continues example `l-plot-torch-export-with-dynamic-cache-201`.\n\n## Custom classes are working fine\n\n``DynamicCache`` is replica of :class:`transformers.cache_utils.DynamicCache`\nbut it does not inherits from :class:`transformers.cache_utils.Cache`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional, Tuple\nimport torch\n\n\nclass DynamicCache:\n    def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n        super().__init__()\n        self._seen_tokens = (\n            0  # Used in `generate` to keep tally of how many tokens the cache has seen\n        )\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the cache\n        if key_states is not None:\n            if len(self.key_cache) <= layer_idx:\n                # There may be skipped layers, fill them with empty lists\n                for _ in range(len(self.key_cache), layer_idx):\n                    self.key_cache.append([])\n                    self.value_cache.append([])\n                self.key_cache.append(key_states)\n                self.value_cache.append(value_states)\n            elif (\n                len(self.key_cache[layer_idx]) == 0\n            ):  # fills previously skipped layers; checking for tensor causes errors\n                self.key_cache[layer_idx] = key_states\n                self.value_cache[layer_idx] = value_states\n            else:\n                self.key_cache[layer_idx] = torch.cat(\n                    [self.key_cache[layer_idx], key_states], dim=-2\n                )\n                self.value_cache[layer_idx] = torch.cat(\n                    [self.value_cache[layer_idx], value_states], dim=-2\n                )\n\n        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        is_empty_layer = (\n            len(self.key_cache) == 0  # no cache in any layer\n            or len(self.key_cache)\n            <= layer_idx  # skipped `layer_idx` and hasn't run a layer with cache after it\n            or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n        )\n        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n        return layer_seq_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model uses the class we introduced.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ModelTakingDynamicCacheAsInput(torch.nn.Module):\n    def forward(self, x, dc):\n        kc = torch.cat(dc.key_cache, axis=1)\n        vc = torch.cat(dc.value_cache, axis=1)\n        length = dc.get_seq_length() if dc is not None else 0\n        ones = torch.zeros(\n            (\n                dc.key_cache[0].shape[0],\n                dc.key_cache[0].shape[1],\n                length,\n                dc.key_cache[0].shape[-1],\n            )\n        )\n        w = vc + kc + ones\n        y = w.sum(axis=2, keepdim=True)\n        return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the model runs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3, 8, 7, 1)\ncache = DynamicCache(1)\ncache.update(torch.ones((3, 8, 5, 6)), (torch.ones((3, 8, 5, 6)) * 2), 0)\n\nmodel = ModelTakingDynamicCacheAsInput()\nexpected = model(x, cache)\n\nprint(expected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check it works with other shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 8, 7, 1)\ncache = DynamicCache(1)\ncache.update(torch.ones((4, 8, 11, 6)), (torch.ones((4, 8, 11, 6)) * 2), 0)\n\nmodel = ModelTakingDynamicCacheAsInput()\nexpected = model(x, cache)\n\nprint(expected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's export after serialization functions were registered as shown in\n`l-plot-torch-export-with-dynamic-cache-201`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def flatten_dynamic_cache(\n    dynamic_cache: DynamicCache,\n) -> Tuple[List[Any], torch.utils._pytree.Context]:\n    flat = [\n        (k, getattr(dynamic_cache, k))\n        for k in [\"key_cache\", \"value_cache\"]\n        if hasattr(dynamic_cache, k)\n    ]\n    return [f[1] for f in flat], [f[0] for f in flat]\n\n\ndef unflatten_dynamic_cache(\n    values: List[Any],\n    context: torch.utils._pytree.Context,\n    output_type=None,\n) -> DynamicCache:\n    cache = DynamicCache()\n    values = dict(zip(context, values))\n    for k, v in values.items():\n        setattr(cache, k, v)\n    return cache\n\n\ndef flatten_with_keys_dynamic_cache(d: Dict[Any, Any]) -> Tuple[\n    List[Tuple[torch.utils._pytree.KeyEntry, Any]],\n    torch.utils._pytree.Context,\n]:\n    values, context = flatten_dynamic_cache(d)\n    return [(torch.utils._pytree.MappingKey(k), v) for k, v in zip(context, values)], context\n\n\ntorch.utils._pytree.register_pytree_node(\n    DynamicCache,\n    flatten_dynamic_cache,\n    unflatten_dynamic_cache,\n    serialized_type_name=f\"{DynamicCache.__module__}.{DynamicCache.__name__}\",\n    flatten_with_keys_fn=flatten_with_keys_dynamic_cache,\n)\ntorch.fx._pytree.register_pytree_flatten_spec(\n    DynamicCache, lambda x, _: [x.key_cache, x.value_cache]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's export with dynamic shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = torch.export.Dim(\"batch\", min=1, max=1024)\nclength = torch.export.Dim(\"clength\", min=1, max=1024)\n\nep = torch.export.export(\n    model,\n    (x, cache),\n    dynamic_shapes=({0: batch}, [[{0: batch, 2: clength}], [{0: batch, 2: clength}]]),\n)\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We remove the changes for pytorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.utils._pytree.SUPPORTED_NODES.pop(DynamicCache)\ntorch.fx._pytree.SUPPORTED_NODES.pop(DynamicCache)\ntorch.fx._pytree.SUPPORTED_NODES_EXACT_MATCH.pop(DynamicCache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything looks fine but now...\n\n## DynamicCache(torch.nn.Module)\n\nThat's the only change we make.\nEverything else is the same.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DynamicCache(torch.nn.Module):\n    def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n        super().__init__()\n        self._seen_tokens = (\n            0  # Used in `generate` to keep tally of how many tokens the cache has seen\n        )\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the cache\n        if key_states is not None:\n            if len(self.key_cache) <= layer_idx:\n                # There may be skipped layers, fill them with empty lists\n                for _ in range(len(self.key_cache), layer_idx):\n                    self.key_cache.append([])\n                    self.value_cache.append([])\n                self.key_cache.append(key_states)\n                self.value_cache.append(value_states)\n            elif (\n                len(self.key_cache[layer_idx]) == 0\n            ):  # fills previously skipped layers; checking for tensor causes errors\n                self.key_cache[layer_idx] = key_states\n                self.value_cache[layer_idx] = value_states\n            else:\n                self.key_cache[layer_idx] = torch.cat(\n                    [self.key_cache[layer_idx], key_states], dim=-2\n                )\n                self.value_cache[layer_idx] = torch.cat(\n                    [self.value_cache[layer_idx], value_states], dim=-2\n                )\n\n        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        is_empty_layer = (\n            len(self.key_cache) == 0  # no cache in any layer\n            or len(self.key_cache)\n            <= layer_idx  # skipped `layer_idx` and hasn't run a layer with cache after it\n            or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n        )\n        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n        return layer_seq_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model uses the class we introduced.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ModelTakingDynamicCacheAsInput(torch.nn.Module):\n    def forward(self, x, dc):\n        kc = torch.cat(dc.key_cache, axis=1)\n        vc = torch.cat(dc.value_cache, axis=1)\n        length = dc.get_seq_length() if dc is not None else 0\n        ones = torch.zeros(\n            (\n                dc.key_cache[0].shape[0],\n                dc.key_cache[0].shape[1],\n                length,\n                dc.key_cache[0].shape[-1],\n            )\n        )\n        w = vc + kc + ones\n        y = w.sum(axis=2, keepdim=True)\n        return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the model runs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3, 8, 7, 1)\ncache = DynamicCache(1)\ncache.update(torch.ones((3, 8, 5, 6)), (torch.ones((3, 8, 5, 6)) * 2), 0)\n\nmodel = ModelTakingDynamicCacheAsInput()\nexpected = model(x, cache)\n\nprint(expected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check it works with other shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(4, 8, 7, 1)\ncache = DynamicCache(1)\ncache.update(torch.ones((4, 8, 11, 6)), (torch.ones((4, 8, 11, 6)) * 2), 0)\n\nmodel = ModelTakingDynamicCacheAsInput()\nexpected = model(x, cache)\n\nprint(expected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's export after serialization functions were registered as shown in\n`l-plot-torch-export-with-dynamic-cache-201`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def flatten_dynamic_cache(\n    dynamic_cache: DynamicCache,\n) -> Tuple[List[Any], torch.utils._pytree.Context]:\n    flat = [\n        (k, getattr(dynamic_cache, k))\n        for k in [\"key_cache\", \"value_cache\"]\n        if hasattr(dynamic_cache, k)\n    ]\n    return [f[1] for f in flat], [f[0] for f in flat]\n\n\ndef unflatten_dynamic_cache(\n    values: List[Any],\n    context: torch.utils._pytree.Context,\n    output_type=None,\n) -> DynamicCache:\n    cache = DynamicCache()\n    values = dict(zip(context, values))\n    for k, v in values.items():\n        setattr(cache, k, v)\n    return cache\n\n\ndef flatten_with_keys_dynamic_cache(d: Dict[Any, Any]) -> Tuple[\n    List[Tuple[torch.utils._pytree.KeyEntry, Any]],\n    torch.utils._pytree.Context,\n]:\n    values, context = flatten_dynamic_cache(d)\n    return [(torch.utils._pytree.MappingKey(k), v) for k, v in zip(context, values)], context\n\n\ntorch.utils._pytree.register_pytree_node(\n    DynamicCache,\n    flatten_dynamic_cache,\n    unflatten_dynamic_cache,\n    serialized_type_name=f\"{DynamicCache.__module__}.{DynamicCache.__name__}\",\n    flatten_with_keys_fn=flatten_with_keys_dynamic_cache,\n)\ntorch.fx._pytree.register_pytree_flatten_spec(\n    DynamicCache, lambda x, _: [x.key_cache, x.value_cache]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's export with dynamic shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = torch.export.Dim(\"batch\", min=1, max=1024)\nclength = torch.export.Dim(\"clength\", min=1, max=1024)\n\ntry:\n    ep = torch.export.export(\n        model,\n        (x, cache),\n        dynamic_shapes=({0: batch}, [[{0: batch, 2: clength}], [{0: batch, 2: clength}]]),\n    )\n    print(ep)\nexcept Exception as e:\n    print(f\"It did not work: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There exists a little trick to bypass that issue:\nwe changed the base class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BaseDummyClass:\n    pass\n\n\nDynamicCache.__bases__ = (BaseDummyClass,)\n\nep = torch.export.export(\n    model,\n    (x, cache),\n    dynamic_shapes=({0: batch}, [[{0: batch, 2: clength}], [{0: batch, 2: clength}]]),\n)\nprint(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We remove the changes for pytorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.utils._pytree.SUPPORTED_NODES.pop(DynamicCache)\ntorch.fx._pytree.SUPPORTED_NODES.pop(DynamicCache)\ntorch.fx._pytree.SUPPORTED_NODES_EXACT_MATCH.pop(DynamicCache)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}