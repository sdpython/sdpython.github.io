{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Convolution and Matrix Multiplication\n\nThe [convolution](https://en.wikipedia.org/wiki/Kernel_(image_processing))\nis a well known image transformation used to transform an image.\nIt can be used to blur, to compute the gradient in one direction and\nit is widely used in deep neural networks.\nHaving a fast implementation is important.\n\n## numpy\n\nImage have often 4 dimensions (N, C, H, W) = (batch, channels, height, width).\nLet's first start with a 2D image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\nimport numpy as np\nfrom numpy.testing import assert_almost_equal\nfrom onnx.reference import ReferenceEvaluator\nfrom onnx_array_api.light_api import start\nfrom onnx_array_api.plotting.graphviz_helper import plot_dot\nfrom onnxruntime import InferenceSession\nfrom torch import from_numpy\nfrom torch.nn import Fold, Unfold\nfrom torch.nn.functional import conv_transpose2d, conv2d\nfrom experimental_experiment.gradient.grad_helper import (\n    onnx_derivative,\n    DerivativeOptions,\n)\n\n\nshape = (5, 7)\nN = np.prod(shape)\ndata = np.arange(N).astype(np.float32).reshape(shape)\n# data[:, :] = 0\n# data[2, 3] = 1\ndata.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's a 2D kernel, the same one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel = (np.arange(9) + 1).reshape(3, 3).astype(np.float32)\nkernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### raw convolution\n\nA raw version of a 2D convolution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def raw_convolution(data: np.array, kernel: Sequence[int]) -> np.array:\n    rx = (kernel.shape[0] - 1) // 2\n    ry = (kernel.shape[1] - 1) // 2\n    res = np.zeros(data.shape, dtype=data.dtype)\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            for x in range(kernel.shape[0]):\n                for y in range(kernel.shape[1]):\n                    a = i + x - rx\n                    b = j + y - ry\n                    if a < 0 or b < 0 or a >= data.shape[0] or b >= data.shape[1]:\n                        continue\n                    res[i, j] += kernel[x, y] * data[a, b]\n    return res\n\n\nres = raw_convolution(data, kernel)\nres.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### With pytorch\n\n*pytorch* is optimized for deep learning and prefers 4D tenors\nto represent multiple images. We add two empty dimension\nto the previous example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rest = conv2d(\n    from_numpy(data[np.newaxis, np.newaxis, ...]),\n    from_numpy(kernel[np.newaxis, np.newaxis, ...]),\n    padding=(1, 1),\n)\nrest.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything works.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert_almost_equal(res, rest[0, 0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### using Gemm?\n\nA fast implementation could reuse whatever exists with a fast implementation\nsuch as a matrix multiplication. The goal is to transform the tensor `data`\ninto a new matrix which can be mutiplied with a flatten kernel and finally\nreshaped into the expected result. pytorch calls this function\n[Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html).\nThis function is also called\n[im2col](https://caffe.berkeleyvision.org/tutorial/layers/im2col.html).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unfold = Unfold(kernel_size=(3, 3), padding=(1, 1))(\n    from_numpy(data[np.newaxis, np.newaxis, ...])\n)\nunfold.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then multiply this matrix with the flattened kernel and reshape it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "impl = kernel.flatten() @ unfold.numpy()\nimpl = impl.reshape(data.shape)\nimpl.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "impl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything works as expected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert_almost_equal(res, impl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is ConvTranspose?\n\nDeep neural network are trained with a stochastic gradient descent.\nThe gradient of every layer needs to be computed including the gradient\nof a convolution transpose. That seems easier with the second expression\nof a convolution relying on a matrix multiplication and function `im2col`.\n`im2col` is just a new matrix built from `data` where every value was\ncopied in 9=3x3 locations. The gradient against an input value `data[i,j]`\nis the sum of 9=3x3 values from the output gradient. If `im2col` plays\nwith indices, the gradient requires to do the same thing in the other way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# impl[:, :] = 0\n# impl[2, 3] = 1\nimpl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ConvTranspose...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ct = conv_transpose2d(\n    from_numpy(impl.reshape(data.shape)[np.newaxis, np.newaxis, ...]),\n    from_numpy(kernel[np.newaxis, np.newaxis, ...]),\n    padding=(1, 1),\n).numpy()\nct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now the version with `col2im` or\n[Fold](https://pytorch.org/docs/stable/generated/torch.nn.Fold.html#torch.nn.Fold)\napplied on the result product of the output from `Conv` and the kernel:\nthe output of `Conv` is multiplied by every coefficient of the kernel.\nThen all these matrices are concatenated to build a matrix of the same\nshape of `unfold`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = kernel.flatten().reshape((-1, 1)) @ impl.flatten().reshape((1, -1))\np.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fold...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fold = Fold(kernel_size=(3, 3), output_size=(5, 7), padding=(1, 1))(\n    from_numpy(p[np.newaxis, ...])\n)\nfold.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## onnxruntime-training\n\nFollowing lines shows how :epkg:`onnxruntime` handles the\ngradient computation. This section still needs work.\n\n### Conv\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = (\n    start()\n    .vin(\"X\", shape=[1, 1, None, None])\n    .cst(kernel[np.newaxis, np.newaxis, ...])\n    .rename(\"W\")\n    .bring(\"X\", \"W\")\n    .Conv(pads=[1, 1, 1, 1])\n    .rename(\"Y\")\n    .vout()\n    .to_onnx()\n)\nplot_dot(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref = ReferenceEvaluator(model)\nref.run(None, {\"X\": data[np.newaxis, np.newaxis, ...]})[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grad = onnx_derivative(\n    model, options=DerivativeOptions.FillGrad | DerivativeOptions.KeepOutputs, verbose=1\n)\nplot_dot(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(grad.SerializeToString(), providers=[\"CPUExecutionProvider\"])\nres = sess.run(\n    None,\n    {\n        \"X\": data[np.newaxis, np.newaxis, ...],\n        \"W\": kernel[np.newaxis, np.newaxis, ...],\n    },\n)\nres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConvTranspose\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = (\n    start()\n    .vin(\"X\", shape=[1, 1, None, None])\n    .cst(kernel[np.newaxis, np.newaxis, ...])\n    .rename(\"W\")\n    .bring(\"X\", \"W\")\n    .ConvTranspose(pads=[1, 1, 1, 1])\n    .rename(\"Y\")\n    .vout()\n    .to_onnx()\n)\nplot_dot(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(model.SerializeToString(), providers=[\"CPUExecutionProvider\"])\nct = sess.run(None, {\"X\": impl[np.newaxis, np.newaxis, ...]})[0]\nct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## im2col and col2im\n\nFunction `im2col` transforms an image so that the convolution of this image\ncan be expressed as a matrix multiplication. It takes the image and the kernel shape.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _get_indices(i: int, shape: Sequence[int]) -> np.array:\n    res = np.empty((len(shape),), dtype=np.int64)\n    k = len(shape) - 1\n    while k > 0:\n        m = i % shape[k]\n        res[k] = m\n        i -= m\n        i /= shape[k]\n        k -= 1\n    res[0] = i\n    return res\n\n\ndef _is_out(ind: Sequence[int], shape: Sequence[int]) -> bool:\n    for i, s in zip(ind, shape):\n        if i < 0:\n            return True\n        if i >= s:\n            return True\n    return False\n\n\ndef im2col_naive_implementation(\n    data: np.array, kernel_shape: Sequence[int], fill_value: int = 0\n) -> np.array:\n    \"\"\"\n    Naive implementation for `im2col` or\n    :func:`torch.nn.Unfold` (but with `padding=1`).\n\n    :param image: image (float)\n    :param kernel_shape: kernel shape\n    :param fill_value: fill value\n    :return: result\n    \"\"\"\n    if not isinstance(kernel_shape, tuple):\n        raise TypeError(f\"Unexpected type {type(kernel_shape)!r} for kernel_shape.\")\n    if len(data.shape) != len(kernel_shape):\n        raise ValueError(f\"Shape mismatch {data.shape!r} and {kernel_shape!r}.\")\n    output_shape = data.shape + kernel_shape\n    res = np.empty(output_shape, dtype=data.dtype)\n    middle = np.array([-m / 2 for m in kernel_shape], dtype=np.int64)\n    kernel_size = np.prod(kernel_shape)\n    data_size = np.prod(data.shape)\n    for i in range(data_size):\n        for j in range(kernel_size):\n            i_data = _get_indices(i, data.shape)\n            i_kernel = _get_indices(j, kernel_shape)\n            ind = i_data + i_kernel + middle\n            t_data = tuple(i_data)\n            t_kernel = tuple(i_kernel)\n            i_out = t_data + t_kernel\n            res[i_out] = fill_value if _is_out(ind, data.shape) else data[tuple(ind)]\n    return res\n\n\nv = np.arange(5).astype(np.float32)\nw = im2col_naive_implementation(v, (3,))\nw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All is left is the matrix multiplication.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "k = np.array([1, 1, 1], dtype=np.float32)\nconv = w @ k\nconv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare with the numpy function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.convolve(v, k, mode=\"same\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "..math::\n\n    conv(v, k) = im2col(v, shape(k)) \\; k = w \\; k` where `w = im2col(v, shape(k))\n\nIn deep neural network, the gradient is propagated from the last layer\nto the first one. At some point, the backpropagation produces the gradient\n$\\frac{d(E)}{d(conv)}$, the gradient of the error against\nthe outputs of the convolution layer. Then\n$\\frac{d(E)}{d(v)} = \\frac{d(E)}{d(conv(v, k))}\\frac{d(conv(v, k))}{d(v)}$.\n\nWe need to compute\n$\\frac{d(conv(v, k))}{d(v)} = \\frac{d(conv(v, k))}{d(w)}\\frac{d(w)}{d(v)}$.\n\nWe can say that $\\frac{d(conv(v, k))}{d(w)} = k$.\n\nThat leaves $\\frac{d(w)}{d(v)} = \\frac{d(im2col(v, shape(k)))}{d(v)}$.\nAnd this last term is equal to $im2col(m, shape(k))$ where $m$\nis a matrix identical to $v$ except that all not null parameter\nare replaced by 1. To summarize:\n$\\frac{d(im2col(v, shape(k)))}{d(v)} = im2col(v \\neq 0, shape(k))$.\n\nFinally:\n\n\\begin{align}\\frac{d(E)}{d(v)} = \\frac{d(E)}{d(conv(v, k))}\\frac{d(conv(v, k))}{d(v)} = \\frac{d(E)}{d(conv(v, k))} \\; k \\; im2col(v \\neq 0, shape(k))\\end{align}\n\nNow, $im2col(v \\neq 0, shape(k))$ is a very simple matrix with only ones or zeros.\nIs there a way we can avoid doing the matrix multiplication but simply\nadding terms? That's the purpose of function ``col2im`` defined so that:\n\n\\begin{align}\\frac{d(E)}{d(v)} = \\frac{d(E)}{d(conv(v, k))} \\; k \\; im2col(v \\neq 0, shape(k)) = col2im\\left(\\frac{d(E)}{d(conv(v, k))} \\; k, shape(k) \\right)\\end{align}\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}