{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Evaluate different ways to export a torch model to ONNX\n\nThe example evaluates the performance of onnxruntime of a simple\ntorch model after it was converted into ONNX through different processes:\n\n* [TorchScript-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter),\n  let's call it **script**\n* [TorchDynamo-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter),\n  let's call it **dynamo**\n* if available, the previous model but optimized, **dynopt**\n* a custom exporter **cus_p0**, this exporter supports a very limited\n  set of models, as **dynamo**, it relies on\n  [torch.fx](https://pytorch.org/docs/stable/fx.html) but the design is closer to\n  what tensorflow-onnx does.\n* the same exporter but unused nodes were removed, **cus_p1**\n* the same exporter but constant where folded, **cus_p2**\n\n## Some helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nimport os\nimport platform\nimport pprint\nimport multiprocessing\nimport time\nimport cProfile\nimport pstats\nimport io\nfrom pstats import SortKey\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nimport onnx\nfrom onnx_extended.ext_test_case import measure_time\nfrom onnx_array_api.plotting.text_plot import onnx_simple_text_plot\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport experimental_experiment\nfrom experimental_experiment.torch_exp.onnx_export import to_onnx\nfrom tqdm import tqdm\n\n\ndef system_info():\n    obs = {}\n    obs[\"processor\"] = platform.processor()\n    obs[\"cores\"] = multiprocessing.cpu_count()\n    try:\n        obs[\"cuda\"] = 1 if torch.cuda.is_available() else 0\n        obs[\"cuda_count\"] = torch.cuda.device_count()\n        obs[\"cuda_name\"] = torch.cuda.get_device_name()\n        obs[\"cuda_capa\"] = torch.cuda.get_device_capability()\n    except RuntimeError:\n        # no cuda\n        pass\n    return obs\n\n\npprint.pprint(system_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The model\n\nA simple model to convert.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 128, 5)\n        self.conv2 = nn.Conv2d(128, 16, 5)\n        self.fc1 = nn.Linear(13456, 1024)\n        self.fc2 = nn.Linear(1024, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The exporters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def export_script(filename, model, *args):\n    torch.onnx.export(model, *args, filename, input_names=[\"input\"])\n\n\ndef export_dynamo(filename, model, *args):\n    export_output = torch.onnx.dynamo_export(model, *args)\n    export_output.save(filename)\n\n\ndef export_dynopt(filename, model, *args):\n    export_output = torch.onnx.dynamo_export(model, *args)\n    export_output.save(filename)\n    model_onnx = onnx.load(filename)\n\n    from onnxrewriter.optimizer import optimize\n\n    optimized_model = optimize(model_onnx)\n    with open(filename, \"wb\") as f:\n        f.write(optimized_model.SerializeToString())\n\n\ndef export_cus_p0(filename, model, *args):\n    onx = to_onnx(model, tuple(args), input_names=[\"input\"])\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())\n\n\ndef export_cus_p1(filename, model, *args):\n    onx = to_onnx(model, tuple(args), input_names=[\"input\"], remove_unused=True)\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())\n\n\ndef export_cus_p2(filename, model, *args):\n    onx = to_onnx(\n        model,\n        tuple(args),\n        input_names=[\"input\"],\n        remove_unused=True,\n        constant_folding=True,\n    )\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check they are working.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "export_functions = [\n    export_script,\n    export_dynamo,\n    export_dynopt,\n    export_cus_p0,\n    export_cus_p1,\n    export_cus_p2,\n]\n\nexporters = {f.__name__.replace(\"export_\", \"\"): f for f in export_functions}\nshape = [1, 1, 128, 128]\ninput_tensor = torch.rand(*shape).to(torch.float32)\nmodel = MyModel()\n\nsupported_exporters = {}\nfor k, v in exporters.items():\n    print(f\"run exporter {k}\")\n    filename = f\"plot_torch_export_{k}.onnx\"\n    try:\n        v(filename, model, input_tensor)\n    except Exception as e:\n        print(f\"skipped due to {e}\")\n        continue\n    supported_exporters[k] = v\n    print(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporter speed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\n\nfor k, v in supported_exporters.items():\n    print(f\"run exporter {k}\")\n    filename = f\"plot_torch_export_{k}.onnx\"\n    times = []\n    for i in range(5):\n        begin = time.perf_counter()\n        v(filename, model, input_tensor)\n        duration = time.perf_counter() - begin\n        times.append(duration)\n    onx = onnx.load(filename)\n    print(\"done.\")\n    data.append(\n        dict(\n            export=k,\n            time=np.mean(duration),\n            min=min(times),\n            max=max(times),\n            first=times[0],\n            last=times[-1],\n            std=np.std(times),\n            nodes=len(onx.graph.node),\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last export to measure time torch spends in export the model\nbefore any other export can begin the translation\nexcept the first one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "begin = time.perf_counter()\nfor i in range(5):\n    exported_mod = torch.export.export(model, (input_tensor,))\nduration = time.perf_counter() - begin\ndata.append(dict(export=\"torch\", time=duration / 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df1 = pandas.DataFrame(data)\nprint(df1)\n\nfig, ax = plt.subplots(1, 1)\ndfi = df1[[\"export\", \"time\", \"std\"]].set_index(\"export\")\ndfi[\"time\"].plot.barh(ax=ax, title=\"Export time\", yerr=dfi[\"std\"])\nfig.tight_layout()\nfig.savefig(\"plot_torch_export.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pr = cProfile.Profile()\npr.enable()\nfor i in range(5):\n    export_cus_p0(\"dummy.onnx\", model, input_tensor)\npr.disable()\ns = io.StringIO()\nsortby = SortKey.CUMULATIVE\nps = pstats.Stats(pr, stream=s).sort_stats(sortby)\nps.print_stats()\n\n\ndef clean_text(text):\n    pathes = [\n        os.path.abspath(\n            os.path.normpath(os.path.join(os.path.dirname(torch.__file__), \"..\"))\n        ),\n        os.path.abspath(\n            os.path.normpath(os.path.join(os.path.dirname(onnx.__file__), \"..\"))\n        ),\n        os.path.abspath(\n            os.path.normpath(\n                os.path.join(os.path.dirname(experimental_experiment.__file__), \"..\")\n            )\n        ),\n    ]\n    for p in pathes:\n        text = text.replace(p, \"\")\n    text = text.replace(\"experimental_experiment\", \"experimental_experiment\".upper())\n    return text\n\n\ntext = \"\\n\".join(s.getvalue().split(\"\\n\")[:200])\nprint(clean_text(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following display helps to understand.\nMost of the tiume added by the custom converter is used to\nconverter the initializer and build the onnx model once the conversion\nis complete.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# from onnx_array_api.profiling import profile2graph\n# root, nodes = profile2graph(ps, clean_text=clean_text)\n# text = root.to_text()\n# print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark():\n    from onnxruntime import InferenceSession, SessionOptions, GraphOptimizationLevel\n\n    shape = [1, 1, 128, 128]\n    data = []\n    confs = list(\n        itertools.product(\n            [_ for _ in os.listdir(\".\") if \".onnx\" in _ and _.startswith(\"plot_torch\")],\n            [\n                [\"CPUExecutionProvider\"],\n                [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n            ],\n            [\"0\", \"1\"],\n        )\n    )\n    loop = tqdm(confs)\n    print(f\"number of experiments: {len(loop)}\")\n    for name, ps, aot in loop:\n        root = os.path.split(name)[-1]\n        _, ext = os.path.splitext(root)\n        if ext != \".onnx\":\n            continue\n\n        obs = {}  # system_info()\n        obs[\"name\"] = name\n        obs[\"providers\"] = \",\".join(ps)\n        p = \"CUDA\" if \"CUDA\" in obs[\"providers\"] else \"CPU\"\n        obs[\"compute\"] = p\n        obs[\"aot\"] = 1 if aot == \"0\" else 0\n        obs[\"export\"] = name.replace(\"plot_torch_export_\", \"\").replace(\".onnx\", \"\")\n\n        onx = onnx.load(name)\n        obs[\"n_nodes\"] = len(onx.graph.node)\n        obs[\"n_function\"] = len(onx.functions or [])\n        obs[\"n_sub\"] = len([n for n in onx.graph.node if n.op_type == \"Sub\"])\n\n        opts = SessionOptions()\n        opts.add_session_config_entry(\"session.disable_aot_function_inlining\", aot)\n        opts.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n        opts.optimized_model_filepath = (\n            f\"ort-{name.replace('.onnx', '')}-{p.lower()}-aot{aot}.onnx\"\n        )\n\n        try:\n            sess = InferenceSession(name, opts, providers=ps)\n        except Exception as e:\n            loop.set_description(f\"ERROR-load: {name} {e}\")\n            obs.update({\"error\": e, \"step\": \"run\"})\n            data.append(obs)\n            continue\n\n        input_name = sess.get_inputs()[0].name\n        feeds = {input_name: np.random.rand(*shape).astype(np.float32)}\n        try:\n            for i in range(0, 5):\n                sess.run(None, feeds)\n        except Exception as e:\n            loop.set_description(f\"ERROR-run: {name} {e}\")\n            obs.update({\"error\": e, \"step\": \"load\"})\n            data.append(obs)\n            continue\n        obs.update(measure_time(lambda: sess.run(None, feeds), max_time=1))\n\n        loop.set_description(f\"{obs['average']} {name} {ps}\")\n        data.append(obs)\n\n    df = pandas.DataFrame(data)\n    df.to_csv(\"benchmark.csv\", index=False)\n    df.to_excel(\"benchmark.xlsx\", index=False)\n    return df\n\n\ndf = benchmark()\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other view\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = pandas.pivot_table(\n    df, index=\"export\", columns=[\"compute\", \"aot\"], values=\"average\"\n)\nprint(piv)\n\nfig, ax = plt.subplots()\npiv.plot.barh(ax=ax, title=\"Compares onnxruntime time on exported models\")\nfig.tight_layout()\nfig.savefig(\"plot_torch_export_ort.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the interesting models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models = [\n    _ for _ in os.listdir(\".\") if \".onnx\" in _ and _.startswith(\"ort-plot_torch_export\")\n]\nfor model in models:\n    if (\n        \"cpu\" not in model\n        and \"cuda\" not in model\n        or \"aot\" not in model\n        or \"cus_p0\" in model\n        or \"cus_p1\" in model\n    ):\n        print(\"skip1\", model)\n        continue\n    if (\"dynamo\" in model or \"dynopt\" in model) and \"aot0\" in model:\n        print(\"skip2\", model)\n        continue\n    if \"aot1\" in model and (\"dynamo\" in model or \"dynopt\" in model):\n        print(\"skip3\", model)\n        continue\n    print()\n    print(\"#################################################\")\n    print(model)\n    print(\"#################################################\")\n    onx = onnx.load(model)\n    print(onnx_simple_text_plot(onx))\n\nprint(\"done.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}