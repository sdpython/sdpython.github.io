{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 101: Some dummy examples with torch.export.export\n\n## Easy Case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n\nclass Neuron(torch.nn.Module):\n    def __init__(self, n_dims: int = 5, n_targets: int = 3):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_dims, n_targets)\n\n    def forward(self, x):\n        z = self.linear(x)\n        return torch.sigmoid(z)\n\n\nexported_program = torch.export.export(Neuron(), (torch.randn(1, 5),))\nprint(exported_program.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With an integer as input\n\nAs [torch.export.export](https://pytorch.org/docs/stable/export.html)\ndocumentation, integer do not show up on the graph.\nAn exporter based on :func:`torch.export.export` cannot consider\nthe integer as an input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NeuronIInt(torch.nn.Module):\n    def __init__(self, n_dims: int = 5, n_targets: int = 3):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_dims, n_targets)\n\n    def forward(self, x: torch.Tensor, i_input: int):\n        z = self.linear(x)\n        return torch.sigmoid(z)[:, i_input]\n\n\nexported_program = torch.export.export(NeuronIInt(), (torch.randn(1, 5), 2))\nprint(exported_program.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With an integer as input\n\nBut if the integer is wrapped into a Tensor, it works.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NeuronIInt(torch.nn.Module):\n    def __init__(self, n_dims: int = 5, n_targets: int = 3):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_dims, n_targets)\n\n    def forward(self, x: torch.Tensor, i_input):\n        z = self.linear(x)\n        return torch.sigmoid(z)[:, i_input]\n\n\nexported_program = torch.export.export(\n    NeuronIInt(), (torch.randn(1, 5), torch.Tensor([2]).to(torch.int32))\n)\nprint(exported_program.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrapped\n\nWrapped, it continues to work.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WrappedNeuronIInt(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, *args, **kwargs):\n        return self.model.forward(*args, **kwargs)\n\n\nexported_program = torch.export.export(\n    WrappedNeuronIInt(NeuronIInt()), (torch.randn(1, 5), torch.Tensor([2]).to(torch.int32))\n)\nprint(exported_program.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List\n\nThe last one does not export. An exporter based on\n:func:`torch.export.export` cannot work.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NeuronNoneListInt(torch.nn.Module):\n    def __init__(self, n_dims: int = 5, n_targets: int = 3):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_dims, n_targets)\n\n    def forward(self, x, yz, i_input):\n        z = self.linear(x + yz[0] * yz[3])\n        return torch.sigmoid(z)[:i_input]\n\n\ntry:\n    exported_program = torch.export.export(\n        NeuronNoneListInt(),\n        (\n            torch.randn(1, 5),\n            [torch.randn(1, 5), None, None, torch.randn(1, 5)],\n            torch.Tensor([2]).to(torch.int32),\n        ),\n    )\n    print(exported_program.graph)\nexcept torch._dynamo.exc.Unsupported as e:\n    print(\"-- an error occured:\")\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loops\n\nLoops are not captured.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NeuronLoop(torch.nn.Module):\n    def __init__(self, n_dims: int = 5, n_targets: int = 3):\n        super().__init__()\n        self.linear = torch.nn.Linear(n_dims, n_targets)\n\n    def forward(self, x, xs):\n        z = self.linear(x)\n        for i in range(len(xs)):\n            x += xs[i] * (i + 1)\n        return z\n\n\nexported_program = torch.export.export(\n    NeuronLoop(),\n    (\n        torch.randn(1, 5),\n        [torch.randn(1, 5), torch.randn(1, 5)],\n    ),\n)\nprint(exported_program.graph)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}