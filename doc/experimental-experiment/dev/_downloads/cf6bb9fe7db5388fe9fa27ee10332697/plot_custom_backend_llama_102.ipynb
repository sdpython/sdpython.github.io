{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 102: Fuse kernels in a small Llama Model\n\nThis example leverages the function :epkg:`torch.compile` and the ability\nto use a custom backend to test the optimization of a model by fusing\nsimple element-wise kernels.\n\nIt takes a small Llama model and uses a backend based on :epkg:`onnxruntime`.\nThe model is converted into ONNX and then optimized by fusing element-wise\nkernels.\n\n::\n\n    python plot_custom_backend_llama --config large\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from experimental_experiment.args import get_parsed_args\n\nscript_args = get_parsed_args(\n    \"plot_custom_backend_llama\",\n    config=(\"medium\", \"large or medium depending, large means closer to the real model\"),\n    num_hidden_layers=(1, \"number of hidden layers\"),\n    with_mask=(0, \"tries with a mask as a secondary input\"),\n    description=__doc__,\n    expose=\"config,num_hidden_layers,with_mask\",\n)\n\nprint(f\"config={script_args.config!r}\")\nprint(f\"num_hidden_layers={script_args.num_hidden_layers!r}\")\nprint(f\"with_mask={script_args.with_mask!r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport numpy as np\nimport pandas\nfrom tqdm import tqdm\nimport torch\nfrom transformers import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaModel\nfrom experimental_experiment.xbuilder import OptimizationOptions\nfrom experimental_experiment.torch_dynamo import onnx_custom_backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The dummy model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "has_cuda = torch.cuda.is_available()\n\n\ndef ids_tensor(shape, vocab_size):\n    total_dims = 1\n    for dim in shape:\n        total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n        values.append(np.random.randint(0, vocab_size - 1))\n\n    return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The size of the input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if script_args.config == \"large\":\n    batch, seq, vocab_size = 2, 1024, 32000\n    intermediate_size = 11008\nelse:\n    batch, seq, vocab_size = 2, 1024, 1024\n    intermediate_size = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The configuration of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config = LlamaConfig(\n    hidden_size=4096,\n    num_hidden_layers=int(script_args.num_hidden_layers),\n    vocab_size=vocab_size,\n    intermediate_size=intermediate_size,\n    max_position_embeddings=2048,\n    num_attention_heads=32,\n)\nconfig._attn_implementation = \"eager\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of time we run the model to measure\nthe inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "warmup = 10 if config == \"medium\" else 5\nN = 50 if config == \"medium\" else 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create the model with dummy inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = LlamaModel(config)\n\ninputs = (ids_tensor([batch, seq], vocab_size),)\nif script_args.with_mask in (1, \"1\"):\n    input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))\n    inputs = (*inputs, input_mask)\n\nprocessor = \"cuda\" if has_cuda else \"cpu\"\nprint(f\"moving model and inputs to processor={processor!r}\")\nmodel = model.to(processor)\ninputs = tuple(i.to(processor) for i in inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure of eager mode\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "times = []\n\nwith torch.no_grad():\n\n    # warmup\n    print(\"warmup eager\")\n    for _ in tqdm(range(warmup)):\n        # model(input_ids, input_mask)\n        model(*inputs)\n        torch.cuda.synchronize()\n\n    # repeat\n    print(\"repeat eager\")\n    begin = time.perf_counter()\n    for _ in tqdm(range(N)):\n        model(*inputs)\n        torch.cuda.synchronize()\n    d = (time.perf_counter() - begin) / N\n    baseline = d\n    times.append(dict(optium=\"eager\", processor=processor, avg_time=d, warmup=warmup, N=N))\n    print(\"avg time eager\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure with the custom backend\n\nThree kind of optimization:\n\n- **default**: the onnx model is optimized with less onnx operators\n- **default+onnxruntime**: the onnx model is optimized with fused kernels\n  implemented by onnxruntime\n- **default+onnxruntime+experimental**: the onnx model is optimized with fused kernels\n  implemented by onnxruntime and also custom kernels, this does not work on\n  CPU.\n\nSome links:\n\n* :class:`experimental_experiment.xbuilder.OptimizationOptions`:\n  that class defines the optimizations to apply after the model\n  is converted to onnx,\n* :func:`experimental_experiment.torch_dynamo.onnx_custom_backend`:\n  that function implements the custom backend based on :epkg:`onnxruntime`,\n  it converts the model into ONNX, optimizes and runs it,\n  it does not support :epkg:`graph break`,\n  it does not work well with dynamic shapes yet.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n\n    for optim in [\"default\", \"default+onnxruntime\", \"default+onnxruntime+experimental\"]:\n        print(\"----------------------\")\n        print(f\"optim={optim}\")\n\n        # This variable is used to retrieve the onnx models created by the backend.\n        # It can be set to None if it is not needed.\n        # Graph are usually small as they do not contain weights.\n        storage = {}\n\n        options = OptimizationOptions(\n            constant_folding=True,\n            patterns=None if optim == \"\" else optim,\n            verbose=0,\n            processor=processor.upper(),\n        )\n\n        # The backend used here overwrite some of the parameters provided by\n        # function onnx_custom_backend.\n        custom_custom_backend = lambda *args, optim=optim, options=options, storage=storage, **kwargs: onnx_custom_backend(  # noqa: E731, E501\n            *args,\n            target_opset=18,\n            verbose=0,\n            options=options,\n            optimize=optim != \"\",\n            storage=storage,\n            dump_prefix=f\"dump_onx_llama_{optim.replace('+', '_')}\",\n            **kwargs,\n        )\n\n        # The function setting the backend.\n        compiled_model = torch.compile(\n            model, backend=custom_custom_backend, fullgraph=True, dynamic=False\n        )\n\n        # warmup\n        print(\"warmup compiled model\")\n        for _ in tqdm(range(warmup)):\n            compiled_model(*inputs)\n            torch.cuda.synchronize()\n\n        # repeat\n        print(\"repeat compiled_model\")\n        begin = time.perf_counter()\n        for _ in tqdm(range(N)):\n            compiled_model(*inputs)\n            torch.cuda.synchronize()\n        d = (time.perf_counter() - begin) / N\n\n        # let's measure the number of custom ops\n        n_custom_ops = None\n        if storage is not None:\n            onnx_model = storage[\"instance\"][0][\"onnx\"]\n            n_custom_ops = len([node for node in onnx_model.graph.node if node.domain != \"\"])\n\n        times.append(\n            dict(\n                optium=optim,\n                processor=processor,\n                avg_time=d,\n                warmup=warmup,\n                N=N,\n                n_custom_ops=n_custom_ops,\n                speedup=baseline / d,\n            )\n        )\n        print(f\"avg time custom backend with optimization={optim!r}\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final results\n\navg_time, lower is better,\nspeedup compare to eager mode, higher is better.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pandas.DataFrame(times)\nprint(df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}