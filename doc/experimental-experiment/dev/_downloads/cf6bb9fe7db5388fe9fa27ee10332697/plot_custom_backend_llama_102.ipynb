{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 102: Fuse kernels in a small Llama Model\n\nThis example leverages the function :epkg:`torch.compile` and the ability\nto use a custom backend (see :epkg:`Custom Backends`)\nto test the optimization of a model by fusing simple element-wise kernels.\n\nIt takes a small Llama model and uses a backend based on :epkg:`onnxruntime`.\nThe model is converted into ONNX and then optimized by fusing element-wise\nkernels.\n\n::\n\n    python plot_custom_backend_llama --config large\n\nThe script requires the following packages beside pytorch,\n:epkg:`onnxruntime-training` (for GPU), :epkg:`onnx-extended`\n(compiled for GPU) and :epkg:`transformers`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from experimental_experiment.args import get_parsed_args\n\nscript_args = get_parsed_args(\n    \"plot_custom_backend_llama\",\n    config=(\"medium\", \"large or medium depending, large means closer to the real model\"),\n    num_hidden_layers=(1, \"number of hidden layers\"),\n    with_mask=(0, \"tries with a mask as a secondary input\"),\n    optim=(\"\", \"Optimization to apply, empty string for all\"),\n    description=__doc__,\n    expose=\"config,num_hidden_layers,with_mask,optim\",\n)\n\nprint(f\"config={script_args.config!r}\")\nprint(f\"num_hidden_layers={script_args.num_hidden_layers!r}\")\nprint(f\"with_mask={script_args.with_mask!r}\")\nprint(f\"optim={script_args.optim!r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport numpy as np\nimport pandas\nfrom tqdm import tqdm\nimport torch\nfrom transformers import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaModel\nfrom experimental_experiment.xbuilder import OptimizationOptions\nfrom experimental_experiment.torch_dynamo import onnx_custom_backend\nfrom experimental_experiment.bench_run import get_machine\nfrom experimental_experiment.ext_test_case import unit_test_going\n\nhas_cuda = torch.cuda.is_available()\nmachine = get_machine()\nprint(f\"has_cuda={has_cuda}\")\nprint(f\"processor: {machine['processor_name']}\")\nprint(f\"device: {machine.get('device_name', '?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The dummy model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ids_tensor(shape, vocab_size):\n    total_dims = 1\n    for dim in shape:\n        total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n        values.append(np.random.randint(0, vocab_size - 1))\n\n    return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The size of the input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if script_args.config == \"large\":\n    batch, seq, vocab_size = 2, 1024, 32000\n    intermediate_size = 11008\n    hidden_size = 4096\n    num_attention_heads = 32\nelse:\n    batch, seq, vocab_size = 2, 1024, 1024\n    intermediate_size = 1024\n    hidden_size = 512\n    num_attention_heads = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The configuration of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config = LlamaConfig(\n    hidden_size=hidden_size,\n    num_hidden_layers=int(script_args.num_hidden_layers),\n    vocab_size=vocab_size,\n    intermediate_size=intermediate_size,\n    max_position_embeddings=2048,\n    num_attention_heads=num_attention_heads,\n)\nconfig._attn_implementation = \"eager\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of time we run the model to measure\nthe inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "warmup = 10 if script_args.config == \"medium\" else 5\nN = 50 if script_args.config == \"medium\" else 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create the model with dummy inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"creates the model\")\nmodel = LlamaModel(config)\n\ninputs = (ids_tensor([batch, seq], vocab_size),)\nif script_args.with_mask in (1, \"1\"):\n    input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))\n    inputs = (*inputs, input_mask)\n\nprocessor = \"cuda\" if has_cuda else \"cpu\"\nprint(f\"moving model and inputs to processor={processor!r}\")\nmodel = model.to(processor)\ninputs = tuple(i.to(processor) for i in inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure of eager mode\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "times = []\n\nwith torch.no_grad():\n\n    # warmup\n    print(\"warmup eager\")\n    for _ in tqdm(range(warmup)):\n        # model(input_ids, input_mask)\n        model(*inputs)\n        if has_cuda:\n            torch.cuda.synchronize()\n\n    # repeat\n    print(\"repeat eager\")\n    begin = time.perf_counter()\n    for _ in tqdm(range(N)):\n        model(*inputs)\n        if has_cuda:\n            torch.cuda.synchronize()\n    d = (time.perf_counter() - begin) / N\n    baseline = d\n    times.append(dict(optim=\"eager\", processor=processor, avg_time=d, warmup=warmup, N=N))\n    print(\"avg time eager\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure with the custom backend\n\nThree kind of optimization:\n\n- **default**: the onnx model is optimized with less onnx operators\n- **default+onnxruntime**: the onnx model is optimized with fused kernels\n  implemented by onnxruntime\n- **default+onnxruntime+experimental**: the onnx model is optimized with fused kernels\n  implemented by onnxruntime and also custom kernels, this does not work on\n  CPU.\n\nSome links:\n\n* :class:`experimental_experiment.xbuilder.OptimizationOptions`:\n  that class defines the optimizations to apply after the model\n  is converted to onnx,\n* :func:`experimental_experiment.torch_dynamo.onnx_custom_backend`:\n  that function implements the custom backend based on :epkg:`onnxruntime`,\n  it converts the model into ONNX, optimizes and runs it,\n  it does not support :epkg:`graph break`,\n  it does not work well with dynamic shapes yet.\n* The CUDA kernels are implemented at\n  [onnx_extended/ortops/optim/cuda](https://github.com/sdpython/onnx-extended/tree/main/onnx_extended/ortops/optim/cuda)\n* See `l-design-pattern-optimizer` to understand how\n  these are applied to modify an onnx model.\n\nThe GPU memory is not fully freed before two iterations. Only one scenario\nshould be handled in the same process.\nResults may be very different with a different chip.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimization = (\n    [script_args.optim]\n    if script_args.optim\n    else [\"default\", \"default+onnxruntime\", \"default+onnxruntime+experimental\"]\n)\n\nif unit_test_going():\n    # It is too long.\n    optimization = []\n    times = []\n\n\nwith torch.no_grad():\n\n    for optim in optimization:\n        print(\"----------------------\")\n        print(f\"optim={optim}\")\n\n        # This variable is used to retrieve the onnx models created by the backend.\n        # It can be set to None if it is not needed.\n        # Graph are usually small as they do not contain weights.\n        storage = None  # {}\n\n        options = OptimizationOptions(\n            constant_folding=True,\n            patterns=None if optim == \"\" else optim,\n            verbose=0,\n            processor=processor.upper(),\n        )\n\n        # The backend used here overwrite some of the parameters provided by\n        # function onnx_custom_backend.\n        custom_custom_backend = lambda *args, optim=optim, options=options, storage=storage, **kwargs: onnx_custom_backend(  # noqa: E731, E501\n            *args,\n            target_opset=18,\n            verbose=0,\n            options=options,\n            optimize=optim != \"\",\n            storage=storage,\n            dump_prefix=f\"dump_onx_llama_{optim.replace('+', '_')}\",\n            **kwargs,\n        )\n\n        # The function setting the backend.\n        compiled_model = torch.compile(\n            model, backend=custom_custom_backend, fullgraph=True, dynamic=False\n        )\n\n        # warmup\n        print(\"warmup compiled model\")\n        for _ in tqdm(range(warmup)):\n            compiled_model(*inputs)\n            if has_cuda:\n                torch.cuda.synchronize()\n\n        # repeat\n        print(\"repeat compiled_model\")\n        begin = time.perf_counter()\n        for _ in tqdm(range(N)):\n            compiled_model(*inputs)\n            if has_cuda:\n                torch.cuda.synchronize()\n        d = (time.perf_counter() - begin) / N\n\n        # let's measure the number of custom ops\n        n_custom_ops = None\n        if storage is not None:\n            onnx_model = storage[\"instance\"][0][\"onnx\"]\n            n_custom_ops = len([node for node in onnx_model.graph.node if node.domain != \"\"])\n\n        times.append(\n            dict(\n                optim=optim,\n                processor=processor,\n                avg_time=d,\n                warmup=warmup,\n                N=N,\n                n_custom_ops=n_custom_ops,\n                speedup=baseline / d,\n            )\n        )\n        print(f\"avg time custom backend with optimization={optim!r}\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final results\n\navg_time, lower is better,\nspeedup compare to eager mode, higher is better.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if times:\n    df = pandas.DataFrame(times)\n    print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if times:\n    df.set_index(\"optim\")[[\"speedup\"]].plot.bar(\n        title=\"Speedup for different optimization scenario\"\n    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}