{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Check the exporter on a dummy from HuggingFace\n\nEvery conversion task must be tested on a large scale. One huge source\nof model is :epkg:`HuggingFace`. We focus on the model\n[Tiny-LLM](https://huggingface.co/arnir0/Tiny-LLM).\nTo avoid downloading any weigths, we write a function creating a\nrandom model based on the same architecture.\n\n## Guess the cache dimension\n\nThe first step is to guess the dummy inputs.\nLet's use the true model for that.\nWe use the dummy example from the model page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Tuple\nimport packaging.version as pv\nimport torch\nimport transformers\n\n\nMODEL_NAME = \"arnir0/Tiny-LLM\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We rewrite the forward method to print the cache dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def string_inputs(args, kwargs):\n    def _cache(a):\n        if len(a.key_cache):\n            return f\"n_caches={len(a.key_cache)}, shape={a.key_cache[0].shape}\"\n        return f\"n_caches={len(a.key_cache)}\"\n\n    for a in args:\n        if isinstance(a, transformers.cache_utils.DynamicCache):\n            return _cache(a)\n    for k, a in kwargs.items():\n        if isinstance(a, transformers.cache_utils.DynamicCache):\n            return f\"{k}={_cache(a)}\"\n    return \"no_cache\"\n\n\ndef _forward_(*args, _f=None, **kwargs):\n    assert _f is not None\n    if hasattr(torch.compiler, \"is_exporting\") and not torch.compiler.is_exporting():\n        # torch.compiler.is_exporting requires torch>=2.7\n        print(string_inputs(args, kwargs))\n    return _f(*args, **kwargs)\n\n\nkeep_model_forward = model.forward\nmodel.forward = lambda *args, _f=keep_model_forward, **kwargs: _forward_(\n    *args, _f=_f, **kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prompt = \"Continue: it rains...\"\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n\noutputs = model.generate(\n    inputs, max_length=50, temperature=1, top_k=50, top_p=0.95, do_sample=True\n)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's restore the forward as it was.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.forward = keep_model_forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The model creation\n\nLet's create an untrained model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if pv.Version(transformers.__version__) > pv.Version(\"4.49.99999\"):\n\n    def make_dynamic_cache(\n        key_value_pairs: List[Tuple[torch.Tensor, torch.Tensor]],\n    ) -> transformers.cache_utils.DynamicCache:\n        \"\"\"\n        Creates an instance of :class:`transformers.cache_utils.DynamicCache`.\n        This version is valid for ``transformers >= 4.50``.\n\n        :param key_value_pairs: list of pairs of (key, values)\n        :return: :class:`transformers.cache_utils.DynamicCache`\n        \"\"\"\n        return transformers.cache_utils.DynamicCache(key_value_pairs)\n\nelse:\n\n    def make_dynamic_cache(\n        key_value_pairs: List[Tuple[torch.Tensor, torch.Tensor]],\n    ) -> transformers.cache_utils.DynamicCache:\n        \"\"\"\n        Creates an instance of :class:`transformers.cache_utils.DynamicCache`.\n        This version is valid for ``transformers < 4.50``.\n\n        :param key_value_pairs: list of pairs of (key, values)\n        :return: :class:`transformers.cache_utils.DynamicCache`\n        \"\"\"\n        cache = transformers.cache_utils.DynamicCache(len(key_value_pairs))\n        for i, (key, value) in enumerate(key_value_pairs):\n            cache.update(key, value, i)\n        return cache\n\n\ndef get_tiny_llm(\n    batch_size: int = 2,\n    input_cache: bool = True,\n    common_dynamic_shapes: bool = True,\n    dynamic_rope: bool = False,\n    **kwargs,\n) -> Dict[str, Any]:\n    \"\"\"\n    Gets a non initialized model.\n\n    :param batch_size: batch size\n    :param input_cache: generate data for this iteration with or without cache\n    :param kwargs: to overwrite the configuration, example ``num_hidden_layers=1``\n    :param common_dynamic_shapes: if True returns dynamic shapes as well\n    :param dynamic_rope: use dynamic rope (see :class:`transformers.LlamaConfig`)\n    :return: dictionary\n    \"\"\"\n    import transformers\n\n    config = {\n        \"architectures\": [\"LlamaForCausalLM\"],\n        \"bos_token_id\": 1,\n        \"eos_token_id\": 2,\n        \"hidden_act\": \"silu\",\n        \"hidden_size\": 192,\n        \"initializer_range\": 0.02,\n        \"intermediate_size\": 1024,\n        \"max_position_embeddings\": 1024,\n        \"model_type\": \"llama\",\n        \"num_attention_heads\": 2,\n        \"num_hidden_layers\": 1,\n        \"num_key_value_heads\": 1,\n        \"pretraining_tp\": 1,\n        \"rms_norm_eps\": 1e-05,\n        \"rope_scaling\": {\"rope_type\": \"dynamic\", \"factor\": 10.0} if dynamic_rope else None,\n        \"tie_word_embeddings\": False,\n        \"torch_dtype\": \"float32\",\n        \"transformers_version\": \"4.31.0.dev0\",\n        \"use_cache\": True,\n        \"vocab_size\": 32000,\n    }\n\n    config.update(**kwargs)\n    conf = transformers.LlamaConfig(**config)\n    model = transformers.LlamaForCausalLM(conf)\n    model.eval()\n\n    # now the inputs\n    cache_last_dim = 96\n    sequence_length = 30\n    sequence_length2 = 3\n    num_key_value_heads = 1\n    max_token_id = config[\"vocab_size\"] - 1\n    n_layers = config[\"num_hidden_layers\"]\n\n    batch = torch.export.Dim(\"batch\", min=1, max=1024)\n    seq_length = torch.export.Dim(\"seq_length\", min=1, max=4096)\n    cache_length = torch.export.Dim(\"cache_length\", min=1, max=4096)\n\n    shapes = {\n        \"input_ids\": {0: batch, 1: seq_length},\n        \"attention_mask\": {\n            0: batch,\n            1: torch.export.Dim.DYNAMIC,  # cache_length + seq_length\n        },\n        \"past_key_values\": [\n            [{0: batch, 2: cache_length} for _ in range(n_layers)],\n            [{0: batch, 2: cache_length} for _ in range(n_layers)],\n        ],\n    }\n    inputs = dict(\n        input_ids=torch.randint(0, max_token_id, (batch_size, sequence_length2)).to(\n            torch.int64\n        ),\n        attention_mask=torch.ones((batch_size, sequence_length + sequence_length2)).to(\n            torch.int64\n        ),\n        past_key_values=make_dynamic_cache(\n            [\n                (\n                    torch.randn(\n                        batch_size, num_key_value_heads, sequence_length, cache_last_dim\n                    ),\n                    torch.randn(\n                        batch_size, num_key_value_heads, sequence_length, cache_last_dim\n                    ),\n                )\n                for i in range(n_layers)\n            ]\n        ),\n    )\n    return dict(inputs=inputs, model=model, dynamic_shapes=shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get the model, inputs and dynamic shapes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "experiment = get_tiny_llm()\nmodel, inputs, dynamic_shapes = (\n    experiment[\"model\"],\n    experiment[\"inputs\"],\n    experiment[\"dynamic_shapes\"],\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expected_output = model(**inputs)\nprint(\"result type\", type(expected_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works.\n\n## ExportedProgram\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    ep = torch.export.export(model, (), inputs, dynamic_shapes=dynamic_shapes)\n    print(\"It worked:\")\n    print(ep)\nexcept Exception as e:\n    # To work, it needs at least PRs:\n    # * https://github.com/huggingface/transformers/pull/36311\n    # * https://github.com/huggingface/transformers/pull/36652\n    print(\"It failed:\", e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}