{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export Phi-3.5-mini-instruct with draft_export\n\nTries :func:`torch.export._draft_export.draft_export`.\n\n## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from contextlib import redirect_stderr\nfrom io import StringIO\nfrom typing import Any, Dict\nimport torch\nimport torch.export._draft_export\nimport transformers\nfrom experimental_experiment.helpers import string_type\nfrom onnx_diagnostic.helpers.cache_helper import make_dynamic_cache\nfrom onnx_diagnostic.torch_export_patches import register_additional_serialization_functions\n\n\ndef get_phi35_untrained(batch_size: int = 2, **kwargs) -> Dict[str, Any]:\n    \"\"\"\n    Gets a non initialized model with two sets of inputs and different shapes.\n\n    :param batch_size: batch size\n    :param kwargs: to overwrite the configuration, example ``num_hidden_layers=1``\n    :return: dictionary\n\n    See `Phi-3.5-mini-instruct/config.json\n    <https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/config.json>`_.\n    \"\"\"\n    config = {\n        \"_name_or_path\": \"Phi-3.5-mini-instruct\",\n        \"architectures\": [\"Phi3ForCausalLM\"],\n        \"attention_dropout\": 0.0,\n        \"auto_map\": {\n            \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n            \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\",\n        },\n        \"bos_token_id\": 1,\n        \"embd_pdrop\": 0.0,\n        \"eos_token_id\": 32000,\n        \"hidden_act\": \"silu\",\n        \"hidden_size\": 3072,\n        \"initializer_range\": 0.02,\n        \"intermediate_size\": 8192,\n        \"max_position_embeddings\": 131072,\n        \"model_type\": \"phi3\",\n        \"num_attention_heads\": 32,\n        \"num_hidden_layers\": 32,\n        \"num_key_value_heads\": 32,\n        \"original_max_position_embeddings\": 4096,\n        \"pad_token_id\": 32000,\n        \"resid_pdrop\": 0.0,\n        \"rms_norm_eps\": 1e-05,\n        \"rope_scaling\": {\n            \"long_factor\": [\n                1.0800000429153442,\n                1.1100000143051147,\n                1.1399999856948853,\n                1.340000033378601,\n                1.5899999141693115,\n                1.600000023841858,\n                1.6200000047683716,\n                2.620000123977661,\n                3.2300000190734863,\n                3.2300000190734863,\n                4.789999961853027,\n                7.400000095367432,\n                7.700000286102295,\n                9.09000015258789,\n                12.199999809265137,\n                17.670000076293945,\n                24.46000099182129,\n                28.57000160217285,\n                30.420001983642578,\n                30.840002059936523,\n                32.590003967285156,\n                32.93000411987305,\n                42.320003509521484,\n                44.96000289916992,\n                50.340003967285156,\n                50.45000457763672,\n                57.55000305175781,\n                57.93000411987305,\n                58.21000289916992,\n                60.1400032043457,\n                62.61000442504883,\n                62.62000274658203,\n                62.71000289916992,\n                63.1400032043457,\n                63.1400032043457,\n                63.77000427246094,\n                63.93000411987305,\n                63.96000289916992,\n                63.970001220703125,\n                64.02999877929688,\n                64.06999969482422,\n                64.08000183105469,\n                64.12000274658203,\n                64.41000366210938,\n                64.4800033569336,\n                64.51000213623047,\n                64.52999877929688,\n                64.83999633789062,\n            ],\n            \"short_factor\": [\n                1.0,\n                1.0199999809265137,\n                1.0299999713897705,\n                1.0299999713897705,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0499999523162842,\n                1.0699999332427979,\n                1.0999999046325684,\n                1.1099998950958252,\n                1.1599998474121094,\n                1.1599998474121094,\n                1.1699998378753662,\n                1.2899998426437378,\n                1.339999794960022,\n                1.679999828338623,\n                1.7899998426437378,\n                1.8199998140335083,\n                1.8499997854232788,\n                1.8799997568130493,\n                1.9099997282028198,\n                1.9399996995925903,\n                1.9899996519088745,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0199997425079346,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0299997329711914,\n                2.0799996852874756,\n                2.0899996757507324,\n                2.189999580383301,\n                2.2199995517730713,\n                2.5899994373321533,\n                2.729999542236328,\n                2.749999523162842,\n                2.8399994373321533,\n            ],\n            \"type\": \"longrope\",\n        },\n        \"rope_theta\": 10000.0,\n        \"sliding_window\": 262144,\n        \"tie_word_embeddings\": False,\n        \"torch_dtype\": \"bfloat16\",\n        \"use_cache\": True,\n        \"attention_bias\": False,\n        \"vocab_size\": 32064,\n    }\n    config.update(**kwargs)\n    conf = transformers.Phi3Config(**config)\n    model = transformers.Phi3ForCausalLM(conf)\n    model.eval()\n\n    cache = make_dynamic_cache(\n        [\n            (torch.randn(batch_size, 32, 30, 96), torch.randn(batch_size, 32, 30, 96))\n            for i in range(config[\"num_hidden_layers\"])\n        ]\n    )\n    cache2 = make_dynamic_cache(\n        [\n            (torch.randn(batch_size + 1, 32, 31, 96), torch.randn(batch_size + 1, 32, 31, 96))\n            for i in range(config[\"num_hidden_layers\"])\n        ]\n    )\n\n    inputs = dict(\n        input_ids=torch.randint(0, 32064, (batch_size, 3)).to(torch.int64),\n        attention_mask=torch.ones((batch_size, 33)).to(torch.int64),\n        past_key_values=cache,\n    )\n    inputs2 = dict(\n        input_ids=torch.randint(0, 32064, (batch_size + 1, 4)).to(torch.int64),\n        attention_mask=torch.ones((batch_size + 1, 35)).to(torch.int64),\n        past_key_values=cache2,\n    )\n    return dict(inputs=inputs, model=model, inputs2=inputs2)\n\n\ndata = get_phi35_untrained(num_hidden_layers=2)\nmodel, inputs, inputs2 = data[\"model\"], data[\"inputs\"], data[\"inputs2\"]\n\nprint(string_type(inputs, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Draft Export\n\nThe function we want to try.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "err = StringIO()\nwith redirect_stderr(err), register_additional_serialization_functions(patch_transformers=True):\n    ep = torch.export._draft_export.draft_export(model, tuple(), kwargs=inputs, strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Errors if any.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(err.getvalue())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print the report.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ep._report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the exported program.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ep)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}