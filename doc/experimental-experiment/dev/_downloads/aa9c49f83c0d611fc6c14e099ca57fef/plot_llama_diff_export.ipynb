{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compares LLAMA exporters\n\nThe script compares the two exporters implemented in :epkg:`pytorch`\nfor a part of llama model. The model are compared after all optimizations\nwere made with :epkg:`onnx-rewriter` and :epkg:`onnxruntime`.\n\n* [TorchScript-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter),\n  let's call it **script**\n* [TorchDynamo-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter),\n  let's call it **dynamo**\n\nTo run the script:\n\n::\n\n    python _doc/examples/plot_llama_diff_export --help\n\n## Some helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import contextlib\nimport os\nimport io\nimport warnings\nimport logging\n\ntry:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        import onnxruntime\n\n        has_cuda = \"CUDAExecutionProvider\" in onnxruntime.get_available_providers()\nexcept ImportError:\n    print(\"onnxruntime not available.\")\n    import sys\n\n    sys.exit(0)\n\nimport numpy as np\nimport onnx\nfrom onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator\nimport torch\nfrom experimental_experiment.ext_test_case import get_parsed_args, unit_test_going\nfrom experimental_experiment.convert.convert_helper import (\n    optimize_model_proto,\n    ort_optimize,\n)\nfrom experimental_experiment.torch_helper.llama_helper import (\n    get_llama_model,\n    get_llama_attention,\n    get_llama_decoder,\n)\n\nhas_cuda = has_cuda and torch.cuda.is_available()\nlogging.disable(logging.ERROR)\nprovider = \"cuda\" if has_cuda else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The exporting functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "script_args = get_parsed_args(\n    \"plot_llama_diff_export\",\n    description=__doc__,\n    part=(\"attention\", \"one value among attention, decoder, model\"),\n    expose=\"part\",\n)\n\nprint(f\"part={script_args.part}\")\n\n\ndef opt_filename(filename: str) -> str:\n    name, ext = os.path.splitext(filename)\n    return f\"{name}.opt{ext}\"\n\n\ndef export_script(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            torch.onnx.export(model, args, filename, input_names=[\"input\"])\n    onx = onnx.load(filename)\n    ort_optimize(onx, opt_filename(filename), providers=provider)\n\n\ndef export_dynamo(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            export_output = torch.onnx.dynamo_export(model, *args)\n            model = export_output.model_proto\n    new_model = optimize_model_proto(model)\n    with open(filename, \"wb\") as f:\n        f.write(new_model.SerializeToString())\n    ort_optimize(new_model, opt_filename(filename), providers=provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model and data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if unit_test_going():\n    kwargs = dict(input_dims=[(2, 1024)] * 2)\nelse:\n    kwargs = dict(\n        input_dims=[(2, 1024)] * 2,\n        _attn_implementation=\"eager\",\n        num_hidden_layers=1,\n        hidden_size=512,\n        vocab_size=4000,\n        intermediate_size=2000,\n        max_position_embeddings=2048,\n        num_attention_heads=8,\n    )\n\nif script_args.part == \"attention\":\n    model, inputs = get_llama_attention(**kwargs)\nelif script_args.part == \"decoder\":\n    model, inputs = get_llama_decoder(**kwargs)\nelif script_args.part == \"model\":\n    model, inputs = get_llama_model(**kwargs)\nelse:\n    raise RuntimeError(f\"Unexpected value for part={script_args.part!r}\")\n\nprint(f\"simple run with {len(inputs)} inputs\")\nexpected = model(*inputs[0])\nprint(f\"eager mode worked {expected.shape}, {expected.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Export\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file1 = f\"llama.{script_args.part}.script.onnx\"\nfile2 = f\"llama.{script_args.part}.dynamo.onnx\"\n\nprint(\"torch script exporter\")\nexport_script(file1, model, *inputs[0])\n\nprint(\"torch dynamo exporter\")\nexport_dynamo(file2, model, *inputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verification\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file1 = f\"llama.{script_args.part}.script.opt.onnx\"\nfile2 = f\"llama.{script_args.part}.dynamo.opt.onnx\"\n\n\nproviders = (\n    [\"CPUExecutionProvider\"]\n    if provider == \"cpu\"\n    else [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n)\nsess1 = onnxruntime.InferenceSession(file1, providers=providers)\nsess2 = onnxruntime.InferenceSession(file2, providers=providers)\n\n\nmodel1 = onnx.load(file1)\nmodel2 = onnx.load(file2)\n\nfeeds1, feeds2 = {}, {}\nfor i in range(len(inputs[0])):\n    x = inputs[0][i].detach().numpy()\n    feeds1[sess1.get_inputs()[i].name] = x\n    feeds2[sess2.get_inputs()[i].name] = x\n\ngot1 = sess1.run(None, feeds1)\ngot2 = sess2.run(None, feeds2)\n\ndiff1 = np.abs(expected.detach().numpy() - got1[0]).max()\ndiff2 = np.abs(expected.detach().numpy() - got2[0]).max()\n\nprint(f\"Error with the eager model: {diff1}, {diff2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the reference evaluator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess1 = ExtendedReferenceEvaluator(file1)\nsess2 = ExtendedReferenceEvaluator(file2)\n\n\ngot1 = sess1.run(None, feeds1)\ngot2 = sess2.run(None, feeds2)\n\ndiff1 = np.abs(expected.detach().numpy() - got1[0]).max()\ndiff2 = np.abs(expected.detach().numpy() - got2[0]).max()\n\nprint(f\"Error with the eager model: {diff1}, {diff2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison and execution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def clean_name(name):\n    return name.replace(\n        \"_inlfunc_transformers_models_llama_modeling_llama_LlamaAttention\", \"\"\n    ).replace(\"_inlfunc_torch_nn_modules_linear_Linear\", \"\")\n\n\nnp_inputs = [i.detach().numpy() for i in inputs[0]]\nres1, res2, align, dc = compare_onnx_execution(\n    model1, model2, inputs=np_inputs, verbose=1\n)\nfor r in res2:\n    r.name = clean_name(r.name)\ntext = dc.to_str(res1, res2, align, column_size=90)\nprint(text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}