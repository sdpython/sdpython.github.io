{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Compares LLAMA exporters\n\nThe script compares the two exporters implemented in :epkg:`pytorch`\nfor a part of llama model. The model are compared after all optimizations\nwere made with :epkg:`onnx-rewriter` and :epkg:`onnxruntime`.\n\n* [TorchScript-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter),\n  let's call it **script**\n* [TorchDynamo-based ONNX Exporter](https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter),\n  let's call it **dynamo**\n\nTo run the script:\n\n::\n\n    python _doc/examples/plot_llama_diff_export --help\n\n## Some helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import contextlib\nimport os\nimport io\nimport warnings\nimport logging\n\ntry:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        import onnxruntime\n\n        has_cuda = \"CUDAExecutionProvider\" in onnxruntime.get_available_providers()\nexcept ImportError:\n    print(\"onnxruntime not available.\")\n    import sys\n\n    sys.exit(0)\n\nimport numpy as np\nimport onnx\nfrom onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator\nimport torch\nfrom experimental_experiment.ext_test_case import get_parsed_args, unit_test_going\nfrom experimental_experiment.torch_exp.onnx_export import to_onnx\nfrom experimental_experiment.convert.convert_helper import (\n    optimize_model_proto,\n    ort_optimize,\n)\nfrom experimental_experiment.torch_helper.llama_helper import (\n    get_llama_model,\n    get_llama_attention,\n    get_llama_decoder,\n)\nfrom experimental_experiment.torch_helper.dump_helper import reorder_functions_in_proto\n\nhas_cuda = has_cuda and torch.cuda.is_available()\nlogging.disable(logging.ERROR)\nprovider = \"cuda\" if has_cuda else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The exporting functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "script_args = get_parsed_args(\n    \"plot_llama_diff_export\",\n    description=__doc__,\n    part=(\"attention\", \"one value among attention, decoder, model\"),\n    exporter=(\"dynamo\", \"one value among dynamo, custom\"),\n    ortopt=(1, \"run onnxruntime optimization\"),\n    expose=\"part,exporter,ortopt\",\n)\n\nprint(f\"part={script_args.part}\")\nprint(f\"exporter={script_args.exporter}\")\nortopt = script_args.ortopt in (1, \"1\")\nprint(f\"ortopt={ortopt}\")\n\n\ndef opt_filename(filename: str) -> str:\n    name, ext = os.path.splitext(filename)\n    return f\"{name}.opt{ext}\"\n\n\ndef export_script(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            torch.onnx.export(model, args, filename, input_names=[\"input\"])\n    if ortopt:\n        onx = onnx.load(filename)\n        ort_optimize(onx, opt_filename(filename), providers=provider)\n\n\ndef export_dynamo(filename, model, *args):\n    with contextlib.redirect_stdout(io.StringIO()):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            export_output = torch.onnx.dynamo_export(model, *args)\n            model = export_output.model_proto\n    try:\n        new_model = optimize_model_proto(model)\n    except ImportError as e:\n        print(\"skipping optimization, missing package:\", e)\n        new_model = model\n    with open(filename, \"wb\") as f:\n        f.write(new_model.SerializeToString())\n    if ortopt:\n        ort_optimize(new_model, opt_filename(filename), providers=provider)\n\n\ndef export_custom(filename, model, *args):\n    new_model = to_onnx(\n        model,\n        tuple(args),\n        input_names=[f\"input{i}\" for i in range(len(args))],\n        remove_unused=True,\n        constant_folding=False,\n    )\n    with open(filename, \"wb\") as f:\n        f.write(new_model.SerializeToString())\n    if ortopt:\n        ort_optimize(new_model, opt_filename(filename), providers=provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if unit_test_going():\n    kwargs = dict(input_dims=[(2, 1024)] * 2)\nelse:\n    kwargs = dict(\n        input_dims=[(2, 1024)] * 2,\n        _attn_implementation=\"eager\",\n        num_hidden_layers=1,\n        hidden_size=512,\n        vocab_size=4000,\n        intermediate_size=2000,\n        max_position_embeddings=2048,\n        num_attention_heads=8,\n    )\n\nif script_args.part == \"attention\":\n    model, inputs = get_llama_attention(**kwargs)\nelif script_args.part == \"decoder\":\n    model, inputs = get_llama_decoder(**kwargs)\nelif script_args.part == \"model\":\n    model, inputs = get_llama_model(**kwargs)\nelse:\n    raise RuntimeError(f\"Unexpected value for part={script_args.part!r}\")\n\nprint(f\"simple run with {len(inputs)} inputs\")\nexpected = model(*inputs[0])\nprint(f\"eager mode worked {expected.shape}, {expected.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exporter = script_args.exporter\nfile1 = f\"llama.{script_args.part}.script.onnx\"\nfile2 = f\"llama.{script_args.part}.{exporter}.onnx\"\n\nprint(\"torch script exporter\")\nexport_script(file1, model, *inputs[0])\n\nif exporter == \"dynamo\":\n    print(\"torch dynamo exporter\")\n    export_dynamo(file2, model, *inputs[0])\nelif exporter == \"custom\":\n    print(\"torch custom exporter\")\n    export_custom(file2, model, *inputs[0])\nelse:\n    raise AssertionError(f\"Unexpected value for exporter={exporter!r}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if ortopt:\n    print(\"Using models optimized by onnxruntime\")\n    file1 = f\"llama.{script_args.part}.script.opt.onnx\"\n    file2 = f\"llama.{script_args.part}.{exporter}.opt.onnx\"\n\n\nproviders = (\n    [\"CPUExecutionProvider\"]\n    if provider == \"cpu\"\n    else [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n)\n\nmodel1 = onnx.load(file1)\nmodel2 = onnx.load(file2)\n\nfeeds1, feeds2 = {}, {}\nfor i in range(len(inputs[0])):\n    x = inputs[0][i].detach().numpy()\n    feeds1[model1.graph.input[i].name] = x\n    feeds2[model2.graph.input[i].name] = x\n\nif ortopt:\n    sess1 = onnxruntime.InferenceSession(file1, providers=providers)\n    sess2 = onnxruntime.InferenceSession(file2, providers=providers)\n\n    got1 = sess1.run(None, feeds1)\n    got2 = sess2.run(None, feeds2)\n\n    diff1 = np.abs(expected.detach().numpy() - got1[0]).max()\n    diff2 = np.abs(expected.detach().numpy() - got2[0]).max()\n\n    print(f\"Error with the eager model and onnxruntime: {diff1}, {diff2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification with the reference evaluator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reorder_functions_in_proto(file1)\nreorder_functions_in_proto(file2)\n\nsess1 = ExtendedReferenceEvaluator(file1)\ntry:\n    sess2 = ExtendedReferenceEvaluator(file2)\nexcept NotImplementedError as e:\n    print(e)\n    sess2 = None\n\ngot1 = sess1.run(None, feeds1)\ngot2 = got1 if sess2 is None else sess2.run(None, feeds2)\n\ndiff1 = np.abs(expected.detach().numpy() - got1[0]).max()\ndiff2 = np.abs(expected.detach().numpy() - got2[0]).max()\n\nprint(f\"Error with the eager model and the reference evaluator: {diff1}, {diff2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison and execution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def clean_name(name):\n    return name.replace(\n        \"_inlfunc_transformers_models_llama_modeling_llama_LlamaAttention\", \"\"\n    ).replace(\"_inlfunc_torch_nn_modules_linear_Linear\", \"\")\n\n\nif sess2 is not None:\n    np_inputs = [i.detach().numpy() for i in inputs[0]]\n    res1, res2, align, dc = compare_onnx_execution(\n        model1, model2, inputs=np_inputs, verbose=1, raise_exc=False\n    )\n    for r in res2:\n        r.name = clean_name(r.name)\n    text = dc.to_str(res1, res2, align, column_size=90)\n    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See `l-long-outputs-llama-diff-export` for a better view.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}