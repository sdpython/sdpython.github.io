{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# to_onnx and submodules from LLMs\n\nBig models are hard to read once converted into onnx.\nLet's see how to improve their readibility.\nThe code is inspired from\n[LLM from scratch with Pytorch](https://medium.com/@msouza.os/llm-from-scratch-with-pytorch-9f21808c6319).\n\n## A simple LLM\n\nAll comments were removed from the code to make it less verbose.\nA few fixes were applied to the original code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nfrom onnx.inliner import inline_local_functions\nfrom onnx_array_api.plotting.graphviz_helper import plot_dot\nfrom onnx_array_api.reference import compare_onnx_execution\nimport torch\nfrom onnxruntime import InferenceSession\nfrom experimental_experiment.torch_interpreter import to_onnx\nfrom experimental_experiment.helpers import pretty_onnx\nfrom experimental_experiment.bench_run import max_diff\nfrom experimental_experiment.xbuilder import OptimizationOptions\n\n\nclass Embedding(torch.nn.Module):\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n        self.pe = torch.nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, x):\n        word_emb = self.embedding(x)\n        word_pe = self.pe(x)\n        return word_emb + word_pe\n\n\nclass AttentionBlock(torch.nn.Module):\n\n    def __init__(self, embedding_dim: int, context_size: int):\n        super().__init__()\n        self.query = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.key = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.value = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n\n        ones = torch.ones(size=[context_size, context_size], dtype=torch.float)\n        self.register_buffer(name=\"mask\", tensor=torch.tril(input=ones))\n\n    def forward(self, x):\n        B, T, C = x.size()\n\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n\n        qk = query @ key.transpose(-2, -1) * C**-0.5\n        attention = qk.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n        attention = torch.nn.functional.softmax(input=attention, dim=-1)\n\n        out = attention @ value\n        return out\n\n\nclass MultiAttentionBlock(torch.nn.Module):\n\n    def __init__(self, embedding_dim: int, num_heads: int, context_size: int):\n        super().__init__()\n        self.attention = torch.nn.ModuleList(\n            modules=[AttentionBlock(embedding_dim, context_size) for _ in range(num_heads)]\n        )\n        self.linear = torch.nn.Linear(\n            in_features=embedding_dim * num_heads, out_features=embedding_dim\n        )\n\n    def forward(self, x):\n        out = torch.cat(tensors=[attention(x) for attention in self.attention], dim=-1)\n        x = self.linear(out)\n        return x\n\n\nclass FeedForward(torch.nn.Module):\n\n    def __init__(self, embedding_dim: int, ff_dim: int):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(embedding_dim, ff_dim)\n        self.relu = torch.nn.ReLU()\n        self.linear_2 = torch.nn.Linear(ff_dim, embedding_dim)\n\n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.relu(x)\n        x = self.linear_2(x)\n        return x\n\n\nclass DecoderLayer(torch.nn.Module):\n\n    def __init__(self, embedding_dim: int, num_heads: int, context_size: int, ff_dim: int):\n        super().__init__()\n        self.attention = MultiAttentionBlock(embedding_dim, num_heads, context_size)\n        self.feed_forward = FeedForward(embedding_dim, ff_dim)\n        self.norm_1 = torch.nn.LayerNorm(normalized_shape=embedding_dim)\n        self.norm_2 = torch.nn.LayerNorm(normalized_shape=embedding_dim)\n\n    def forward(self, x):\n        x_norm = self.norm_1(x)\n        attention = self.attention(x_norm)\n        attention = attention + x\n\n        attention_norm = self.norm_2(attention)\n        ff = self.feed_forward(attention_norm)\n        ff = ff + attention\n\n        return ff\n\n\nclass LLM(torch.nn.Module):\n\n    def __init__(\n        self,\n        vocab_size: int = 1024,\n        embedding_dim: int = 16,\n        num_heads: int = 2,\n        context_size: int = 256,\n        ff_dim: int = 128,\n    ):\n        super().__init__()\n        self.embedding = Embedding(vocab_size, embedding_dim)\n        self.decoder = DecoderLayer(embedding_dim, num_heads, context_size, ff_dim)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        y = self.decoder(x)\n        return y\n\n\nllm = LLM()\ndim = (1, 30)\ninput_ids = torch.randint(0, 1024, dim).to(torch.int64)\ny = llm(input_ids)\n\nprint(f\"output: shape={y.shape}, min={y.min()}, max={y.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First conversion to ONNX\n\nThe conversion relies on :func:`torch.export.export`.\nwhich gives:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = torch.export.export(llm, (input_ids,))\nprint(ep.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then function :func:`to_onnx <experimental_experiment.torch_interpreter.to_onnx>`\nconverts it into ONNX.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(llm, (input_ids,))\nprint(pretty_onnx(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check there is no discrepancy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(onx.SerializeToString(), providers=[\"CPUExecutionProvider\"])\nfeeds = dict(input_ids=input_ids.numpy())\ngot = sess.run(None, feeds)[0]\n\ndiff = max_diff(y, got)\nprint(f\"output: shape={got.shape}, min={got.min()}, max={got.max()}\")\nprint(f\"max discrepancy={diff['abs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save the ONNX model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onnx.save(onx, \"plot_exporter_recipes_c_modules.inlined.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNX with submodules\n\nLet's produce an ONNX model with submodules.\nFunction :func:`to_onnx <experimental_experiment.torch_interpreter.to_onnx>`\nis calling the function :func:`torch.export.unflatten.unflatten`\nunder the hood. The fx graph looks like the following.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ep = torch.export.export(llm, (input_ids,))\nunflatten_ep = torch.export.unflatten(ep)\nprint(unflatten_ep.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The exported graph looks simpler and shows something like::\n\n  %decoder : [num_users=1] = call_module[target=decoder](args = (%embedding,), kwargs = {})\n\nIt preserves the hierarchy but it does not necessarily preserves the signatures\nof the initial modules. That's was not one of our goals.\nThe tricky part is module called (*embedding*) is not an instance ``Embedding``\nbut an instance of [InterpreterModule](https://github.com/pytorch/pytorch/blob/main/torch/export/unflatten.py#L116)\nand contains the fx nodes contributing to the submodule and coming from the\nprevious graph.\n\nNow the ONNX graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_module = to_onnx(llm, (input_ids,), export_modules_as_functions=True)\nprint(pretty_onnx(onx_module))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check again there is no new discrepancies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(onx_module.SerializeToString(), providers=[\"CPUExecutionProvider\"])\nfeeds = dict(input_ids=input_ids.numpy())\ngot = sess.run(None, feeds)[0]\n\ndiff = max_diff(y, got)\nprint(f\"output: shape={got.shape}, min={got.min()}, max={got.max()}\")\nprint(f\"max discrepancy={diff['abs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save the ONNX model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onnx.save(onx_module, \"plot_exporter_recipes_c_modules.module.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And visually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_dot(onx_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inlining\n\nThe ONNX graph can still be inline after this.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_inlined = inline_local_functions(onx_module)\nprint(pretty_onnx(onx_inlined))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizations\n\nThe ONNX graph produced by the exporter without any optimization is very verbose\nand less efficient. That's why some optimizations are made to the model by default.\nIt is also possible to introduce kernels implemented in :epkg:`onnxruntime`.\nLet's how it goes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_optimized = to_onnx(\n    llm,\n    (input_ids,),\n    options=OptimizationOptions(\n        patterns=\"default+onnxruntime\", constant_folding=True, verbose=2\n    ),\n)\nprint(pretty_onnx(onx_optimized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows a kernel ``FusedMatMul[com.microsoft]`` which implement a kernel equivalent Gemm\nbut working for any tensors, not only 2D.\nHow does it work on the model which keeps exports the moduels as local functions?\nThe optimizer optimizes every local function independantly.\nWe reduce the verbosity...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_module_optimized = to_onnx(\n    llm,\n    (input_ids,),\n    options=OptimizationOptions(patterns=\"default+onnxruntime\", constant_folding=True),\n    export_modules_as_functions=True,\n)\nprint(pretty_onnx(onx_module_optimized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems to be working as well on this simple case even though the optimizers were\nnot tested on such models. However, keeping the submodule information might be useful\nto implement optimizer for a fmaily of models sharing the same components.\n\n## Optimizations for CUDA\n\nThe optimizer may have a different behaviour knowning the model is running on CUDA.\nIt may use different kernels and do different optimization if needed.\nThat may not be the good place to do it as the runtime may choose to run one kernel on CPU,\nanother one on CUDA. The current optimization does not know that and\nis not able to decide which provider would be more useful for some kernels.\nThis coudl even be decided at runtime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_cuda_optimized = to_onnx(\n    llm,\n    (input_ids,),\n    options=OptimizationOptions(\n        patterns=\"default+onnxruntime\", constant_folding=True, verbose=2, processor=\"CUDA\"\n    ),\n)\nprint(pretty_onnx(onx_cuda_optimized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison optimized and not optimized?\n\nThe following tools is trying to match the node and shape inference\nfrom two models. If they are not too different, the functions\nis able to find out the differences. We can use to see\nwhich operators were fused into bigger ones only implemented by\n:epkg:`onnxruntime`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res1, res2, align, dc = compare_onnx_execution(onx, onx_optimized, verbose=1)\nprint(\"------------\")\ntext = dc.to_str(res1, res2, align)\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The conversion should handle dynamic shapes as well as the input sequence\ncan be of any length. But that's a topic for another example.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}