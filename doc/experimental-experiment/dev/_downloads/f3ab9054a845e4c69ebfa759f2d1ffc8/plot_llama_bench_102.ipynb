{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 102: Measure LLAMA speed\n\nThe script is calling many times the script ``experimental_experiment.torch_bench.dort_bench.py``.\n\n::\n\n    python _doc/examples/plot_llama_bench_102.py --help\n    \nFor exemple, to check mixed precision on multiple backend:\n\n::\n\n    python _doc/examples/plot_llama_bench_102.py --device=cuda --num_hidden_layers=2 --mixed=1\n\n::\n\n    python _doc/examples/plot_llama_bench_102.py --device=cuda --num_hidden_layers=2 --mixed=1 --backend=eager,dynger,ortmodule,inductor,ort+,custom --config=large\n\nWith 32Gb GPU memory, the script runs with 6 layers.\n\n::\n\n    python _doc/examples/plot_llama_bench_102.py --device=cuda --num_hidden_layers=6 --mixed=1 --backend=eager,dynger,ortmodule,inductor,trt,ort+,custom --config=large\n\n    python _doc/examples/plot_llama_bench_102.py --device=cuda --num_hidden_layers=2 --mixed=1 --backend=eager,ort+,custom --config=large\n\nRun the following command to run one experiment and get the available options:\n\n::\n\n    python -m experimental_experiment.torch_bench.dort_bench --help\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from experimental_experiment.args import get_parsed_args, check_cuda_availability\n\nparsed_args = get_parsed_args(\n    \"plot_llama_bench\",\n    description=__doc__,\n    warmup=5,\n    repeat=10,\n    model=(\"llama\", \"model to benchmark\"),\n    backend=(\n        \"eager,dynger,inductor,ort,ort+,custom,ortmodule\",\n        \"backend to test, among eager,dynger,inductor,ort,ort+,custom,plug,ortmodule,backort\",\n    ),\n    device=(\"cuda\" if check_cuda_availability() else \"cpu\", \"device to test\"),\n    num_hidden_layers=(\"1\", \"hidden layers to test\"),\n    mixed=(\"0\", \"boolean value to test (mixed precision or not)\"),\n    dynamic=(\"0\", \"boolean value to test dynamic shapes or not\"),\n    script_name=(\"experimental_experiment.torch_bench.dort_bench\", \"script to run\"),\n    dump=(0, \"dump the models with env ONNXRT_DUMP_PATH\"),\n    check=(0, \"just check the script is working, ignores all other parameters\"),\n    config=(\"medium\", \"configuration to use, default or medium\"),\n    patterns=(\n        \"none,default,default+onnxruntime,\" \"default+onnxruntime+experimental\",\n        \"optimization patterns to use\",\n    ),\n    implementation=(\"eager\", \"eager or sdpa or both values comma separated value\"),\n    with_mask=(1, \"with or without a second input (mask\"),\n    disable_pattern=(\"none\", \"pattern or patterns to disable\"),\n    ort_optimize=(\n        \"0,1\",\n        \"enable or disable onnxruntime optimization, \" \"by default, tries both\",\n    ),\n    order=(\"none\", \"optimization order see class OrderAlgorithm, none by default\"),\n    shape_scenario=(\n        \"\",\n        \"shapes to use, 2x1024 by default, 'batch' to get \"\n        \"shapes with different batch dimensions, 'length' to get \"\n        \"different length sizes\",\n    ),\n    verbose=(1, \"verbosity\"),\n    expose=\"backend,device,num_hidden_layers,mixed,scipt_name,repeat,\"\n    \"warmup,dump,check,config,patterns,dynamic,disable_pattern,model\"\n    \"implementation,with_mask,ort_optimize,verbose,order,shape_scenario\",\n)\n\nimport onnxruntime  # noqa: F401\nimport numpy as np\nimport pandas\nimport matplotlib.pyplot as plt\nimport itertools\nimport torch\nfrom experimental_experiment.ext_test_case import unit_test_going\nfrom experimental_experiment.bench_run import run_benchmark, get_machine, BenchmarkError\n\nscript_name = \"experimental_experiment.torch_bench.dort_bench\"\nmachine = {} if unit_test_going() else get_machine()\n\n\nrepeat = parsed_args.repeat\nwarmup = parsed_args.warmup\n\n\ndef make_config(\n    model,\n    backend,\n    device,\n    num_hidden_layers,\n    repeat,\n    mixed,\n    dynamic,\n    config,\n    warmup,\n    pattern,\n    disable_pattern,\n    implementation,\n    with_mask,\n    ort_optimize,\n    order,\n    shape_scenario,\n    verbose,\n    existing=None,\n):\n    if backend not in (\"custom\", \"ort+\"):\n        ort_optimize = None\n        pattern = None\n        disable_pattern = None\n    cf = dict(\n        model=model,\n        backend=backend,\n        device=device,\n        num_hidden_layers=num_hidden_layers,\n        repeat=repeat,\n        mixed=mixed,\n        dynamic=dynamic,\n        config=config,\n        warmup=warmup,\n        implementation=implementation,\n        with_mask=with_mask,\n        ort_optimize=ort_optimize,\n        order=order,\n        shape_scenario=shape_scenario,\n        verbose=verbose,\n    )\n    cf = {k: v for k, v in cf.items() if v is not None}\n\n    if existing and backend not in (\"custom\", \"ort+\"):\n        for ex in existing:\n            if not ex:\n                continue\n            equal = True\n            for k in cf:\n                if cf[k] != ex[k]:\n                    equal = False\n                    break\n            if equal:\n                return None\n\n    if pattern is None:\n        opt = {}\n    elif pattern == \"none\":\n        opt = dict(enable_pattern=\"default\", disable_pattern=\"default\")\n    elif pattern in \"default\" or \"+\" in pattern:\n        opt = dict(enable_pattern=pattern)\n    else:\n        raise AssertionError(f\"unexpected value for pattern={pattern!r}\")\n    cf.update(opt)\n    if disable_pattern not in (\"none\", None):\n        if \"disable_pattern\" in cf:\n            cf[\"disable_pattern\"] += f\",{disable_pattern}\"\n        else:\n            cf[\"disable_pattern\"] = disable_pattern\n    if \"enable_pattern\" in cf and \"+experimental\" in cf[\"enable_pattern\"]:\n        try:\n            import onnx_extended  # noqa: F401\n        except ImportError:\n            return None\n    elif not ort_optimize and backend in (\"custom\", \"ort+\"):\n        return None\n    assert (\n        cf[\"backend\"] != \"eager\" or cf.get(\"ort_optimize\", None) is None\n    ), f\"Wrong configuration {cf}\"\n    return cf\n\n\nif parsed_args.check not in (1, \"1\"):\n    verbose = parsed_args.verbose\n    configs = []\n    for (\n        backend,\n        device,\n        num_hidden_layers,\n        mixed,\n        dynamic,\n        pattern,\n        impl,\n        ort_optimize,\n    ) in itertools.product(\n        parsed_args.backend.split(\",\"),\n        parsed_args.device.split(\",\"),\n        list(map(int, parsed_args.num_hidden_layers.split(\",\"))),\n        list(map(int, parsed_args.mixed.split(\",\"))),\n        list(map(int, parsed_args.dynamic.split(\",\"))),\n        parsed_args.patterns.split(\",\"),\n        parsed_args.implementation.split(\",\"),\n        list(map(int, parsed_args.ort_optimize.split(\",\"))),\n    ):\n        if mixed == 1 and device == \"cpu\":\n            continue\n        if machine.get(\"capability\", (0, 0)) < (7, 0) and backend == \"inductor\":\n            continue\n        configs.append(\n            make_config(\n                model=parsed_args.model,\n                backend=backend,\n                device=device,\n                num_hidden_layers=num_hidden_layers,\n                repeat=repeat,\n                mixed=mixed,\n                dynamic=dynamic,\n                config=parsed_args.config,\n                warmup=warmup,\n                pattern=pattern,\n                disable_pattern=parsed_args.disable_pattern,\n                existing=configs,\n                implementation=impl,\n                with_mask=parsed_args.with_mask,\n                ort_optimize=ort_optimize,\n                order=parsed_args.order,\n                shape_scenario=parsed_args.shape_scenario,\n                verbose=verbose,\n            )\n        )\nelse:\n    verbose = 5\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    configs = [\n        dict(\n            model=parsed_args.model,\n            backend=\"custom\",\n            device=device,\n            num_hidden_layers=1,\n            repeat=1,\n            mixed=0,\n            dynamic=0,\n            warmup=1,\n            config=\"small\",\n        ),\n    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All configurations to consider.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configs = [cf for cf in configs if cf]\nif verbose:\n    for i, cf in enumerate(configs):\n        print(f\"config {i+1}: {cf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running configuration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    data = run_benchmark(\n        parsed_args.script_name,\n        configs,\n        verbose=verbose,\n        stop_if_exception=False,\n        dump=parsed_args.dump in (\"1\", 1),\n    )\n    data_collected = True\nexcept BenchmarkError as e:\n    if verbose:\n        print(e)\n    data_collected = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's process the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prefix = (\n    f\"plot_{parsed_args.model}-{parsed_args.with_mask}-\"\n    f\"m{parsed_args.mixed}d{parsed_args.dynamic}h{parsed_args.num_hidden_layers}-\"\n    f\"{parsed_args.implementation}\"\n)\n\nif data_collected:\n\n    def clean_pattern(s):\n        s = s.replace(\"+default-default\", \"\")\n        return s\n\n    def make_legend(row):\n        row = row.to_dict()\n        val = [\n            row[\"device\"],\n            f\"h{row['num_hidden_layers']}\",\n            row[\"implementation\"],\n            row[\"backend\"],\n        ]\n        if row[\"mixed\"]:\n            val.append(\"mix\")\n        if row[\"dynamic\"]:\n            val.append(\"dyn\")\n        if \"patterns\" in row and row[\"patterns\"] and \"nan\" not in str(row[\"patterns\"]):\n            val.append(f\"({clean_pattern(row['patterns'])})\")\n        s = \"-\".join(map(str, val))\n        assert \"nan\" not in s, f\"Legend {s!r} is wrong, row={row}\"\n        return s\n\n    df = pandas.DataFrame(data)\n    df = df.drop([\"OUTPUT\", \"ERROR\"], axis=1)\n    df[\"legend\"] = df.apply(make_legend, axis=1)\n    df[\"time\"] = df[\"time\"].astype(float)\n    df_eager = df[(df[\"implementation\"] == \"eager\") & (df[\"backend\"] == \"eager\")][\n        \"time\"\n    ].dropna()\n    if df_eager.shape[0] > 0:\n        min_eager = df_eager.min()\n        df[\"increase\"] = df[\"time\"] / min_eager - 1\n        # df[\"ERROR\"] = df[\"ERROR\"].apply(lambda s: s.replace(\"\\n\", \" \"))\n    filename = f\"plot_{prefix}_bench_with_cmd.csv\"\n    df.to_csv(filename, index=False)\n    filename = f\"plot_{prefix}_bench_with_cmd.xlsx\"\n    df.to_excel(filename, index=False)\n\n    df = df.drop([\"CMD\"], axis=1)\n    filename = f\"plot_{prefix}_bench.csv\"\n    df.to_csv(filename, index=False)\n    df = pandas.read_csv(filename)  # to cast type\n    print(df)\n\n    # summary\n    cs = [\n        c\n        for c in [\"backend\", \"patterns\", \"warmup_time\", \"time\", \"increase\"]\n        if c in df.columns\n    ]\n    dfs = df[cs]\n    filename = f\"plot_{prefix}_summary.xlsx\"\n    dfs.to_excel(filename, index=False)\n    filename = f\"plot_{prefix}_summary.csv\"\n    dfs.to_csv(filename, index=False)\n    print(dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First lines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df.head(2).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More simple\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for c in [\"time\", \"warmup_time\"]:\n    if c not in df.columns:\n        df[c] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simplified data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df.sort_values(\"legend\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot warmup time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch_version = list(set(df[\"torch\"].dropna()))\ntransformers_version = list(set(df[\"transformers\"].dropna()))\nver = f\"{torch_version[0]} - {transformers_version[0]}\"\nmodel = parsed_args.model\nmodeldf = list(set(df[model].dropna()))[0]\ntitle_prefix = (\n    f\"lower better\\n\"\n    f\"{parsed_args.model} - {ver} - mask{parsed_args.with_mask}\"\n    f\"\\n<device>-h<hidden-layers>-<implementation>-<backend>-(optimization)\"\n)\n\n\nif data_collected:\n    fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))\n\n    df = df.sort_values(\"time\").set_index(\"legend\")\n    df[[\"warmup_time\"]].plot.barh(ax=ax, title=f\"warmup time\\n{title_prefix}\")\n    ax.grid(True)\n\n    fig.tight_layout()\n    fig.savefig(f\"plot_{prefix}_bench_warmup_time.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data_collected:\n    fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))\n\n    df[[\"time\"]].plot.barh(ax=ax, title=f\"computation time\\n{title_prefix}\")\n    mi, ma = df[\"time\"].min(), df[\"time\"].max()\n    mi = mi - (ma - mi) / 10\n    ax.set_xlim(left=mi)\n    ax.grid(True)\n\n    fig.tight_layout()\n    fig.savefig(f\"plot_{prefix}_bench_time.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot increase.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if data_collected:\n    fig, ax = plt.subplots(1, 1, figsize=(12, df.shape[0] // 3 + 1))\n\n    df[[\"increase\"]].plot.barh(ax=ax, title=f\"comparison to eager %\\n{title_prefix}\")\n    ax.grid(True)\n\n    fig.tight_layout()\n    fig.savefig(f\"plot_{prefix}_bench_relative.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}