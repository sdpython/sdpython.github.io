{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 301: Compares LLAMA exporters for onnxrt backend\n\nThe script compares exported models in :epkg:`pytorch`\nusing :epkg:`onnxrt backend`. It tries to do a side by side\nof the execution of both models.\n\nTo run the script:\n\n::\n\n    python _doc/examples/plot_llama_diff_dort --help\n\n\nThe following example compares the forward step for mixed precision on cuda\nand produces all the intermediate onnx graphs.\n\n::\n\n    python _doc/examples/plot_llama_diff_dort.py --part model --ortopt 1 \\\n            --cuda 1 --backward 0 --mixed 1\n\nYou may use ``--mixed=1`` to compare the backward graphs.\n\n## Some helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from experimental_experiment.args import get_parsed_args\n\nscript_args = get_parsed_args(\n    \"plot_llama_diff_export\",\n    description=__doc__,\n    part=(\"attention\", \"one value among attention, decoder, model\"),\n    ortopt=(1, \"run onnxruntime optimization\"),\n    backward=(0, \"does one operator for backward\"),\n    cuda=(0, \"use cuda or not\"),\n    mixed=(0, \"use miwed precision\"),\n    opset=(18, \"onnx opset\"),\n    expose=\"part,exporter,ortopt,cuda,mixed,opset\",\n)\n\n\nimport copy\nimport os\nimport warnings\nimport logging\n\ntry:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        import onnxruntime\n\n        has_cuda = \"CUDAExecutionProvider\" in onnxruntime.get_available_providers()\nexcept ImportError:\n    print(\"onnxruntime not available.\")\n    import sys\n\n    sys.exit(0)\n\nimport onnx\nfrom onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator\nimport torch\nfrom torch._dynamo.backends.common import aot_autograd\nfrom experimental_experiment.ext_test_case import unit_test_going\nfrom experimental_experiment.convert.convert_helper import (\n    optimize_model_proto_oxs,\n    ort_optimize,\n)\nfrom experimental_experiment.torch_models.llama_helper import (\n    get_llama_model,\n    get_llama_attention,\n    get_llama_decoder,\n)\nfrom experimental_experiment.torch_models.dump_helper import (\n    assert_all_close,\n    dump_onnx,\n    reorder_functions_in_proto,\n    inputs_from_onnx_model,\n    build_matching_inputs,\n    results_to_string,\n)\nfrom experimental_experiment.torch_models.training_helper import (\n    train_loop,\n    make_aot_ort,\n)\nfrom experimental_experiment.torch_dynamo import (\n    onnx_debug_backend,\n    get_decomposition_table,\n)\n\nhas_cuda = has_cuda and torch.cuda.is_available()\nlogging.disable(logging.ERROR)\nprovider = \"cuda\" if has_cuda else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The exporting functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"part={script_args.part}\")\nortopt = script_args.ortopt in (1, \"1\")\nprint(f\"ortopt={ortopt}\")\nbackward = script_args.backward in (1, \"1\")\nprint(f\"backward={backward}\")\nuse_cuda = script_args.cuda in (1, \"1\")\nprint(f\"cuda={use_cuda}\")\nuse_mixed = script_args.mixed in (1, \"1\")\nprint(f\"mixed={use_mixed}\")\nopset = int(script_args.opset)\nprint(f\"opset={opset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if unit_test_going():\n    kwargs = dict(input_dims=[(2, 1024)] * 2)\nelse:\n    kwargs = dict(\n        input_dims=[(2, 1024)] * 2,\n        _attn_implementation=\"eager\",\n        num_hidden_layers=1,\n        hidden_size=512,\n        vocab_size=4000,\n        intermediate_size=2000,\n        max_position_embeddings=2048,\n        num_attention_heads=8,\n    )\n\nif script_args.part == \"attention\":\n    model, inputs = get_llama_attention(**kwargs)\nelif script_args.part == \"decoder\":\n    model, inputs = get_llama_decoder(**kwargs)\nelif script_args.part == \"model\":\n    model, inputs = get_llama_model(**kwargs)\nelse:\n    raise RuntimeError(f\"Unexpected value for part={script_args.part!r}\")\n\nif use_cuda:\n    model = model.to(\"cuda\")\n    inputs = [[i.to(\"cuda\") for i in inp] for inp in inputs]\n\nprint(f\"simple run with {len(inputs)} inputs\")\nif backward:\n    if use_mixed:\n        assert use_cuda, \"mixed precision only works with cuda\"\n        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n            torch.cuda.synchronize()\n            expected = train_loop(copy.deepcopy(model), *inputs[0])\n            torch.cuda.synchronize()\n    else:\n        expected = train_loop(copy.deepcopy(model), *inputs[0])\n    print(\n        f\"-- eager mode worked, {len(expected)} gradients, first one is \"\n        f\"{expected[0].shape}, {expected[0].dtype}\"\n    )\nelse:\n    if use_mixed:\n        assert use_cuda, \"mixed precision only works with cuda\"\n        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n            torch.cuda.synchronize()\n            expected = model(*inputs[0])\n            torch.cuda.synchronize()\n    else:\n        expected = model(*inputs[0])\n    print(results_to_string(expected))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if hasattr(torch._dynamo.variables.misc, \"LoggingLoggerVariable\"):\n    # A tweak to make torch.export.export work.\n    torch._dynamo.variables.misc.LoggingLoggerVariable.call_method = lambda *_, **__: None\n\n\nfolder = \"dump_models\"\nstorage = {}\n\nif backward:\n    # onnxrt backend\n    local_aot_ort, _ = make_aot_ort(dynamic=False, rewrite=True)\n\n    optimized_mod = torch.compile(\n        copy.deepcopy(model), backend=local_aot_ort, dynamic=False, fullgraph=True\n    )\n\n    with dump_onnx(\"llama_onnxrt\", folder=folder, clean=True):\n        if use_mixed:\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                torch.cuda.synchronize()\n                expected_onnxrt = train_loop(optimized_mod, *inputs[0])\n                torch.cuda.synchronize()\n        else:\n            expected_onnxrt = train_loop(optimized_mod, *inputs[0])\n    assert_all_close(expected[0], expected_onnxrt[0], atol=1e-3)\n    print(\n        f\"-- onnxrt backend worked, {len(expected_onnxrt)} gradients, first one is \"\n        f\"{expected_onnxrt[0].shape}, {expected_onnxrt[0].dtype}\"\n    )\n\n    # debugging backend\n    aot_compiler = aot_autograd(\n        fw_compiler=lambda *args, **kwargs: onnx_debug_backend(\n            *args,\n            dump_prefix=os.path.join(folder, \"llama_debug\"),\n            target_opset=opset,\n            storage=storage,\n            **kwargs,\n        ),\n        decompositions=get_decomposition_table(),\n    )\n    onnx_mod = torch.compile(copy.deepcopy(model), backend=aot_compiler, fullgraph=True)\n\n    if use_mixed:\n        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n            torch.cuda.synchronize()\n            got = train_loop(onnx_mod, *inputs[0])\n            torch.cuda.synchronize()\n    else:\n        got = train_loop(onnx_mod, *inputs[0])\n    assert_all_close(expected[0], got[0], atol=1e-2 if use_mixed else 1e-4)\n    print(\n        f\"-- debug backend worked, {len(got)} gradients, first one is \"\n        f\"{got[0].shape}, {got[0].dtype}\"\n    )\n\nelse:\n    # onnxrt backend\n    local_aot_ort, _ = make_aot_ort(dynamic=True, rewrite=True)\n    optimized_mod = torch.compile(model, backend=local_aot_ort, fullgraph=True)\n    with dump_onnx(\"llama_onnxrt\", folder=folder, clean=True):\n        if use_mixed:\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                torch.cuda.synchronize()\n                expected_onnxrt = optimized_mod(*inputs[0])\n                torch.cuda.synchronize()\n        else:\n            expected_onnxrt = optimized_mod(*inputs[0])\n    assert_all_close(expected, expected_onnxrt, atol=1e-2)\n\n    # debugging backend\n    aot_compiler = aot_autograd(\n        fw_compiler=lambda *args, **kwargs: onnx_debug_backend(\n            *args,\n            dump_prefix=os.path.join(folder, \"llama_debug\"),\n            target_opset=17,\n            storage=storage,\n            **kwargs,\n        )\n    )\n\n    onnx_mod = torch.compile(model, backend=aot_compiler, fullgraph=True)\n    if use_mixed:\n        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n            got = onnx_mod(*inputs[0])\n    else:\n        try:\n            got = onnx_mod(*inputs[0])\n        except Exception as e:\n            print(f\"ERROR: {e}\")\n            got = None\n    if got is not None:\n        assert_all_close(expected, got, atol=1 if use_mixed else 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For forward, there are two files, one onnx model and the graph module\nprinted in a txt file. For backward, there are two onnx models.\nThen it is multiplied by the number of backends.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models = os.listdir(folder)\nprint(f\"exported models: {models}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inputs used by the debug backend\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    feeds = storage[\"instance\"][0][\"inputs\"][0]\n    for k, v in feeds.items():\n        print(f\"-- {k} {v.dtype} {v.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's the first line of the graph module\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    graph_module = storage[\"instance\"][0][\"graph_module\"]\n    print(\"\\n\".join(str(graph_module.graph).split(\"\\n\")[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison and execution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    if backward:\n        print(f\"-- {len(storage['instance'])} onnx models were creates\")\n        for i, inst in enumerate(storage[\"instance\"]):\n            print(f\"  model {i}: {len(inst['inputs'])} runs\")\n\n        # deal with backward\n        onnx_models = list(sorted([m for m in models if m.endswith(\".onnx\")]))\n        assert len(onnx_models) == 4, f\"unexpected value {onnx_models}\"\n        onnx_models = list(sorted([m for m in models if m.endswith(\".onnx\") and \"_1\" in m]))\n        assert len(onnx_models) == 2, f\"unexpected value {onnx_models}\"\n        model_onnxrt = os.path.join(folder, onnx_models[1])\n        model_debug = os.path.join(folder, onnx_models[0])\n    else:\n        onnx_models = list(sorted([m for m in models if m.endswith(\".onnx\")]))\n        if len(onnx_models) == 2:\n            model_onnxrt = os.path.join(folder, onnx_models[1])\n            model_debug = os.path.join(folder, onnx_models[0])\n        else:\n            model_debug = os.path.join(folder, onnx_models[0])\n            # the following error may appear:\n            # Node type 'Rank' from domain 'pkg.onnxscript.torch_lib.common' is unknown\n            print(f\"One model is missing, onnx_models={onnx_models}\")\n            model_onnxrt = model_debug\n\n    print(f\"model_onnxrt={model_onnxrt}\")\n    print(f\"model_debug={model_debug}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The inputs of both models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    print(\"onnxrt:\", inputs_from_onnx_model(model_onnxrt))\n    print(\"debug:\", inputs_from_onnx_model(model_debug))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inputs are not the same. The first model has more and some inputs were\nmoved into the initializer list into for `model_debug`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    print(\"debug:\", inputs_from_onnx_model(model_debug, init=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization and Verification\n\nLet's try the model with a python backend (reference implementation).\nFirst step, onnxscript uses many functions. The reference evaluation expects\nevery function to be defined so the order of functions in the model matters.\nNo recursivity is allowed by this runtime.\nWe need to reorder as function Rank is usually placed\nat the end of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    reorder_functions_in_proto(model_onnxrt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the model and optimize them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    debug = onnx.load(model_debug)\n    try:\n        onnxrt = optimize_model_proto_oxs(onnx.load(model_onnxrt))\n    except ImportError as e:\n        print(\"missing library\", e)\n        onnxrt = debug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's apply onnxruntime optimization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage and ortopt:\n    providers = (\n        [(\"CUDAExecutionProvider\", {}), (\"CPUExecutionProvider\", {})]\n        if use_cuda\n        else [\"CPUExecutionProvider\"]\n    )\n    with open(model_onnxrt.replace(\".onnx\", \".before.opt.onnx\"), \"wb\") as f:\n        f.write(onnxrt.SerializeToString())\n    print(f\"run onnxruntime optimization on {model_onnxrt}\")\n    optimized = model_onnxrt.replace(\".onnx\", \".opt.onnx\")\n    ort_optimize(onnxrt, output=optimized, providers=providers)\n    onnxrt = onnx.load(optimized)\n\n    print(f\"run onnxruntime optimization on {model_debug}\")\n    optimized = model_debug.replace(\".onnx\", \".opt.onnx\")\n    ort_optimize(debug, output=optimized, disable_aot=True, providers=providers)\n    debug = onnx.load(optimized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For what's following, we need to build two lists of matching inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    print(\"build_matching_inputs\")\n    feedsrt = build_matching_inputs(model_debug, feeds, model_onnxrt)\n    print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check both models are running.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    out_onnxrt = ExtendedReferenceEvaluator(onnxrt).run(None, feedsrt)\n    out_debug = ExtendedReferenceEvaluator(debug).run(None, feeds)\n    assert out_onnxrt\n    assert out_debug\n\n# assert_all_close(out_onnxrt, out_debug)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Side by side\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if \"instance\" in storage:\n    res1, res2, align, dc = compare_onnx_execution(\n        onnxrt,\n        debug,\n        verbose=1,\n        raise_exc=True,\n        inputs=(feedsrt, feeds),\n    )\n    text = dc.to_str(res1, res2, align, column_size=90)\n    print(text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}