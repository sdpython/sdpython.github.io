
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_recipes/plot_exporter_exporter_phi35_piece.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_recipes_plot_exporter_exporter_phi35_piece.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_recipes_plot_exporter_exporter_phi35_piece.py:


.. _l-plot-exporter-exporter-phi35-piece:

Export Phi-3.5-mini-instruct piece by piece
===========================================

:func:`torch.export.export` often breaks on big models because there
are control flows or instructions breaking the propagation of
dynamic shapes (see ...). The function usually gives an indication where
the model implementation can be fixed but in case, that is not possible,
we can try to export the model piece by piece: every module
is converted separately from its submodule. A model can be exported even
if one of its submodules cannot.

Model
+++++

.. GENERATED FROM PYTHON SOURCE LINES 18-212

.. code-block:: Python


    import pprint
    from typing import Any, Dict
    import torch
    import torch._export.tools
    import transformers
    from experimental_experiment.helpers import string_type
    from experimental_experiment.torch_interpreter.piece_by_piece import (
        trace_execution_piece_by_piece,
    )


    def get_phi35_untrained(batch_size: int = 2, **kwargs) -> Dict[str, Any]:
        """
        Gets a non initialized model with two sets of inputs and different shapes.

        :param batch_size: batch size
        :param kwargs: to overwrite the configuration, example ``num_hidden_layers=1``
        :return: dictionary

        See `Phi-3.5-mini-instruct/config.json
        <https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/config.json>`_.
        """
        config = {
            "_name_or_path": "Phi-3.5-mini-instruct",
            "architectures": ["Phi3ForCausalLM"],
            "attention_dropout": 0.0,
            "auto_map": {
                "AutoConfig": "configuration_phi3.Phi3Config",
                "AutoModelForCausalLM": "modeling_phi3.Phi3ForCausalLM",
            },
            "bos_token_id": 1,
            "embd_pdrop": 0.0,
            "eos_token_id": 32000,
            "hidden_act": "silu",
            "hidden_size": 3072,
            "initializer_range": 0.02,
            "intermediate_size": 8192,
            "max_position_embeddings": 131072,
            "model_type": "phi3",
            "num_attention_heads": 32,
            "num_hidden_layers": 32,
            "num_key_value_heads": 32,
            "original_max_position_embeddings": 4096,
            "pad_token_id": 32000,
            "resid_pdrop": 0.0,
            "rms_norm_eps": 1e-05,
            "rope_scaling": {
                "long_factor": [
                    1.0800000429153442,
                    1.1100000143051147,
                    1.1399999856948853,
                    1.340000033378601,
                    1.5899999141693115,
                    1.600000023841858,
                    1.6200000047683716,
                    2.620000123977661,
                    3.2300000190734863,
                    3.2300000190734863,
                    4.789999961853027,
                    7.400000095367432,
                    7.700000286102295,
                    9.09000015258789,
                    12.199999809265137,
                    17.670000076293945,
                    24.46000099182129,
                    28.57000160217285,
                    30.420001983642578,
                    30.840002059936523,
                    32.590003967285156,
                    32.93000411987305,
                    42.320003509521484,
                    44.96000289916992,
                    50.340003967285156,
                    50.45000457763672,
                    57.55000305175781,
                    57.93000411987305,
                    58.21000289916992,
                    60.1400032043457,
                    62.61000442504883,
                    62.62000274658203,
                    62.71000289916992,
                    63.1400032043457,
                    63.1400032043457,
                    63.77000427246094,
                    63.93000411987305,
                    63.96000289916992,
                    63.970001220703125,
                    64.02999877929688,
                    64.06999969482422,
                    64.08000183105469,
                    64.12000274658203,
                    64.41000366210938,
                    64.4800033569336,
                    64.51000213623047,
                    64.52999877929688,
                    64.83999633789062,
                ],
                "short_factor": [
                    1.0,
                    1.0199999809265137,
                    1.0299999713897705,
                    1.0299999713897705,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0499999523162842,
                    1.0699999332427979,
                    1.0999999046325684,
                    1.1099998950958252,
                    1.1599998474121094,
                    1.1599998474121094,
                    1.1699998378753662,
                    1.2899998426437378,
                    1.339999794960022,
                    1.679999828338623,
                    1.7899998426437378,
                    1.8199998140335083,
                    1.8499997854232788,
                    1.8799997568130493,
                    1.9099997282028198,
                    1.9399996995925903,
                    1.9899996519088745,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0199997425079346,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0299997329711914,
                    2.0799996852874756,
                    2.0899996757507324,
                    2.189999580383301,
                    2.2199995517730713,
                    2.5899994373321533,
                    2.729999542236328,
                    2.749999523162842,
                    2.8399994373321533,
                ],
                "type": "longrope",
            },
            "rope_theta": 10000.0,
            "sliding_window": 262144,
            "tie_word_embeddings": False,
            "torch_dtype": "bfloat16",
            "use_cache": True,
            "attention_bias": False,
            "vocab_size": 32064,
        }
        config.update(**kwargs)
        conf = transformers.Phi3Config(**config)
        model = transformers.Phi3ForCausalLM(conf)
        model.eval()

        cache = transformers.cache_utils.DynamicCache(config["num_hidden_layers"])
        for i in range(config["num_hidden_layers"]):
            cache.update(
                torch.randn(batch_size, 32, 30, 96), torch.randn(batch_size, 32, 30, 96), i
            )
        cache2 = transformers.cache_utils.DynamicCache(config["num_hidden_layers"])
        for i in range(config["num_hidden_layers"]):
            cache2.update(
                torch.randn(batch_size + 1, 32, 31, 96),
                torch.randn(batch_size + 1, 32, 31, 96),
                i,
            )

        inputs = dict(
            input_ids=torch.randint(0, 32064, (batch_size, 3)).to(torch.int64),
            attention_mask=torch.ones((batch_size, 33)).to(torch.int64),
            past_key_values=cache,
        )
        inputs2 = dict(
            input_ids=torch.randint(0, 32064, (batch_size + 1, 4)).to(torch.int64),
            attention_mask=torch.ones((batch_size + 1, 35)).to(torch.int64),
            past_key_values=cache2,
        )
        return dict(inputs=inputs, model=model, inputs2=inputs2)


    data = get_phi35_untrained(num_hidden_layers=2)
    model, inputs, inputs2 = data["model"], data["inputs"], data["inputs2"]

    print(string_type(inputs, with_shape=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    dict(input_ids:T7s2x3,attention_mask:T7s2x33,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x96,T1s2x32x30x96], value_cache=#2[T1s2x32x30x96,T1s2x32x30x96]))




.. GENERATED FROM PYTHON SOURCE LINES 213-220

Dynamic Shapes
++++++++++++++

We want to infer the dynamic shapes from the two sets of inputs we gave.
For that, we use a function to trace the execution of the model
including its submodules. It is going to execute the model twice
with the two sets of inputs and stores every intermediate input and output.

.. GENERATED FROM PYTHON SOURCE LINES 220-223

.. code-block:: Python


    diag = trace_execution_piece_by_piece(model, [inputs, inputs2], verbose=2)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [_trace_forward_execution]  __main__-Phi3ForCausalLM.forward
    [_trace_forward_execution] .. model-Phi3Model.forward
    [_trace_forward_execution] .... embed_tokens-Embedding.forward
    [_trace_forward_execution] .... layers[0]-Phi3DecoderLayer.forward
    [_trace_forward_execution] ...... self_attn-Phi3Attention.forward
    [_trace_forward_execution] ........ o_proj-Linear.forward
    [_trace_forward_execution] ........ qkv_proj-Linear.forward
    [_trace_forward_execution] ...... mlp-Phi3MLP.forward
    [_trace_forward_execution] ........ gate_up_proj-Linear.forward
    [_trace_forward_execution] ........ down_proj-Linear.forward
    [_trace_forward_execution] ........ activation_fn-SiLU.forward
    [_trace_forward_execution] ...... input_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... resid_attn_dropout-Dropout.forward
    [_trace_forward_execution] ...... resid_mlp_dropout-Dropout.forward
    [_trace_forward_execution] .... layers[1]-Phi3DecoderLayer.forward
    [_trace_forward_execution] ...... self_attn-Phi3Attention.forward
    [_trace_forward_execution] ........ o_proj-Linear.forward
    [_trace_forward_execution] ........ qkv_proj-Linear.forward
    [_trace_forward_execution] ...... mlp-Phi3MLP.forward
    [_trace_forward_execution] ........ gate_up_proj-Linear.forward
    [_trace_forward_execution] ........ down_proj-Linear.forward
    [_trace_forward_execution] ........ activation_fn-SiLU.forward
    [_trace_forward_execution] ...... input_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm.forward
    [_trace_forward_execution] ...... resid_attn_dropout-Dropout.forward
    [_trace_forward_execution] ...... resid_mlp_dropout-Dropout.forward
    [_trace_forward_execution] .... norm-Phi3RMSNorm.forward
    [_trace_forward_execution] .... rotary_emb-Phi3RotaryEmbedding.forward
    [_trace_forward_execution] .. lm_head-Linear.forward
    [trace_execution_piece_by_piece] run with dict(args:(),kwargs:dict(input_ids:T7s2x3,attention_mask:T7s2x33,past_key_values:DynamicCache(key_cache=#2[T1s2x32x30x96,T1s2x32x30x96], value_cache=#2[T1s2x32x30x96,T1s2x32x30x96])))
    [__main__:Phi3ForCausalLM] > **dict(input_ids:T7r2,attention_mask:T7r2,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [model:Phi3Model]   > **dict(input_ids:T7r2,attention_mask:T7r2,position_ids:None,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),inputs_embeds:None,use_cache:None,output_attentions:bool,output_hidden_states:bool,return_dict:bool,cache_position:None)
    [embed_tokens:Embedding]     > T7r2
    [embed_tokens:Embedding]     < T1r3
    [rotary_emb:Phi3RotaryEmbedding]     > *(T1r3,T7r2)
    [rotary_emb:Phi3RotaryEmbedding]     < *(T1r3,T1r3)
    [layers[0]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[0]:Phi3DecoderLayer]     < *(T1r3,)
    [layers[1]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[1]:Phi3DecoderLayer]     < *(T1r3,)
    [norm:Phi3RMSNorm]     > T1r3
    [norm:Phi3RMSNorm]     < T1r3
    [model:Phi3Model]   < *dict(last_hidden_state:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [lm_head:Linear]   > T1r3
    [lm_head:Linear]   < T1r3
    [__main__:Phi3ForCausalLM] < *dict(logits:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [trace_execution_piece_by_piece] run with dict(args:(),kwargs:dict(input_ids:T7s3x4,attention_mask:T7s3x35,past_key_values:DynamicCache(key_cache=#2[T1s3x32x31x96,T1s3x32x31x96], value_cache=#2[T1s3x32x31x96,T1s3x32x31x96])))
    [__main__:Phi3ForCausalLM] > **dict(input_ids:T7r2,attention_mask:T7r2,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [model:Phi3Model]   > **dict(input_ids:T7r2,attention_mask:T7r2,position_ids:None,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),inputs_embeds:None,use_cache:None,output_attentions:bool,output_hidden_states:bool,return_dict:bool,cache_position:None)
    [embed_tokens:Embedding]     > T7r2
    [embed_tokens:Embedding]     < T1r3
    [rotary_emb:Phi3RotaryEmbedding]     > *(T1r3,T7r2)
    [rotary_emb:Phi3RotaryEmbedding]     < *(T1r3,T1r3)
    [layers[0]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[0]:Phi3DecoderLayer]     < *(T1r3,)
    [layers[1]:Phi3DecoderLayer]     > *(T1r3,), **dict(attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [input_layernorm:Phi3RMSNorm]       > T1r3
    [input_layernorm:Phi3RMSNorm]       < T1r3
    [self_attn:Phi3Attention]       > **dict(hidden_states:T1r3,attention_mask:T1r4,position_ids:T7r2,past_key_value:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]),output_attentions:bool,use_cache:bool,cache_position:T7r1,position_embeddings:(T1r3,T1r3))
    [qkv_proj:Linear]         > T1r3
    [qkv_proj:Linear]         < T1r3
    [o_proj:Linear]         > T1r3
    [o_proj:Linear]         < T1r3
    [self_attn:Phi3Attention]       < *(T1r3,None)
    [resid_attn_dropout:Dropout]       > T1r3
    [resid_attn_dropout:Dropout]       < T1r3
    [post_attention_layernorm:Phi3RMSNorm]       > T1r3
    [post_attention_layernorm:Phi3RMSNorm]       < T1r3
    [mlp:Phi3MLP]       > T1r3
    [gate_up_proj:Linear]         > T1r3
    [gate_up_proj:Linear]         < T1r3
    [activation_fn:SiLU]         > T1r3
    [activation_fn:SiLU]         < T1r3
    [down_proj:Linear]         > T1r3
    [down_proj:Linear]         < T1r3
    [mlp:Phi3MLP]       < T1r3
    [resid_mlp_dropout:Dropout]       > T1r3
    [resid_mlp_dropout:Dropout]       < T1r3
    [layers[1]:Phi3DecoderLayer]     < *(T1r3,)
    [norm:Phi3RMSNorm]     > T1r3
    [norm:Phi3RMSNorm]     < T1r3
    [model:Phi3Model]   < *dict(last_hidden_state:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [lm_head:Linear]   > T1r3
    [lm_head:Linear]   < T1r3
    [__main__:Phi3ForCausalLM] < *dict(logits:T1r3,past_key_values:DynamicCache(key_cache=#2[T1r4,T1r4], value_cache=#2[T1r4,T1r4]))
    [trace_forward_execution] traced execution of model Phi3ForCausalLM
    >>> __main__: Phi3ForCausalLM
      > ((),dict(input_ids:CT7s2x3[1926,30157:A15287.666666666666],attention_mask:CT7s2x33[1,1:A1.0],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.6383233070373535,4.422898292541504:A-0.0035022671144108],CT1s2x32x30x96[-4.655252456665039,4.282486438751221:A-0.0009285174642691407]], value_cache=#2[CT1s2x32x30x96[-4.561790943145752,4.394318103790283:A0.002781494748319777],CT1s2x32x30x96[-4.386584758758545,5.087398052215576:A-0.000443269511274274]])))
      > ((),dict(input_ids:CT7s3x4[913,26609:A14189.0],attention_mask:CT7s3x35[1,1:A1.0],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.731935024261475,4.380406379699707:A-0.002659257337925612],CT1s3x32x31x96[-4.263330936431885,4.456257343292236:A-0.001896037205008988]], value_cache=#2[CT1s3x32x31x96[-4.831547260284424,4.720435619354248:A0.0003485915722994343],CT1s3x32x31x96[-4.535431861877441,4.804716110229492:A-0.0013735615768043663]])))
        >>> model: Phi3Model
          > ((),dict(input_ids:CT7s2x3[1926,30157:A15287.666666666666],attention_mask:CT7s2x33[1,1:A1.0],position_ids:None,past_key_values:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.6383233070373535,4.422898292541504:A-0.0035022671144108],CT1s2x32x30x96[-4.655252456665039,4.282486438751221:A-0.0009285174642691407]], value_cache=#2[CT1s2x32x30x96[-4.561790943145752,4.394318103790283:A0.002781494748319777],CT1s2x32x30x96[-4.386584758758545,5.087398052215576:A-0.000443269511274274]]),inputs_embeds:None,use_cache:None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,cache_position:None))
          > ((),dict(input_ids:CT7s3x4[913,26609:A14189.0],attention_mask:CT7s3x35[1,1:A1.0],position_ids:None,past_key_values:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.731935024261475,4.380406379699707:A-0.002659257337925612],CT1s3x32x31x96[-4.263330936431885,4.456257343292236:A-0.001896037205008988]], value_cache=#2[CT1s3x32x31x96[-4.831547260284424,4.720435619354248:A0.0003485915722994343],CT1s3x32x31x96[-4.535431861877441,4.804716110229492:A-0.0013735615768043663]]),inputs_embeds:None,use_cache:None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,cache_position:None))
            >>> embed_tokens: Embedding
              > ((CT7s2x3[1926,30157:A15287.666666666666],),{})
              > ((CT7s3x4[913,26609:A14189.0],),{})
              < (CT1s2x3x3072[-0.08306732773780823,0.08018296211957932:A0.00011067436948413044],)
              < (CT1s3x4x3072[-0.08019158989191055,0.08220747858285904:A8.251460977446893e-05],)
            <<<
            >>> layers[0]: Phi3DecoderLayer
              > ((CT1s2x3x3072[-0.08306732773780823,0.08018296211957932:A0.00011067436948413044],),dict(attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.6383233070373535,4.422898292541504:A-0.0035022671144108],CT1s2x32x30x96[-4.655252456665039,4.282486438751221:A-0.0009285174642691407]], value_cache=#2[CT1s2x32x30x96[-4.561790943145752,4.394318103790283:A0.002781494748319777],CT1s2x32x30x96[-4.386584758758545,5.087398052215576:A-0.000443269511274274]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
              > ((CT1s3x4x3072[-0.08019158989191055,0.08220747858285904:A8.251460977446893e-05],),dict(attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.731935024261475,4.380406379699707:A-0.002659257337925612],CT1s3x32x31x96[-4.263330936431885,4.456257343292236:A-0.001896037205008988]], value_cache=#2[CT1s3x32x31x96[-4.831547260284424,4.720435619354248:A0.0003485915722994343],CT1s3x32x31x96[-4.535431861877441,4.804716110229492:A-0.0013735615768043663]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                >>> self_attn: Phi3Attention
                  > ((),dict(hidden_states:CT1s2x3x3072[-4.07381534576416,3.967752695083618:A0.005547375635928849],attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x30x96[-4.6383233070373535,4.422898292541504:A-0.0035022671144108],CT1s2x32x30x96[-4.655252456665039,4.282486438751221:A-0.0009285174642691407]], value_cache=#2[CT1s2x32x30x96[-4.561790943145752,4.394318103790283:A0.002781494748319777],CT1s2x32x30x96[-4.386584758758545,5.087398052215576:A-0.000443269511274274]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
                  > ((),dict(hidden_states:CT1s3x4x3072[-3.992448091506958,4.066098213195801:A0.004094581482353199],attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x31x96[-4.731935024261475,4.380406379699707:A-0.002659257337925612],CT1s3x32x31x96[-4.263330936431885,4.456257343292236:A-0.001896037205008988]], value_cache=#2[CT1s3x32x31x96[-4.831547260284424,4.720435619354248:A0.0003485915722994343],CT1s3x32x31x96[-4.535431861877441,4.804716110229492:A-0.0013735615768043663]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                    >>> o_proj: Linear
                      > ((CT1s2x3x3072[-1.827728033065796,2.0529401302337646:A0.0046290373323797],),{})
                      > ((CT1s3x4x3072[-1.831414818763733,1.9729397296905518:A-0.0016202278740744288],),{})
                      < (CT1s2x3x3072[-1.427620768547058,1.4870259761810303:A-0.0034037739528369254],)
                      < (CT1s3x4x3072[-1.4522504806518555,1.604917049407959:A0.006740730577790701],)
                    <<<
                    >>> qkv_proj: Linear
                      > ((CT1s2x3x3072[-4.07381534576416,3.967752695083618:A0.005547375635928849],),{})
                      > ((CT1s3x4x3072[-3.992448091506958,4.066098213195801:A0.004094581482353199],),{})
                      < (CT1s2x3x9216[-4.315618515014648,4.688002109527588:A0.0031265301673405097],)
                      < (CT1s3x4x9216[-4.777982711791992,4.703427791595459:A-0.005237230659852693],)
                    <<<
                  < (CT1s2x3x3072[-1.427620768547058,1.4870259761810303:A-0.0034037739528369254],None)
                  < (CT1s3x4x3072[-1.4522504806518555,1.604917049407959:A0.006740730577790701],None)
                <<<
                >>> mlp: Phi3MLP
                  > ((CT1s2x3x3072[-3.7734475135803223,3.673424005508423:A-0.008422457108856918],),{})
                  > ((CT1s3x4x3072[-3.722879648208618,4.036346435546875:A0.018334169059275936],),{})
                    >>> gate_up_proj: Linear
                      > ((CT1s2x3x3072[-3.7734475135803223,3.673424005508423:A-0.008422457108856918],),{})
                      > ((CT1s3x4x3072[-3.722879648208618,4.036346435546875:A0.018334169059275936],),{})
                      < (CT1s2x3x16384[-4.821575164794922,4.749272346496582:A-0.004448725162994549],)
                      < (CT1s3x4x16384[-4.964662075042725,4.792863845825195:A0.00240860671094012],)
                    <<<
                    >>> down_proj: Linear
                      > ((CT1s2x3x8192[-9.801362037658691,10.829329490661621:A-0.00024725358225864805],),{})
                      > ((CT1s3x4x8192[-8.595328330993652,9.225577354431152:A0.0038693266681385234],),{})
                      < (CT1s2x3x3072[-5.5982537269592285,6.071797847747803:A0.0014455462089068129],)
                      < (CT1s3x4x3072[-5.901676654815674,5.963698863983154:A-0.002790240660424893],)
                    <<<
                    >>> activation_fn: SiLU
                      > ((CT1s2x3x8192[-4.821575164794922,4.4840168952941895:A-0.002275999769707937],),{})
                      > ((CT1s3x4x8192[-4.964662075042725,4.792863845825195:A-0.0009792285298739027],),{})
                      < (CT1s2x3x8192[-0.27846455574035645,4.433966636657715:A0.24414615958645203],)
                      < (CT1s3x4x8192[-0.27846455574035645,4.7534637451171875:A0.24591329612694465],)
                    <<<
                  < (CT1s2x3x3072[-5.5982537269592285,6.071797847747803:A0.0014455462089068129],)
                  < (CT1s3x4x3072[-5.901676654815674,5.963698863983154:A-0.002790240660424893],)
                <<<
                >>> input_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-0.08306732773780823,0.08018296211957932:A0.00011067436948413044],),{})
                  > ((CT1s3x4x3072[-0.08019158989191055,0.08220747858285904:A8.251460977446893e-05],),{})
                  < (CT1s2x3x3072[-4.07381534576416,3.967752695083618:A0.005547375635928849],)
                  < (CT1s3x4x3072[-3.992448091506958,4.066098213195801:A0.004094581482353199],)
                <<<
                >>> post_attention_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-1.4386417865753174,1.4993441104888916:A-0.0032930996335791304],),{})
                  > ((CT1s3x4x3072[-1.4525972604751587,1.6054736375808716:A0.006823245127608383],),{})
                  < (CT1s2x3x3072[-3.7734475135803223,3.673424005508423:A-0.008422457108856918],)
                  < (CT1s3x4x3072[-3.722879648208618,4.036346435546875:A0.018334169059275936],)
                <<<
                >>> resid_attn_dropout: Dropout
                  > ((CT1s2x3x3072[-1.427620768547058,1.4870259761810303:A-0.0034037739528369254],),{})
                  > ((CT1s3x4x3072[-1.4522504806518555,1.604917049407959:A0.006740730577790701],),{})
                  < (CT1s2x3x3072[-1.427620768547058,1.4870259761810303:A-0.0034037739528369254],)
                  < (CT1s3x4x3072[-1.4522504806518555,1.604917049407959:A0.006740730577790701],)
                <<<
                >>> resid_mlp_dropout: Dropout
                  > ((CT1s2x3x3072[-5.5982537269592285,6.071797847747803:A0.0014455462089068129],),{})
                  > ((CT1s3x4x3072[-5.901676654815674,5.963698863983154:A-0.002790240660424893],),{})
                  < (CT1s2x3x3072[-5.5982537269592285,6.071797847747803:A0.0014455462089068129],)
                  < (CT1s3x4x3072[-5.901676654815674,5.963698863983154:A-0.002790240660424893],)
                <<<
              < (CT1s2x3x3072[-5.81913948059082,5.713836669921875:A-0.0018475531795856012],)
              < (CT1s3x4x3072[-6.7342939376831055,6.567790985107422:A0.00403300459763361],)
            <<<
            >>> layers[1]: Phi3DecoderLayer
              > ((CT1s2x3x3072[-5.81913948059082,5.713836669921875:A-0.0018475531795856012],),dict(attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.257540702819824,5.124209403991699:A-0.003196788084440659],CT1s2x32x30x96[-4.655252456665039,4.282486438751221:A-0.0009285174642691407]], value_cache=#2[CT1s2x32x33x96[-4.561790943145752,4.394318103790283:A0.002350516678099157],CT1s2x32x30x96[-4.386584758758545,5.087398052215576:A-0.000443269511274274]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
              > ((CT1s3x4x3072[-6.7342939376831055,6.567790985107422:A0.00403300459763361],),dict(attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.593832492828369,5.213327407836914:A-0.003202959405071999],CT1s3x32x31x96[-4.263330936431885,4.456257343292236:A-0.001896037205008988]], value_cache=#2[CT1s3x32x35x96[-4.831547260284424,4.720435619354248:A-0.0005023535074368147],CT1s3x32x31x96[-4.535431861877441,4.804716110229492:A-0.0013735615768043663]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                >>> self_attn: Phi3Attention
                  > ((),dict(hidden_states:CT1s2x3x3072[-4.09100341796875,4.030111789703369:A-0.001383308456963069],attention_mask:CT1s2x1x3x33[-3.4028234663852886e+38,-0.0:A-1.0311586261773601e+37],position_ids:CT7s1x3[30,32:A31.0],past_key_value:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.257540702819824,5.124209403991699:A-0.003196788084440659],CT1s2x32x30x96[-4.655252456665039,4.282486438751221:A-0.0009285174642691407]], value_cache=#2[CT1s2x32x33x96[-4.561790943145752,4.394318103790283:A0.002350516678099157],CT1s2x32x30x96[-4.386584758758545,5.087398052215576:A-0.000443269511274274]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s3[30,32:A31.0],position_embeddings:(CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])))
                  > ((),dict(hidden_states:CT1s3x4x3072[-4.805418491363525,4.775954246520996:A0.0029569381308761754],attention_mask:CT1s3x1x4x35[-3.4028234663852886e+38,-0.0:A-1.4583529141651237e+37],position_ids:CT7s1x4[31,34:A32.5],past_key_value:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.593832492828369,5.213327407836914:A-0.003202959405071999],CT1s3x32x31x96[-4.263330936431885,4.456257343292236:A-0.001896037205008988]], value_cache=#2[CT1s3x32x35x96[-4.831547260284424,4.720435619354248:A-0.0005023535074368147],CT1s3x32x31x96[-4.535431861877441,4.804716110229492:A-0.0013735615768043663]]),output_attentions:bool=False,use_cache:bool=True,cache_position:CT7s4[31,34:A32.5],position_embeddings:(CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])))
                    >>> o_proj: Linear
                      > ((CT1s2x3x3072[-4.020685195922852,2.674964666366577:A-0.00013759772180973093],),{})
                      > ((CT1s3x4x3072[-2.6122562885284424,2.658212661743164:A-0.0002009009938143248],),{})
                      < (CT1s2x3x3072[-1.8699411153793335,1.553274154663086:A0.0054730174840579415],)
                      < (CT1s3x4x3072[-1.613895058631897,1.6024682521820068:A0.0034523024632070096],)
                    <<<
                    >>> qkv_proj: Linear
                      > ((CT1s2x3x3072[-4.09100341796875,4.030111789703369:A-0.001383308456963069],),{})
                      > ((CT1s3x4x3072[-4.805418491363525,4.775954246520996:A0.0029569381308761754],),{})
                      < (CT1s2x3x9216[-4.736824989318848,4.913894176483154:A0.005468381911252133],)
                      < (CT1s3x4x9216[-4.8616557121276855,4.58002233505249:A0.0018776633773416893],)
                    <<<
                  < (CT1s2x3x3072[-1.8699411153793335,1.553274154663086:A0.0054730174840579415],None)
                  < (CT1s3x4x3072[-1.613895058631897,1.6024682521820068:A0.0034523024632070096],None)
                <<<
                >>> mlp: Phi3MLP
                  > ((CT1s2x3x3072[-3.806696653366089,4.0461812019348145:A0.002403160079169927],),{})
                  > ((CT1s3x4x3072[-4.820797443389893,4.334407329559326:A0.005289986026752026],),{})
                    >>> gate_up_proj: Linear
                      > ((CT1s2x3x3072[-3.806696653366089,4.0461812019348145:A0.002403160079169927],),{})
                      > ((CT1s3x4x3072[-4.820797443389893,4.334407329559326:A0.005289986026752026],),{})
                      < (CT1s2x3x16384[-4.276189804077148,4.889461994171143:A0.004154219779817225],)
                      < (CT1s3x4x16384[-5.150392055511475,4.979555130004883:A-0.00304460610584556],)
                    <<<
                    >>> down_proj: Linear
                      > ((CT1s2x3x8192[-8.113507270812988,9.472539901733398:A0.0020876293706956314],),{})
                      > ((CT1s3x4x8192[-11.213486671447754,11.590143203735352:A0.000469281663918795],),{})
                      < (CT1s2x3x3072[-5.216437339782715,6.053864479064941:A-0.007060474994255451],)
                      < (CT1s3x4x3072[-5.934596538543701,5.2051215171813965:A-0.0035336170861784114],)
                    <<<
                    >>> activation_fn: SiLU
                      > ((CT1s2x3x8192[-4.276189804077148,4.889461994171143:A0.004177494788758433],),{})
                      > ((CT1s3x4x8192[-5.150392055511475,4.41855525970459:A-0.0038667097259761363],),{})
                      < (CT1s2x3x8192[-0.27846452593803406,4.852941513061523:A0.2492913767233189],)
                      < (CT1s3x4x8192[-0.27846455574035645,4.365938663482666:A0.2424479720547118],)
                    <<<
                  < (CT1s2x3x3072[-5.216437339782715,6.053864479064941:A-0.007060474994255451],)
                  < (CT1s3x4x3072[-5.934596538543701,5.2051215171813965:A-0.0035336170861784114],)
                <<<
                >>> input_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-5.81913948059082,5.713836669921875:A-0.0018475531795856012],),{})
                  > ((CT1s3x4x3072[-6.7342939376831055,6.567790985107422:A0.00403300459763361],),{})
                  < (CT1s2x3x3072[-4.09100341796875,4.030111789703369:A-0.001383308456963069],)
                  < (CT1s3x4x3072[-4.805418491363525,4.775954246520996:A0.0029569381308761754],)
                <<<
                >>> post_attention_layernorm: Phi3RMSNorm
                  > ((CT1s2x3x3072[-5.5596604347229,5.9727888107299805:A0.003625464379739343],),{})
                  > ((CT1s3x4x3072[-7.030299663543701,6.196286201477051:A0.007485306892375674],),{})
                  < (CT1s2x3x3072[-3.806696653366089,4.0461812019348145:A0.002403160079169927],)
                  < (CT1s3x4x3072[-4.820797443389893,4.334407329559326:A0.005289986026752026],)
                <<<
                >>> resid_attn_dropout: Dropout
                  > ((CT1s2x3x3072[-1.8699411153793335,1.553274154663086:A0.0054730174840579415],),{})
                  > ((CT1s3x4x3072[-1.613895058631897,1.6024682521820068:A0.0034523024632070096],),{})
                  < (CT1s2x3x3072[-1.8699411153793335,1.553274154663086:A0.0054730174840579415],)
                  < (CT1s3x4x3072[-1.613895058631897,1.6024682521820068:A0.0034523024632070096],)
                <<<
                >>> resid_mlp_dropout: Dropout
                  > ((CT1s2x3x3072[-5.216437339782715,6.053864479064941:A-0.007060474994255451],),{})
                  > ((CT1s3x4x3072[-5.934596538543701,5.2051215171813965:A-0.0035336170861784114],),{})
                  < (CT1s2x3x3072[-5.216437339782715,6.053864479064941:A-0.007060474994255451],)
                  < (CT1s3x4x3072[-5.934596538543701,5.2051215171813965:A-0.0035336170861784114],)
                <<<
              < (CT1s2x3x3072[-8.007316589355469,7.950407981872559:A-0.003435011149122147],)
              < (CT1s3x4x3072[-8.958703994750977,7.521454334259033:A0.003951689586352182],)
            <<<
            >>> norm: Phi3RMSNorm
              > ((CT1s2x3x3072[-8.007316589355469,7.950407981872559:A-0.003435011149122147],),{})
              > ((CT1s3x4x3072[-8.958703994750977,7.521454334259033:A0.003951689586352182],),{})
              < (CT1s2x3x3072[-3.9787144660949707,4.065525054931641:A-0.0015723454703411794],)
              < (CT1s3x4x3072[-4.457744598388672,3.877598524093628:A0.002080924662687068],)
            <<<
            >>> rotary_emb: Phi3RotaryEmbedding
              > ((CT1s2x3x3072[-0.08306732773780823,0.08018296211957932:A0.00011067436948413044],CT7s1x3[30,32:A31.0]),{})
              > ((CT1s3x4x3072[-0.08019158989191055,0.08220747858285904:A8.251460977446893e-05],CT7s1x4[31,34:A32.5]),{})
              < (CT1s1x3x96[-1.1855769157409668,1.1902371644973755:A0.746652018013669],CT1s1x3x96[-1.1887905597686768,1.190193772315979:A0.1589894221542636])
              < (CT1s1x4x96[-1.1855769157409668,1.190237045288086:A0.7129333875218435],CT1s1x4x96[-1.1719439029693604,1.1902378797531128:A0.18296290554159592])
            <<<
          < (dict(last_hidden_state:CT1s2x3x3072[-3.9787144660949707,4.065525054931641:A-0.0015723454703411794],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.257540702819824,5.124209403991699:A-0.003196788084440659],CT1s2x32x33x96[-5.2950439453125,5.101772785186768:A0.0005665109786626474]], value_cache=#2[CT1s2x32x33x96[-4.561790943145752,4.394318103790283:A0.002350516678099157],CT1s2x32x33x96[-4.736824989318848,5.087398052215576:A-0.0007748850698348344]])),)
          < (dict(last_hidden_state:CT1s3x4x3072[-4.457744598388672,3.877598524093628:A0.002080924662687068],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.593832492828369,5.213327407836914:A-0.003202959405071999],CT1s3x32x35x96[-5.725250244140625,5.474720001220703:A-0.001029906491431944]], value_cache=#2[CT1s3x32x35x96[-4.831547260284424,4.720435619354248:A-0.0005023535074368147],CT1s3x32x35x96[-4.8616557121276855,4.804716110229492:A-0.0020244930790110586]])),)
        <<<
        >>> lm_head: Linear
          > ((CT1s2x3x3072[-3.9787144660949707,4.065525054931641:A-0.0015723454703411794],),{})
          > ((CT1s3x4x3072[-4.457744598388672,3.877598524093628:A0.002080924662687068],),{})
          < (CT1s2x3x32064[-5.129799842834473,5.195614337921143:A-0.0013133137086102555],)
          < (CT1s3x4x32064[-4.8343825340271,5.137273788452148:A0.0004936027517846812],)
        <<<
      < (dict(logits:CT1s2x3x32064[-5.129799842834473,5.195614337921143:A-0.0013133137086102555],past_key_values:DynamicCache(key_cache=#2[CT1s2x32x33x96[-5.257540702819824,5.124209403991699:A-0.003196788084440659],CT1s2x32x33x96[-5.2950439453125,5.101772785186768:A0.0005665109786626474]], value_cache=#2[CT1s2x32x33x96[-4.561790943145752,4.394318103790283:A0.002350516678099157],CT1s2x32x33x96[-4.736824989318848,5.087398052215576:A-0.0007748850698348344]])),)
      < (dict(logits:CT1s3x4x32064[-4.8343825340271,5.137273788452148:A0.0004936027517846812],past_key_values:DynamicCache(key_cache=#2[CT1s3x32x35x96[-5.593832492828369,5.213327407836914:A-0.003202959405071999],CT1s3x32x35x96[-5.725250244140625,5.474720001220703:A-0.001029906491431944]], value_cache=#2[CT1s3x32x35x96[-4.831547260284424,4.720435619354248:A-0.0005023535074368147],CT1s3x32x35x96[-4.8616557121276855,4.804716110229492:A-0.0020244930790110586]])),)
    <<<
    [_untrace_forward_execution]  __main__-Phi3ForCausalLM
    [_untrace_forward_execution] .. model-Phi3Model
    [_untrace_forward_execution] .... embed_tokens-Embedding
    [_untrace_forward_execution] .... layers[0]-Phi3DecoderLayer
    [_untrace_forward_execution] ...... self_attn-Phi3Attention
    [_untrace_forward_execution] ........ o_proj-Linear
    [_untrace_forward_execution] ........ qkv_proj-Linear
    [_untrace_forward_execution] ...... mlp-Phi3MLP
    [_untrace_forward_execution] ........ gate_up_proj-Linear
    [_untrace_forward_execution] ........ down_proj-Linear
    [_untrace_forward_execution] ........ activation_fn-SiLU
    [_untrace_forward_execution] ...... input_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... resid_attn_dropout-Dropout
    [_untrace_forward_execution] ...... resid_mlp_dropout-Dropout
    [_untrace_forward_execution] .... layers[1]-Phi3DecoderLayer
    [_untrace_forward_execution] ...... self_attn-Phi3Attention
    [_untrace_forward_execution] ........ o_proj-Linear
    [_untrace_forward_execution] ........ qkv_proj-Linear
    [_untrace_forward_execution] ...... mlp-Phi3MLP
    [_untrace_forward_execution] ........ gate_up_proj-Linear
    [_untrace_forward_execution] ........ down_proj-Linear
    [_untrace_forward_execution] ........ activation_fn-SiLU
    [_untrace_forward_execution] ...... input_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... post_attention_layernorm-Phi3RMSNorm
    [_untrace_forward_execution] ...... resid_attn_dropout-Dropout
    [_untrace_forward_execution] ...... resid_mlp_dropout-Dropout
    [_untrace_forward_execution] .... norm-Phi3RMSNorm
    [_untrace_forward_execution] .... rotary_emb-Phi3RotaryEmbedding
    [_untrace_forward_execution] .. lm_head-Linear




.. GENERATED FROM PYTHON SOURCE LINES 224-227

Now we keep in memory every input/output for the submodules,
we can guess the dynamic shapes for every of them.
The final ones:

.. GENERATED FROM PYTHON SOURCE LINES 227-231

.. code-block:: Python

    dynamic_shapes = diag.guess_dynamic_shapes()
    print("The dynamic shapes are:")
    pprint.pprint(dynamic_shapes)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The dynamic shapes are:
    ((),
     {'attention_mask': {0: <_DimHint.DYNAMIC: 3>, 1: <_DimHint.DYNAMIC: 3>},
      'input_ids': {0: <_DimHint.DYNAMIC: 3>, 1: <_DimHint.DYNAMIC: 3>},
      'past_key_values': [[{0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>},
                           {0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>}],
                          [{0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>},
                           {0: <_DimHint.DYNAMIC: 3>, 2: <_DimHint.DYNAMIC: 3>}]]})




.. GENERATED FROM PYTHON SOURCE LINES 232-233

And all the dynamic shapes all along the traced submodules.

.. GENERATED FROM PYTHON SOURCE LINES 233-243

.. code-block:: Python

    print(
        diag.pretty_text(
            with_dynamic_shape=True,
            with_shape=False,
            with_min_max=False,
            with_device=False,
            with_inputs=False,
        ).replace("<_DimHint.DYNAMIC: 3>", "DYN")
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    >>> __main__: Phi3ForCausalLM
      DS=((), {'attention_mask': {0: DYN, 1: DYN}, 'input_ids': {0: DYN, 1: DYN}, 'past_key_values': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]]})
        >>> model: Phi3Model
          DS=((), {'attention_mask': {0: DYN, 1: DYN}, 'cache_position': None, 'input_ids': {0: DYN, 1: DYN}, 'inputs_embeds': None, 'output_attentions': None, 'output_hidden_states': None, 'past_key_values': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_ids': None, 'return_dict': None, 'use_cache': None})
            >>> embed_tokens: Embedding: DS=(({0: DYN, 1: DYN},), {}) <<<
            >>> layers[0]: Phi3DecoderLayer
              DS=(({0: DYN, 1: DYN},), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                >>> self_attn: Phi3Attention
                  DS=((), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'hidden_states': {0: DYN, 1: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                    >>> o_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> qkv_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> mlp: Phi3MLP
                  DS=(({0: DYN, 1: DYN},), {})
                    >>> gate_up_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> down_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> activation_fn: SiLU: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> post_attention_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_attn_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_mlp_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
            <<<
            >>> layers[1]: Phi3DecoderLayer
              DS=(({0: DYN, 1: DYN},), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                >>> self_attn: Phi3Attention
                  DS=((), {'attention_mask': {0: DYN, 2: DYN, 3: DYN}, 'cache_position': {0: DYN}, 'hidden_states': {0: DYN, 1: DYN}, 'output_attentions': None, 'past_key_value': [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]], 'position_embeddings': ({1: DYN}, {1: DYN}), 'position_ids': {1: DYN}, 'use_cache': None})
                    >>> o_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> qkv_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> mlp: Phi3MLP
                  DS=(({0: DYN, 1: DYN},), {})
                    >>> gate_up_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> down_proj: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
                    >>> activation_fn: SiLU: DS=(({0: DYN, 1: DYN},), {}) <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> post_attention_layernorm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_attn_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
                >>> resid_mlp_dropout: Dropout: DS=(({0: DYN, 1: DYN},), {}) <<<
            <<<
            >>> norm: Phi3RMSNorm: DS=(({0: DYN, 1: DYN},), {}) <<<
            >>> rotary_emb: Phi3RotaryEmbedding: DS=(({0: DYN, 1: DYN}, {1: DYN}), {}) <<<
        <<<
        >>> lm_head: Linear: DS=(({0: DYN, 1: DYN},), {}) <<<
    <<<




.. GENERATED FROM PYTHON SOURCE LINES 244-254

Evaluate the export
+++++++++++++++++++

In many cases, the export (to :class:`torch.fx.Graph`, to ONNX)
does not work on the first try. We need a way to understand
how much the model can be exported. It can be used to evaluate
the how much code needs to be rewritten or patched to be exportable.
The verbosity can be increase to show dynamic shapes, results
of the discrepancies.
Let's display the module and its submodule first.

.. GENERATED FROM PYTHON SOURCE LINES 254-265

.. code-block:: Python


    print(
        diag.pretty_text(
            with_dynamic_shape=False,
            with_shape=False,
            with_min_max=False,
            with_device=False,
            with_inputs=False,
        )
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    >>> __main__: Phi3ForCausalLM
        >>> model: Phi3Model
            >>> embed_tokens: Embedding <<<
            >>> layers[0]: Phi3DecoderLayer
                >>> self_attn: Phi3Attention
                    >>> o_proj: Linear <<<
                    >>> qkv_proj: Linear <<<
                <<<
                >>> mlp: Phi3MLP
                    >>> gate_up_proj: Linear <<<
                    >>> down_proj: Linear <<<
                    >>> activation_fn: SiLU <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm <<<
                >>> post_attention_layernorm: Phi3RMSNorm <<<
                >>> resid_attn_dropout: Dropout <<<
                >>> resid_mlp_dropout: Dropout <<<
            <<<
            >>> layers[1]: Phi3DecoderLayer
                >>> self_attn: Phi3Attention
                    >>> o_proj: Linear <<<
                    >>> qkv_proj: Linear <<<
                <<<
                >>> mlp: Phi3MLP
                    >>> gate_up_proj: Linear <<<
                    >>> down_proj: Linear <<<
                    >>> activation_fn: SiLU <<<
                <<<
                >>> input_layernorm: Phi3RMSNorm <<<
                >>> post_attention_layernorm: Phi3RMSNorm <<<
                >>> resid_attn_dropout: Dropout <<<
                >>> resid_mlp_dropout: Dropout <<<
            <<<
            >>> norm: Phi3RMSNorm <<<
            >>> rotary_emb: Phi3RotaryEmbedding <<<
        <<<
        >>> lm_head: Linear <<<
    <<<




.. GENERATED FROM PYTHON SOURCE LINES 266-269

The we try to export to see the submodule failing the whole model.
We can pickle the failing model and restore it to speedup
the refactoring to make it work.

.. GENERATED FROM PYTHON SOURCE LINES 269-279

.. code-block:: Python

    print("----------------------")
    ep = diag.try_export(
        exporter="fx",
        use_dynamic_shapes=True,
        exporter_kwargs=dict(strict=False),
        verbose=1,
    )
    print(f"success: {ep.status}")
    print(diag.get_export_report())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ----------------------

    [try_export-FX]  __main__-Phi3ForCausalLM --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_values']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_values']` (expected None)
    [try_export-FX] .. model-Phi3Model --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_values']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_values']` (expected None)
    [try_export-FX] .... embed_tokens-Embedding --- OK: 
    [try_export-FX] .... layers[0]-Phi3DecoderLayer --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ...... self_attn-Phi3Attention --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ........ o_proj-Linear --- OK: 
    [try_export-FX] ........ qkv_proj-Linear --- OK: 
    [try_export-FX] ...... mlp-Phi3MLP --- OK: 
    [try_export-FX] ...... input_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... post_attention_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... resid_attn_dropout-Dropout --- OK: 
    [try_export-FX] ...... resid_mlp_dropout-Dropout --- OK: 
    [try_export-FX] .... layers[1]-Phi3DecoderLayer --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ...... self_attn-Phi3Attention --- FAIL, step=EXPORT, reason=Cannot associate shape [[{0: DYN, 2: DYN}, {0: DYN, 2: DYN}], [{0: DYN, 2: DYN}, {0: DYN, 2: DYN}]] specified at `dynamic_shapes['past_key_value']` to non-tensor type <class 'transformers.cache_utils.DynamicCache'> at `inputs['past_key_value']` (expected None)
    [try_export-FX] ........ o_proj-Linear --- OK: 
    [try_export-FX] ........ qkv_proj-Linear --- OK: 
    [try_export-FX] ...... mlp-Phi3MLP --- OK: 
    [try_export-FX] ...... input_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... post_attention_layernorm-Phi3RMSNorm --- OK: 
    [try_export-FX] ...... resid_attn_dropout-Dropout --- OK: 
    [try_export-FX] ...... resid_mlp_dropout-Dropout --- OK: 
    [try_export-FX] .... norm-Phi3RMSNorm --- OK: 
    [try_export-FX] .... rotary_emb-Phi3RotaryEmbedding --- FAIL, step=EXPORT, reason=Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)
    [try_export-FX] .... rotary_emb-Phi3RotaryEmbedding --- FAIL: Could not guard on data-depend...
    [try_export-FX] .. lm_head-Linear --- OK: 
    success: 2
    __main__                         Phi3ForCausalLM       FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ..model                          Phi3Model             FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ....embed_tokens                 Embedding             OK -- ExportedProgram
    ....layers[0]                    Phi3DecoderLayer      FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ......self_attn                  Phi3Attention         FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ........o_proj                   Linear                OK -- ExportedProgram
    ........qkv_proj                 Linear                OK -- ExportedProgram
    ......mlp                        Phi3MLP               OK -- ExportedProgram
    ........gate_up_proj             Linear                OK as part of its owner
    ........down_proj                Linear                OK as part of its owner
    ........activation_fn            SiLU                  OK as part of its owner
    ......input_layernorm            Phi3RMSNorm           OK -- ExportedProgram
    ......post_attention_layernorm   Phi3RMSNorm           OK -- ExportedProgram
    ......resid_attn_dropout         Dropout               OK -- ExportedProgram
    ......resid_mlp_dropout          Dropout               OK -- ExportedProgram
    ....layers[1]                    Phi3DecoderLayer      FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ......self_attn                  Phi3Attention         FAIL -- step=EXPORT, reason='Cannot associate shape [[{0: DYN, 2: DYN...'
    ........o_proj                   Linear                OK -- ExportedProgram
    ........qkv_proj                 Linear                OK -- ExportedProgram
    ......mlp                        Phi3MLP               OK -- ExportedProgram
    ........gate_up_proj             Linear                OK as part of its owner
    ........down_proj                Linear                OK as part of its owner
    ........activation_fn            SiLU                  OK as part of its owner
    ......input_layernorm            Phi3RMSNorm           OK -- ExportedProgram
    ......post_attention_layernorm   Phi3RMSNorm           OK -- ExportedProgram
    ......resid_attn_dropout         Dropout               OK -- ExportedProgram
    ......resid_mlp_dropout          Dropout               OK -- ExportedProgram
    ....norm                         Phi3RMSNorm           OK -- ExportedProgram
    ....rotary_emb                   Phi3RotaryEmbedding   FAIL -- step=EXPORT, reason='Could not guard on data-dependent expres...'
    ..lm_head                        Linear                OK -- ExportedProgram




.. GENERATED FROM PYTHON SOURCE LINES 280-287

Replace the failing module by a custom op
+++++++++++++++++++++++++++++++++++++++++

The main module is not exportable because one piece cannot be exported.
But maybe if we assume it works, maybe everything else is working.
So let's try to replace this class by a custom op.
This will be something for another example.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 6.108 seconds)


.. _sphx_glr_download_auto_recipes_plot_exporter_exporter_phi35_piece.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_exporter_exporter_phi35_piece.ipynb <plot_exporter_exporter_phi35_piece.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_exporter_exporter_phi35_piece.py <plot_exporter_exporter_phi35_piece.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_exporter_exporter_phi35_piece.zip <plot_exporter_exporter_phi35_piece.zip>`


.. include:: plot_exporter_exporter_phi35_piece.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
