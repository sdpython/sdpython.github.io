
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_recipes/plot_exporter_exporter_lost_dynamic_dimension.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_recipes_plot_exporter_exporter_lost_dynamic_dimension.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_recipes_plot_exporter_exporter_lost_dynamic_dimension.py:


.. _l-plot-exporter-lost_dynamic_dimension:

A dynamic dimension lost by torch.export.export
===============================================

Dynamic shapes ensures a model is valid not matter what the
dimension value is for a dynamic dimension.
:func:`torch.export.export` is trying to keep track of that information
for every intermediate result the model produces.
But something it fails. Let's see one case.

A dynamic dimension is replaced by a constant by function pad
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

It could by any other function. A function is taking an integer as an argument.
Despite the fact this value may change with different input, the exporter
loses than information as it consider the value as an integer, therefore,
a constant.

.. GENERATED FROM PYTHON SOURCE LINES 21-43

.. code-block:: Python


    import torch


    def dummy_function(idx, x_len):
        # [1, 2, 3] becomes [1, 2, 3, x_len]
        return torch.nn.functional.pad(idx, (0, 1), value=x_len)


    class Model(torch.nn.Module):
        def forward(self, x, y):
            padded = dummy_function(x, y.shape[0])
            return padded.reshape((-1, 1)) + torch.arange(padded.max()).reshape((1, -1))


    model = Model()
    inputs = (
        (torch.arange(3) + 1).to(torch.int64),
        torch.tensor([0, 5], dtype=torch.int64),
    )
    print(model(*inputs))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[1, 2, 3],
            [2, 3, 4],
            [3, 4, 5],
            [2, 3, 4]])




.. GENERATED FROM PYTHON SOURCE LINES 44-45

Let's export.

.. GENERATED FROM PYTHON SOURCE LINES 45-50

.. code-block:: Python

    AUTO = torch.export.Dim.AUTO
    ep = torch.export.export(
        model, inputs, dynamic_shapes={"x": {0: AUTO}, "y": {0: AUTO}}, strict=False
    )








.. GENERATED FROM PYTHON SOURCE LINES 51-52

Let's check it works.

.. GENERATED FROM PYTHON SOURCE LINES 52-54

.. code-block:: Python

    print(ep.module()(*inputs))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[1, 2, 3],
            [2, 3, 4],
            [3, 4, 5],
            [2, 3, 4]])




.. GENERATED FROM PYTHON SOURCE LINES 55-56

Let's print the graph.

.. GENERATED FROM PYTHON SOURCE LINES 56-58

.. code-block:: Python

    print(ep.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %x : [num_users=1] = placeholder[target=x]
        %y : [num_users=0] = placeholder[target=y]
        %pad : [num_users=2] = call_function[target=torch.ops.aten.pad.default](args = (%x, [0, 1], constant, 2.0), kwargs = {})
        %reshape : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%pad, [-1, 1]), kwargs = {})
        %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%pad,), kwargs = {})
        %item : [num_users=3] = call_function[target=torch.ops.aten.item.default](args = (%max_1,), kwargs = {})
        %sym_constrain_range_for_size_default : [num_users=0] = call_function[target=torch.ops.aten.sym_constrain_range_for_size.default](args = (%item,), kwargs = {})
        %ge : [num_users=1] = call_function[target=operator.ge](args = (%item, 0), kwargs = {})
        %_assert_scalar_default : [num_users=0] = call_function[target=torch.ops.aten._assert_scalar.default](args = (%ge, Runtime assertion failed for expression u0 >= 0 on node 'ge'), kwargs = {})
        %arange : [num_users=1] = call_function[target=torch.ops.aten.arange.default](args = (%item,), kwargs = {device: cpu, pin_memory: False})
        %reshape_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%arange, [1, -1]), kwargs = {})
        %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%reshape, %reshape_1), kwargs = {})
        return (add,)




.. GENERATED FROM PYTHON SOURCE LINES 59-64

It shows the following line
``[torch.ops.aten.pad.default](args = (%x, [0, 1], constant, 2.0)``
which corresponds to ``torch.nn.functional.pad(idx, (0, 1), value=x_len)``.
But in this case, ``x_len`` is equal to ``y.shape[0]`` which was defined
as a dynamic dimension. Se if we choose something like the following:

.. GENERATED FROM PYTHON SOURCE LINES 64-70

.. code-block:: Python


    inputs2 = (
        (torch.arange(3) + 1).to(torch.int64),
        torch.tensor([0, 5, 6], dtype=torch.int64),
    )








.. GENERATED FROM PYTHON SOURCE LINES 71-72

The original model works.

.. GENERATED FROM PYTHON SOURCE LINES 72-74

.. code-block:: Python

    print(model(*inputs2))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[1, 2, 3],
            [2, 3, 4],
            [3, 4, 5],
            [3, 4, 5]])




.. GENERATED FROM PYTHON SOURCE LINES 75-76

But the exported program does not.

.. GENERATED FROM PYTHON SOURCE LINES 76-81

.. code-block:: Python

    try:
        print(ep.module()(*inputs2))
    except Exception as e:
        print(e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Expected input at *args[1].shape[0] to be equal to 2, but got 3




.. GENERATED FROM PYTHON SOURCE LINES 82-87

How to fix it?
++++++++++++++

In this particular case, function is not the only way ``pad``
to produce the desired result.

.. GENERATED FROM PYTHON SOURCE LINES 87-103

.. code-block:: Python



    def dummy_function_cat(idx, x_len):
        # [1, 2, 3] becomes [1, 2, 3, x_len]
        return torch.cat([idx, torch.tensor([x_len], dtype=torch.int64)], dim=0)


    class ModelCat(torch.nn.Module):
        def forward(self, x, y):
            padded = dummy_function_cat(x, y.shape[0])
            return padded.reshape((-1, 1)) + torch.arange(padded.max()).reshape((1, -1))


    modelcat = ModelCat()
    print(modelcat(*inputs))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[1, 2, 3],
            [2, 3, 4],
            [3, 4, 5],
            [2, 3, 4]])




.. GENERATED FROM PYTHON SOURCE LINES 104-105

Let's export.

.. GENERATED FROM PYTHON SOURCE LINES 105-109

.. code-block:: Python

    epcat = torch.export.export(
        modelcat, inputs, dynamic_shapes={"x": {0: AUTO}, "y": {0: AUTO}}, strict=False
    )








.. GENERATED FROM PYTHON SOURCE LINES 110-111

Let's check it works.

.. GENERATED FROM PYTHON SOURCE LINES 111-113

.. code-block:: Python

    print(epcat.module()(*inputs))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[1, 2, 3],
            [2, 3, 4],
            [3, 4, 5],
            [2, 3, 4]])




.. GENERATED FROM PYTHON SOURCE LINES 114-115

Let's print the graph.

.. GENERATED FROM PYTHON SOURCE LINES 115-117

.. code-block:: Python

    print(epcat.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %x : [num_users=1] = placeholder[target=x]
        %y : [num_users=1] = placeholder[target=y]
        %sym_size_int_2 : [num_users=1] = call_function[target=torch.ops.aten.sym_size.int](args = (%y, 0), kwargs = {})
        %scalar_tensor : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (%sym_size_int_2,), kwargs = {dtype: torch.int64, device: cpu, pin_memory: False})
        %stack : [num_users=1] = call_function[target=torch.ops.aten.stack.default](args = ([%scalar_tensor],), kwargs = {})
        %to : [num_users=1] = call_function[target=torch.ops.aten.to.device](args = (%stack, cpu, torch.int64), kwargs = {})
        %detach_ : [num_users=1] = call_function[target=torch.ops.aten.detach_.default](args = (%to,), kwargs = {})
        %cat : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%x, %detach_],), kwargs = {})
        %reshape : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%cat, [-1, 1]), kwargs = {})
        %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%cat,), kwargs = {})
        %item : [num_users=3] = call_function[target=torch.ops.aten.item.default](args = (%max_1,), kwargs = {})
        %sym_constrain_range_for_size_default : [num_users=0] = call_function[target=torch.ops.aten.sym_constrain_range_for_size.default](args = (%item,), kwargs = {})
        %ge : [num_users=1] = call_function[target=operator.ge](args = (%item, 0), kwargs = {})
        %_assert_scalar_default : [num_users=0] = call_function[target=torch.ops.aten._assert_scalar.default](args = (%ge, Runtime assertion failed for expression u0 >= 0 on node 'ge'), kwargs = {})
        %arange : [num_users=1] = call_function[target=torch.ops.aten.arange.default](args = (%item,), kwargs = {device: cpu, pin_memory: False})
        %reshape_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%arange, [1, -1]), kwargs = {})
        %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%reshape, %reshape_1), kwargs = {})
        return (add,)




.. GENERATED FROM PYTHON SOURCE LINES 118-119

And the final verification.

.. GENERATED FROM PYTHON SOURCE LINES 119-121

.. code-block:: Python

    print(epcat.module()(*inputs2))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[1, 2, 3],
            [2, 3, 4],
            [3, 4, 5],
            [3, 4, 5]])




.. GENERATED FROM PYTHON SOURCE LINES 122-123

It finally works.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 5.957 seconds)


.. _sphx_glr_download_auto_recipes_plot_exporter_exporter_lost_dynamic_dimension.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_exporter_exporter_lost_dynamic_dimension.ipynb <plot_exporter_exporter_lost_dynamic_dimension.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_exporter_exporter_lost_dynamic_dimension.py <plot_exporter_exporter_lost_dynamic_dimension.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_exporter_exporter_lost_dynamic_dimension.zip <plot_exporter_exporter_lost_dynamic_dimension.zip>`


.. include:: plot_exporter_exporter_lost_dynamic_dimension.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
