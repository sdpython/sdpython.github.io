
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_recipes/plot_exporter_recipes_c_scan_pdist.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_recipes_plot_exporter_recipes_c_scan_pdist.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_recipes_plot_exporter_recipes_c_scan_pdist.py:


.. _l-plot-exporter-recipes-custom-pdist:

to_onnx and a model with a loop (scan)
======================================

Control flow cannot be exported with a change.
The code of the model can be changed or patched
to introduce function :func:`torch.ops.higher_order.scan`.

Pairwise Distance
+++++++++++++++++

We appy loops to the pairwise distances (:class:`torch.nn.PairwiseDistance`).

.. GENERATED FROM PYTHON SOURCE LINES 16-41

.. code-block:: Python


    import scipy.spatial.distance as spd
    import torch
    from onnx_array_api.plotting.graphviz_helper import plot_dot
    from experimental_experiment.helpers import pretty_onnx
    from experimental_experiment.torch_interpreter import to_onnx, ExportOptions


    class ModuleWithControlFlowLoop(torch.nn.Module):
        def forward(self, x, y):
            dist = torch.empty((x.shape[0], y.shape[0]), dtype=x.dtype)
            for i in range(x.shape[0]):
                sub = y - x[i : i + 1]
                d = torch.sqrt((sub * sub).sum(axis=1))
                dist[i, :] = d
            return dist


    model = ModuleWithControlFlowLoop()
    x = torch.randn(3, 4)
    y = torch.randn(5, 4)
    pwd = spd.cdist(x.numpy(), y.numpy())
    expected = torch.from_numpy(pwd)
    print(f"shape={pwd.shape}, discrepancies={torch.abs(expected - model(x,y)).max()}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    shape=(3, 5), discrepancies=2.4455857339233944e-07




.. GENERATED FROM PYTHON SOURCE LINES 42-44

:func:`torch.export.export` works because it unrolls the loop.
It works if the input size never change.

.. GENERATED FROM PYTHON SOURCE LINES 44-49

.. code-block:: Python



    ep = torch.export.export(model, (x, y))
    print(ep.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %x : [num_users=3] = placeholder[target=x]
        %y : [num_users=3] = placeholder[target=y]
        %empty : [num_users=4] = call_function[target=torch.ops.aten.empty.memory_format](args = ([3, 5],), kwargs = {dtype: torch.float32, device: cpu, pin_memory: False})
        %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%x, 0, 0, 1), kwargs = {})
        %sub : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%y, %slice_1), kwargs = {})
        %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub, %sub), kwargs = {})
        %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul, [1]), kwargs = {})
        %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_1,), kwargs = {})
        %select : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%empty, 0, 0), kwargs = {})
        %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%select, 0, 0, 9223372036854775807), kwargs = {})
        %copy_ : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%slice_2, %sqrt), kwargs = {})
        %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%x, 0, 1, 2), kwargs = {})
        %sub_1 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%y, %slice_3), kwargs = {})
        %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_1, %sub_1), kwargs = {})
        %sum_2 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1, [1]), kwargs = {})
        %sqrt_1 : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_2,), kwargs = {})
        %select_1 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%empty, 0, 1), kwargs = {})
        %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%select_1, 0, 0, 9223372036854775807), kwargs = {})
        %copy__1 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%slice_4, %sqrt_1), kwargs = {})
        %slice_5 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%x, 0, 2, 3), kwargs = {})
        %sub_2 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%y, %slice_5), kwargs = {})
        %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_2, %sub_2), kwargs = {})
        %sum_3 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_2, [1]), kwargs = {})
        %sqrt_2 : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_3,), kwargs = {})
        %select_2 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%empty, 0, 2), kwargs = {})
        %slice_6 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%select_2, 0, 0, 9223372036854775807), kwargs = {})
        %copy__2 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%slice_6, %sqrt_2), kwargs = {})
        return (empty,)




.. GENERATED FROM PYTHON SOURCE LINES 50-51

However, with dynamic shapes, that's another story.

.. GENERATED FROM PYTHON SOURCE LINES 51-63

.. code-block:: Python


    x_rows = torch.export.Dim("x_rows")
    y_rows = torch.export.Dim("y_rows")
    dim = torch.export.Dim("dim")
    try:
        ep = torch.export.export(
            model, (x, y), dynamic_shapes={"x": {0: x_rows, 1: dim}, "y": {0: y_rows, 1: dim}}
        )
        print(ep.graph)
    except Exception as e:
        print(e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Constraints violated (x_rows)! For more information, run with TORCH_LOGS="+dynamic".
      - Not all values of x_rows = L['x'].size()[0] in the specified range are valid because x_rows was inferred to be a constant (3).

    Suggested fixes:
      x_rows = 3




.. GENERATED FROM PYTHON SOURCE LINES 64-69

Suggested Patch
+++++++++++++++

We need to rewrite the module with function
:func:`torch.ops.higher_order.scan`.

.. GENERATED FROM PYTHON SOURCE LINES 69-97

.. code-block:: Python



    def dist(y: torch.Tensor, scanned_x: torch.Tensor):
        sub = y - scanned_x.reshape((1, -1))
        sq = sub * sub
        rd = torch.sqrt(sq.sum(axis=1))
        # clone --> UnsupportedAliasMutationException:
        # Combine_fn might be aliasing the input!
        return [y.clone(), rd]


    class ModuleWithControlFlowLoopScan(torch.nn.Module):

        def forward(self, x, y):
            carry, out = torch.ops.higher_order.scan(
                dist,
                [y],
                [x],
                dim=0,
                reverse=False,
                additional_inputs=[],
            )
            return out


    model = ModuleWithControlFlowLoopScan()
    print(f"shape={pwd.shape}, discrepancies={torch.abs(expected - model(x,y)).max()}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    shape=(3, 5), discrepancies=2.4455857339233944e-07




.. GENERATED FROM PYTHON SOURCE LINES 98-99

That works. Let's export again.

.. GENERATED FROM PYTHON SOURCE LINES 99-105

.. code-block:: Python


    ep = torch.export.export(
        model, (x, y), dynamic_shapes={"x": {0: x_rows, 1: dim}, "y": {0: y_rows, 1: dim}}
    )
    print(ep.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %x : [num_users=1] = placeholder[target=x]
        %y : [num_users=1] = placeholder[target=y]
        %scan_combine_graph_0 : [num_users=1] = get_attr[target=scan_combine_graph_0]
        %scan : [num_users=2] = call_function[target=torch.ops.higher_order.scan](args = (%scan_combine_graph_0, [%y], [%x], 0, False, []), kwargs = {})
        %getitem : [num_users=0] = call_function[target=operator.getitem](args = (%scan, 0), kwargs = {})
        %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%scan, 1), kwargs = {})
        return (getitem_1,)




.. GENERATED FROM PYTHON SOURCE LINES 106-108

The graph shows some unused results and this might confuse the exporter.
We need to run :meth:`torch.export.ExportedProgram.run_decompositions`.

.. GENERATED FROM PYTHON SOURCE LINES 108-111

.. code-block:: Python

    ep = ep.run_decompositions({})
    print(ep.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %x : [num_users=1] = placeholder[target=x]
        %y : [num_users=1] = placeholder[target=y]
        %scan_combine_graph_0 : [num_users=1] = get_attr[target=scan_combine_graph_0]
        %scan : [num_users=1] = call_function[target=torch.ops.higher_order.scan](args = (%scan_combine_graph_0, [%y], [%x], 0, False, []), kwargs = {})
        %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%scan, 1), kwargs = {})
        return (getitem_1,)




.. GENERATED FROM PYTHON SOURCE LINES 112-113

Let's export again with ONNX.

.. GENERATED FROM PYTHON SOURCE LINES 113-122

.. code-block:: Python


    onx = to_onnx(
        model,
        (x, y),
        dynamic_shapes={"x": {0: x_rows, 1: dim}, "y": {0: y_rows, 1: dim}},
        export_options=ExportOptions(decomposition_table="default"),
    )
    print(pretty_onnx(onx))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    opset: domain='local_functions' version=1
    input: name='x' type=dtype('float32') shape=['x_rows', 'dim']
    input: name='y' type=dtype('float32') shape=['y_rows', 'dim']
    Scan(y, x, body=G1, num_scan_inputs=1, scan_input_directions=[0], scan_output_axes=[0], scan_output_directions=[0]) -> scan#0, output_0
    output: name='output_0' type=dtype('float32') shape=['x_rows', 'y_rows']
    ----- subgraph ---- Scan - scan - att.body=G1 -- level=1 -- init_0_y,scan_0_x -> output_0,output_1
    input: name='init_0_y' type='NOTENSOR' shape=None
    input: name='scan_0_x' type='NOTENSOR' shape=None
    scan_combine_graph_0[local_functions](init_0_y, scan_0_x) -> output_0, output_1
    output: name='output_0' type='NOTENSOR' shape=None
    output: name='output_1' type='NOTENSOR' shape=None
    ----- function name=scan_combine_graph_0 domain=local_functions
    ----- doc_string: -- function_options=FunctionOptions(export_as_function=...
    opset: domain='' version=18
    input: 'arg0_1'
    input: 'arg1_1'
    Constant(value=[1, -1]) -> init7_s2_1_-1
      Reshape(arg1_1, init7_s2_1_-1) -> view
        Sub(arg0_1, view) -> sub_4
          Mul(sub_4, sub_4) -> mul_7
    Constant(value=[1]) -> init7_s1_1
      ReduceSum(mul_7, init7_s1_1, keepdims=0) -> sum_1
        Sqrt(sum_1) -> output_1
    Identity(arg0_1) -> output_0
    output: name='output_0' type=? shape=?
    output: name='output_1' type=? shape=?




.. GENERATED FROM PYTHON SOURCE LINES 123-124

We can also inline the local function.

.. GENERATED FROM PYTHON SOURCE LINES 124-135

.. code-block:: Python


    onx = to_onnx(
        model,
        (x, y),
        dynamic_shapes={"x": {0: x_rows, 1: dim}, "y": {0: y_rows, 1: dim}},
        inline=True,
        export_options=ExportOptions(decomposition_table="default"),
    )
    print(pretty_onnx(onx))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    opset: domain='local_functions' version=1
    input: name='x' type=dtype('float32') shape=['x_rows', 'dim']
    input: name='y' type=dtype('float32') shape=['y_rows', 'dim']
    Scan(y, x, body=G1, num_scan_inputs=1, scan_input_directions=[0], scan_output_axes=[0], scan_output_directions=[0]) -> scan#0, output_0
    output: name='output_0' type=dtype('float32') shape=['x_rows', 'y_rows']
    ----- subgraph ---- Scan - scan - att.body=G1 -- level=1 -- init_0_y,scan_0_x -> output_0,output_1
    input: name='init_0_y' type='NOTENSOR' shape=None
    input: name='scan_0_x' type='NOTENSOR' shape=None
    Constant(value=[1]) -> init7_s1_12
    Constant(value=[1, -1]) -> init7_s2_1_-12
      Reshape(scan_0_x, init7_s2_1_-12) -> view2
        Sub(init_0_y, view2) -> sub_42
          Mul(sub_42, sub_42) -> mul_72
      ReduceSum(mul_72, init7_s1_12, keepdims=0) -> sum_12
        Sqrt(sum_12) -> output_1
    Identity(init_0_y) -> output_0
    output: name='output_0' type='NOTENSOR' shape=None
    output: name='output_1' type='NOTENSOR' shape=None




.. GENERATED FROM PYTHON SOURCE LINES 136-137

And visually.

.. GENERATED FROM PYTHON SOURCE LINES 137-139

.. code-block:: Python


    plot_dot(onx)



.. image-sg:: /auto_recipes/images/sphx_glr_plot_exporter_recipes_c_scan_pdist_001.png
   :alt: plot exporter recipes c scan pdist
   :srcset: /auto_recipes/images/sphx_glr_plot_exporter_recipes_c_scan_pdist_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.878 seconds)


.. _sphx_glr_download_auto_recipes_plot_exporter_recipes_c_scan_pdist.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_exporter_recipes_c_scan_pdist.ipynb <plot_exporter_recipes_c_scan_pdist.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_exporter_recipes_c_scan_pdist.py <plot_exporter_recipes_c_scan_pdist.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_exporter_recipes_c_scan_pdist.zip <plot_exporter_recipes_c_scan_pdist.zip>`


.. include:: plot_exporter_recipes_c_scan_pdist.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
