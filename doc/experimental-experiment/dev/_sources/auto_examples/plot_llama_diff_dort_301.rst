
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_dort_301.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_dort_301.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_dort_301.py:


.. _l-plot-onnxrt-diff:

301: Compares LLAMA exporters for onnxrt backend
================================================

The script compares exported models in :epkg:`pytorch`
using :epkg:`onnxrt backend`. It tries to do a side by side
of the execution of both models.

To run the script:

::

    python _doc/examples/plot_llama_diff_dort --help


The following example compares the forward step for mixed precision on cuda
and produces all the intermediate onnx graphs.

::

    python _doc/examples/plot_llama_diff_dort.py --part model --ortopt 1 --cuda 1 --backward 0 --mixed 1

You may use ``--mixed=1`` to compare the backward graphs.

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 30-98

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("attention", "one value among attention, decoder, model"),
        ortopt=(1, "run onnxruntime optimization"),
        backward=(0, "does one operator for backward"),
        cuda=(0, "use cuda or not"),
        mixed=(0, "use miwed precision"),
        expose="part,exporter,ortopt,cuda,mixed",
    )


    import copy
    import os
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import onnx
    from onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator
    import torch
    from torch._dynamo.backends.common import aot_autograd
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.convert.convert_helper import (
        optimize_model_proto,
        ort_optimize,
    )
    from experimental_experiment.torch_helper.llama_helper import (
        get_llama_model,
        get_llama_attention,
        get_llama_decoder,
    )
    from experimental_experiment.torch_helper.dump_helper import (
        assert_all_close,
        dump_onnx,
        reorder_functions_in_proto,
        inputs_from_onnx_model,
        build_matching_inputs,
        results_to_string,
    )
    from experimental_experiment.torch_helper.training_helper import (
        train_loop,
        make_aot_ort,
    )
    from experimental_experiment.torch_dynamo import (
        onnx_debug_backend,
        get_decomposition_table,
    )

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 99-101

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 101-112

.. code-block:: Python


    print(f"part={script_args.part}")
    ortopt = script_args.ortopt in (1, "1")
    print(f"ortopt={ortopt}")
    backward = script_args.backward in (1, "1")
    print(f"backward={backward}")
    use_cuda = script_args.cuda in (1, "1")
    print(f"cuda={use_cuda}")
    use_mixed = script_args.mixed in (1, "1")
    print(f"mixed={use_mixed}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=attention
    ortopt=True
    backward=False
    cuda=False
    mixed=False




.. GENERATED FROM PYTHON SOURCE LINES 113-115

Model and data
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 115-169

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "attention":
        model, inputs = get_llama_attention(**kwargs)
    elif script_args.part == "decoder":
        model, inputs = get_llama_decoder(**kwargs)
    elif script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    if use_cuda:
        model = model.to("cuda")
        inputs = [[i.to("cuda") for i in inp] for inp in inputs]

    print(f"simple run with {len(inputs)} inputs")
    if backward:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = train_loop(copy.deepcopy(model), *inputs[0])
                torch.cuda.synchronize()
        else:
            expected = train_loop(copy.deepcopy(model), *inputs[0])
        print(
            f"-- eager mode worked, {len(expected)} gradients, first one is "
            f"{expected[0].shape}, {expected[0].dtype}"
        )
    else:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = model(*inputs[0])
                torch.cuda.synchronize()
        else:
            expected = model(*inputs[0])
        print(results_to_string(expected))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    torch.float32 (2, 1024, 512) [sum=-130]




.. GENERATED FROM PYTHON SOURCE LINES 170-172

Exporting
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 172-256

.. code-block:: Python


    folder = "dump_models"
    storage = {}

    if backward:
        # onnxrt backend
        local_aot_ort, _ = make_aot_ort(dynamic=False)

        optimized_mod = torch.compile(
            copy.deepcopy(model), backend=local_aot_ort, dynamic=False, fullgraph=True
        )

        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = train_loop(optimized_mod, *inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = train_loop(optimized_mod, *inputs[0])
        assert_all_close(expected[0], expected_onnxrt[0], atol=1e-3)
        print(
            f"-- onnxrt backend worked, {len(expected_onnxrt)} gradients, first one is "
            f"{expected_onnxrt[0].shape}, {expected_onnxrt[0].dtype}"
        )

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=17,
                storage=storage,
                **kwargs,
            ),
            decompositions=get_decomposition_table(),
        )
        onnx_mod = torch.compile(copy.deepcopy(model), backend=aot_compiler, fullgraph=True)

        if False and use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                got = train_loop(onnx_mod, *inputs[0])
                torch.cuda.synchronize()
        else:
            got = train_loop(onnx_mod, *inputs[0])
        assert_all_close(expected[0], got[0], atol=1e-2 if use_mixed else 1e-4)
        print(
            f"-- debug backend worked, {len(got)} gradients, first one is "
            f"{got[0].shape}, {got[0].dtype}"
        )

    else:
        # onnxrt backend
        optimized_mod = torch.compile(model, backend="onnxrt", fullgraph=True)
        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = optimized_mod(*inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = optimized_mod(*inputs[0])
        assert_all_close(expected, expected_onnxrt, atol=1e-2)

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=17,
                storage=storage,
                **kwargs,
            )
        )

        onnx_mod = torch.compile(model, backend=aot_compiler, fullgraph=True)
        if use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                got = onnx_mod(*inputs[0])
        else:
            got = onnx_mod(*inputs[0])
        assert_all_close(expected, got, atol=1 if use_mixed else 1e-3)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:136: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 257-260

For forward, there are two files, one onnx model and the graph module
printed in a txt file. For backward, there are two onnx models.
Then it is multiplied by the number of backends.

.. GENERATED FROM PYTHON SOURCE LINES 260-264

.. code-block:: Python


    models = os.listdir(folder)
    print(f"exported models: {models}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    exported models: ['llama_onnxrt_0.onnx', 'llama_debug_0.onnx', 'llama_debug_0.txt', 'llama_onnxrt_0.txt']




.. GENERATED FROM PYTHON SOURCE LINES 265-266

Inputs used by the debug backend

.. GENERATED FROM PYTHON SOURCE LINES 266-271

.. code-block:: Python


    feeds = storage["instance"][0]["inputs"][0]
    for k, v in feeds.items():
        print(f"-- {k} {v.dtype} {v.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- input0 float32 (512, 512)
    -- input1 float32 (512, 512)
    -- input2 float32 (512, 512)
    -- input3 float32 (512, 512)
    -- input4 float32 (2048, 64)
    -- input5 float32 (2048, 64)
    -- input6 float32 (2, 1024, 512)
    -- input7 int64 (1, 1024)
    -- input8 float32 (2, 1, 1024, 1024)




.. GENERATED FROM PYTHON SOURCE LINES 272-273

Let's the first line of the graph module

.. GENERATED FROM PYTHON SOURCE LINES 273-278

.. code-block:: Python


    graph_module = storage["instance"][0]["graph_module"]
    print("\n".join(str(graph_module.graph).split("\n")[:10]))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %primals_1 : [num_users=1] = placeholder[target=primals_1]
        %primals_2 : [num_users=1] = placeholder[target=primals_2]
        %primals_3 : [num_users=1] = placeholder[target=primals_3]
        %primals_4 : [num_users=1] = placeholder[target=primals_4]
        %primals_5 : [num_users=1] = placeholder[target=primals_5]
        %primals_6 : [num_users=1] = placeholder[target=primals_6]
        %primals_7 : [num_users=3] = placeholder[target=primals_7]
        %primals_8 : [num_users=2] = placeholder[target=primals_8]
        %primals_9 : [num_users=1] = placeholder[target=primals_9]




.. GENERATED FROM PYTHON SOURCE LINES 279-281

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 281-309

.. code-block:: Python


    if backward:
        print(f"-- {len(storage['instance'])} onnx models were creates")
        for i, inst in enumerate(storage["instance"]):
            print(f"  model {i}: {len(inst['inputs'])} runs")

        # deal with backward
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        assert len(onnx_models) == 4, f"unexpected value {onnx_models}"
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx") and "_1" in m]))
        assert len(onnx_models) == 2, f"unexpected value {onnx_models}"
        model_onnxrt = os.path.join(folder, onnx_models[1])
        model_debug = os.path.join(folder, onnx_models[0])
    else:
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        if len(onnx_models) == 2:
            model_onnxrt = os.path.join(folder, onnx_models[1])
            model_debug = os.path.join(folder, onnx_models[0])
        else:
            model_debug = os.path.join(folder, onnx_models[0])
            # the following error may appear:
            # Node type 'Rank' from domain 'pkg.onnxscript.torch_lib.common' is unknown
            print(f"One model is missing, onnx_models={onnx_models}")
            model_onnxrt = model_debug

    print(f"model_onnxrt={model_onnxrt}")
    print(f"model_debug={model_debug}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    model_onnxrt=dump_models/llama_onnxrt_0.onnx
    model_debug=dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 310-311

The inputs of both models

.. GENERATED FROM PYTHON SOURCE LINES 311-315

.. code-block:: Python


    print("onnxrt:", inputs_from_onnx_model(model_onnxrt))
    print("debug:", inputs_from_onnx_model(model_debug))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    onnxrt: [('INPUT', 'primals_2', 1, (512, 512)), ('INPUT', 'primals_4', 1, (512, 512)), ('INPUT', 'primals_3', 1, (512, 512)), ('INPUT', 'primals_5', 1, (2048, 64)), ('INPUT', 'primals_6', 1, (2048, 64)), ('INPUT', 'primals_1', 1, (512, 512)), ('INPUT', 'primals_7', 1, (2, 1024, 512)), ('INPUT', 'primals_8', 7, (1, 1024)), ('INPUT', 'primals_9', 1, (2, 1, 1024, 1024))]
    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024))]




.. GENERATED FROM PYTHON SOURCE LINES 316-318

Inputs are not the same. The first model has more and some inputs were
moved into the initializer list into for `model_debug`.

.. GENERATED FROM PYTHON SOURCE LINES 318-321

.. code-block:: Python


    print("debug:", inputs_from_onnx_model(model_debug, init=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024)), ('INIT', 'init1_s_', 1, ()), ('INIT', 'init7_s1_0', 7, (1,)), ('INIT', 'init7_s1_1', 7, (1,)), ('INIT', 'init7_s1_1024', 7, (1,)), ('INIT', 'init7_s1_3', 7, (1,)), ('INIT', 'init7_s1_32', 7, (1,)), ('INIT', 'init7_s1_9223372036854775807', 7, (1,)), ('INIT', 'init7_s2_2048_512', 7, (2,)), ('INIT', 'init7_s3_16_1024_1024', 7, (3,)), ('INIT', 'init7_s3_16_1024_64', 7, (3,)), ('INIT', 'init7_s3_16_64_1024', 7, (3,)), ('INIT', 'init7_s3_2_1024_512', 7, (3,)), ('INIT', 'init7_s4_2_1024_8_64', 7, (4,))]




.. GENERATED FROM PYTHON SOURCE LINES 322-330

Optimization and Verification
+++++++++++++++++++++++++++++

Let's try the model with a python backend (reference implementation).
First step, onnx-script uses many functions. The reference evaluation expects
every function to be defined so the order of functions in the model matters.
No recursivity is allowed by this runtime. We need to reorder as function Rank is usually placed
at the end of the model.

.. GENERATED FROM PYTHON SOURCE LINES 330-333

.. code-block:: Python


    reorder_functions_in_proto(model_onnxrt)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    'dump_models/llama_onnxrt_0.onnx'



.. GENERATED FROM PYTHON SOURCE LINES 334-335

Let's load the model and optimize them.

.. GENERATED FROM PYTHON SOURCE LINES 335-343

.. code-block:: Python


    debug = onnx.load(model_debug)
    try:
        onnxrt = optimize_model_proto(onnx.load(model_onnxrt))
    except ImportError as e:
        print("missing library", e)
        onnxrt = debug





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 344-345

Let's apply onnxruntime optimization

.. GENERATED FROM PYTHON SOURCE LINES 345-364

.. code-block:: Python


    if ortopt:
        providers = (
            [("CUDAExecutionProvider", {}), ("CPUExecutionProvider", {})]
            if use_cuda
            else ["CPUExecutionProvider"]
        )
        with open(model_onnxrt.replace(".onnx", ".before.opt.onnx"), "wb") as f:
            f.write(onnxrt.SerializeToString())
        print(f"run onnxruntime optimization on {model_onnxrt}")
        optimized = model_onnxrt.replace(".onnx", ".opt.onnx")
        ort_optimize(onnxrt, output=optimized, providers=providers)
        onnxrt = onnx.load(optimized)

        print(f"run onnxruntime optimization on {model_debug}")
        optimized = model_debug.replace(".onnx", ".opt.onnx")
        ort_optimize(debug, output=optimized, disable_aot=True, providers=providers)
        debug = onnx.load(optimized)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run onnxruntime optimization on dump_models/llama_onnxrt_0.onnx
    run onnxruntime optimization on dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 365-366

For what's following, we need to build two lists of matching inputs.

.. GENERATED FROM PYTHON SOURCE LINES 366-372

.. code-block:: Python


    print("build_matching_inputs")
    feedsrt = build_matching_inputs(model_debug, feeds, model_onnxrt)
    print("done")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    build_matching_inputs
    done




.. GENERATED FROM PYTHON SOURCE LINES 373-374

We check both models are running.

.. GENERATED FROM PYTHON SOURCE LINES 374-382

.. code-block:: Python


    out_onnxrt = ExtendedReferenceEvaluator(onnxrt).run(None, feedsrt)
    out_debug = ExtendedReferenceEvaluator(debug).run(None, feeds)
    assert out_onnxrt
    assert out_debug

    # assert_all_close(out_onnxrt, out_debug)








.. GENERATED FROM PYTHON SOURCE LINES 383-384

Side by side

.. GENERATED FROM PYTHON SOURCE LINES 384-395

.. code-block:: Python



    res1, res2, align, dc = compare_onnx_execution(
        onnxrt,
        debug,
        verbose=1,
        raise_exc=True,
        inputs=(feedsrt, feeds),
    )
    text = dc.to_str(res1, res2, align, column_size=90)
    print(text)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] execute with 2 inputs
    [compare_onnx_execution] execute first model
    [compare_onnx_execution] got 104 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 81 results
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 109 pairs
    [compare_onnx_execution] done
    001 ~ | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_1_token_172      | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_113     
    002 ~ | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_3_token_174      | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_2_token_119     
    003 ~ | INITIA int64    1:4                  CIKK                 ortshared_7_1_4_1_token_168      | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_0_token_114     
    004 - | INITIA int64    1:4                  CIKM                 ortshared_7_1_4_2_token_173      |                                                                                           
    005 - | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_3_token_180      |                                                                                           
    006 ~ | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_1_token_169      | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_1_token_118     
    007 ~ | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_0_token_162      | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_4_token_122     
    008 ~ | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_0_token_171      | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_3_token_121     
    009 - | INITIA float32                       IAAA                 ortshared_1_0_1_1_token_178      |                                                                                           
    010 - | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_2_token_177      |                                                                                           
    011 - | INITIA int64                         ZAAA                 ortshared_7_0_1_0_token_164      |                                                                                           
    012 - | INITIA int64                         BAAA                 ortshared_7_0_1_1_token_165      |                                                                                           
    013 ~ | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_5_token_179      | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_5_token_123     
    014 = | INITIA int64    1:2                  USAA                 ortshared_7_1_2_1_token_175      | INITIA int64    1:2                  USAA                 ortshared_7_1_2_0_token_125     
    015 ~ | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_2_token_170      | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_3_token_124     
    016 - | INITIA float32                       BAAA                 ortshared_1_0_1_0_token_166      |                                                                                           
    017 ~ | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_167      | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_0_token_115     
    018 ~ | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_4_token_176      | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_1_token_116     
    019 ~ | INITIA int64    1:2                  BKAA                 ortshared_7_1_2_0_token_163      | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_2_token_120     
    020 = | INPUT  float32  2:512x512            DALZ                 primals_2                        | INPUT  float32  2:512x512            DALZ                 input0                          
    021 = | INPUT  float32  2:512x512            CDSE                 primals_4                        | INPUT  float32  2:512x512            CDSE                 input1                          
    022 = | INPUT  float32  2:512x512            VYQD                 primals_3                        | INPUT  float32  2:512x512            VYQD                 input2                          
    023 + |                                                                                            | INPUT  float32  2:512x512            SVXS                 input3                           
    024 = | INPUT  float32  2:2048x64            MDRB                 primals_5                        | INPUT  float32  2:2048x64            MDRB                 input4                          
    025 = | INPUT  float32  2:2048x64            ZHDU                 primals_6                        | INPUT  float32  2:2048x64            ZHDU                 input5                          
    026 - | INPUT  float32  2:512x512            SVXS                 primals_1                        |                                                                                           
    027 = | INPUT  float32  3:2x1024x512         WPVA                 primals_7                        | INPUT  float32  3:2x1024x512         WPVA                 input6                          
    028 = | INPUT  int64    2:1x1024             KAQG                 primals_8                        | INPUT  int64    2:1x1024             KAQG                 input7                          
    029 = | INPUT  float32  4:2x1x1024x1024      AAAA                 primals_9                        | INPUT  float32  4:2x1x1024x1024      AAAA                 input8                          
    030 - | RESULT float32  2:512x512            CDSE Identity        t_6                              |                                                                                           
    031 - | RESULT float32  4:2x1x1024x1024      AAAA Mul             _inlfunc_aten_add|folded_2_other |                                                                                           
    032 - | RESULT int64    2:1x1024             KAQG Expand          _val_62                          |                                                                                           
    033 - | RESULT int64    3:1x1024x1           KAQG Unsqueeze       _val_64                          |                                                                                           
    034 - | RESULT int64    3:1x1024x1           KAQG Concat          _val_65                          |                                                                                           
    035 = | RESULT float32  2:1024x64            GSEC Slice           slice_2                          | RESULT float32  2:1024x64            GSEC Slice           slice_2                         
    036 - | RESULT float32  2:1024x64            GSEC Transpose       _val_59                          |                                                                                           
    037 ~ | RESULT float32  3:1x1024x64          GSEC GatherND        _val_66                          | RESULT float32  3:1x1024x64          GSEC Gather          index_1                         
    038 = | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       aten_unsqueeze_110_n2            | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       output_5                        
    039 = | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_5_out0           | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_4_out0          
    040 = | RESULT float32  2:2048x512           WPVA Reshape         view                             | RESULT float32  2:2048x512           WPVA Reshape         output_2                        
    041 ~ | RESULT float32  2:2048x512           FODE FusedMatMul     mm_1                             | RESULT float32  2:2048x512           MBIK Gemm            mm_1                            
    042 - | RESULT float32  3:2x1024x512         FODE Reshape         view_3                           |                                                                                           
    043 ~ | RESULT float32  4:2x1024x8x64        FODE Reshape         view_7                           | RESULT float32  4:2x1024x8x64        MBIK Reshape         view_7                          
    044 ~ | RESULT float32  4:2x1024x8x32        WKDM Slice           Slice_195                        | RESULT float32  4:2x1024x8x32        ZKLR Slice           slice_Tensor6                   
    045 ~ | RESULT float32  4:2x1024x8x32        EQXO Neg             aten_neg_199_n0                  | RESULT float32  4:2x1024x8x32        BQPJ Neg             neg2                            
    046 ~ | RESULT float32  4:2x1024x8x32        JEAT Slice           Slice_178                        | RESULT float32  4:2x1024x8x32        MQYU Slice           slice_Tensor5                   
    047 ~ | RESULT float32  4:2x1024x8x64        NUXH Concat          aten_cat_204_n0                  | RESULT float32  4:2x1024x8x64        NHND Concat          cat2                            
    048 ~ | RESULT float32  4:2x1024x8x64        ULYE Mul             aten_mul_208_n0                  | RESULT float32  4:2x1024x8x64        MKNQ Mul             mul4                            
    049 = | RESULT float32  2:1024x64            CJYF Slice           slice_1                          | RESULT float32  2:1024x64            CJYF Slice           slice_1                         
    050 - | RESULT float32  2:1024x64            CJYF Transpose       _val_50                          |                                                                                           
    051 ~ | RESULT float32  3:1x1024x64          CJYF GatherND        _val_57                          | RESULT float32  3:1x1024x64          CJYF Gather          index                           
    052 = | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       aten_unsqueeze_109_n2            | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       output_4                        
    053 = | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_8_out0           | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_6_out0          
    054 ~ | RESULT float32  4:2x1024x8x64        CNJF Mul             aten_mul_161_n0                  | RESULT float32  4:2x1024x8x64        CNYL Mul             mul3                            
    055 ~ | RESULT float32  4:2x1024x8x64        WYHJ Add             _inlfunc_aten_add|folded_1_n3    | RESULT float32  4:2x1024x8x64        NWLB Add             add_Tensor2                     
    056 ~ | RESULT float32  4:2x8x64x1024        SBTX Transpose       transpose_3                      | RESULT float32  4:2x8x64x1024        FGZO Transpose       transpose_3                     
    057 - | RESULT float32  3:16x64x1024         SBTX Reshape         view_10                          |                                                                                           
    058 - | RESULT float32  4:1x1x1024x64        GSEC Transpose       unsqueeze_1                      |                                                                                           
    059 ~ | RESULT float32  2:2048x512           TPVY FusedMatMul     mm                               | RESULT float32  2:2048x512           WPVA Reshape         output_1                        
    060 ~ | RESULT float32  3:2x1024x512         TPVY Reshape         view_1                           | RESULT float32  2:2048x512           FODE Gemm            mm                              
    061 ~ | RESULT float32  4:2x1024x8x64        TPVY Reshape         view_6                           | RESULT float32  4:2x1024x8x64        FODE Reshape         view_6                          
    062 ~ | RESULT float32  4:2x8x1024x64        BGDP Transpose       transpose                        | RESULT float32  4:2x8x1024x64        YWMV Transpose       transpose                       
    063 ~ | RESULT float32  4:2x8x1024x32        JUGM Slice           slice_4                          | RESULT float32  4:2x8x1024x32        VMVU Slice           slice_4                         
    064 ~ | RESULT float32  4:2x8x1024x32        RGUO Neg             neg                              | RESULT float32  4:2x8x1024x32        FOFG Neg             neg                             
    065 ~ | RESULT float32  4:2x8x1024x32        RNYD Slice           slice_3                          | RESULT float32  4:2x8x1024x32        EKSA Slice           slice_3                         
    066 ~ | RESULT float32  4:2x8x1024x64        HTSQ Concat          cat                              | RESULT float32  4:2x8x1024x64        JYYG Concat          cat                             
    067 ~ | RESULT float32  4:2x8x1024x64        AXRB Mul             mul_1                            | RESULT float32  4:2x8x1024x64        ZHYD Mul             mul_1                           
    068 - | RESULT float32  4:1x1x1024x64        CJYF Transpose       unsqueeze                        |                                                                                           
    069 ~ | RESULT float32  4:2x8x1024x64        EOKH Mul             mul                              | RESULT float32  4:2x8x1024x64        TUVU Mul             mul                             
    070 ~ | RESULT float32  4:2x8x1024x64        ELBI Add             add                              | RESULT float32  4:2x8x1024x64        SBTX Add             add                             
    071 - | RESULT float32  3:16x1024x64         ELBI Reshape         view_9                           |                                                                                           
    072 - | RESULT float32  3:16x1024x1024       NJJA MatMul          bmm                              |                                                                                           
    073 - | RESULT float32  4:2x8x1024x1024      NJJA Reshape         view_11                          |                                                                                           
    074 ~ | RESULT float32  4:2x8x1024x1024      WOPG Div             div                              | RESULT float32  4:2x8x1024x1024      YMIL FusedMatMul     div                             
    075 ~ | RESULT float32  4:2x8x1024x1024      WOPG Add             add_2                            | RESULT float32  4:2x8x1024x1024      YMIL Add             add_2                           
    076 ~ | RESULT float32  4:2x8x1024x1024      OONO Softmax         _softmax                         | RESULT float32  4:2x8x1024x1024      NOOO Softmax         output_8                        
    077 - | RESULT float32  3:16x1024x1024       OONO Reshape         view_12                          |                                                                                           
    078 ~ | RESULT float32  2:2048x512           QTHG FusedMatMul     mm_2                             | RESULT float32  2:2048x512           WPVA Reshape         output_3                        
    079 ~ | RESULT float32  3:2x1024x512         QTHG Reshape         view_5                           | RESULT float32  2:2048x512           QTHG Gemm            mm_2                            
    080 = | RESULT float32  4:2x1024x8x64        QTHG Reshape         view_8                           | RESULT float32  4:2x1024x8x64        QTHG Reshape         view_8                          
    081 = | RESULT float32  4:2x8x1024x64        GFBN Transpose       transpose_2                      | RESULT float32  4:2x8x1024x64        GFBN Transpose       transpose_2                     
    082 ~ | RESULT float32  3:16x1024x64         GFBN Reshape         view_13                          | RESULT float32  4:2x8x1024x64        VEAS MatMul          view_11                         
    083 ~ | RESULT float32  3:16x1024x64         MHEB MatMul          bmm_1                            | RESULT float32  4:2x1024x8x64        AZMG Transpose       transpose_4                     
    084 ~ | RESULT float32  4:2x8x1024x64        MHEB Reshape         view_14                          | RESULT float32  2:2048x512           AZMG Reshape         output_12                       
    085 ~ | RESULT float32  4:2x1024x8x64        EOAE Transpose       transpose_4                      | RESULT float32  2:2048x512           LJIA Gemm            mm_3                            
    086 ~ | RESULT float32  3:2x1024x512         EOAE Reshape         view_15                          | RESULT float32  3:2x1024x512         LJIA Reshape         output_0                        
    087 + |                                                                                            | RESULT float32  2:512x512            XVIA Transpose       output_11                        
    088 ~ | RESULT float32  2:2048x512           EOAE Reshape         view_16                          | RESULT float32  3:16x1024x64         GFBN Reshape         output_10                       
    089 - | RESULT float32  2:2048x512           SZBY FusedMatMul     mm_3                             |                                                                                           
    090 - | RESULT float32  3:2x1024x512         SZBY Reshape         view_17                          |                                                                                           
    091 ~ | RESULT float32  3:16x1024x1024       OONO Transpose       transpose_6                      | RESULT float32  3:16x1024x1024       NOOO Reshape         output_9                        
    092 + |                                                                                            | RESULT float32  3:16x64x1024         FGZO Reshape         output_7                         
    093 - | RESULT float32  4:2x8x1024x1024      OONO Identity        detach_3                         |                                                                                           
    094 ~ | RESULT float32  3:16x1024x64         SBTX Transpose       transpose_9                      | RESULT float32  3:16x1024x64         SBTX Reshape         output_6                        
    095 + |                                                                                            | OUTPUT float32  3:2x1024x512         LJIA                 output_0                         
    096 ~ | RESULT float32  3:16x64x1024         ELBI Transpose       transpose_8                      | OUTPUT float32  2:2048x512           WPVA                 output_1                        
    097 ~ | RESULT float32  3:16x64x1024         GFBN Transpose       transpose_7                      | OUTPUT float32  2:2048x512           WPVA                 output_2                        
    098 = | OUTPUT float32  2:2048x512           WPVA                 view                             | OUTPUT float32  2:2048x512           WPVA                 output_3                        
    099 - | OUTPUT float32  2:512x512            CDSE                 t_6                              |                                                                                           
    100 = | OUTPUT float32  4:1x1x1024x64        CJYF                 unsqueeze                        | OUTPUT float32  4:1x1x1024x64        CJYF                 output_4                        
    101 = | OUTPUT float32  4:1x1x1024x64        GSEC                 unsqueeze_1                      | OUTPUT float32  4:1x1x1024x64        GSEC                 output_5                        
    102 ~ | OUTPUT float32  3:16x64x1024         GFBN                 transpose_7                      | OUTPUT float32  3:16x1024x64         SBTX                 output_6                        
    103 ~ | OUTPUT float32  3:16x64x1024         ELBI                 transpose_8                      | OUTPUT float32  3:16x64x1024         FGZO                 output_7                        
    104 - | OUTPUT float32  3:16x1024x64         SBTX                 transpose_9                      |                                                                                           
    105 ~ | OUTPUT float32  4:2x8x1024x1024      OONO                 detach_3                         | OUTPUT float32  4:2x8x1024x1024      NOOO                 output_8                        
    106 ~ | OUTPUT float32  3:16x1024x1024       OONO                 transpose_6                      | OUTPUT float32  3:16x1024x1024       NOOO                 output_9                        
    107 ~ | OUTPUT float32  2:2048x512           EOAE                 view_16                          | OUTPUT float32  3:16x1024x64         GFBN                 output_10                       
    108 + |                                                                                            | OUTPUT float32  2:512x512            XVIA                 output_11                        
    109 ~ | OUTPUT float32  3:2x1024x512         SZBY                 view_17                          | OUTPUT float32  2:2048x512           AZMG                 output_12                       





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 14.180 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_dort_301.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_dort_301.ipynb <plot_llama_diff_dort_301.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_dort_301.py <plot_llama_diff_dort_301.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
