
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_dort_301.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_dort_301.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_dort_301.py:


.. _l-plot-onnxrt-diff:

301: Compares LLAMA exporters for onnxrt backend
================================================

The script compares exported models in :epkg:`pytorch`
using :epkg:`onnxrt backend`. It tries to do a side by side
of the execution of both models.

To run the script:

::

    python _doc/examples/plot_llama_diff_dort --help


The following example compares the forward step for mixed precision on cuda
and produces all the intermediate onnx graphs.

::

    python _doc/examples/plot_llama_diff_dort.py --part model --ortopt 1 --cuda 1 --backward 0 --mixed 1

You may use ``--mixed=1`` to compare the backward graphs.

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 30-98

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("attention", "one value among attention, decoder, model"),
        ortopt=(1, "run onnxruntime optimization"),
        backward=(0, "does one operator for backward"),
        cuda=(0, "use cuda or not"),
        mixed=(0, "use miwed precision"),
        expose="part,exporter,ortopt,cuda,mixed",
    )


    import copy
    import os
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import onnx
    from onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator
    import torch
    from torch._dynamo.backends.common import aot_autograd
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.convert.convert_helper import (
        optimize_model_proto,
        ort_optimize,
    )
    from experimental_experiment.torch_helper.llama_helper import (
        get_llama_model,
        get_llama_attention,
        get_llama_decoder,
    )
    from experimental_experiment.torch_helper.dump_helper import (
        assert_all_close,
        dump_onnx,
        reorder_functions_in_proto,
        inputs_from_onnx_model,
        build_matching_inputs,
        results_to_string,
    )
    from experimental_experiment.torch_helper.training_helper import (
        train_loop,
        make_aot_ort,
    )
    from experimental_experiment.torch_dynamo import (
        onnx_debug_backend,
        get_decomposition_table,
    )

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 99-101

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 101-112

.. code-block:: Python


    print(f"part={script_args.part}")
    ortopt = script_args.ortopt in (1, "1")
    print(f"ortopt={ortopt}")
    backward = script_args.backward in (1, "1")
    print(f"backward={backward}")
    use_cuda = script_args.cuda in (1, "1")
    print(f"cuda={use_cuda}")
    use_mixed = script_args.mixed in (1, "1")
    print(f"mixed={use_mixed}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=attention
    ortopt=True
    backward=False
    cuda=False
    mixed=False




.. GENERATED FROM PYTHON SOURCE LINES 113-115

Model and data
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 115-169

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "attention":
        model, inputs = get_llama_attention(**kwargs)
    elif script_args.part == "decoder":
        model, inputs = get_llama_decoder(**kwargs)
    elif script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    if use_cuda:
        model = model.to("cuda")
        inputs = [[i.to("cuda") for i in inp] for inp in inputs]

    print(f"simple run with {len(inputs)} inputs")
    if backward:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = train_loop(copy.deepcopy(model), *inputs[0])
                torch.cuda.synchronize()
        else:
            expected = train_loop(copy.deepcopy(model), *inputs[0])
        print(
            f"-- eager mode worked, {len(expected)} gradients, first one is "
            f"{expected[0].shape}, {expected[0].dtype}"
        )
    else:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = model(*inputs[0])
                torch.cuda.synchronize()
        else:
            expected = model(*inputs[0])
        print(results_to_string(expected))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    torch.float32 (2, 1024, 512) [sum=213]




.. GENERATED FROM PYTHON SOURCE LINES 170-172

Exporting
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 172-257

.. code-block:: Python


    folder = "dump_models"
    storage = {}

    if backward:
        # onnxrt backend
        local_aot_ort, _ = make_aot_ort(dynamic=False, rewrite=True)

        optimized_mod = torch.compile(
            copy.deepcopy(model), backend=local_aot_ort, dynamic=False, fullgraph=True
        )

        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = train_loop(optimized_mod, *inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = train_loop(optimized_mod, *inputs[0])
        assert_all_close(expected[0], expected_onnxrt[0], atol=1e-3)
        print(
            f"-- onnxrt backend worked, {len(expected_onnxrt)} gradients, first one is "
            f"{expected_onnxrt[0].shape}, {expected_onnxrt[0].dtype}"
        )

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=17,
                storage=storage,
                **kwargs,
            ),
            decompositions=get_decomposition_table(),
        )
        onnx_mod = torch.compile(copy.deepcopy(model), backend=aot_compiler, fullgraph=True)

        if False and use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                got = train_loop(onnx_mod, *inputs[0])
                torch.cuda.synchronize()
        else:
            got = train_loop(onnx_mod, *inputs[0])
        assert_all_close(expected[0], got[0], atol=1e-2 if use_mixed else 1e-4)
        print(
            f"-- debug backend worked, {len(got)} gradients, first one is "
            f"{got[0].shape}, {got[0].dtype}"
        )

    else:
        # onnxrt backend
        local_aot_ort, _ = make_aot_ort(dynamic=True, rewrite=True)
        optimized_mod = torch.compile(model, backend=local_aot_ort, fullgraph=True)
        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = optimized_mod(*inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = optimized_mod(*inputs[0])
        assert_all_close(expected, expected_onnxrt, atol=1e-2)

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=17,
                storage=storage,
                **kwargs,
            )
        )

        onnx_mod = torch.compile(model, backend=aot_compiler, fullgraph=True)
        if use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                got = onnx_mod(*inputs[0])
        else:
            got = onnx_mod(*inputs[0])
        assert_all_close(expected, got, atol=1 if use_mixed else 1e-3)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:136: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 258-261

For forward, there are two files, one onnx model and the graph module
printed in a txt file. For backward, there are two onnx models.
Then it is multiplied by the number of backends.

.. GENERATED FROM PYTHON SOURCE LINES 261-265

.. code-block:: Python


    models = os.listdir(folder)
    print(f"exported models: {models}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    exported models: ['llama_onnxrt_0.onnx', 'llama_debug_0.onnx', 'llama_debug_0.txt', 'llama_onnxrt_0.txt']




.. GENERATED FROM PYTHON SOURCE LINES 266-267

Inputs used by the debug backend

.. GENERATED FROM PYTHON SOURCE LINES 267-272

.. code-block:: Python


    feeds = storage["instance"][0]["inputs"][0]
    for k, v in feeds.items():
        print(f"-- {k} {v.dtype} {v.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- input0 float32 (512, 512)
    -- input1 float32 (512, 512)
    -- input2 float32 (512, 512)
    -- input3 float32 (512, 512)
    -- input4 float32 (2048, 64)
    -- input5 float32 (2048, 64)
    -- input6 float32 (2, 1024, 512)
    -- input7 int64 (1, 1024)
    -- input8 float32 (2, 1, 1024, 1024)




.. GENERATED FROM PYTHON SOURCE LINES 273-274

Let's the first line of the graph module

.. GENERATED FROM PYTHON SOURCE LINES 274-279

.. code-block:: Python


    graph_module = storage["instance"][0]["graph_module"]
    print("\n".join(str(graph_module.graph).split("\n")[:10]))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %primals_1 : [num_users=1] = placeholder[target=primals_1]
        %primals_2 : [num_users=1] = placeholder[target=primals_2]
        %primals_3 : [num_users=1] = placeholder[target=primals_3]
        %primals_4 : [num_users=1] = placeholder[target=primals_4]
        %primals_5 : [num_users=1] = placeholder[target=primals_5]
        %primals_6 : [num_users=1] = placeholder[target=primals_6]
        %primals_7 : [num_users=3] = placeholder[target=primals_7]
        %primals_8 : [num_users=2] = placeholder[target=primals_8]
        %primals_9 : [num_users=1] = placeholder[target=primals_9]




.. GENERATED FROM PYTHON SOURCE LINES 280-282

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 282-310

.. code-block:: Python


    if backward:
        print(f"-- {len(storage['instance'])} onnx models were creates")
        for i, inst in enumerate(storage["instance"]):
            print(f"  model {i}: {len(inst['inputs'])} runs")

        # deal with backward
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        assert len(onnx_models) == 4, f"unexpected value {onnx_models}"
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx") and "_1" in m]))
        assert len(onnx_models) == 2, f"unexpected value {onnx_models}"
        model_onnxrt = os.path.join(folder, onnx_models[1])
        model_debug = os.path.join(folder, onnx_models[0])
    else:
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        if len(onnx_models) == 2:
            model_onnxrt = os.path.join(folder, onnx_models[1])
            model_debug = os.path.join(folder, onnx_models[0])
        else:
            model_debug = os.path.join(folder, onnx_models[0])
            # the following error may appear:
            # Node type 'Rank' from domain 'pkg.onnxscript.torch_lib.common' is unknown
            print(f"One model is missing, onnx_models={onnx_models}")
            model_onnxrt = model_debug

    print(f"model_onnxrt={model_onnxrt}")
    print(f"model_debug={model_debug}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    model_onnxrt=dump_models/llama_onnxrt_0.onnx
    model_debug=dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 311-312

The inputs of both models

.. GENERATED FROM PYTHON SOURCE LINES 312-316

.. code-block:: Python


    print("onnxrt:", inputs_from_onnx_model(model_onnxrt))
    print("debug:", inputs_from_onnx_model(model_debug))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    onnxrt: [('INPUT', 'primals_5', 1, (2048, 64)), ('INPUT', 'primals_6', 1, (2048, 64)), ('INPUT', 'primals_1', 1, (512, 512)), ('INPUT', 'primals_7', 1, (2, 1024, 512)), ('INPUT', 'primals_2', 1, (512, 512)), ('INPUT', 'primals_4', 1, (512, 512)), ('INPUT', 'primals_3', 1, (512, 512)), ('INPUT', 'primals_8', 7, (1, 1024)), ('INPUT', 'primals_9', 1, (2, 1, 1024, 1024))]
    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024))]




.. GENERATED FROM PYTHON SOURCE LINES 317-319

Inputs are not the same. The first model has more and some inputs were
moved into the initializer list into for `model_debug`.

.. GENERATED FROM PYTHON SOURCE LINES 319-322

.. code-block:: Python


    print("debug:", inputs_from_onnx_model(model_debug, init=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024)), ('INIT', 'init1_s_', 1, ()), ('INIT', 'init7_s1_0', 7, (1,)), ('INIT', 'init7_s1_1', 7, (1,)), ('INIT', 'init7_s1_1024', 7, (1,)), ('INIT', 'init7_s1_3', 7, (1,)), ('INIT', 'init7_s1_32', 7, (1,)), ('INIT', 'init7_s1_9223372036854775807', 7, (1,)), ('INIT', 'init7_s2_2048_512', 7, (2,)), ('INIT', 'init7_s3_16_1024_1024', 7, (3,)), ('INIT', 'init7_s3_16_1024_64', 7, (3,)), ('INIT', 'init7_s3_16_64_1024', 7, (3,)), ('INIT', 'init7_s3_2_1024_512', 7, (3,)), ('INIT', 'init7_s4_2_1024_8_64', 7, (4,))]




.. GENERATED FROM PYTHON SOURCE LINES 323-331

Optimization and Verification
+++++++++++++++++++++++++++++

Let's try the model with a python backend (reference implementation).
First step, onnx-script uses many functions. The reference evaluation expects
every function to be defined so the order of functions in the model matters.
No recursivity is allowed by this runtime. We need to reorder as function Rank is usually placed
at the end of the model.

.. GENERATED FROM PYTHON SOURCE LINES 331-334

.. code-block:: Python


    reorder_functions_in_proto(model_onnxrt)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    'dump_models/llama_onnxrt_0.onnx'



.. GENERATED FROM PYTHON SOURCE LINES 335-336

Let's load the model and optimize them.

.. GENERATED FROM PYTHON SOURCE LINES 336-344

.. code-block:: Python


    debug = onnx.load(model_debug)
    try:
        onnxrt = optimize_model_proto(onnx.load(model_onnxrt))
    except ImportError as e:
        print("missing library", e)
        onnxrt = debug





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Applied 0 pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 345-346

Let's apply onnxruntime optimization

.. GENERATED FROM PYTHON SOURCE LINES 346-365

.. code-block:: Python


    if ortopt:
        providers = (
            [("CUDAExecutionProvider", {}), ("CPUExecutionProvider", {})]
            if use_cuda
            else ["CPUExecutionProvider"]
        )
        with open(model_onnxrt.replace(".onnx", ".before.opt.onnx"), "wb") as f:
            f.write(onnxrt.SerializeToString())
        print(f"run onnxruntime optimization on {model_onnxrt}")
        optimized = model_onnxrt.replace(".onnx", ".opt.onnx")
        ort_optimize(onnxrt, output=optimized, providers=providers)
        onnxrt = onnx.load(optimized)

        print(f"run onnxruntime optimization on {model_debug}")
        optimized = model_debug.replace(".onnx", ".opt.onnx")
        ort_optimize(debug, output=optimized, disable_aot=True, providers=providers)
        debug = onnx.load(optimized)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run onnxruntime optimization on dump_models/llama_onnxrt_0.onnx
    run onnxruntime optimization on dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 366-367

For what's following, we need to build two lists of matching inputs.

.. GENERATED FROM PYTHON SOURCE LINES 367-373

.. code-block:: Python


    print("build_matching_inputs")
    feedsrt = build_matching_inputs(model_debug, feeds, model_onnxrt)
    print("done")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    build_matching_inputs
    done




.. GENERATED FROM PYTHON SOURCE LINES 374-375

We check both models are running.

.. GENERATED FROM PYTHON SOURCE LINES 375-383

.. code-block:: Python


    out_onnxrt = ExtendedReferenceEvaluator(onnxrt).run(None, feedsrt)
    out_debug = ExtendedReferenceEvaluator(debug).run(None, feeds)
    assert out_onnxrt
    assert out_debug

    # assert_all_close(out_onnxrt, out_debug)








.. GENERATED FROM PYTHON SOURCE LINES 384-385

Side by side

.. GENERATED FROM PYTHON SOURCE LINES 385-396

.. code-block:: Python



    res1, res2, align, dc = compare_onnx_execution(
        onnxrt,
        debug,
        verbose=1,
        raise_exc=True,
        inputs=(feedsrt, feeds),
    )
    text = dc.to_str(res1, res2, align, column_size=90)
    print(text)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] execute with 2 inputs
    [compare_onnx_execution] execute first model
    [compare_onnx_execution] got 104 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 81 results
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 111 pairs
    [compare_onnx_execution] done
    001 ~ | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_5_token_179      | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_113     
    002 ~ | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_2_token_165      | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_2_token_119     
    003 ~ | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_1_token_164      | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_0_token_114     
    004 ~ | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_4_token_173      | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_1_token_118     
    005 - | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_3_token_177      |                                                                                           
    006 - | INITIA float32                       BAAA                 ortshared_1_0_1_1_token_180      |                                                                                           
    007 ~ | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_3_token_170      | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_4_token_122     
    008 ~ | INITIA int64    1:4                  CIKK                 ortshared_7_1_4_2_token_174      | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_3_token_121     
    009 - | INITIA float32                       IAAA                 ortshared_1_0_1_0_token_172      |                                                                                           
    010 ~ | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_0_token_166      | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_5_token_123     
    011 - | INITIA int64                         ZAAA                 ortshared_7_0_1_1_token_168      |                                                                                           
    012 = | INITIA int64    1:2                  USAA                 ortshared_7_1_2_1_token_178      | INITIA int64    1:2                  USAA                 ortshared_7_1_2_0_token_125     
    013 - | INITIA int64                         BAAA                 ortshared_7_0_1_0_token_162      |                                                                                           
    014 ~ | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_169      | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_3_token_124     
    015 ~ | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_1_token_167      | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_0_token_115     
    016 ~ | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_0_token_163      | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_1_token_116     
    017 = | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_2_token_176      | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_2_token_120     
    018 - | INITIA int64    1:4                  CIKM                 ortshared_7_1_4_1_token_171      |                                                                                           
    019 - | INITIA int64    1:2                  BKAA                 ortshared_7_1_2_0_token_175      |                                                                                           
    020 - | INPUT  float32  2:2048x64            MDRB                 primals_5                        |                                                                                           
    021 - | INPUT  float32  2:2048x64            ZHDU                 primals_6                        |                                                                                           
    022 = | INPUT  float32  2:512x512            VZBT                 primals_1                        | INPUT  float32  2:512x512            VZBT                 input0                          
    023 - | INPUT  float32  3:2x1024x512         JNNC                 primals_7                        |                                                                                           
    024 = | INPUT  float32  2:512x512            AAFW                 primals_2                        | INPUT  float32  2:512x512            AAFW                 input1                          
    025 = | INPUT  float32  2:512x512            MWON                 primals_4                        | INPUT  float32  2:512x512            MWON                 input2                          
    026 = | INPUT  float32  2:512x512            TAWX                 primals_3                        | INPUT  float32  2:512x512            TAWX                 input3                          
    027 + |                                                                                            | INPUT  float32  2:2048x64            MDRB                 input4                           
    028 + |                                                                                            | INPUT  float32  2:2048x64            ZHDU                 input5                           
    029 + |                                                                                            | INPUT  float32  3:2x1024x512         JNNC                 input6                           
    030 = | INPUT  int64    2:1x1024             KAQG                 primals_8                        | INPUT  int64    2:1x1024             KAQG                 input7                          
    031 = | INPUT  float32  4:2x1x1024x1024      AAAA                 primals_9                        | INPUT  float32  4:2x1x1024x1024      AAAA                 input8                          
    032 - | RESULT float32  2:512x512            MWON Identity        t_6                              |                                                                                           
    033 - | RESULT float32  4:2x1x1024x1024      AAAA Mul             _inlfunc_aten_add|folded_2_other |                                                                                           
    034 - | RESULT int64    2:1x1024             KAQG Expand          _val_61                          |                                                                                           
    035 - | RESULT int64    3:1x1024x1           KAQG Unsqueeze       _val_63                          |                                                                                           
    036 - | RESULT int64    3:1x1024x1           KAQG Concat          _val_64                          |                                                                                           
    037 = | RESULT float32  2:1024x64            GSEC Slice           slice_2                          | RESULT float32  2:1024x64            GSEC Slice           slice_2                         
    038 - | RESULT float32  2:1024x64            GSEC Transpose       _val_58                          |                                                                                           
    039 ~ | RESULT float32  3:1x1024x64          GSEC GatherND        _val_65                          | RESULT float32  3:1x1024x64          GSEC Gather          index_1                         
    040 = | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       aten_unsqueeze_110_n2            | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       output_5                        
    041 = | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_5_out0           | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_4_out0          
    042 = | RESULT float32  2:2048x512           JNNC Reshape         view                             | RESULT float32  2:2048x512           JNNC Reshape         output_2                        
    043 ~ | RESULT float32  2:2048x512           YZQI FusedMatMul     mm_1                             | RESULT float32  2:2048x512           YZQI Gemm            mm_1                            
    044 - | RESULT float32  3:2x1024x512         YZQI Reshape         view_3                           |                                                                                           
    045 = | RESULT float32  4:2x1024x8x64        YZQI Reshape         view_7                           | RESULT float32  4:2x1024x8x64        YZQI Reshape         view_7                          
    046 = | RESULT float32  4:2x1024x8x32        XVOF Slice           Slice_195                        | RESULT float32  4:2x1024x8x32        XVOF Slice           slice_Tensor6                   
    047 = | RESULT float32  4:2x1024x8x32        DFMV Neg             aten_neg_199_n0                  | RESULT float32  4:2x1024x8x32        DFMV Neg             neg2                            
    048 = | RESULT float32  4:2x1024x8x32        BDCD Slice           Slice_178                        | RESULT float32  4:2x1024x8x32        BDCD Slice           slice_Tensor5                   
    049 = | RESULT float32  4:2x1024x8x64        FIOY Concat          aten_cat_204_n0                  | RESULT float32  4:2x1024x8x64        FIOY Concat          cat2                            
    050 = | RESULT float32  4:2x1024x8x64        ZYNV Mul             aten_mul_208_n0                  | RESULT float32  4:2x1024x8x64        ZYNV Mul             mul4                            
    051 = | RESULT float32  2:1024x64            CJYF Slice           slice_1                          | RESULT float32  2:1024x64            CJYF Slice           slice_1                         
    052 - | RESULT float32  2:1024x64            CJYF Transpose       _val_49                          |                                                                                           
    053 ~ | RESULT float32  3:1x1024x64          CJYF GatherND        _val_56                          | RESULT float32  3:1x1024x64          CJYF Gather          index                           
    054 = | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       aten_unsqueeze_109_n2            | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       output_4                        
    055 = | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_8_out0           | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_6_out0          
    056 = | RESULT float32  4:2x1024x8x64        SFVR Mul             aten_mul_161_n0                  | RESULT float32  4:2x1024x8x64        SFVR Mul             mul3                            
    057 = | RESULT float32  4:2x1024x8x64        QDIM Add             _inlfunc_aten_add|folded_1_n3    | RESULT float32  4:2x1024x8x64        QDIM Add             add_Tensor2                     
    058 = | RESULT float32  4:2x8x64x1024        QEVZ Transpose       transpose_3                      | RESULT float32  4:2x8x64x1024        QEVZ Transpose       transpose_3                     
    059 ~ | RESULT float32  3:16x64x1024         QEVZ Reshape         view_10                          | RESULT float32  2:2048x512           JNNC Reshape         output_1                        
    060 - | RESULT float32  4:1x1x1024x64        GSEC Transpose       unsqueeze_1                      |                                                                                           
    061 ~ | RESULT float32  2:2048x512           UDZO FusedMatMul     mm                               | RESULT float32  2:2048x512           UDZO Gemm            mm                              
    062 - | RESULT float32  3:2x1024x512         UDZO Reshape         view_1                           |                                                                                           
    063 = | RESULT float32  4:2x1024x8x64        UDZO Reshape         view_6                           | RESULT float32  4:2x1024x8x64        UDZO Reshape         view_6                          
    064 = | RESULT float32  4:2x8x1024x64        ZZCL Transpose       transpose                        | RESULT float32  4:2x8x1024x64        ZZCL Transpose       transpose                       
    065 = | RESULT float32  4:2x8x1024x32        IGFY Slice           slice_4                          | RESULT float32  4:2x8x1024x32        IGFY Slice           slice_4                         
    066 = | RESULT float32  4:2x8x1024x32        SUVC Neg             neg                              | RESULT float32  4:2x8x1024x32        SUVC Neg             neg                             
    067 = | RESULT float32  4:2x8x1024x32        RTYM Slice           slice_3                          | RESULT float32  4:2x8x1024x32        RTYM Slice           slice_3                         
    068 = | RESULT float32  4:2x8x1024x64        JNTP Concat          cat                              | RESULT float32  4:2x8x1024x64        JNTP Concat          cat                             
    069 = | RESULT float32  4:2x8x1024x64        SJWX Mul             mul_1                            | RESULT float32  4:2x8x1024x64        SJWX Mul             mul_1                           
    070 - | RESULT float32  4:1x1x1024x64        CJYF Transpose       unsqueeze                        |                                                                                           
    071 = | RESULT float32  4:2x8x1024x64        HKEJ Mul             mul                              | RESULT float32  4:2x8x1024x64        HKEJ Mul             mul                             
    072 = | RESULT float32  4:2x8x1024x64        ZTAH Add             add                              | RESULT float32  4:2x8x1024x64        ZTAH Add             add                             
    073 - | RESULT float32  3:16x1024x64         ZTAH Reshape         view_9                           |                                                                                           
    074 - | RESULT float32  3:16x1024x1024       RZIS MatMul          bmm                              |                                                                                           
    075 - | RESULT float32  4:2x8x1024x1024      RZIS Reshape         view_11                          |                                                                                           
    076 ~ | RESULT float32  4:2x8x1024x1024      VTBM Div             div                              | RESULT float32  4:2x8x1024x1024      VTBM FusedMatMul     div                             
    077 = | RESULT float32  4:2x8x1024x1024      VTBM Add             add_2                            | RESULT float32  4:2x8x1024x1024      VTBM Add             add_2                           
    078 = | RESULT float32  4:2x8x1024x1024      NNOO Softmax         _softmax                         | RESULT float32  4:2x8x1024x1024      NNOO Softmax         output_8                        
    079 - | RESULT float32  3:16x1024x1024       NNOO Reshape         view_12                          |                                                                                           
    080 ~ | RESULT float32  2:2048x512           LODB FusedMatMul     mm_2                             | RESULT float32  2:2048x512           JNNC Reshape         output_3                        
    081 ~ | RESULT float32  3:2x1024x512         LODB Reshape         view_5                           | RESULT float32  2:2048x512           LAEM Gemm            mm_2                            
    082 ~ | RESULT float32  4:2x1024x8x64        LODB Reshape         view_8                           | RESULT float32  4:2x1024x8x64        LAEM Reshape         view_8                          
    083 ~ | RESULT float32  4:2x8x1024x64        JQXJ Transpose       transpose_2                      | RESULT float32  4:2x8x1024x64        FFCP Transpose       transpose_2                     
    084 ~ | RESULT float32  3:16x1024x64         JQXJ Reshape         view_13                          | RESULT float32  4:2x8x1024x64        FLHP MatMul          view_11                         
    085 ~ | RESULT float32  3:16x1024x64         LJUL MatMul          bmm_1                            | RESULT float32  4:2x1024x8x64        IJUB Transpose       transpose_4                     
    086 ~ | RESULT float32  4:2x8x1024x64        LJUL Reshape         view_14                          | RESULT float32  2:2048x512           IJUB Reshape         output_12                       
    087 ~ | RESULT float32  4:2x1024x8x64        QEAF Transpose       transpose_4                      | RESULT float32  2:2048x512           CZEY Gemm            mm_3                            
    088 ~ | RESULT float32  3:2x1024x512         QEAF Reshape         view_15                          | RESULT float32  3:2x1024x512         CZEY Reshape         output_0                        
    089 + |                                                                                            | RESULT float32  2:512x512            AWUV Transpose       output_11                        
    090 ~ | RESULT float32  2:2048x512           QEAF Reshape         view_16                          | RESULT float32  3:16x1024x64         FFCP Reshape         output_10                       
    091 - | RESULT float32  2:2048x512           ADNN FusedMatMul     mm_3                             |                                                                                           
    092 - | RESULT float32  3:2x1024x512         ADNN Reshape         view_17                          |                                                                                           
    093 ~ | RESULT float32  3:16x1024x1024       NNOO Transpose       transpose_6                      | RESULT float32  3:16x1024x1024       NNOO Reshape         output_9                        
    094 + |                                                                                            | RESULT float32  3:16x64x1024         QEVZ Reshape         output_7                         
    095 - | RESULT float32  4:2x8x1024x1024      NNOO Identity        detach_3                         |                                                                                           
    096 ~ | RESULT float32  3:16x1024x64         QEVZ Transpose       transpose_9                      | RESULT float32  3:16x1024x64         ZTAH Reshape         output_6                        
    097 + |                                                                                            | OUTPUT float32  3:2x1024x512         CZEY                 output_0                         
    098 ~ | RESULT float32  3:16x64x1024         ZTAH Transpose       transpose_8                      | OUTPUT float32  2:2048x512           JNNC                 output_1                        
    099 ~ | RESULT float32  3:16x64x1024         JQXJ Transpose       transpose_7                      | OUTPUT float32  2:2048x512           JNNC                 output_2                        
    100 = | OUTPUT float32  2:2048x512           JNNC                 view                             | OUTPUT float32  2:2048x512           JNNC                 output_3                        
    101 - | OUTPUT float32  2:512x512            MWON                 t_6                              |                                                                                           
    102 = | OUTPUT float32  4:1x1x1024x64        CJYF                 unsqueeze                        | OUTPUT float32  4:1x1x1024x64        CJYF                 output_4                        
    103 = | OUTPUT float32  4:1x1x1024x64        GSEC                 unsqueeze_1                      | OUTPUT float32  4:1x1x1024x64        GSEC                 output_5                        
    104 ~ | OUTPUT float32  3:16x64x1024         JQXJ                 transpose_7                      | OUTPUT float32  3:16x1024x64         ZTAH                 output_6                        
    105 ~ | OUTPUT float32  3:16x64x1024         ZTAH                 transpose_8                      | OUTPUT float32  3:16x64x1024         QEVZ                 output_7                        
    106 - | OUTPUT float32  3:16x1024x64         QEVZ                 transpose_9                      |                                                                                           
    107 = | OUTPUT float32  4:2x8x1024x1024      NNOO                 detach_3                         | OUTPUT float32  4:2x8x1024x1024      NNOO                 output_8                        
    108 = | OUTPUT float32  3:16x1024x1024       NNOO                 transpose_6                      | OUTPUT float32  3:16x1024x1024       NNOO                 output_9                        
    109 ~ | OUTPUT float32  2:2048x512           QEAF                 view_16                          | OUTPUT float32  3:16x1024x64         FFCP                 output_10                       
    110 + |                                                                                            | OUTPUT float32  2:512x512            AWUV                 output_11                        
    111 ~ | OUTPUT float32  3:2x1024x512         ADNN                 view_17                          | OUTPUT float32  2:2048x512           IJUB                 output_12                       





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 14.777 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_dort_301.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_dort_301.ipynb <plot_llama_diff_dort_301.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_dort_301.py <plot_llama_diff_dort_301.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
