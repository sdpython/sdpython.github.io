
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_dort_301.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_dort_301.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_dort_301.py:


.. _l-plot-onnxrt-diff:

301: Compares LLAMA exporters for onnxrt backend
================================================

The script compares exported models in :epkg:`pytorch`
using :epkg:`onnxrt backend`. It tries to do a side by side
of the execution of both models.

To run the script:

::

    python _doc/examples/plot_llama_diff_dort --help


The following example compares the forward step for mixed precision on cuda
and produces all the intermediate onnx graphs.

::

    python _doc/examples/plot_llama_diff_dort.py --part model --ortopt 1 --cuda 1 --backward 0 --mixed 1

You may use ``--mixed=1`` to compare the backward graphs.

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 30-99

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("attention", "one value among attention, decoder, model"),
        ortopt=(1, "run onnxruntime optimization"),
        backward=(0, "does one operator for backward"),
        cuda=(0, "use cuda or not"),
        mixed=(0, "use miwed precision"),
        opset=(18, "onnx opset"),
        expose="part,exporter,ortopt,cuda,mixed,opset",
    )


    import copy
    import os
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import onnx
    from onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator
    import torch
    from torch._dynamo.backends.common import aot_autograd
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.convert.convert_helper import (
        optimize_model_proto,
        ort_optimize,
    )
    from experimental_experiment.torch_helper.llama_helper import (
        get_llama_model,
        get_llama_attention,
        get_llama_decoder,
    )
    from experimental_experiment.torch_helper.dump_helper import (
        assert_all_close,
        dump_onnx,
        reorder_functions_in_proto,
        inputs_from_onnx_model,
        build_matching_inputs,
        results_to_string,
    )
    from experimental_experiment.torch_helper.training_helper import (
        train_loop,
        make_aot_ort,
    )
    from experimental_experiment.torch_dynamo import (
        onnx_debug_backend,
        get_decomposition_table,
    )

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 100-102

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 102-115

.. code-block:: Python


    print(f"part={script_args.part}")
    ortopt = script_args.ortopt in (1, "1")
    print(f"ortopt={ortopt}")
    backward = script_args.backward in (1, "1")
    print(f"backward={backward}")
    use_cuda = script_args.cuda in (1, "1")
    print(f"cuda={use_cuda}")
    use_mixed = script_args.mixed in (1, "1")
    print(f"mixed={use_mixed}")
    opset = int(script_args.opset)
    print(f"opset={opset}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=attention
    ortopt=True
    backward=False
    cuda=False
    mixed=False
    opset=18




.. GENERATED FROM PYTHON SOURCE LINES 116-118

Model and data
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 118-172

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "attention":
        model, inputs = get_llama_attention(**kwargs)
    elif script_args.part == "decoder":
        model, inputs = get_llama_decoder(**kwargs)
    elif script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    if use_cuda:
        model = model.to("cuda")
        inputs = [[i.to("cuda") for i in inp] for inp in inputs]

    print(f"simple run with {len(inputs)} inputs")
    if backward:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = train_loop(copy.deepcopy(model), *inputs[0])
                torch.cuda.synchronize()
        else:
            expected = train_loop(copy.deepcopy(model), *inputs[0])
        print(
            f"-- eager mode worked, {len(expected)} gradients, first one is "
            f"{expected[0].shape}, {expected[0].dtype}"
        )
    else:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = model(*inputs[0])
                torch.cuda.synchronize()
        else:
            expected = model(*inputs[0])
        print(results_to_string(expected))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    torch.float32 (2, 1024, 512) [sum=-355]




.. GENERATED FROM PYTHON SOURCE LINES 173-175

Exporting
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 175-260

.. code-block:: Python


    folder = "dump_models"
    storage = {}

    if backward:
        # onnxrt backend
        local_aot_ort, _ = make_aot_ort(dynamic=False, rewrite=True)

        optimized_mod = torch.compile(
            copy.deepcopy(model), backend=local_aot_ort, dynamic=False, fullgraph=True
        )

        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = train_loop(optimized_mod, *inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = train_loop(optimized_mod, *inputs[0])
        assert_all_close(expected[0], expected_onnxrt[0], atol=1e-3)
        print(
            f"-- onnxrt backend worked, {len(expected_onnxrt)} gradients, first one is "
            f"{expected_onnxrt[0].shape}, {expected_onnxrt[0].dtype}"
        )

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=opset,
                storage=storage,
                **kwargs,
            ),
            decompositions=get_decomposition_table(),
        )
        onnx_mod = torch.compile(copy.deepcopy(model), backend=aot_compiler, fullgraph=True)

        if False and use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                got = train_loop(onnx_mod, *inputs[0])
                torch.cuda.synchronize()
        else:
            got = train_loop(onnx_mod, *inputs[0])
        assert_all_close(expected[0], got[0], atol=1e-2 if use_mixed else 1e-4)
        print(
            f"-- debug backend worked, {len(got)} gradients, first one is "
            f"{got[0].shape}, {got[0].dtype}"
        )

    else:
        # onnxrt backend
        local_aot_ort, _ = make_aot_ort(dynamic=True, rewrite=True)
        optimized_mod = torch.compile(model, backend=local_aot_ort, fullgraph=True)
        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = optimized_mod(*inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = optimized_mod(*inputs[0])
        assert_all_close(expected, expected_onnxrt, atol=1e-2)

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=17,
                storage=storage,
                **kwargs,
            )
        )

        onnx_mod = torch.compile(model, backend=aot_compiler, fullgraph=True)
        if use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                got = onnx_mod(*inputs[0])
        else:
            got = onnx_mod(*inputs[0])
        assert_all_close(expected, got, atol=1 if use_mixed else 1e-3)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 261-264

For forward, there are two files, one onnx model and the graph module
printed in a txt file. For backward, there are two onnx models.
Then it is multiplied by the number of backends.

.. GENERATED FROM PYTHON SOURCE LINES 264-268

.. code-block:: Python


    models = os.listdir(folder)
    print(f"exported models: {models}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    exported models: ['llama_onnxrt_0.onnx', 'llama_debug_0.onnx', 'llama_debug_0.txt', 'llama_onnxrt_0.txt']




.. GENERATED FROM PYTHON SOURCE LINES 269-270

Inputs used by the debug backend

.. GENERATED FROM PYTHON SOURCE LINES 270-275

.. code-block:: Python


    feeds = storage["instance"][0]["inputs"][0]
    for k, v in feeds.items():
        print(f"-- {k} {v.dtype} {v.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- input0 float32 (512, 512)
    -- input1 float32 (512, 512)
    -- input2 float32 (512, 512)
    -- input3 float32 (512, 512)
    -- input4 float32 (2048, 64)
    -- input5 float32 (2048, 64)
    -- input6 float32 (2, 1024, 512)
    -- input7 int64 (1, 1024)
    -- input8 float32 (2, 1, 1024, 1024)




.. GENERATED FROM PYTHON SOURCE LINES 276-277

Let's the first line of the graph module

.. GENERATED FROM PYTHON SOURCE LINES 277-282

.. code-block:: Python


    graph_module = storage["instance"][0]["graph_module"]
    print("\n".join(str(graph_module.graph).split("\n")[:10]))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %primals_1 : [num_users=1] = placeholder[target=primals_1]
        %primals_2 : [num_users=1] = placeholder[target=primals_2]
        %primals_3 : [num_users=1] = placeholder[target=primals_3]
        %primals_4 : [num_users=1] = placeholder[target=primals_4]
        %primals_5 : [num_users=1] = placeholder[target=primals_5]
        %primals_6 : [num_users=1] = placeholder[target=primals_6]
        %primals_7 : [num_users=3] = placeholder[target=primals_7]
        %primals_8 : [num_users=2] = placeholder[target=primals_8]
        %primals_9 : [num_users=1] = placeholder[target=primals_9]




.. GENERATED FROM PYTHON SOURCE LINES 283-285

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 285-313

.. code-block:: Python


    if backward:
        print(f"-- {len(storage['instance'])} onnx models were creates")
        for i, inst in enumerate(storage["instance"]):
            print(f"  model {i}: {len(inst['inputs'])} runs")

        # deal with backward
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        assert len(onnx_models) == 4, f"unexpected value {onnx_models}"
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx") and "_1" in m]))
        assert len(onnx_models) == 2, f"unexpected value {onnx_models}"
        model_onnxrt = os.path.join(folder, onnx_models[1])
        model_debug = os.path.join(folder, onnx_models[0])
    else:
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        if len(onnx_models) == 2:
            model_onnxrt = os.path.join(folder, onnx_models[1])
            model_debug = os.path.join(folder, onnx_models[0])
        else:
            model_debug = os.path.join(folder, onnx_models[0])
            # the following error may appear:
            # Node type 'Rank' from domain 'pkg.onnxscript.torch_lib.common' is unknown
            print(f"One model is missing, onnx_models={onnx_models}")
            model_onnxrt = model_debug

    print(f"model_onnxrt={model_onnxrt}")
    print(f"model_debug={model_debug}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    model_onnxrt=dump_models/llama_onnxrt_0.onnx
    model_debug=dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 314-315

The inputs of both models

.. GENERATED FROM PYTHON SOURCE LINES 315-319

.. code-block:: Python


    print("onnxrt:", inputs_from_onnx_model(model_onnxrt))
    print("debug:", inputs_from_onnx_model(model_debug))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    onnxrt: [('INPUT', 'primals_2', 1, (512, 512)), ('INPUT', 'primals_4', 1, (512, 512)), ('INPUT', 'primals_3', 1, (512, 512)), ('INPUT', 'primals_5', 1, (2048, 64)), ('INPUT', 'primals_6', 1, (2048, 64)), ('INPUT', 'primals_1', 1, (512, 512)), ('INPUT', 'primals_7', 1, (2, 1024, 512)), ('INPUT', 'primals_8', 7, (1, 1024)), ('INPUT', 'primals_9', 1, (2, 1, 1024, 1024))]
    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024))]




.. GENERATED FROM PYTHON SOURCE LINES 320-322

Inputs are not the same. The first model has more and some inputs were
moved into the initializer list into for `model_debug`.

.. GENERATED FROM PYTHON SOURCE LINES 322-325

.. code-block:: Python


    print("debug:", inputs_from_onnx_model(model_debug, init=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024)), ('INIT', 'init7_s2_2048_512', 7, (2,)), ('INIT', 'init7_s3_2_1024_512', 7, (3,)), ('INIT', 'init7_s4_2_1024_8_64', 7, (4,)), ('INIT', 'init7_s1_0', 7, (1,)), ('INIT', 'init7_s1_1024', 7, (1,)), ('INIT', 'init7_s1_1', 7, (1,)), ('INIT', 'init7_s1_32', 7, (1,)), ('INIT', 'init7_s1_3', 7, (1,)), ('INIT', 'init7_s1_9223372036854775807', 7, (1,)), ('INIT', 'init7_s3_16_1024_64', 7, (3,)), ('INIT', 'init7_s3_16_64_1024', 7, (3,)), ('INIT', 'init1_s_', 1, ()), ('INIT', 'init7_s3_16_1024_1024', 7, (3,))]




.. GENERATED FROM PYTHON SOURCE LINES 326-334

Optimization and Verification
+++++++++++++++++++++++++++++

Let's try the model with a python backend (reference implementation).
First step, onnx-script uses many functions. The reference evaluation expects
every function to be defined so the order of functions in the model matters.
No recursivity is allowed by this runtime. We need to reorder as function Rank is usually placed
at the end of the model.

.. GENERATED FROM PYTHON SOURCE LINES 334-337

.. code-block:: Python


    reorder_functions_in_proto(model_onnxrt)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    'dump_models/llama_onnxrt_0.onnx'



.. GENERATED FROM PYTHON SOURCE LINES 338-339

Let's load the model and optimize them.

.. GENERATED FROM PYTHON SOURCE LINES 339-347

.. code-block:: Python


    debug = onnx.load(model_debug)
    try:
        onnxrt = optimize_model_proto(onnx.load(model_onnxrt))
    except ImportError as e:
        print("missing library", e)
        onnxrt = debug





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 348-349

Let's apply onnxruntime optimization

.. GENERATED FROM PYTHON SOURCE LINES 349-368

.. code-block:: Python


    if ortopt:
        providers = (
            [("CUDAExecutionProvider", {}), ("CPUExecutionProvider", {})]
            if use_cuda
            else ["CPUExecutionProvider"]
        )
        with open(model_onnxrt.replace(".onnx", ".before.opt.onnx"), "wb") as f:
            f.write(onnxrt.SerializeToString())
        print(f"run onnxruntime optimization on {model_onnxrt}")
        optimized = model_onnxrt.replace(".onnx", ".opt.onnx")
        ort_optimize(onnxrt, output=optimized, providers=providers)
        onnxrt = onnx.load(optimized)

        print(f"run onnxruntime optimization on {model_debug}")
        optimized = model_debug.replace(".onnx", ".opt.onnx")
        ort_optimize(debug, output=optimized, disable_aot=True, providers=providers)
        debug = onnx.load(optimized)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run onnxruntime optimization on dump_models/llama_onnxrt_0.onnx
    run onnxruntime optimization on dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 369-370

For what's following, we need to build two lists of matching inputs.

.. GENERATED FROM PYTHON SOURCE LINES 370-376

.. code-block:: Python


    print("build_matching_inputs")
    feedsrt = build_matching_inputs(model_debug, feeds, model_onnxrt)
    print("done")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    build_matching_inputs
    done




.. GENERATED FROM PYTHON SOURCE LINES 377-378

We check both models are running.

.. GENERATED FROM PYTHON SOURCE LINES 378-386

.. code-block:: Python


    out_onnxrt = ExtendedReferenceEvaluator(onnxrt).run(None, feedsrt)
    out_debug = ExtendedReferenceEvaluator(debug).run(None, feeds)
    assert out_onnxrt
    assert out_debug

    # assert_all_close(out_onnxrt, out_debug)








.. GENERATED FROM PYTHON SOURCE LINES 387-388

Side by side

.. GENERATED FROM PYTHON SOURCE LINES 388-399

.. code-block:: Python



    res1, res2, align, dc = compare_onnx_execution(
        onnxrt,
        debug,
        verbose=1,
        raise_exc=True,
        inputs=(feedsrt, feeds),
    )
    text = dc.to_str(res1, res2, align, column_size=90)
    print(text)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] execute with 2 inputs
    [compare_onnx_execution] execute first model
    [compare_onnx_execution] got 104 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 81 results
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 109 pairs
    [compare_onnx_execution] done
    001 ~ | INITIA float32                       BAAA                 ortshared_1_0_1_0_token_164      | INITIA int64    1:2                  USAA                 ortshared_7_1_2_0_token_121     
    002 - | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_2_token_172      |                                                                                           
    003 = | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_0_token_168      | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_3_token_125     
    004 ~ | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_3_token_180      | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_115     
    005 ~ | INITIA int64    1:2                  USAA                 ortshared_7_1_2_0_token_170      | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_0_token_113     
    006 = | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_2_token_169      | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_4_token_119     
    007 ~ | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_5_token_179      | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_3_token_118     
    008 - | INITIA int64    1:4                  CIKM                 ortshared_7_1_4_2_token_178      |                                                                                           
    009 ~ | INITIA int64    1:4                  CIKK                 ortshared_7_1_4_0_token_162      | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_5_token_120     
    010 - | INITIA int64                         ZAAA                 ortshared_7_0_1_0_token_166      |                                                                                           
    011 - | INITIA float32                       IAAA                 ortshared_1_0_1_1_token_165      |                                                                                           
    012 ~ | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_4_token_174      | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_1_token_114     
    013 - | INITIA int64                         BAAA                 ortshared_7_0_1_1_token_176      |                                                                                           
    014 ~ | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_1_token_167      | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_2_token_117     
    015 ~ | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_1_token_171      | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_0_token_116     
    016 - | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_0_token_163      |                                                                                           
    017 ~ | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_1_token_177      | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_1_token_123     
    018 - | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_3_token_173      |                                                                                           
    019 ~ | INITIA int64    1:2                  BKAA                 ortshared_7_1_2_1_token_175      | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_2_token_124     
    020 = | INPUT  float32  2:512x512            AYTK                 primals_2                        | INPUT  float32  2:512x512            AYTK                 input0                          
    021 = | INPUT  float32  2:512x512            OIBB                 primals_4                        | INPUT  float32  2:512x512            OIBB                 input1                          
    022 = | INPUT  float32  2:512x512            QHCJ                 primals_3                        | INPUT  float32  2:512x512            QHCJ                 input2                          
    023 + |                                                                                            | INPUT  float32  2:512x512            ZYEX                 input3                           
    024 = | INPUT  float32  2:2048x64            MDRB                 primals_5                        | INPUT  float32  2:2048x64            MDRB                 input4                          
    025 = | INPUT  float32  2:2048x64            ZHDU                 primals_6                        | INPUT  float32  2:2048x64            ZHDU                 input5                          
    026 - | INPUT  float32  2:512x512            ZYEX                 primals_1                        |                                                                                           
    027 = | INPUT  float32  3:2x1024x512         KPQW                 primals_7                        | INPUT  float32  3:2x1024x512         KPQW                 input6                          
    028 = | INPUT  int64    2:1x1024             KAQG                 primals_8                        | INPUT  int64    2:1x1024             KAQG                 input7                          
    029 = | INPUT  float32  4:2x1x1024x1024      AAAA                 primals_9                        | INPUT  float32  4:2x1x1024x1024      AAAA                 input8                          
    030 - | RESULT float32  2:512x512            OIBB Identity        t_6                              |                                                                                           
    031 - | RESULT float32  4:2x1x1024x1024      AAAA Mul             other_1__45                      |                                                                                           
    032 - | RESULT int64    2:1x1024             KAQG Expand          _val_62                          |                                                                                           
    033 - | RESULT int64    3:1x1024x1           KAQG Unsqueeze       _val_64                          |                                                                                           
    034 - | RESULT int64    3:1x1024x1           KAQG Concat          _val_65                          |                                                                                           
    035 = | RESULT float32  2:1024x64            GSEC Slice           slice_2                          | RESULT float32  2:1024x64            GSEC Slice           slice_2                         
    036 - | RESULT float32  2:1024x64            GSEC Transpose       _val_59                          |                                                                                           
    037 ~ | RESULT float32  3:1x1024x64          GSEC GatherND        _val_66                          | RESULT float32  3:1x1024x64          GSEC Gather          index_1                         
    038 = | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       n2__16                           | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       output_5                        
    039 = | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_5_out0           | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_4_out0          
    040 = | RESULT float32  2:2048x512           KPQW Reshape         view                             | RESULT float32  2:2048x512           KPQW Reshape         output_2                        
    041 ~ | RESULT float32  2:2048x512           TIUU FusedMatMul     mm_1                             | RESULT float32  2:2048x512           MSDE Gemm            mm_1                            
    042 - | RESULT float32  3:2x1024x512         TIUU Reshape         view_3                           |                                                                                           
    043 ~ | RESULT float32  4:2x1024x8x64        TIUU Reshape         view_7                           | RESULT float32  4:2x1024x8x64        MSDE Reshape         view_7                          
    044 ~ | RESULT float32  4:2x1024x8x32        MOPZ Slice           Slice_195                        | RESULT float32  4:2x1024x8x32        YGAY Slice           slice_Tensor6                   
    045 ~ | RESULT float32  4:2x1024x8x32        OMLB Neg             n0__27                           | RESULT float32  4:2x1024x8x32        CUAC Neg             neg2                            
    046 ~ | RESULT float32  4:2x1024x8x32        GUEV Slice           Slice_178                        | RESULT float32  4:2x1024x8x32        NLDH Slice           slice_Tensor5                   
    047 ~ | RESULT float32  4:2x1024x8x64        UGPV Concat          n0__30                           | RESULT float32  4:2x1024x8x64        PGEK Concat          cat2                            
    048 ~ | RESULT float32  4:2x1024x8x64        CZOT Mul             n0__33                           | RESULT float32  4:2x1024x8x64        PFQC Mul             mul4                            
    049 = | RESULT float32  2:1024x64            CJYF Slice           slice_1                          | RESULT float32  2:1024x64            CJYF Slice           slice_1                         
    050 - | RESULT float32  2:1024x64            CJYF Transpose       _val_50                          |                                                                                           
    051 ~ | RESULT float32  3:1x1024x64          CJYF GatherND        _val_57                          | RESULT float32  3:1x1024x64          CJYF Gather          index                           
    052 = | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       n2__15                           | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       output_4                        
    053 = | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_8_out0           | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_6_out0          
    054 ~ | RESULT float32  4:2x1024x8x64        LCFD Mul             n0__24                           | RESULT float32  4:2x1024x8x64        UURX Mul             mul3                            
    055 ~ | RESULT float32  4:2x1024x8x64        MBTW Add             n3__35                           | RESULT float32  4:2x1024x8x64        JZIZ Add             add_Tensor2                     
    056 ~ | RESULT float32  4:2x8x64x1024        YPAP Transpose       transpose_3                      | RESULT float32  4:2x8x64x1024        SQMV Transpose       transpose_3                     
    057 - | RESULT float32  3:16x64x1024         YPAP Reshape         view_10                          |                                                                                           
    058 - | RESULT float32  4:1x1x1024x64        GSEC Transpose       unsqueeze_1                      |                                                                                           
    059 ~ | RESULT float32  2:2048x512           BMQW FusedMatMul     mm                               | RESULT float32  2:2048x512           KPQW Reshape         output_1                        
    060 ~ | RESULT float32  3:2x1024x512         BMQW Reshape         view_1                           | RESULT float32  2:2048x512           TIUU Gemm            mm                              
    061 ~ | RESULT float32  4:2x1024x8x64        BMQW Reshape         view_6                           | RESULT float32  4:2x1024x8x64        TIUU Reshape         view_6                          
    062 ~ | RESULT float32  4:2x8x1024x64        JFAN Transpose       transpose                        | RESULT float32  4:2x8x1024x64        ZCLC Transpose       transpose                       
    063 ~ | RESULT float32  4:2x8x1024x32        DTUX Slice           slice_4                          | RESULT float32  4:2x8x1024x32        NNPA Slice           slice_4                         
    064 ~ | RESULT float32  4:2x8x1024x32        XHGD Neg             neg                              | RESULT float32  4:2x8x1024x32        NNLA Neg             neg                             
    065 ~ | RESULT float32  4:2x8x1024x32        GNFP Slice           slice_3                          | RESULT float32  4:2x8x1024x32        MOWC Slice           slice_3                         
    066 ~ | RESULT float32  4:2x8x1024x64        DULR Concat          cat                              | RESULT float32  4:2x8x1024x64        ZAHD Concat          cat                             
    067 ~ | RESULT float32  4:2x8x1024x64        GLPW Mul             mul_1                            | RESULT float32  4:2x8x1024x64        CAQR Mul             mul_1                           
    068 - | RESULT float32  4:1x1x1024x64        CJYF Transpose       unsqueeze                        |                                                                                           
    069 ~ | RESULT float32  4:2x8x1024x64        WAVQ Mul             mul                              | RESULT float32  4:2x8x1024x64        XQKZ Mul             mul                             
    070 ~ | RESULT float32  4:2x8x1024x64        CLLN Add             add                              | RESULT float32  4:2x8x1024x64        YPAP Add             add                             
    071 - | RESULT float32  3:16x1024x64         CLLN Reshape         view_9                           |                                                                                           
    072 - | RESULT float32  3:16x1024x1024       UAFW MatMul          bmm                              |                                                                                           
    073 - | RESULT float32  4:2x8x1024x1024      UAFW Reshape         view_11                          |                                                                                           
    074 ~ | RESULT float32  4:2x8x1024x1024      PDOG Div             div                              | RESULT float32  4:2x8x1024x1024      EZUQ FusedMatMul     div                             
    075 ~ | RESULT float32  4:2x8x1024x1024      PDOG Add             add_2                            | RESULT float32  4:2x8x1024x1024      EZUQ Add             add_2                           
    076 = | RESULT float32  4:2x8x1024x1024      ONNN Softmax         result__46                       | RESULT float32  4:2x8x1024x1024      ONNN Softmax         output_8                        
    077 - | RESULT float32  3:16x1024x1024       ONNN Reshape         view_12                          |                                                                                           
    078 ~ | RESULT float32  2:2048x512           QBUY FusedMatMul     mm_2                             | RESULT float32  2:2048x512           KPQW Reshape         output_3                        
    079 ~ | RESULT float32  3:2x1024x512         QBUY Reshape         view_5                           | RESULT float32  2:2048x512           QBUY Gemm            mm_2                            
    080 = | RESULT float32  4:2x1024x8x64        QBUY Reshape         view_8                           | RESULT float32  4:2x1024x8x64        QBUY Reshape         view_8                          
    081 = | RESULT float32  4:2x8x1024x64        WWIL Transpose       transpose_2                      | RESULT float32  4:2x8x1024x64        WWIL Transpose       transpose_2                     
    082 ~ | RESULT float32  3:16x1024x64         WWIL Reshape         view_13                          | RESULT float32  4:2x8x1024x64        VVDS MatMul          view_11                         
    083 ~ | RESULT float32  3:16x1024x64         RUGP MatMul          bmm_1                            | RESULT float32  4:2x1024x8x64        TVXX Transpose       transpose_4                     
    084 ~ | RESULT float32  4:2x8x1024x64        RUGP Reshape         view_14                          | RESULT float32  2:2048x512           TVXX Reshape         output_12                       
    085 ~ | RESULT float32  4:2x1024x8x64        PWAV Transpose       transpose_4                      | RESULT float32  2:2048x512           EECA Gemm            mm_3                            
    086 ~ | RESULT float32  3:2x1024x512         PWAV Reshape         view_15                          | RESULT float32  3:2x1024x512         EECA Reshape         output_0                        
    087 + |                                                                                            | RESULT float32  2:512x512            AETA Transpose       output_11                        
    088 ~ | RESULT float32  2:2048x512           PWAV Reshape         view_16                          | RESULT float32  3:16x1024x64         WWIL Reshape         output_10                       
    089 - | RESULT float32  2:2048x512           TWDD FusedMatMul     mm_3                             |                                                                                           
    090 - | RESULT float32  3:2x1024x512         TWDD Reshape         view_17                          |                                                                                           
    091 ~ | RESULT float32  3:16x1024x1024       ONNN Transpose       transpose_6                      | RESULT float32  3:16x1024x1024       ONNN Reshape         output_9                        
    092 + |                                                                                            | RESULT float32  3:16x64x1024         SQMV Reshape         output_7                         
    093 - | RESULT float32  4:2x8x1024x1024      ONNN Identity        detach_3                         |                                                                                           
    094 ~ | RESULT float32  3:16x1024x64         YPAP Transpose       transpose_9                      | RESULT float32  3:16x1024x64         YPAP Reshape         output_6                        
    095 + |                                                                                            | OUTPUT float32  3:2x1024x512         EECA                 output_0                         
    096 ~ | RESULT float32  3:16x64x1024         CLLN Transpose       transpose_8                      | OUTPUT float32  2:2048x512           KPQW                 output_1                        
    097 ~ | RESULT float32  3:16x64x1024         WWIL Transpose       transpose_7                      | OUTPUT float32  2:2048x512           KPQW                 output_2                        
    098 = | OUTPUT float32  2:2048x512           KPQW                 view                             | OUTPUT float32  2:2048x512           KPQW                 output_3                        
    099 - | OUTPUT float32  2:512x512            OIBB                 t_6                              |                                                                                           
    100 = | OUTPUT float32  4:1x1x1024x64        CJYF                 unsqueeze                        | OUTPUT float32  4:1x1x1024x64        CJYF                 output_4                        
    101 = | OUTPUT float32  4:1x1x1024x64        GSEC                 unsqueeze_1                      | OUTPUT float32  4:1x1x1024x64        GSEC                 output_5                        
    102 ~ | OUTPUT float32  3:16x64x1024         WWIL                 transpose_7                      | OUTPUT float32  3:16x1024x64         YPAP                 output_6                        
    103 ~ | OUTPUT float32  3:16x64x1024         CLLN                 transpose_8                      | OUTPUT float32  3:16x64x1024         SQMV                 output_7                        
    104 - | OUTPUT float32  3:16x1024x64         YPAP                 transpose_9                      |                                                                                           
    105 = | OUTPUT float32  4:2x8x1024x1024      ONNN                 detach_3                         | OUTPUT float32  4:2x8x1024x1024      ONNN                 output_8                        
    106 = | OUTPUT float32  3:16x1024x1024       ONNN                 transpose_6                      | OUTPUT float32  3:16x1024x1024       ONNN                 output_9                        
    107 ~ | OUTPUT float32  2:2048x512           PWAV                 view_16                          | OUTPUT float32  3:16x1024x64         WWIL                 output_10                       
    108 + |                                                                                            | OUTPUT float32  2:512x512            AETA                 output_11                        
    109 ~ | OUTPUT float32  3:2x1024x512         TWDD                 view_17                          | OUTPUT float32  2:2048x512           TVXX                 output_12                       





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 33.751 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_dort_301.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_dort_301.ipynb <plot_llama_diff_dort_301.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_dort_301.py <plot_llama_diff_dort_301.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
