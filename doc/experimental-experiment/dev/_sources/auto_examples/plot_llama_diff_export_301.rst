
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_export_301.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_export_301.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_export_301.py:


.. _l-plot-llama-diff-export:

301: Compares LLAMA exporters
=============================

The script compares the two exporters implemented in :epkg:`pytorch`
for a part of llama model. The model are compared after all optimizations
were made with and :epkg:`onnxruntime`.

* `TorchScript-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter>`_,
  let's call it **script**
* `TorchDynamo-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter>`_,
  let's call it **dynamo**

To run the script:

::

    python _doc/examples/plot_llama_diff_export --help

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 27-76

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("model", "one value among model, ..."),
        exporter=("dynamo", "one value among dynamo, custom"),
        ortopt=(1, "run onnxruntime optimization"),
        opset=(18, "onnx opset"),
        expose="part,exporter,ortopt,opset",
    )

    import contextlib
    import os
    import io
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import numpy as np
    import onnx
    from onnx_array_api.reference import compare_onnx_execution
    import torch
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.reference import ExtendedReferenceEvaluator
    from experimental_experiment.torch_interpreter import to_onnx
    from experimental_experiment.helpers import string_type
    from experimental_experiment.xbuilder import OptimizationOptions
    from experimental_experiment.convert.convert_helper import ort_optimize
    from experimental_experiment.torch_models.llama_helper import get_llama_model
    from experimental_experiment.torch_models.dump_helper import reorder_functions_in_proto

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 77-79

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 79-136

.. code-block:: Python



    print(f"part={script_args.part}")
    print(f"exporter={script_args.exporter}")
    ortopt = script_args.ortopt in (1, "1")
    print(f"ortopt={ortopt}")
    opset = int(script_args.opset)
    print(f"opset={opset}")


    def opt_filename(filename: str) -> str:
        name, ext = os.path.splitext(filename)
        return f"{name}.opt{ext}"


    def export_script(filename, model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                torch.onnx.export(
                    model, args, filename, input_names=["input"], opset_version=opset
                )
        if ortopt:
            onx = onnx.load(filename)
            ort_optimize(onx, opt_filename(filename), providers=provider)


    def export_dynamo(filename, model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                export_output = torch.onnx.export(model, args, dynamo=True)
                export_output.optimize()
                model = export_output.model_proto
        with open(filename, "wb") as f:
            f.write(model.SerializeToString())
        if ortopt:
            ort_optimize(model, opt_filename(filename), providers=provider)


    def export_custom(filename, model, *args):
        new_model = to_onnx(
            model,
            tuple(args),
            input_names=[f"input{i}" for i in range(len(args))],
            options=OptimizationOptions(
                remove_unused=True,
                constant_folding=False,
            ),
            target_opset=opset,
        )
        with open(filename, "wb") as f:
            f.write(new_model.SerializeToString())
        if ortopt:
            ort_optimize(new_model, opt_filename(filename), providers=provider)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=model
    exporter=dynamo
    ortopt=True
    opset=18




.. GENERATED FROM PYTHON SOURCE LINES 137-139

Model and data
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 139-164

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    print(f"simple run with {len(inputs)} inputs")
    expected = model(*inputs[0])
    print(f"eager worked: {string_type(expected, with_shape=True)}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    eager worked: (T1s2x1024x512,)




.. GENERATED FROM PYTHON SOURCE LINES 165-167

Exporting
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 167-184

.. code-block:: Python


    exporter = script_args.exporter
    file1 = f"llama.{script_args.part}.script.onnx"
    file2 = f"llama.{script_args.part}.{exporter}.onnx"

    print("torch script exporter")
    export_script(file1, model, *inputs[0])

    if exporter == "dynamo":
        print("torch dynamo exporter")
        export_dynamo(file2, model, *inputs[0])
    elif exporter == "custom":
        print("torch custom exporter")
        export_custom(file2, model, *inputs[0])
    else:
        raise AssertionError(f"Unexpected value for exporter={exporter!r}.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch script exporter
    torch dynamo exporter




.. GENERATED FROM PYTHON SOURCE LINES 185-187

Verification
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 187-223

.. code-block:: Python


    if ortopt:
        print("Using models optimized by onnxruntime")
        file1 = f"llama.{script_args.part}.script.opt.onnx"
        file2 = f"llama.{script_args.part}.{exporter}.opt.onnx"


    providers = (
        ["CPUExecutionProvider"]
        if provider == "cpu"
        else [("CUDAExecutionProvider", {}), ("CPUExecutionProvider", {})]
    )

    model1 = onnx.load(file1)
    model2 = onnx.load(file2)

    feeds1, feeds2 = {}, {}
    for i in range(len(inputs[0])):
        x = inputs[0][i].detach().numpy()
        feeds1[model1.graph.input[i].name] = x
        feeds2[model2.graph.input[i].name] = x

    if ortopt:
        sess1 = onnxruntime.InferenceSession(file1, providers=providers)
        sess2 = onnxruntime.InferenceSession(file2, providers=providers)

        got1 = sess1.run(None, feeds1)
        got2 = sess2.run(None, feeds2)

        if isinstance(expected, tuple) and len(expected) == 1:
            expected = expected[0]
        diff1 = np.abs(expected.detach().numpy() - got1[0]).max()
        diff2 = np.abs(expected.detach().numpy() - got2[0]).max()

        print(f"Error with the eager model and onnxruntime: {diff1}, {diff2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using models optimized by onnxruntime
    Error with the eager model and onnxruntime: 0.003202080726623535, 0.00327947735786438




.. GENERATED FROM PYTHON SOURCE LINES 224-226

Verification with the reference evaluator
+++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 226-249

.. code-block:: Python


    reorder_functions_in_proto(file1)
    reorder_functions_in_proto(file2)

    sess1 = ExtendedReferenceEvaluator(file1)
    try:
        sess2 = ExtendedReferenceEvaluator(file2)
    except NotImplementedError as e:
        print(e)
        sess2 = None

    got1 = sess1.run(None, feeds1)
    got2 = got1 if sess2 is None else sess2.run(None, feeds2)

    if isinstance(expected, tuple):
        diff1 = np.abs(expected[0].detach().numpy() - got1[0]).max()
        diff2 = np.abs(expected[0].detach().numpy() - got2[0]).max()
    else:
        diff1 = np.abs(expected.detach().numpy() - got1[0]).max()
        diff2 = np.abs(expected.detach().numpy() - got2[0]).max()

    print(f"Error with the eager model and the reference evaluator: {diff1}, {diff2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Error with the eager model and the reference evaluator: 3.814697265625e-06, 3.814697265625e-06




.. GENERATED FROM PYTHON SOURCE LINES 250-252

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 252-280

.. code-block:: Python



    def clean_name(name):
        return name.replace(
            "_inlfunc_transformers_models_llama_modeling_llama_LlamaAttention", ""
        ).replace("_inlfunc_torch_nn_modules_linear_Linear", "")


    if sess2 is not None:
        try:
            np_inputs = [i.detach().numpy() for i in inputs[0]]
            res1, res2, align, dc = compare_onnx_execution(
                model1,
                model2,
                inputs=np_inputs,
                verbose=1,
                raise_exc=False,
                cls=ExtendedReferenceEvaluator,
            )
            for r in res2:
                r.name = clean_name(r.name)
            text = dc.to_str(res1, res2, align, column_size=90)
            print(text)
        except AssertionError as e:
            if "Unexpected type <class 'list'> for value, it must be a numpy array." not in str(e):
                raise
            print(e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] execute with 2 inputs
    [compare_onnx_execution] execute first model
    [compare_onnx_execution] got 83 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 83 results (first model)
    [compare_onnx_execution] got 99 results (second model)
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 112 pairs
    [compare_onnx_execution] done
    001 = | INITIA float32  2:4000x512           XNVV                 model.embed_tokens.weight        | INITIA float32  2:4000x512           XNVV                 model.embed_tokens.weight       
    002 + |                                                                                            | INITIA float32  4:2x1x1024x1024      ????                 expand_1                         
    003 + |                                                                                            | INITIA float32  4:1x1024x1x64        CJYF                 unsqueeze_10                     
    004 - | INITIA float32  1:512                YYYY                 model.layers.0.input_layernorm.w |                                                                                           
    005 - | INITIA float32  2:512x512            ZXFZ                 onnx::MatMul_381                 |                                                                                           
    006 - | INITIA float32  2:512x512            AHZB                 onnx::MatMul_397                 |                                                                                           
    007 - | INITIA float32  2:512x512            WHXA                 onnx::MatMul_398                 |                                                                                           
    008 = | INITIA float32  2:512x512            YAYZ                 onnx::MatMul_423                 | INITIA float32  2:512x512            YAYZ                 val_321                         
    009 - | INITIA float32  2:512x2000           XJZT                 onnx::MatMul_424                 |                                                                                           
    010 = | INITIA float32  2:512x2000           ZPHS                 onnx::MatMul_425                 | INITIA float32  2:512x2000           ZPHS                 val_327                         
    011 + |                                                                                            | INITIA float32  2:512x512            ZXFZ                 val_239                          
    012 - | INITIA float32  2:2000x512           CVYE                 onnx::MatMul_426                 |                                                                                           
    013 - | INITIA int64    5:2x1x1024x1024x4    AQYO                 /model/Concat_output_0           |                                                                                           
    014 ~ | INITIA int64    1:4                  CBKK                 /model/Concat_1_output_0         | INITIA float32                       AAAA                 val_237                         
    015 - | INITIA float32  4:1x1024x1x64        GSEC                 /model/layers.0/self_attn/Unsque |                                                                                           
    016 ~ | INITIA int64    1:2                  GGAA                 splits                           | INITIA int64    1:1                  ZAAA                 val_323                         
    017 + |                                                                                            | INITIA float32  1:512                YYYY                 model.layers.0.input_layernorm.w 
    018 + |                                                                                            | INITIA float32  1:512                YYYY                 model.layers.0.post_attention_la 
    019 ~ | INITIA float32                       ?AAA                 /model/Constant_19_output_0      | INITIA float32  1:512                YYYY                 model.norm.weight               
    020 - | INITIA float32  4:2x1x1024x1024      ????                 /model/Slice_2_output_0          |                                                                                           
    021 - | INITIA float32  4:1x1024x1x64        CJYF                 /model/layers.0/self_attn/Unsque |                                                                                           
    022 ~ | INITIA int64    1:4                  CKZM                 /model/layers.0/self_attn/Consta | INITIA int64    1:2                  GGAA                 splits_token_10                 
    023 + |                                                                                            | INITIA float32  2:2000x512           CVYE                 val_328                          
    024 ~ | INITIA int64    1:2                  GGAA                 splits_token_12                  | INITIA float32                       ?AAA                 val_5                           
    025 + |                                                                                            | INITIA float32  2:512x2000           XJZT                 val_325                          
    026 ~ | INITIA int64    1:1                  AAAA                 /model/layers.0/self_attn/Consta | INITIA int64                         BAAA                 dim_0_10                        
    027 + |                                                                                            | INITIA float32  2:512x512            AHZB                 val_242                          
    028 + |                                                                                            | INITIA float32  4:1024x1x2x1024      ????                 val_180                          
    029 ~ | INITIA int64    1:1                  BAAA                 /model/layers.0/self_attn/Consta | INITIA int64    1:3                  CKZA                 val_320                         
    030 + |                                                                                            | INITIA float32  2:512x512            WHXA                 val_244                          
    031 ~ | INITIA int64    1:1                  KAAA                 /model/layers.0/self_attn/Consta | INITIA int64                         CAAA                 val_20                          
    032 + |                                                                                            | INITIA int64    2:1024x1             KAQG                 val_178                          
    033 ~ | INITIA int64    1:1                  DAAA                 const_transpose_optimizer_token_ | INITIA int64    1:2                  GGAA                 splits                          
    034 ~ | INITIA int64    1:1                  CAAA                 /model/Constant_13_output_0      | INITIA int64    1:4                  CKZM                 val_243                         
    035 = | INITIA float32                       AAAA                 /model/Constant_14_output_0      | INITIA float32                       AAAA                 scalar_tensor_default           
    036 - | INITIA float32  4:2x1x1024x1024      ????                 /model/Expand_output_0           |                                                                                           
    037 = | INITIA float32  4:1x1x1024x64        GSEC                 Transpose_token_4_out0           | INITIA float32  4:1x1x1024x64        GSEC                 Transpose_token_4_out0          
    038 + |                                                                                            | INITIA float32  4:1x1024x1x64        GSEC                 unsqueeze_11                     
    039 ~ | INITIA int64    1:3                  CKZA                 /model/layers.0/self_attn/Consta | INITIA int64                         CAAA                 val_20_token_11                 
    040 = | INPUT  int64    2:2x1024             ZJWE                 input                            | INPUT  int64    2:2x1024             ZJWE                 input_ids                       
    041 = | INPUT  float32  2:2x1024             BACA                 attention_mask.1                 | INPUT  float32  2:2x1024             BACA                 attention_mask                  
    042 = | RESULT float32  3:2x1024x512         DEIK Gather          /model/embed_tokens/Gather_outpu | RESULT float32  3:2x1024x512         DEIK Gather          embedding                       
    043 ~ | RESULT float32  3:2x1024x512         SVQN SimplifiedLayer /model/layers.0/input_layernorm/ | RESULT float32  3:2x1024x512         BAAA Pow             pow_1                           
    044 + |                                                                                            | RESULT float32  3:2x1024x1           AAAA ReduceMean      mean                             
    045 + |                                                                                            | RESULT float32  3:2x1024x1           AAAA Add             add_1                            
    046 + |                                                                                            | RESULT float32  3:2x1024x1           KKKK Sqrt            val_238                          
    047 ~ | RESULT float32  3:2x1024x1           WISP SimplifiedLayer saved_inv_std_var                | RESULT float32  3:2x1024x1           WISP Reciprocal      rsqrt                           
    048 + |                                                                                            | RESULT float32  3:2x1024x512         SVQN Mul             mul_3                            
    049 + |                                                                                            | RESULT float32  3:2x1024x512         SVQN Mul             mul_4                            
    050 = | RESULT float32  3:2x1024x512         XGMT MatMul          /model/layers.0/self_attn/k_proj | RESULT float32  3:2x1024x512         XGMT MatMul          linear_1                        
    051 = | RESULT float32  4:2x1024x8x64        XGMT Reshape         /model/layers.0/self_attn/Reshap | RESULT float32  4:2x1024x8x64        XGMT Reshape         view_2                          
    052 = | RESULT float32  4:2x1024x8x32        BLII Split           /model/layers.0/self_attn/Slice_ | RESULT float32  4:2x1024x8x32        BLII Split           node_Slice_363                  
    053 = | RESULT float32  4:2x1024x8x32        WWEK Split           /model/layers.0/self_attn/Slice_ | RESULT float32  4:2x1024x8x32        WWEK Split           node_Slice_374                  
    054 = | RESULT float32  4:2x1024x8x32        EEWQ Neg             /model/layers.0/self_attn/Neg_1  | RESULT float32  4:2x1024x8x32        EEWQ Neg             node_Neg_375                    
    055 = | RESULT float32  4:2x1024x8x64        FPDY Concat          /model/layers.0/self_attn/Concat | RESULT float32  4:2x1024x8x64        FPDY Concat          node_Concat_376                 
    056 = | RESULT float32  4:2x1024x8x64        UYJS Mul             /model/layers.0/self_attn/Mul_3  | RESULT float32  4:2x1024x8x64        UYJS Mul             node_Mul_377                    
    057 = | RESULT float32  4:2x1024x8x64        RKRD Mul             /model/layers.0/self_attn/Mul_2  | RESULT float32  4:2x1024x8x64        RKRD Mul             node_Mul_352                    
    058 = | RESULT float32  4:2x1024x8x64        KHAV Add             /model/layers.0/self_attn/Add_1  | RESULT float32  4:2x1024x8x64        KHAV Add             node_Add_378                    
    059 = | RESULT float32  4:2x8x64x1024        RAWZ Transpose       /model/layers.0/self_attn/Transp | RESULT float32  4:2x8x64x1024        RAWZ Transpose       transpose_4                     
    060 = | RESULT float32  3:2x1024x512         OONN MatMul          /model/layers.0/self_attn/q_proj | RESULT float32  3:2x1024x512         OONN MatMul          linear                          
    061 = | RESULT float32  4:2x1024x8x64        OONN Reshape         /model/layers.0/self_attn/Reshap | RESULT float32  4:2x1024x8x64        OONN Reshape         view_1                          
    062 = | RESULT float32  4:2x1024x8x64        XUSC Mul             /model/layers.0/self_attn/Mul    | RESULT float32  4:2x1024x8x64        XUSC Mul             node_Mul_324                    
    063 = | RESULT float32  4:2x8x1024x64        NDBT Transpose       /model/layers.0/self_attn/Mul_ou | RESULT float32  4:2x8x1024x64        NDBT Transpose       mul_5                           
    064 = | RESULT float32  4:2x8x1024x64        GXHU Transpose       /model/layers.0/self_attn/Transp | RESULT float32  4:2x8x1024x64        GXHU Transpose       transpose_1                     
    065 = | RESULT float32  4:2x8x1024x32        RERG Split           /model/layers.0/self_attn/Slice_ | RESULT float32  4:2x8x1024x32        RERG Split           slice_24                        
    066 = | RESULT float32  4:2x8x1024x32        PSPN Split           /model/layers.0/self_attn/Slice_ | RESULT float32  4:2x8x1024x32        PSPN Split           slice_25                        
    067 = | RESULT float32  4:2x8x1024x32        LILN Neg             /model/layers.0/self_attn/Neg_ou | RESULT float32  4:2x8x1024x32        LILN Neg             neg                             
    068 = | RESULT float32  4:2x8x1024x64        BLBT Concat          /model/layers.0/self_attn/Concat | RESULT float32  4:2x8x1024x64        BLBT Concat          cat_1                           
    069 = | RESULT float32  4:2x8x1024x64        GEUD Mul             /model/layers.0/self_attn/Mul_1_ | RESULT float32  4:2x8x1024x64        GEUD Mul             mul_6                           
    070 = | RESULT float32  4:2x8x1024x64        UHUW Add             /model/layers.0/self_attn/Add_ou | RESULT float32  4:2x8x1024x64        UHUW Add             add_2                           
    071 = | RESULT float32  4:2x8x1024x1024      XTYC FusedMatMul     /model/layers.0/self_attn/Mul_4_ | RESULT float32  4:2x8x1024x1024      XTYC FusedMatMul     mul_9                           
    072 = | RESULT float32  3:2x1x1024           BACA Unsqueeze       /model/Unsqueeze_2_output_0      | RESULT float32  3:2x1x1024           BACA Unsqueeze       unsqueeze_5                     
    073 = | RESULT float32  4:2x1x1x1024         BACA Unsqueeze       /model/Unsqueeze_3_output_0      | RESULT float32  4:2x1x1x1024         BACA Unsqueeze       unsqueeze_6                     
    074 = | RESULT float32  4:2x1x1024x1024      ???? Add             /model/Add_output_0              | RESULT float32  4:2x1x1024x1024      ???? Add             add                             
    075 = | RESULT bool     4:2x1x1024x1024      KWTE Equal           /model/Equal_1_output_0          | RESULT bool     4:2x1x1024x1024      KWTE Equal           eq                              
    076 = | RESULT float32  4:2x1x1024x1024      ???? Where           /model/Where_1_output_0          | RESULT float32  4:2x1x1024x1024      ???? Where           masked_fill                     
    077 + |                                                                                            | RESULT float32  4:1024x1x2x1024      ???? Transpose       val_179                          
    078 + |                                                                                            | RESULT float32  4:1024x1x2x1024      ???? ScatterND       val_181                          
    079 - | RESULT float32  4:2x1x1024x1024      ???? Reshape         /model/Reshape_output_0          |                                                                                           
    080 - | RESULT float32  4:2x1x1024x1024      ???? ScatterND       /model/ScatterND_output_0        |                                                                                           
    081 ~ | RESULT float32  4:2x1x1024x1024      ???? Slice           /model/layers.0/self_attn/Slice_ | RESULT float32  4:2x1x1024x1024      ???? Transpose       slice_scatter_1                 
    082 = | RESULT float32  4:2x8x1024x1024      ???? Add             /model/layers.0/self_attn/Add_2_ | RESULT float32  4:2x8x1024x1024      ???? Add             add_4                           
    083 ~ | RESULT float32  4:2x8x1024x1024      OONN Softmax         /model/layers.0/self_attn/Softma | RESULT float32  4:2x8x1024x1024      OOON Softmax         val_318                         
    084 = | RESULT float32  3:2x1024x512         GWGZ MatMul          /model/layers.0/self_attn/v_proj | RESULT float32  3:2x1024x512         GWGZ MatMul          linear_2                        
    085 = | RESULT float32  4:2x1024x8x64        GWGZ Reshape         /model/layers.0/self_attn/Reshap | RESULT float32  4:2x1024x8x64        GWGZ Reshape         view_3                          
    086 = | RESULT float32  4:2x8x1024x64        XFAE Transpose       /model/layers.0/self_attn/Transp | RESULT float32  4:2x8x1024x64        XFAE Transpose       transpose_3                     
    087 = | RESULT float32  4:2x8x1024x64        JCII MatMul          /model/layers.0/self_attn/MatMul | RESULT float32  4:2x8x1024x64        JCII MatMul          matmul_2                        
    088 = | RESULT float32  4:2x1024x8x64        TTBQ Transpose       /model/layers.0/self_attn/Transp | RESULT float32  4:2x1024x8x64        TTBQ Transpose       transpose_5                     
    089 = | RESULT float32  3:2x1024x512         TTBQ Reshape         /model/layers.0/self_attn/Reshap | RESULT float32  3:2x1024x512         TTBQ Reshape         view_4                          
    090 = | RESULT float32  3:2x1024x512         VVZD MatMul          /model/layers.0/self_attn/o_proj | RESULT float32  3:2x1024x512         VVZD MatMul          linear_3                        
    091 = | RESULT float32  3:2x1024x512         YZGN Add             /model/layers.0/Add_output_0     | RESULT float32  3:2x1024x512         YZGN Add             add_5                           
    092 ~ | RESULT float32  3:2x1024x512         XWRG SimplifiedLayer /model/layers.0/post_attention_l | RESULT float32  3:2x1024x512         ODAT Pow             pow_2                           
    093 + |                                                                                            | RESULT float32  3:2x1024x1           YYLL ReduceMean      mean_1                           
    094 + |                                                                                            | RESULT float32  3:2x1024x1           YYLL Add             add_6                            
    095 + |                                                                                            | RESULT float32  3:2x1024x1           IIXX Sqrt            val_324                          
    096 ~ | RESULT float32  3:2x1024x1           TUHI SimplifiedLayer saved_inv_std_var_token_10       | RESULT float32  3:2x1024x1           TUHI Reciprocal      rsqrt_1                         
    097 + |                                                                                            | RESULT float32  3:2x1024x512         XWRG Mul             mul_10                           
    098 + |                                                                                            | RESULT float32  3:2x1024x512         XWRG Mul             mul_11                           
    099 = | RESULT float32  3:2x1024x2000        LCJA MatMul          /model/layers.0/mlp/gate_proj/Ma | RESULT float32  3:2x1024x2000        LCJA MatMul          linear_4                        
    100 = | RESULT float32  3:2x1024x2000        OWWJ QuickGelu       /model/layers.0/mlp/act_fn/Mul_o | RESULT float32  3:2x1024x2000        OWWJ QuickGelu       silu                            
    101 = | RESULT float32  3:2x1024x2000        IVET MatMul          /model/layers.0/mlp/up_proj/MatM | RESULT float32  3:2x1024x2000        IVET MatMul          linear_5                        
    102 = | RESULT float32  3:2x1024x2000        XQBH Mul             /model/layers.0/mlp/Mul_output_0 | RESULT float32  3:2x1024x2000        XQBH Mul             mul_12                          
    103 = | RESULT float32  3:2x1024x512         HMOL MatMul          /model/layers.0/mlp/down_proj/Ma | RESULT float32  3:2x1024x512         HMOL MatMul          linear_6                        
    104 = | RESULT float32  3:2x1024x512         GMUX Add             /model/layers.0/Add_1_output_0   | RESULT float32  3:2x1024x512         GMUX Add             add_7                           
    105 ~ | RESULT float32  3:2x1024x512         GDPI SimplifiedLayer 347                              | RESULT float32  3:2x1024x512         JIXP Pow             pow_3                           
    106 + |                                                                                            | RESULT float32  3:2x1024x1           EEOO ReduceMean      mean_2                           
    107 + |                                                                                            | RESULT float32  3:2x1024x1           EEOO Add             add_8                            
    108 + |                                                                                            | RESULT float32  3:2x1024x1           VVJJ Sqrt            val_331                          
    109 ~ | RESULT float32  3:2x1024x1           EEEG SimplifiedLayer saved_inv_std_var_token_11       | RESULT float32  3:2x1024x1           EEEG Reciprocal      rsqrt_2                         
    110 + |                                                                                            | RESULT float32  3:2x1024x512         GDPI Mul             mul_13                           
    111 + |                                                                                            | RESULT float32  3:2x1024x512         GDPI Mul             mul_14                           
    112 = | OUTPUT float32  3:2x1024x512         GDPI                 347                              | OUTPUT float32  3:2x1024x512         GDPI                 mul_14                          




.. GENERATED FROM PYTHON SOURCE LINES 281-282

See :ref:`l-long-outputs-llama-diff-export` for a better view.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 38.708 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_export_301.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_export_301.ipynb <plot_llama_diff_export_301.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_export_301.py <plot_llama_diff_export_301.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_llama_diff_export_301.zip <plot_llama_diff_export_301.zip>`


.. include:: plot_llama_diff_export_301.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
