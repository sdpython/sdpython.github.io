
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_dort.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_dort.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_dort.py:


.. _l-plot-onnxrt-diff:

Compares LLAMA exporters for onnxrt backend
===========================================

The script compares exported models in :epkg:`pytorch`
using :epkg:`onnxrt backend`. It tries to do a side by side
of the execution of both models.

To run the script:

::

    python _doc/examples/plot_llama_diff_dort --help


The following example compares the forward step for mixed precision on cuda
and produces all the intermediate onnx graphs.

::

    python _doc/examples/plot_llama_diff_dort.py --part model --ortopt 1 --cuda 1 --backward 0 --mixed 1

You may use ``--mixed=1`` to compare the backward graphs.

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 30-98

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("attention", "one value among attention, decoder, model"),
        ortopt=(1, "run onnxruntime optimization"),
        backward=(0, "does one operator for backward"),
        cuda=(0, "use cuda or not"),
        mixed=(0, "use miwed precision"),
        expose="part,exporter,ortopt,cuda,mixed",
    )


    import copy
    import os
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import onnx
    from onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator
    import torch
    from torch._dynamo.backends.common import aot_autograd
    from experimental_experiment.ext_test_case import unit_test_going
    from experimental_experiment.convert.convert_helper import (
        optimize_model_proto,
        ort_optimize,
    )
    from experimental_experiment.torch_helper.llama_helper import (
        get_llama_model,
        get_llama_attention,
        get_llama_decoder,
    )
    from experimental_experiment.torch_helper.dump_helper import (
        assert_all_close,
        dump_onnx,
        reorder_functions_in_proto,
        inputs_from_onnx_model,
        build_matching_inputs,
        results_to_string,
    )
    from experimental_experiment.torch_helper.training_helper import (
        train_loop,
        make_aot_ort,
    )
    from experimental_experiment.torch_dynamo import (
        onnx_debug_backend,
        get_decomposition_table,
    )

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 99-101

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 101-112

.. code-block:: Python


    print(f"part={script_args.part}")
    ortopt = script_args.ortopt in (1, "1")
    print(f"ortopt={ortopt}")
    backward = script_args.backward in (1, "1")
    print(f"backward={backward}")
    use_cuda = script_args.cuda in (1, "1")
    print(f"cuda={use_cuda}")
    use_mixed = script_args.mixed in (1, "1")
    print(f"mixed={use_mixed}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=attention
    ortopt=True
    backward=False
    cuda=False
    mixed=False




.. GENERATED FROM PYTHON SOURCE LINES 113-115

Model and data
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 115-169

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "attention":
        model, inputs = get_llama_attention(**kwargs)
    elif script_args.part == "decoder":
        model, inputs = get_llama_decoder(**kwargs)
    elif script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    if use_cuda:
        model = model.to("cuda")
        inputs = [[i.to("cuda") for i in inp] for inp in inputs]

    print(f"simple run with {len(inputs)} inputs")
    if backward:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = train_loop(copy.deepcopy(model), *inputs[0])
                torch.cuda.synchronize()
        else:
            expected = train_loop(copy.deepcopy(model), *inputs[0])
        print(
            f"-- eager mode worked, {len(expected)} gradients, first one is "
            f"{expected[0].shape}, {expected[0].dtype}"
        )
    else:
        if use_mixed:
            assert use_cuda, "mixed precision only works with cuda"
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                expected = model(*inputs[0])
                torch.cuda.synchronize()
        else:
            expected = model(*inputs[0])
        print(results_to_string(expected))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    torch.float32 (2, 1024, 512) [sum=518]




.. GENERATED FROM PYTHON SOURCE LINES 170-172

Exporting
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 172-256

.. code-block:: Python


    folder = "dump_models"
    storage = {}

    if backward:
        # onnxrt backend
        local_aot_ort, _ = make_aot_ort(dynamic=False)

        optimized_mod = torch.compile(
            copy.deepcopy(model), backend=local_aot_ort, dynamic=False, fullgraph=True
        )

        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = train_loop(optimized_mod, *inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = train_loop(optimized_mod, *inputs[0])
        assert_all_close(expected[0], expected_onnxrt[0], atol=1e-3)
        print(
            f"-- onnxrt backend worked, {len(expected_onnxrt)} gradients, first one is "
            f"{expected_onnxrt[0].shape}, {expected_onnxrt[0].dtype}"
        )

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=17,
                storage=storage,
                **kwargs,
            ),
            decompositions=get_decomposition_table(),
        )
        onnx_mod = torch.compile(copy.deepcopy(model), backend=aot_compiler, fullgraph=True)

        if False and use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                torch.cuda.synchronize()
                got = train_loop(onnx_mod, *inputs[0])
                torch.cuda.synchronize()
        else:
            got = train_loop(onnx_mod, *inputs[0])
        assert_all_close(expected[0], got[0], atol=1e-2 if use_mixed else 1e-4)
        print(
            f"-- debug backend worked, {len(got)} gradients, first one is "
            f"{got[0].shape}, {got[0].dtype}"
        )

    else:
        # onnxrt backend
        optimized_mod = torch.compile(model, backend="onnxrt", fullgraph=True)
        with dump_onnx("llama_onnxrt", folder=folder, clean=True):
            if use_mixed:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    torch.cuda.synchronize()
                    expected_onnxrt = optimized_mod(*inputs[0])
                    torch.cuda.synchronize()
            else:
                expected_onnxrt = optimized_mod(*inputs[0])
        assert_all_close(expected, expected_onnxrt, atol=1e-2)

        # debugging backend
        aot_compiler = aot_autograd(
            fw_compiler=lambda *args, **kwargs: onnx_debug_backend(
                *args,
                dump_prefix=os.path.join(folder, "llama_debug"),
                target_opset=17,
                storage=storage,
                **kwargs,
            )
        )

        onnx_mod = torch.compile(model, backend=aot_compiler, fullgraph=True)
        if use_mixed:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                got = onnx_mod(*inputs[0])
        else:
            got = onnx_mod(*inputs[0])
        assert_all_close(expected, got, atol=1 if use_mixed else 1e-3)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ********************B None
    **********A False
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:136: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 257-260

For forward, there are two files, one onnx model and the graph module
printed in a txt file. For backward, there are two onnx models.
Then it is multiplied by the number of backends.

.. GENERATED FROM PYTHON SOURCE LINES 260-264

.. code-block:: Python


    models = os.listdir(folder)
    print(f"exported models: {models}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    exported models: ['llama_onnxrt_0.onnx', 'llama_debug_0.onnx', 'llama_debug_0.txt', 'llama_onnxrt_0.txt']




.. GENERATED FROM PYTHON SOURCE LINES 265-266

Inputs used by the debug backend

.. GENERATED FROM PYTHON SOURCE LINES 266-271

.. code-block:: Python


    feeds = storage["instance"][0]["inputs"][0]
    for k, v in feeds.items():
        print(f"-- {k} {v.dtype} {v.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- input0 float32 (512, 512)
    -- input1 float32 (512, 512)
    -- input2 float32 (512, 512)
    -- input3 float32 (512, 512)
    -- input4 float32 (2048, 64)
    -- input5 float32 (2048, 64)
    -- input6 float32 (2, 1024, 512)
    -- input7 int64 (1, 1024)
    -- input8 float32 (2, 1, 1024, 1024)




.. GENERATED FROM PYTHON SOURCE LINES 272-273

Let's the first line of the graph module

.. GENERATED FROM PYTHON SOURCE LINES 273-278

.. code-block:: Python


    graph_module = storage["instance"][0]["graph_module"]
    print("\n".join(str(graph_module.graph).split("\n")[:10]))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    graph():
        %primals_1 : [num_users=1] = placeholder[target=primals_1]
        %primals_2 : [num_users=1] = placeholder[target=primals_2]
        %primals_3 : [num_users=1] = placeholder[target=primals_3]
        %primals_4 : [num_users=1] = placeholder[target=primals_4]
        %primals_5 : [num_users=1] = placeholder[target=primals_5]
        %primals_6 : [num_users=1] = placeholder[target=primals_6]
        %primals_7 : [num_users=3] = placeholder[target=primals_7]
        %primals_8 : [num_users=2] = placeholder[target=primals_8]
        %primals_9 : [num_users=1] = placeholder[target=primals_9]




.. GENERATED FROM PYTHON SOURCE LINES 279-281

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 281-309

.. code-block:: Python


    if backward:
        print(f"-- {len(storage['instance'])} onnx models were creates")
        for i, inst in enumerate(storage["instance"]):
            print(f"  model {i}: {len(inst['inputs'])} runs")

        # deal with backward
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        assert len(onnx_models) == 4, f"unexpected value {onnx_models}"
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx") and "_1" in m]))
        assert len(onnx_models) == 2, f"unexpected value {onnx_models}"
        model_onnxrt = os.path.join(folder, onnx_models[1])
        model_debug = os.path.join(folder, onnx_models[0])
    else:
        onnx_models = list(sorted([m for m in models if m.endswith(".onnx")]))
        if len(onnx_models) == 2:
            model_onnxrt = os.path.join(folder, onnx_models[1])
            model_debug = os.path.join(folder, onnx_models[0])
        else:
            model_debug = os.path.join(folder, onnx_models[0])
            # the following error may appear:
            # Node type 'Rank' from domain 'pkg.onnxscript.torch_lib.common' is unknown
            print(f"One model is missing, onnx_models={onnx_models}")
            model_onnxrt = model_debug

    print(f"model_onnxrt={model_onnxrt}")
    print(f"model_debug={model_debug}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    model_onnxrt=dump_models/llama_onnxrt_0.onnx
    model_debug=dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 310-311

The inputs of both models

.. GENERATED FROM PYTHON SOURCE LINES 311-315

.. code-block:: Python


    print("onnxrt:", inputs_from_onnx_model(model_onnxrt))
    print("debug:", inputs_from_onnx_model(model_debug))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    onnxrt: [('INPUT', 'primals_6', 1, (2048, 64)), ('INPUT', 'primals_1', 1, (512, 512)), ('INPUT', 'primals_7', 1, (2, 1024, 512)), ('INPUT', 'primals_2', 1, (512, 512)), ('INPUT', 'primals_4', 1, (512, 512)), ('INPUT', 'primals_3', 1, (512, 512)), ('INPUT', 'primals_5', 1, (2048, 64)), ('INPUT', 'primals_8', 7, (1, 1024)), ('INPUT', 'primals_9', 1, (2, 1, 1024, 1024))]
    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024))]




.. GENERATED FROM PYTHON SOURCE LINES 316-318

Inputs are not the same. The first model has more and some inputs were
moved into the initializer list into for `model_debug`.

.. GENERATED FROM PYTHON SOURCE LINES 318-321

.. code-block:: Python


    print("debug:", inputs_from_onnx_model(model_debug, init=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    debug: [('INPUT', 'input0', 1, (512, 512)), ('INPUT', 'input1', 1, (512, 512)), ('INPUT', 'input2', 1, (512, 512)), ('INPUT', 'input3', 1, (512, 512)), ('INPUT', 'input4', 1, (2048, 64)), ('INPUT', 'input5', 1, (2048, 64)), ('INPUT', 'input6', 1, (2, 1024, 512)), ('INPUT', 'input7', 7, (1, 1024)), ('INPUT', 'input8', 1, (2, 1, 1024, 1024)), ('INIT', 'init1_s_', 1, ()), ('INIT', 'init7_s1_0', 7, (1,)), ('INIT', 'init7_s1_1', 7, (1,)), ('INIT', 'init7_s1_1024', 7, (1,)), ('INIT', 'init7_s1_3', 7, (1,)), ('INIT', 'init7_s1_32', 7, (1,)), ('INIT', 'init7_s1_9223372036854775807', 7, (1,)), ('INIT', 'init7_s2_2048_512', 7, (2,)), ('INIT', 'init7_s3_16_1024_1024', 7, (3,)), ('INIT', 'init7_s3_16_1024_64', 7, (3,)), ('INIT', 'init7_s3_16_64_1024', 7, (3,)), ('INIT', 'init7_s3_2_1024_512', 7, (3,)), ('INIT', 'init7_s4_2_1024_8_64', 7, (4,))]




.. GENERATED FROM PYTHON SOURCE LINES 322-330

Optimization and Verification
+++++++++++++++++++++++++++++

Let's try the model with a python backend (reference implementation).
First step, onnx-script uses many functions. The reference evaluation expects
every function to be defined so the order of functions in the model matters.
No recursivity is allowed by this runtime. We need to reorder as function Rank is usually placed
at the end of the model.

.. GENERATED FROM PYTHON SOURCE LINES 330-333

.. code-block:: Python


    reorder_functions_in_proto(model_onnxrt)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    'dump_models/llama_onnxrt_0.onnx'



.. GENERATED FROM PYTHON SOURCE LINES 334-335

Let's load the model and optimize them.

.. GENERATED FROM PYTHON SOURCE LINES 335-343

.. code-block:: Python


    debug = onnx.load(model_debug)
    try:
        onnxrt = optimize_model_proto(onnx.load(model_onnxrt))
    except ImportError as e:
        print("missing library", e)
        onnxrt = debug





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Applied 0 pattern rewrite rules.
    Applied 0 pattern rewrite rules.




.. GENERATED FROM PYTHON SOURCE LINES 344-345

Let's apply onnxruntime optimization

.. GENERATED FROM PYTHON SOURCE LINES 345-364

.. code-block:: Python


    if ortopt:
        providers = (
            [("CUDAExecutionProvider", {}), ("CPUExecutionProvider", {})]
            if use_cuda
            else ["CPUExecutionProvider"]
        )
        with open(model_onnxrt.replace(".onnx", ".before.opt.onnx"), "wb") as f:
            f.write(onnxrt.SerializeToString())
        print(f"run onnxruntime optimization on {model_onnxrt}")
        optimized = model_onnxrt.replace(".onnx", ".opt.onnx")
        ort_optimize(onnxrt, output=optimized, providers=providers)
        onnxrt = onnx.load(optimized)

        print(f"run onnxruntime optimization on {model_debug}")
        optimized = model_debug.replace(".onnx", ".opt.onnx")
        ort_optimize(debug, output=optimized, disable_aot=True, providers=providers)
        debug = onnx.load(optimized)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run onnxruntime optimization on dump_models/llama_onnxrt_0.onnx
    run onnxruntime optimization on dump_models/llama_debug_0.onnx




.. GENERATED FROM PYTHON SOURCE LINES 365-366

For what's following, we need to build two lists of matching inputs.

.. GENERATED FROM PYTHON SOURCE LINES 366-372

.. code-block:: Python


    print("build_matching_inputs")
    feedsrt = build_matching_inputs(model_debug, feeds, model_onnxrt)
    print("done")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    build_matching_inputs
    done




.. GENERATED FROM PYTHON SOURCE LINES 373-374

We check both models are running.

.. GENERATED FROM PYTHON SOURCE LINES 374-382

.. code-block:: Python


    out_onnxrt = ExtendedReferenceEvaluator(onnxrt).run(None, feedsrt)
    out_debug = ExtendedReferenceEvaluator(debug).run(None, feeds)
    assert out_onnxrt
    assert out_debug

    # assert_all_close(out_onnxrt, out_debug)








.. GENERATED FROM PYTHON SOURCE LINES 383-384

Side by side

.. GENERATED FROM PYTHON SOURCE LINES 384-395

.. code-block:: Python



    res1, res2, align, dc = compare_onnx_execution(
        onnxrt,
        debug,
        verbose=1,
        raise_exc=True,
        inputs=(feedsrt, feeds),
    )
    text = dc.to_str(res1, res2, align, column_size=90)
    print(text)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] execute with 2 inputs
    [compare_onnx_execution] execute first model
    [compare_onnx_execution] got 104 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 81 results
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 110 pairs
    [compare_onnx_execution] done
    001 ~ | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_5_token_187      | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_0_token_113     
    002 ~ | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_2_token_183      | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_2_token_119     
    003 - | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_1_token_180      |                                                                                           
    004 - | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_3_token_189      |                                                                                           
    005 - | INITIA int64    1:2                  USAA                 ortshared_7_1_2_1_token_181      |                                                                                           
    006 ~ | INITIA int64    1:1                  AAAA                 ortshared_7_1_1_3_token_184      | INITIA int64    1:1                  BAAA                 ortshared_7_1_1_0_token_114     
    007 = | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_1_token_176      | INITIA int64    1:1                  KAAA                 ortshared_7_1_1_1_token_118     
    008 ~ | INITIA int64    1:4                  CKIM                 ortshared_7_1_4_2_token_182      | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_4_token_122     
    009 - | INITIA float32                       IAAA                 ortshared_1_0_1_0_token_179      |                                                                                           
    010 - | INITIA float32                       BAAA                 ortshared_1_0_1_1_token_188      |                                                                                           
    011 - | INITIA int64                         ZAAA                 ortshared_7_0_1_1_token_190      |                                                                                           
    012 - | INITIA int64                         BAAA                 ortshared_7_0_1_0_token_175      |                                                                                           
    013 ~ | INITIA int64    1:4                  CIKK                 ortshared_7_1_4_1_token_177      | INITIA int64    1:1                  GAAA                 ortshared_7_1_1_3_token_121     
    014 ~ | INITIA int64    1:4                  CIKM                 ortshared_7_1_4_0_token_173      | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_5_token_123     
    015 ~ | INITIA int64    1:1                  ?AAA                 ortshared_7_1_1_4_token_186      | INITIA int64    1:2                  USAA                 ortshared_7_1_2_0_token_125     
    016 ~ | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_0_token_172      | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_3_token_124     
    017 ~ | INITIA int64    1:3                  QKKA                 ortshared_7_1_3_2_token_185      | INITIA int64    1:3                  QKMA                 ortshared_7_1_3_0_token_115     
    018 ~ | INITIA int64    1:1                  DAAA                 ortshared_7_1_1_0_token_174      | INITIA int64    1:3                  QMKA                 ortshared_7_1_3_1_token_116     
    019 ~ | INITIA int64    1:2                  BKAA                 ortshared_7_1_2_0_token_178      | INITIA int64    1:3                  CKSA                 ortshared_7_1_3_2_token_120     
    020 - | INPUT  float32  2:2048x64            MDRB                 primals_6                        |                                                                                           
    021 = | INPUT  float32  2:512x512            QAYQ                 primals_1                        | INPUT  float32  2:512x512            QAYQ                 input0                          
    022 - | INPUT  float32  3:2x1024x512         GRQM                 primals_7                        |                                                                                           
    023 = | INPUT  float32  2:512x512            IQMD                 primals_2                        | INPUT  float32  2:512x512            IQMD                 input1                          
    024 = | INPUT  float32  2:512x512            VBHY                 primals_4                        | INPUT  float32  2:512x512            VBHY                 input2                          
    025 = | INPUT  float32  2:512x512            GHVD                 primals_3                        | INPUT  float32  2:512x512            GHVD                 input3                          
    026 + |                                                                                            | INPUT  float32  2:2048x64            MDRB                 input4                           
    027 = | INPUT  float32  2:2048x64            ZHDU                 primals_5                        | INPUT  float32  2:2048x64            ZHDU                 input5                          
    028 + |                                                                                            | INPUT  float32  3:2x1024x512         GRQM                 input6                           
    029 = | INPUT  int64    2:1x1024             KAQG                 primals_8                        | INPUT  int64    2:1x1024             KAQG                 input7                          
    030 = | INPUT  float32  4:2x1x1024x1024      AAAA                 primals_9                        | INPUT  float32  4:2x1x1024x1024      AAAA                 input8                          
    031 - | RESULT float32  2:512x512            VBHY Identity        t_6                              |                                                                                           
    032 - | RESULT float32  4:2x1x1024x1024      AAAA Mul             _inlfunc_aten_add|folded_2_other |                                                                                           
    033 - | RESULT int64    2:1x1024             KAQG Expand          _val_52                          |                                                                                           
    034 - | RESULT int64    3:1x1024x1           KAQG Unsqueeze       _val_54                          |                                                                                           
    035 - | RESULT int64    3:1x1024x1           KAQG Concat          _val_55                          |                                                                                           
    036 ~ | RESULT float32  2:1024x64            CJYF Slice           slice_2                          | RESULT float32  2:1024x64            GSEC Slice           slice_2                         
    037 - | RESULT float32  2:1024x64            CJYF Transpose       _val_49                          |                                                                                           
    038 ~ | RESULT float32  3:1x1024x64          CJYF GatherND        _val_56                          | RESULT float32  3:1x1024x64          GSEC Gather          index_1                         
    039 ~ | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       _token_9                         | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       output_5                        
    040 ~ | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_10_out0          | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_4_out0          
    041 = | RESULT float32  2:2048x512           GRQM Reshape         view                             | RESULT float32  2:2048x512           GRQM Reshape         output_2                        
    042 ~ | RESULT float32  2:2048x512           FBYW FusedMatMul     mm_1                             | RESULT float32  2:2048x512           FBYW Gemm            mm_1                            
    043 - | RESULT float32  3:2x1024x512         FBYW Reshape         view_3                           |                                                                                           
    044 = | RESULT float32  4:2x1024x8x64        FBYW Reshape         view_7                           | RESULT float32  4:2x1024x8x64        FBYW Reshape         view_7                          
    045 = | RESULT float32  4:2x1024x8x32        WKLE Slice           _token_0                         | RESULT float32  4:2x1024x8x32        WKLE Slice           slice_Tensor6                   
    046 = | RESULT float32  4:2x1024x8x32        EQPW Neg             _token_2                         | RESULT float32  4:2x1024x8x32        EQPW Neg             neg2                            
    047 = | RESULT float32  4:2x1024x8x32        IQMS Slice           _token_5                         | RESULT float32  4:2x1024x8x32        IQMS Slice           slice_Tensor5                   
    048 = | RESULT float32  4:2x1024x8x64        MHAP Concat          _token_7                         | RESULT float32  4:2x1024x8x64        MHAP Concat          cat2                            
    049 ~ | RESULT float32  4:2x1024x8x64        GJGB Mul             _token_12                        | RESULT float32  4:2x1024x8x64        XWIK Mul             mul4                            
    050 ~ | RESULT float32  2:1024x64            GSEC Slice           slice_1                          | RESULT float32  2:1024x64            CJYF Slice           slice_1                         
    051 - | RESULT float32  2:1024x64            GSEC Transpose       _val_62                          |                                                                                           
    052 ~ | RESULT float32  3:1x1024x64          GSEC GatherND        _val_69                          | RESULT float32  3:1x1024x64          CJYF Gather          index                           
    053 ~ | RESULT float32  4:1x1x1024x64        GSEC Unsqueeze       _token_14                        | RESULT float32  4:1x1x1024x64        CJYF Unsqueeze       output_4                        
    054 ~ | RESULT float32  4:1x1024x1x64        GSEC Transpose       Transpose_token_15_out0          | RESULT float32  4:1x1024x1x64        CJYF Transpose       Transpose_token_6_out0          
    055 ~ | RESULT float32  4:2x1024x8x64        XAFI Mul             _token_17                        | RESULT float32  4:2x1024x8x64        MTYK Mul             mul3                            
    056 ~ | RESULT float32  4:2x1024x8x64        CKLI Add             _token_19                        | RESULT float32  4:2x1024x8x64        JPGU Add             add_Tensor2                     
    057 ~ | RESULT float32  4:2x8x64x1024        JDAT Transpose       transpose_3                      | RESULT float32  4:2x8x64x1024        VCBZ Transpose       transpose_3                     
    058 ~ | RESULT float32  3:16x64x1024         JDAT Reshape         view_10                          | RESULT float32  2:2048x512           GRQM Reshape         output_1                        
    059 - | RESULT float32  4:1x1x1024x64        CJYF Transpose       unsqueeze_1                      |                                                                                           
    060 ~ | RESULT float32  2:2048x512           SEFW FusedMatMul     mm                               | RESULT float32  2:2048x512           SEFW Gemm            mm                              
    061 - | RESULT float32  3:2x1024x512         SEFW Reshape         view_1                           |                                                                                           
    062 = | RESULT float32  4:2x1024x8x64        SEFW Reshape         view_6                           | RESULT float32  4:2x1024x8x64        SEFW Reshape         view_6                          
    063 = | RESULT float32  4:2x8x1024x64        FQZC Transpose       transpose                        | RESULT float32  4:2x8x1024x64        FQZC Transpose       transpose                       
    064 = | RESULT float32  4:2x8x1024x32        RKIX Slice           slice_4                          | RESULT float32  4:2x8x1024x32        RKIX Slice           slice_4                         
    065 = | RESULT float32  4:2x8x1024x32        JQSD Neg             neg                              | RESULT float32  4:2x8x1024x32        JQSD Neg             neg                             
    066 = | RESULT float32  4:2x8x1024x32        PHRF Slice           slice_3                          | RESULT float32  4:2x8x1024x32        PHRF Slice           slice_3                         
    067 = | RESULT float32  4:2x8x1024x64        YWJI Concat          cat                              | RESULT float32  4:2x8x1024x64        YWJI Concat          cat                             
    068 ~ | RESULT float32  4:2x8x1024x64        FDYC Mul             mul_1                            | RESULT float32  4:2x8x1024x64        ACLL Mul             mul_1                           
    069 - | RESULT float32  4:1x1x1024x64        GSEC Transpose       unsqueeze                        |                                                                                           
    070 ~ | RESULT float32  4:2x8x1024x64        WSVJ Mul             mul                              | RESULT float32  4:2x8x1024x64        FUHC Mul             mul                             
    071 ~ | RESULT float32  4:2x8x1024x64        CUSK Add             add                              | RESULT float32  4:2x8x1024x64        FWSO Add             add                             
    072 - | RESULT float32  3:16x1024x64         CUSK Reshape         view_9                           |                                                                                           
    073 - | RESULT float32  3:16x1024x1024       NLQF MatMul          bmm                              |                                                                                           
    074 ~ | RESULT float32  4:2x8x1024x1024      NLQF Reshape         view_11                          | RESULT float32  4:2x8x1024x1024      XKZT FusedMatMul     div                             
    075 - | RESULT float32  4:2x8x1024x1024      OBMH Div             div                              |                                                                                           
    076 ~ | RESULT float32  4:2x8x1024x1024      OBMH Add             add_2                            | RESULT float32  4:2x8x1024x1024      XKZT Add             add_2                           
    077 ~ | RESULT float32  4:2x8x1024x1024      ONOO Softmax         aten_softmax_no_dtype_231_result | RESULT float32  4:2x8x1024x1024      NNNN Softmax         output_8                        
    078 - | RESULT float32  3:16x1024x1024       ONOO Reshape         view_12                          |                                                                                           
    079 ~ | RESULT float32  2:2048x512           SWSA FusedMatMul     mm_2                             | RESULT float32  2:2048x512           GRQM Reshape         output_3                        
    080 ~ | RESULT float32  3:2x1024x512         SWSA Reshape         view_5                           | RESULT float32  2:2048x512           GLOU Gemm            mm_2                            
    081 ~ | RESULT float32  4:2x1024x8x64        SWSA Reshape         view_8                           | RESULT float32  4:2x1024x8x64        GLOU Reshape         view_8                          
    082 ~ | RESULT float32  4:2x8x1024x64        IGZR Transpose       transpose_2                      | RESULT float32  4:2x8x1024x64        NEMV Transpose       transpose_2                     
    083 ~ | RESULT float32  3:16x1024x64         IGZR Reshape         view_13                          | RESULT float32  4:2x8x1024x64        PEQS MatMul          view_11                         
    084 ~ | RESULT float32  3:16x1024x64         ZEWQ MatMul          bmm_1                            | RESULT float32  4:2x1024x8x64        HMPT Transpose       transpose_4                     
    085 ~ | RESULT float32  4:2x8x1024x64        ZEWQ Reshape         view_14                          | RESULT float32  2:2048x512           HMPT Reshape         output_12                       
    086 ~ | RESULT float32  4:2x1024x8x64        OPJE Transpose       transpose_4                      | RESULT float32  2:2048x512           UWEC Gemm            mm_3                            
    087 ~ | RESULT float32  3:2x1024x512         OPJE Reshape         view_15                          | RESULT float32  3:2x1024x512         UWEC Reshape         output_0                        
    088 + |                                                                                            | RESULT float32  2:512x512            OAET Transpose       output_11                        
    089 ~ | RESULT float32  2:2048x512           OPJE Reshape         view_16                          | RESULT float32  3:16x1024x64         NEMV Reshape         output_10                       
    090 - | RESULT float32  2:2048x512           ACQR FusedMatMul     mm_3                             |                                                                                           
    091 - | RESULT float32  3:2x1024x512         ACQR Reshape         view_17                          |                                                                                           
    092 ~ | RESULT float32  3:16x1024x1024       ONOO Transpose       transpose_6                      | RESULT float32  3:16x1024x1024       NNNN Reshape         output_9                        
    093 + |                                                                                            | RESULT float32  3:16x64x1024         VCBZ Reshape         output_7                         
    094 - | RESULT float32  4:2x8x1024x1024      ONOO Identity        detach_3                         |                                                                                           
    095 ~ | RESULT float32  3:16x1024x64         JDAT Transpose       transpose_9                      | RESULT float32  3:16x1024x64         FWSO Reshape         output_6                        
    096 + |                                                                                            | OUTPUT float32  3:2x1024x512         UWEC                 output_0                         
    097 ~ | RESULT float32  3:16x64x1024         CUSK Transpose       transpose_8                      | OUTPUT float32  2:2048x512           GRQM                 output_1                        
    098 ~ | RESULT float32  3:16x64x1024         IGZR Transpose       transpose_7                      | OUTPUT float32  2:2048x512           GRQM                 output_2                        
    099 = | OUTPUT float32  2:2048x512           GRQM                 view                             | OUTPUT float32  2:2048x512           GRQM                 output_3                        
    100 - | OUTPUT float32  2:512x512            VBHY                 t_6                              |                                                                                           
    101 = | OUTPUT float32  4:1x1x1024x64        CJYF                 unsqueeze_1                      | OUTPUT float32  4:1x1x1024x64        CJYF                 output_4                        
    102 = | OUTPUT float32  4:1x1x1024x64        GSEC                 unsqueeze                        | OUTPUT float32  4:1x1x1024x64        GSEC                 output_5                        
    103 ~ | OUTPUT float32  3:16x64x1024         IGZR                 transpose_7                      | OUTPUT float32  3:16x1024x64         FWSO                 output_6                        
    104 ~ | OUTPUT float32  3:16x64x1024         CUSK                 transpose_8                      | OUTPUT float32  3:16x64x1024         VCBZ                 output_7                        
    105 - | OUTPUT float32  3:16x1024x64         JDAT                 transpose_9                      |                                                                                           
    106 ~ | OUTPUT float32  4:2x8x1024x1024      ONOO                 detach_3                         | OUTPUT float32  4:2x8x1024x1024      NNNN                 output_8                        
    107 ~ | OUTPUT float32  3:16x1024x1024       ONOO                 transpose_6                      | OUTPUT float32  3:16x1024x1024       NNNN                 output_9                        
    108 ~ | OUTPUT float32  2:2048x512           OPJE                 view_16                          | OUTPUT float32  3:16x1024x64         NEMV                 output_10                       
    109 + |                                                                                            | OUTPUT float32  2:512x512            OAET                 output_11                        
    110 ~ | OUTPUT float32  3:2x1024x512         ACQR                 view_17                          | OUTPUT float32  2:2048x512           HMPT                 output_12                       





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.019 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_dort.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_dort.ipynb <plot_llama_diff_dort.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_dort.py <plot_llama_diff_dort.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
