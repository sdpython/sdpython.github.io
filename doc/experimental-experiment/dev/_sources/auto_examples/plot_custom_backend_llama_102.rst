
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_custom_backend_llama_102.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_custom_backend_llama_102.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_custom_backend_llama_102.py:


========================================
102: Fuse kernels in a small Llama Model
========================================

This example leverages the function :epkg:`torch.compile` and the ability
to use a custom backend to test the optimization of a model by fusing
simple element-wise kernels.

It takes a small Llama model and uses a backend based on :epkg:`onnxruntime`.
The model is converted into ONNX and then optimized by fusing element-wise
kernels.

::

    python plot_custom_backend_llama --config large

.. GENERATED FROM PYTHON SOURCE LINES 18-34

.. code-block:: Python


    from experimental_experiment.args import get_parsed_args

    script_args = get_parsed_args(
        "plot_custom_backend_llama",
        config=("medium", "large or medium depending, large means closer to the real model"),
        num_hidden_layers=(1, "number of hidden layers"),
        with_mask=(0, "tries with a mask as a secondary input"),
        description=__doc__,
        expose="config,num_hidden_layers,with_mask",
    )

    print(f"config={script_args.config!r}")
    print(f"num_hidden_layers={script_args.num_hidden_layers!r}")
    print(f"with_mask={script_args.with_mask!r}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    config='medium'
    num_hidden_layers=1
    with_mask=0




.. GENERATED FROM PYTHON SOURCE LINES 35-36

Imports.

.. GENERATED FROM PYTHON SOURCE LINES 36-47

.. code-block:: Python


    import time
    import numpy as np
    import pandas
    from tqdm import tqdm
    import torch
    from transformers import LlamaConfig
    from transformers.models.llama.modeling_llama import LlamaModel
    from experimental_experiment.xbuilder import OptimizationOptions
    from experimental_experiment.torch_dynamo import onnx_custom_backend








.. GENERATED FROM PYTHON SOURCE LINES 48-50

The dummy model
===============

.. GENERATED FROM PYTHON SOURCE LINES 50-66

.. code-block:: Python


    has_cuda = torch.cuda.is_available()


    def ids_tensor(shape, vocab_size):
        total_dims = 1
        for dim in shape:
            total_dims *= dim

        values = []
        for _ in range(total_dims):
            values.append(np.random.randint(0, vocab_size - 1))

        return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()









.. GENERATED FROM PYTHON SOURCE LINES 67-68

The size of the input.

.. GENERATED FROM PYTHON SOURCE LINES 68-75

.. code-block:: Python

    if script_args.config == "large":
        batch, seq, vocab_size = 2, 1024, 32000
        intermediate_size = 11008
    else:
        batch, seq, vocab_size = 2, 1024, 1024
        intermediate_size = 1024








.. GENERATED FROM PYTHON SOURCE LINES 76-77

The configuration of the model.

.. GENERATED FROM PYTHON SOURCE LINES 77-88

.. code-block:: Python


    config = LlamaConfig(
        hidden_size=4096,
        num_hidden_layers=int(script_args.num_hidden_layers),
        vocab_size=vocab_size,
        intermediate_size=intermediate_size,
        max_position_embeddings=2048,
        num_attention_heads=32,
    )
    config._attn_implementation = "eager"








.. GENERATED FROM PYTHON SOURCE LINES 89-91

The number of time we run the model to measure
the inference.

.. GENERATED FROM PYTHON SOURCE LINES 91-94

.. code-block:: Python

    warmup = 10 if config == "medium" else 5
    N = 50 if config == "medium" else 25








.. GENERATED FROM PYTHON SOURCE LINES 95-96

Let's create the model with dummy inputs.

.. GENERATED FROM PYTHON SOURCE LINES 96-109

.. code-block:: Python

    model = LlamaModel(config)

    inputs = (ids_tensor([batch, seq], vocab_size),)
    if script_args.with_mask in (1, "1"):
        input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))
        inputs = (*inputs, input_mask)

    processor = "cuda" if has_cuda else "cpu"
    print(f"moving model and inputs to processor={processor!r}")
    model = model.to(processor)
    inputs = tuple(i.to(processor) for i in inputs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    moving model and inputs to processor='cuda'




.. GENERATED FROM PYTHON SOURCE LINES 110-112

Measure of eager mode
=====================

.. GENERATED FROM PYTHON SOURCE LINES 112-135

.. code-block:: Python


    times = []

    with torch.no_grad():

        # warmup
        print("warmup eager")
        for _ in tqdm(range(warmup)):
            # model(input_ids, input_mask)
            model(*inputs)
            torch.cuda.synchronize()

        # repeat
        print("repeat eager")
        begin = time.perf_counter()
        for _ in tqdm(range(N)):
            model(*inputs)
            torch.cuda.synchronize()
        d = (time.perf_counter() - begin) / N
        baseline = d
        times.append(dict(optium="eager", processor=processor, avg_time=d, warmup=warmup, N=N))
        print("avg time eager", d)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    warmup eager
      0%|          | 0/5 [00:00<?, ?it/s]     20%|██        | 1/5 [00:00<00:01,  3.06it/s]     60%|██████    | 3/5 [00:00<00:00,  7.25it/s]    100%|██████████| 5/5 [00:00<00:00,  9.39it/s]    100%|██████████| 5/5 [00:00<00:00,  8.00it/s]
    repeat eager
      0%|          | 0/25 [00:00<?, ?it/s]      8%|▊         | 2/25 [00:00<00:01, 12.87it/s]     16%|█▌        | 4/25 [00:00<00:01, 12.87it/s]     24%|██▍       | 6/25 [00:00<00:01, 12.89it/s]     32%|███▏      | 8/25 [00:00<00:01, 12.73it/s]     40%|████      | 10/25 [00:00<00:01, 12.60it/s]     48%|████▊     | 12/25 [00:00<00:01, 12.69it/s]     56%|█████▌    | 14/25 [00:01<00:00, 12.76it/s]     64%|██████▍   | 16/25 [00:01<00:00, 12.78it/s]     72%|███████▏  | 18/25 [00:01<00:00, 12.84it/s]     80%|████████  | 20/25 [00:01<00:00, 12.84it/s]     88%|████████▊ | 22/25 [00:01<00:00, 12.71it/s]     96%|█████████▌| 24/25 [00:01<00:00, 12.79it/s]    100%|██████████| 25/25 [00:01<00:00, 12.78it/s]
    avg time eager 0.07832281335999142




.. GENERATED FROM PYTHON SOURCE LINES 136-158

Measure with the custom backend
===============================

Three kind of optimization:

- **default**: the onnx model is optimized with less onnx operators
- **default+onnxruntime**: the onnx model is optimized with fused kernels
  implemented by onnxruntime
- **default+onnxruntime+experimental**: the onnx model is optimized with fused kernels
  implemented by onnxruntime and also custom kernels, this does not work on
  CPU.

Some links:

* :class:`experimental_experiment.xbuilder.OptimizationOptions`:
  that class defines the optimizations to apply after the model
  is converted to onnx,
* :func:`experimental_experiment.torch_dynamo.onnx_custom_backend`:
  that function implements the custom backend based on :epkg:`onnxruntime`,
  it converts the model into ONNX, optimizes and runs it,
  it does not support :epkg:`graph break`,
  it does not work well with dynamic shapes yet.

.. GENERATED FROM PYTHON SOURCE LINES 158-228

.. code-block:: Python


    with torch.no_grad():

        for optim in ["default", "default+onnxruntime", "default+onnxruntime+experimental"]:
            print("----------------------")
            print(f"optim={optim}")

            # This variable is used to retrieve the onnx models created by the backend.
            # It can be set to None if it is not needed.
            # Graph are usually small as they do not contain weights.
            storage = {}

            options = OptimizationOptions(
                constant_folding=True,
                patterns=None if optim == "" else optim,
                verbose=0,
                processor=processor.upper(),
            )

            # The backend used here overwrite some of the parameters provided by
            # function onnx_custom_backend.
            custom_custom_backend = lambda *args, optim=optim, options=options, storage=storage, **kwargs: onnx_custom_backend(  # noqa: E731, E501
                *args,
                target_opset=18,
                verbose=0,
                options=options,
                optimize=optim != "",
                storage=storage,
                dump_prefix=f"dump_onx_llama_{optim.replace('+', '_')}",
                **kwargs,
            )

            # The function setting the backend.
            compiled_model = torch.compile(
                model, backend=custom_custom_backend, fullgraph=True, dynamic=False
            )

            # warmup
            print("warmup compiled model")
            for _ in tqdm(range(warmup)):
                compiled_model(*inputs)
                torch.cuda.synchronize()

            # repeat
            print("repeat compiled_model")
            begin = time.perf_counter()
            for _ in tqdm(range(N)):
                compiled_model(*inputs)
                torch.cuda.synchronize()
            d = (time.perf_counter() - begin) / N

            # let's measure the number of custom ops
            n_custom_ops = None
            if storage is not None:
                onnx_model = storage["instance"][0]["onnx"]
                n_custom_ops = len([node for node in onnx_model.graph.node if node.domain != ""])

            times.append(
                dict(
                    optium=optim,
                    processor=processor,
                    avg_time=d,
                    warmup=warmup,
                    N=N,
                    n_custom_ops=n_custom_ops,
                    speedup=baseline / d,
                )
            )
            print(f"avg time custom backend with optimization={optim!r}", d)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ----------------------
    optim=default
    warmup compiled model
      0%|          | 0/5 [00:00<?, ?it/s]     20%|██        | 1/5 [00:01<00:06,  1.60s/it]     80%|████████  | 4/5 [00:01<00:00,  2.95it/s]    100%|██████████| 5/5 [00:01<00:00,  2.81it/s]
    repeat compiled_model
      0%|          | 0/25 [00:00<?, ?it/s]     12%|█▏        | 3/25 [00:00<00:00, 22.28it/s]     24%|██▍       | 6/25 [00:00<00:00, 22.00it/s]     36%|███▌      | 9/25 [00:00<00:00, 22.05it/s]     48%|████▊     | 12/25 [00:00<00:00, 22.08it/s]     60%|██████    | 15/25 [00:00<00:00, 22.05it/s]     72%|███████▏  | 18/25 [00:00<00:00, 21.82it/s]     84%|████████▍ | 21/25 [00:00<00:00, 21.86it/s]     96%|█████████▌| 24/25 [00:01<00:00, 21.93it/s]    100%|██████████| 25/25 [00:01<00:00, 21.97it/s]
    avg time custom backend with optimization='default' 0.045571124720008814
    ----------------------
    optim=default+onnxruntime
    warmup compiled model
      0%|          | 0/5 [00:00<?, ?it/s]     20%|██        | 1/5 [00:00<00:03,  1.14it/s]     80%|████████  | 4/5 [00:01<00:00,  4.97it/s]    100%|██████████| 5/5 [00:01<00:00,  4.75it/s]
    repeat compiled_model
      0%|          | 0/25 [00:00<?, ?it/s]     12%|█▏        | 3/25 [00:00<00:00, 23.19it/s]     24%|██▍       | 6/25 [00:00<00:00, 22.85it/s]     36%|███▌      | 9/25 [00:00<00:00, 22.92it/s]     48%|████▊     | 12/25 [00:00<00:00, 22.97it/s]     60%|██████    | 15/25 [00:00<00:00, 22.91it/s]     72%|███████▏  | 18/25 [00:00<00:00, 22.77it/s]     84%|████████▍ | 21/25 [00:00<00:00, 22.70it/s]     96%|█████████▌| 24/25 [00:01<00:00, 22.66it/s]    100%|██████████| 25/25 [00:01<00:00, 22.78it/s]
    avg time custom backend with optimization='default+onnxruntime' 0.043930231600024856
    ----------------------
    optim=default+onnxruntime+experimental
    warmup compiled model
      0%|          | 0/5 [00:00<?, ?it/s]     20%|██        | 1/5 [00:00<00:03,  1.12it/s]     80%|████████  | 4/5 [00:01<00:00,  4.94it/s]    100%|██████████| 5/5 [00:01<00:00,  4.72it/s]
    repeat compiled_model
      0%|          | 0/25 [00:00<?, ?it/s]     12%|█▏        | 3/25 [00:00<00:00, 23.59it/s]     24%|██▍       | 6/25 [00:00<00:00, 23.53it/s]     36%|███▌      | 9/25 [00:00<00:00, 23.42it/s]     48%|████▊     | 12/25 [00:00<00:00, 23.47it/s]     60%|██████    | 15/25 [00:00<00:00, 23.55it/s]     72%|███████▏  | 18/25 [00:00<00:00, 23.44it/s]     84%|████████▍ | 21/25 [00:00<00:00, 23.34it/s]     96%|█████████▌| 24/25 [00:01<00:00, 23.42it/s]    100%|██████████| 25/25 [00:01<00:00, 23.40it/s]
    avg time custom backend with optimization='default+onnxruntime+experimental' 0.04276459920001798




.. GENERATED FROM PYTHON SOURCE LINES 229-234

Final results
=============

avg_time, lower is better,
speedup compare to eager mode, higher is better.

.. GENERATED FROM PYTHON SOURCE LINES 234-237

.. code-block:: Python


    df = pandas.DataFrame(times)
    print(df)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                 optium processor  avg_time  warmup   N  n_custom_ops   speedup
    0                             eager      cuda  0.078323       5  25           NaN       NaN
    1                           default      cuda  0.045571       5  25           0.0  1.718694
    2               default+onnxruntime      cuda  0.043930       5  25           9.0  1.782891
    3  default+onnxruntime+experimental      cuda  0.042765       5  25          15.0  1.831487





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 16.896 seconds)


.. _sphx_glr_download_auto_examples_plot_custom_backend_llama_102.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_custom_backend_llama_102.ipynb <plot_custom_backend_llama_102.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_custom_backend_llama_102.py <plot_custom_backend_llama_102.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_custom_backend_llama_102.zip <plot_custom_backend_llama_102.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
