
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_torch_export.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_torch_export.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_torch_export.py:


Evaluate different ways to export a torch model to ONNX
=======================================================

The example evaluates the performance of onnxruntime of a simple
torch model after it was converted into ONNX through different processes:

* `TorchScript-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter>`_,
  let's call it **script**
* `TorchDynamo-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter>`_,
  let's call it **dynamo**
* if available, the previous model but optimized, **dynopt**
* a custom exporter **cus_p0**, this exporter supports a very limited
  set of models, as **dynamo**, it relies on
  `torch.fx <https://pytorch.org/docs/stable/fx.html>`_ but the design is closer to
  what tensorflow-onnx does.
* the same exporter but unused nodes were removed, **cus_p1**
* the same exporter but constant where folded, **cus_p2**

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 25-67

.. code-block:: Python

    import itertools
    import os
    import platform
    import pprint
    import multiprocessing
    import time
    import cProfile
    import pstats
    import io
    from pstats import SortKey
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas
    import onnx
    from onnx_extended.ext_test_case import measure_time
    from onnx_array_api.plotting.text_plot import onnx_simple_text_plot
    import torch
    from torch import nn
    import torch.nn.functional as F
    import experimental_experiment
    from experimental_experiment.torch_exp.onnx_export import to_onnx
    from tqdm import tqdm


    def system_info():
        obs = {}
        obs["processor"] = platform.processor()
        obs["cores"] = multiprocessing.cpu_count()
        try:
            obs["cuda"] = 1 if torch.cuda.is_available() else 0
            obs["cuda_count"] = torch.cuda.device_count()
            obs["cuda_name"] = torch.cuda.get_device_name()
            obs["cuda_capa"] = torch.cuda.get_device_capability()
        except RuntimeError:
            # no cuda
            pass
        return obs


    pprint.pprint(system_info())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'cores': 8,
     'cuda': 1,
     'cuda_capa': (6, 1),
     'cuda_count': 1,
     'cuda_name': 'NVIDIA GeForce GTX 1060',
     'processor': 'x86_64'}




.. GENERATED FROM PYTHON SOURCE LINES 68-72

The model
+++++++++

A simple model to convert.

.. GENERATED FROM PYTHON SOURCE LINES 72-93

.. code-block:: Python



    class MyModel(nn.Module):
        def __init__(self):
            super(MyModel, self).__init__()
            self.conv1 = nn.Conv2d(1, 128, 5)
            self.conv2 = nn.Conv2d(128, 16, 5)
            self.fc1 = nn.Linear(13456, 1024)
            self.fc2 = nn.Linear(1024, 128)
            self.fc3 = nn.Linear(128, 10)

        def forward(self, x):
            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
            x = F.max_pool2d(F.relu(self.conv2(x)), 2)
            x = torch.flatten(x, 1)
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            return x









.. GENERATED FROM PYTHON SOURCE LINES 94-96

The exporters
+++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 96-143

.. code-block:: Python



    def export_script(filename, model, *args):
        torch.onnx.export(model, *args, filename, input_names=["input"])


    def export_dynamo(filename, model, *args):
        export_output = torch.onnx.dynamo_export(model, *args)
        export_output.save(filename)


    def export_dynopt(filename, model, *args):
        export_output = torch.onnx.dynamo_export(model, *args)
        export_output.save(filename)
        model_onnx = onnx.load(filename)

        from onnxrewriter.optimizer import optimize

        optimized_model = optimize(model_onnx)
        with open(filename, "wb") as f:
            f.write(optimized_model.SerializeToString())


    def export_cus_p0(filename, model, *args):
        onx = to_onnx(model, tuple(args), input_names=["input"])
        with open(filename, "wb") as f:
            f.write(onx.SerializeToString())


    def export_cus_p1(filename, model, *args):
        onx = to_onnx(model, tuple(args), input_names=["input"], remove_unused=True)
        with open(filename, "wb") as f:
            f.write(onx.SerializeToString())


    def export_cus_p2(filename, model, *args):
        onx = to_onnx(
            model,
            tuple(args),
            input_names=["input"],
            remove_unused=True,
            constant_folding=True,
        )
        with open(filename, "wb") as f:
            f.write(onx.SerializeToString())









.. GENERATED FROM PYTHON SOURCE LINES 144-145

Let's check they are working.

.. GENERATED FROM PYTHON SOURCE LINES 145-173

.. code-block:: Python


    export_functions = [
        export_script,
        export_dynamo,
        export_dynopt,
        export_cus_p0,
        export_cus_p1,
        export_cus_p2,
    ]

    exporters = {f.__name__.replace("export_", ""): f for f in export_functions}
    shape = [1, 1, 128, 128]
    input_tensor = torch.rand(*shape).to(torch.float32)
    model = MyModel()

    supported_exporters = {}
    for k, v in exporters.items():
        print(f"run exporter {k}")
        filename = f"plot_torch_export_{k}.onnx"
        try:
            v(filename, model, input_tensor)
        except Exception as e:
            print(f"skipped due to {e}")
            continue
        supported_exporters[k] = v
        print("done.")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run exporter script
    [2023-12-05 14:03:15,850] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
    done.
    run exporter dynamo
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    done.
    run exporter dynopt
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    done.
    run exporter cus_p0
    done.
    run exporter cus_p1
    done.
    run exporter cus_p2
    done.




.. GENERATED FROM PYTHON SOURCE LINES 174-176

Exporter speed
++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 176-204

.. code-block:: Python


    data = []

    for k, v in supported_exporters.items():
        print(f"run exporter {k}")
        filename = f"plot_torch_export_{k}.onnx"
        times = []
        for i in range(5):
            begin = time.perf_counter()
            v(filename, model, input_tensor)
            duration = time.perf_counter() - begin
            times.append(duration)
        onx = onnx.load(filename)
        print("done.")
        data.append(
            dict(
                export=k,
                time=np.mean(duration),
                min=min(times),
                max=max(times),
                first=times[0],
                last=times[-1],
                std=np.std(times),
                nodes=len(onx.graph.node),
            )
        )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    run exporter script
    done.
    run exporter dynamo
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    done.
    run exporter dynopt
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    /home/xadupre/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.
      warnings.warn(
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue result_1 due to large size 55115776.
    WARNING:onnxrewriter.optimizer.constant_folding:Skip storing constant folded nvalue t due to large size 55115776.
    done.
    run exporter cus_p0
    done.
    run exporter cus_p1
    done.
    run exporter cus_p2
    done.




.. GENERATED FROM PYTHON SOURCE LINES 205-208

The last export to measure time torch spends in export the model
before any other export can begin the translation
except the first one.

.. GENERATED FROM PYTHON SOURCE LINES 208-215

.. code-block:: Python


    begin = time.perf_counter()
    for i in range(5):
        exported_mod = torch.export.export(model, (input_tensor,))
    duration = time.perf_counter() - begin
    data.append(dict(export="torch", time=duration / 5))








.. GENERATED FROM PYTHON SOURCE LINES 216-217

The result.

.. GENERATED FROM PYTHON SOURCE LINES 217-226

.. code-block:: Python

    df1 = pandas.DataFrame(data)
    print(df1)

    fig, ax = plt.subplots(1, 1)
    dfi = df1[["export", "time", "std"]].set_index("export")
    dfi["time"].plot.barh(ax=ax, title="Export time", yerr=dfi["std"])
    fig.tight_layout()
    fig.savefig("plot_torch_export.png")




.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_export_001.png
   :alt: Export time
   :srcset: /auto_examples/images/sphx_glr_plot_torch_export_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

       export      time       min       max     first      last       std  nodes
    0  script  0.195931  0.183295  0.257570  0.210905  0.195931  0.025777   12.0
    1  dynamo  0.625513  0.488551  0.786897  0.656923  0.625513  0.109937   13.0
    2  dynopt  0.858001  0.814010  0.936153  0.885437  0.858001  0.039646   13.0
    3  cus_p0  0.460740  0.360371  0.460740  0.360371  0.460740  0.037929   27.0
    4  cus_p1  0.350459  0.350459  0.452602  0.362487  0.350459  0.039809   15.0
    5  cus_p2  0.459033  0.396157  0.486254  0.416429  0.459033  0.032243   12.0
    6   torch  0.214853       NaN       NaN       NaN       NaN       NaN    NaN




.. GENERATED FROM PYTHON SOURCE LINES 227-229

Profiling
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 229-264

.. code-block:: Python


    pr = cProfile.Profile()
    pr.enable()
    for i in range(5):
        export_cus_p0("dummy.onnx", model, input_tensor)
    pr.disable()
    s = io.StringIO()
    sortby = SortKey.CUMULATIVE
    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
    ps.print_stats()


    def clean_text(text):
        pathes = [
            os.path.abspath(
                os.path.normpath(os.path.join(os.path.dirname(torch.__file__), ".."))
            ),
            os.path.abspath(
                os.path.normpath(os.path.join(os.path.dirname(onnx.__file__), ".."))
            ),
            os.path.abspath(
                os.path.normpath(
                    os.path.join(os.path.dirname(experimental_experiment.__file__), "..")
                )
            ),
        ]
        for p in pathes:
            text = text.replace(p, "")
        text = text.replace("experimental_experiment", "experimental_experiment".upper())
        return text


    text = "\n".join(s.getvalue().split("\n")[:200])
    print(clean_text(text))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

             1782086 function calls (1681746 primitive calls) in 4.993 seconds

       Ordered by: cumulative time

       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
            5    0.002    0.000    5.138    1.028 /_doc/examples/plot_torch_export.py:119(export_cus_p0)
            5    0.003    0.001    4.799    0.960 /EXPERIMENTAL_EXPERIMENT/torch_exp/onnx_export.py:8(to_onnx)
            5    0.000    0.000    3.930    0.786 /torch/export/__init__.py:930(export)
            5    0.002    0.000    3.930    0.786 /torch/_export/__init__.py:212(export)
        15/10    0.001    0.000    2.819    0.282 /torch/_dynamo/utils.py:182(time_wrapper)
        20/10    0.000    0.000    2.732    0.273 /torch/_dynamo/eval_frame.py:307(_fn)
    3520/1655    0.012    0.000    2.595    0.002 /torch/utils/_stats.py:15(wrapper)
       115/55    0.001    0.000    2.482    0.045 /torch/nn/modules/module.py:1514(_wrapped_call_impl)
       115/55    0.001    0.000    2.482    0.045 /torch/nn/modules/module.py:1520(_call_impl)
    2910/1810    0.018    0.000    2.121    0.001 /torch/_subclasses/fake_tensor.py:1246(__torch_dispatch__)
    2910/1810    0.137    0.000    2.088    0.001 /torch/_subclasses/fake_tensor.py:1270(dispatch)
            5    0.001    0.000    1.692    0.338 /torch/_dynamo/eval_frame.py:1028(inner)
           20    0.004    0.000    1.691    0.085 /torch/fx/interpreter.py:99(run)
          355    0.004    0.000    1.670    0.005 /torch/fx/interpreter.py:177(run_node)
            5    0.000    0.000    1.590    0.318 /torch/_functorch/aot_autograd.py:3914(aot_export_module)
            5    0.000    0.000    1.586    0.317 /torch/_functorch/aot_autograd.py:4164(_aot_export_function)
            5    0.001    0.000    1.583    0.317 /torch/_functorch/aot_autograd.py:3277(create_aot_dispatcher_function)
        15/10    0.000    0.000    1.491    0.149 /torch/_dynamo/external_utils.py:15(inner)
           10    0.000    0.000    1.243    0.124 /torch/_functorch/aot_autograd.py:3519(flat_fn)
           10    0.001    0.000    1.241    0.124 /torch/_functorch/aot_autograd.py:3486(functional_call)
            5    0.000    0.000    1.240    0.248 /torch/_dynamo/eval_frame.py:456(catch_errors)
            5    0.000    0.000    1.240    0.248 /torch/_dynamo/convert_frame.py:122(_fn)
            5    0.000    0.000    1.237    0.247 /torch/_dynamo/convert_frame.py:249(_convert_frame_assert)
            5    0.000    0.000    1.236    0.247 /torch/_dynamo/convert_frame.py:414(_compile)
            5    0.000    0.000    1.234    0.247 /torch/_dynamo/convert_frame.py:481(compile_inner)
            5    0.000    0.000    1.160    0.232 /torch/_functorch/aot_autograd.py:2182(aot_wrapper_dedupe)
            5    0.000    0.000    1.160    0.232 /torch/_functorch/aot_autograd.py:2375(aot_wrapper_synthetic_base)
            5    0.000    0.000    1.159    0.232 /torch/_functorch/aot_autograd.py:1518(aot_dispatch_base_graph)
            5    0.000    0.000    1.121    0.224 /torch/_functorch/aot_autograd.py:1349(create_functionalized_graph)
            5    0.000    0.000    1.121    0.224 /torch/fx/experimental/proxy_tensor.py:721(wrapped)
    2590/1295    0.007    0.000    1.118    0.001 /torch/_ops.py:447(__call__)
            5    0.000    0.000    1.114    0.223 /torch/_compile.py:20(inner)
            5    0.000    0.000    1.113    0.223 /torch/fx/experimental/proxy_tensor.py:462(dispatch_trace)
    45710/11325    0.224    0.000    1.094    0.000 /torch/utils/_pytree.py:230(tree_flatten)
            5    0.000    0.000    1.085    0.217 /torch/fx/_symbolic_trace.py:695(trace)
            5    0.000    0.000    1.058    0.212 /torch/fx/experimental/proxy_tensor.py:477(wrapped)
            5    0.000    0.000    1.046    0.209 /torch/_dynamo/bytecode_transformation.py:1020(transform_code_object)
            5    0.000    0.000    1.038    0.208 /torch/_dynamo/convert_frame.py:439(transform)
            5    0.000    0.000    1.000    0.200 /torch/_dynamo/symbolic_convert.py:2068(run)
            5    0.001    0.000    1.000    0.200 /torch/_dynamo/symbolic_convert.py:712(run)
          265    0.005    0.000    0.998    0.004 /torch/_dynamo/symbolic_convert.py:617(step)
            5    0.000    0.000    0.920    0.184 /torch/_functorch/aot_autograd.py:1411(fwd_helper)
            5    0.000    0.000    0.920    0.184 /torch/_functorch/aot_autograd.py:1357(functionalized_f_helper)
      600/175    0.003    0.000    0.911    0.005 /torch/_prims_common/wrappers.py:221(_fn)
            5    0.000    0.000    0.908    0.182 /torch/_functorch/aot_autograd.py:1164(inner_fn)
           60    0.001    0.000    0.890    0.015 /torch/nn/modules/linear.py:113(forward)
           60    0.013    0.000    0.889    0.015 {built-in method torch._C._nn.linear}
      410/380    0.003    0.000    0.884    0.002 /torch/fx/experimental/proxy_tensor.py:552(__torch_dispatch__)
    5660/3350    0.022    0.000    0.879    0.000 /torch/utils/_pytree.py:281(tree_map)
      410/380    0.001    0.000    0.871    0.002 /torch/fx/experimental/proxy_tensor.py:573(inner_torch_dispatch)
           50    0.000    0.000    0.867    0.017 /torch/fx/interpreter.py:291(call_module)
       105/75    0.006    0.000    0.860    0.011 /torch/fx/experimental/proxy_tensor.py:243(proxy_call)
            5    0.000    0.000    0.838    0.168 /EXPERIMENTAL_EXPERIMENT/torch_exp/graph_builder.py:325(to_onnx)
           60    0.000    0.000    0.838    0.014 /torch/_dynamo/symbolic_convert.py:384(wrapper)
           60    0.000    0.000    0.832    0.014 /torch/_dynamo/symbolic_convert.py:1106(CALL_FUNCTION)
           60    0.001    0.000    0.830    0.014 /torch/_dynamo/symbolic_convert.py:537(call_function)
           65    0.000    0.000    0.797    0.012 /torch/_dynamo/variables/builder.py:1190(wrap_fx_proxy)
           65    0.004    0.000    0.796    0.012 /torch/_dynamo/variables/builder.py:1240(wrap_fx_proxy_cls)
           75    0.002    0.000    0.761    0.010 /torch/_decomp/decompositions.py:48(inner)
           60    0.003    0.000    0.711    0.012 /torch/_dynamo/utils.py:1291(get_fake_value)
           90    0.000    0.000    0.699    0.008 /torch/_dynamo/utils.py:914(wrap_fake_exception)
           25    0.000    0.000    0.632    0.025 /torch/fx/_symbolic_trace.py:785(module_call_wrapper)
           25    0.000    0.000    0.630    0.025 /torch/fx/experimental/proxy_tensor.py:422(call_module)
           25    0.000    0.000    0.630    0.025 /torch/fx/_symbolic_trace.py:787(forward)
          155    0.001    0.000    0.621    0.004 /torch/fx/interpreter.py:249(call_function)
           25    0.001    0.000    0.619    0.025 /torch/_dynamo/variables/nn_module.py:243(call_function)
           10    0.000    0.000    0.590    0.059 /torch/export/__init__.py:395(_transform)
    5660/3350    0.013    0.000    0.547    0.000 /torch/utils/_pytree.py:283(<listcomp>)
           10    0.000    0.000    0.525    0.052 /torch/fx/passes/infra/pass_manager.py:242(__call__)
           75    0.008    0.000    0.498    0.007 /torch/_decomp/decompositions.py:1222(addmm)
           10    0.000    0.000    0.483    0.048 /torch/fx/passes/infra/pass_base.py:34(__call__)
            5    0.000    0.000    0.482    0.096 /torch/_export/passes/add_runtime_assertions_for_constraints_pass.py:138(call)
            5    0.000    0.000    0.476    0.095 /torch/_export/pass_base.py:400(call)
            5    0.000    0.000    0.475    0.095 /torch/_export/pass_base.py:376(call_submodule)
         4410    0.012    0.000    0.475    0.000 /torch/_subclasses/fake_tensor.py:207(tree_flatten_only)
      310/260    0.010    0.000    0.440    0.002 {method 'detach' of 'torch._C._TensorBase' objects}
         1920    0.009    0.000    0.440    0.000 /torch/utils/_pytree.py:352(tree_map_only)
           40    0.000    0.000    0.434    0.011 /torch/nn/modules/conv.py:459(forward)
           40    0.000    0.000    0.433    0.011 /torch/nn/modules/conv.py:451(_conv_forward)
          145    0.001    0.000    0.433    0.003 /torch/_export/pass_base.py:230(run_node)
           40    0.007    0.000    0.433    0.011 {built-in method torch.conv2d}
           90    0.002    0.000    0.387    0.004 /torch/_export/pass_base.py:244(_fx)
           85    0.001    0.000    0.387    0.005 /torch/_export/pass_base.py:173(call_function)
            5    0.002    0.000    0.380    0.076 /EXPERIMENTAL_EXPERIMENT/torch_exp/graph_builder.py:305(_build_initializers)
            5    0.001    0.000    0.378    0.076 /torch/_dynamo/eval_frame.py:1075(result_capturing_wrapper)
           50    0.149    0.003    0.378    0.008 /EXPERIMENTAL_EXPERIMENT/torch_exp/graph_builder.py:270(from_array)
           75    0.001    0.000    0.376    0.005 /torch/_export/passes/add_runtime_assertions_for_constraints_pass.py:84(call_operator)
           75    0.000    0.000    0.374    0.005 /torch/_export/pass_base.py:312(call_operator)
        20520    0.258    0.000    0.371    0.000 /torch/utils/_pytree.py:223(__init__)
           60    0.000    0.000    0.368    0.006 /torch/_dynamo/utils.py:1338(<lambda>)
           60    0.000    0.000    0.368    0.006 /torch/_dynamo/utils.py:1379(run_node)
           80    0.000    0.000    0.351    0.004 /torch/nn/functional.py:1460(relu)
           80    0.008    0.000    0.351    0.004 {built-in method torch.relu}
            5    0.001    0.000    0.343    0.069 /torch/_functorch/aot_autograd.py:742(inner)
            5    0.000    0.000    0.343    0.069 /torch/_functorch/functional_call.py:10(functional_call)
            5    0.000    0.000    0.343    0.069 /torch/nn/utils/stateless.py:230(_functional_call)
            5    0.000    0.000    0.340    0.068 /torch/fx/graph_module.py:677(call_wrapped)
            5    0.000    0.000    0.340    0.068 /torch/fx/graph_module.py:269(__call__)
      655/250    0.003    0.000    0.333    0.001 /usr/lib/python3.10/copy.py:259(_reconstruct)
      2405/70    0.012    0.000    0.332    0.005 /usr/lib/python3.10/copy.py:128(deepcopy)
       105/45    0.002    0.000    0.327    0.007 /usr/lib/python3.10/copy.py:227(_deepcopy_dict)
           25    0.000    0.000    0.325    0.013 /torch/_dynamo/utils.py:925(deepcopy_to_fake_tensor)
           25    0.000    0.000    0.324    0.013 /torch/_dynamo/utils.py:927(<lambda>)
         1430    0.004    0.000    0.315    0.000 /torch/_subclasses/fake_tensor.py:1569(validate_and_convert_non_fake_tensors)
           50    0.002    0.000    0.302    0.006 /torch/nn/parameter.py:54(__deepcopy__)
          250    0.002    0.000    0.297    0.001 /torch/_subclasses/fake_tensor.py:1799(__torch_function__)
         1005    0.006    0.000    0.296    0.000 /torch/_subclasses/fake_tensor.py:1604(wrap_meta_outputs_with_default_device_logic)
            5    0.000    0.000    0.292    0.058 /onnx/helper.py:278(make_model)
           15    0.000    0.000    0.292    0.019 /google/protobuf/message.py:118(CopyFrom)
           15    0.291    0.019    0.291    0.019 {method 'MergeFrom' of 'google._upb._message.Message' objects}
      525/325    0.011    0.000    0.289    0.001 /torch/_prims_common/wrappers.py:110(_fn)
           80    0.000    0.000    0.288    0.004 /torch/fx/experimental/proxy_tensor.py:182(track_tensor_tree)
       155/80    0.001    0.000    0.287    0.004 /torch/fx/experimental/proxy_tensor.py:183(wrap_with_proxy)
          150    0.001    0.000    0.267    0.002 /torch/fx/experimental/proxy_tensor.py:144(set_meta)
      170/150    0.001    0.000    0.262    0.002 /torch/fx/experimental/proxy_tensor.py:117(extract_val)
          160    0.000    0.000    0.261    0.002 /torch/fx/experimental/proxy_tensor.py:114(snapshot_fake)
      200/150    0.003    0.000    0.254    0.002 /torch/_subclasses/fake_tensor.py:1059(__torch_dispatch__)
         1055    0.009    0.000    0.252    0.000 /torch/_subclasses/fake_tensor.py:1620(wrap)
          180    0.001    0.000    0.247    0.001 /torch/utils/_pytree.py:375(tree_all_only)
           70    0.001    0.000    0.240    0.003 /torch/fx/graph_module.py:649(recompile)
        70900    0.091    0.000    0.237    0.000 /torch/utils/_pytree.py:186(_get_node_type)
          400    0.010    0.000    0.233    0.001 {method 'to' of 'torch._C._TensorBase' objects}
        45710    0.062    0.000    0.218    0.000 /torch/utils/_pytree.py:192(_is_leaf)
           70    0.001    0.000    0.215    0.003 /torch/fx/graph.py:1208(python_code)
           35    0.005    0.000    0.208    0.006 /torch/_dynamo/variables/torch.py:208(call_function)
           70    0.001    0.000    0.202    0.003 /torch/fx/graph.py:1270(_python_code)
           70    0.012    0.000    0.202    0.003 /torch/fx/graph.py:326(_gen_python_code)
    265050/258585    0.171    0.000    0.200    0.000 {built-in method builtins.isinstance}
      290/240    0.074    0.000    0.189    0.001 {method 'clone' of 'torch._C._TensorBase' objects}
            5    0.001    0.000    0.186    0.037 /torch/_dynamo/guards.py:879(__init__)
           50    0.000    0.000    0.181    0.004 /torch/nn/parameter.py:33(__new__)
           40    0.000    0.000    0.174    0.004 /torch/_jit_internal.py:478(fn)
           40    0.001    0.000    0.174    0.004 /torch/nn/functional.py:769(_max_pool2d)
           40    0.005    0.000    0.173    0.004 {built-in method torch.max_pool2d}
          155    0.166    0.001    0.172    0.001 {method 'extend' of 'google._upb._message.RepeatedCompositeContainer' objects}
            5    0.172    0.034    0.172    0.034 {method 'write' of '_io.BufferedWriter' objects}
        45715    0.104    0.000    0.170    0.000 /torch/utils/_pytree.py:207(__post_init__)
          225    0.001    0.000    0.169    0.001 /torch/_decomp/decompositions.py:58(increase_prec)
            5    0.000    0.000    0.165    0.033 /onnx/helper.py:191(make_graph)
          420    0.003    0.000    0.165    0.000 /torch/fx/proxy.py:170(create_proxy)
         1430    0.006    0.000    0.165    0.000 /torch/_subclasses/fake_tensor.py:1557(check_for_subclass)
           55    0.159    0.003    0.159    0.003 {method 'tobytes' of 'numpy.ndarray' objects}
           45    0.002    0.000    0.158    0.004 /torch/fx/graph_module.py:318(__init__)
      490/400    0.004    0.000    0.154    0.000 /torch/nn/modules/module.py:1697(__setattr__)
            5    0.001    0.000    0.152    0.030 /torch/_dynamo/guards.py:943(compile_check_fn)
         1005    0.006    0.000    0.151    0.000 /torch/_subclasses/fake_tensor.py:1115(_find_common_device)
          100    0.001    0.000    0.150    0.002 /torch/_refs/nn/functional/__init__.py:134(_fn)
           45    0.000    0.000    0.148    0.003 /torch/fx/graph_module.py:416(graph)
            5    0.001    0.000    0.147    0.029 /torch/_dynamo/guards.py:1162(build_guard_function)
        70900    0.105    0.000    0.146    0.000 /torch/utils/_pytree.py:176(_is_namedtuple_instance)
         6670    0.014    0.000    0.141    0.000 /torch/fx/node.py:632(map_arg)
    17415/5695    0.085    0.000    0.134    0.000 /torch/utils/_pytree.py:252(tree_unflatten)
          100    0.001    0.000    0.130    0.001 /torch/_refs/nn/functional/__init__.py:246(relu)
    14150/6675    0.057    0.000    0.123    0.000 /torch/fx/node.py:640(map_aggregate)
           35    0.118    0.003    0.118    0.003 {method 'SerializeToString' of 'google._upb._message.Message' objects}
     5390/330    0.013    0.000    0.116    0.000 /usr/lib/python3.10/ast.py:414(visit)
          225    0.001    0.000    0.110    0.000 /torch/_subclasses/fake_tensor.py:358(__call__)
          225    0.002    0.000    0.109    0.000 /torch/_subclasses/fake_tensor.py:283(from_real_tensor)
          165    0.006    0.000    0.100    0.001 /torch/_subclasses/meta_utils.py:494(__call__)
    15445/14455    0.019    0.000    0.096    0.000 {built-in method builtins.next}
          430    0.006    0.000    0.096    0.000 /torch/fx/proxy.py:114(create_node)
          165    0.011    0.000    0.093    0.001 /torch/_subclasses/meta_utils.py:177(meta_tensor)
           60    0.000    0.000    0.092    0.002 /torch/_refs/__init__.py:4265(t)
           60    0.001    0.000    0.092    0.002 {built-in method torch.transpose}
           75    0.002    0.000    0.091    0.001 {built-in method torch.mm}
          250    0.003    0.000    0.089    0.000 /torch/_refs/__init__.py:957(_ref)
         1450    0.012    0.000    0.088    0.000 /torch/fx/graph.py:482(emit_node)
           60    0.001    0.000    0.087    0.001 /torch/_refs/__init__.py:4301(transpose)
           60    0.001    0.000    0.087    0.001 /torch/_dynamo/symbolic_convert.py:1184(LOAD_ATTR)
           60    0.001    0.000    0.086    0.001 {built-in method torch.permute}
         1055    0.010    0.000    0.086    0.000 /torch/_subclasses/fake_tensor.py:341(from_meta_and_device)
            1    0.000    0.000    0.083    0.083 <eval_with_key>.518:4(forward)
    8925/8365    0.010    0.000    0.080    0.000 /torch/fx/node.py:646(<genexpr>)
          440    0.005    0.000    0.079    0.000 /torch/fx/graph.py:805(create_node)
          165    0.001    0.000    0.077    0.000 /torch/_subclasses/fake_tensor.py:1702(from_tensor)
          425    0.009    0.000    0.076    0.000 /torch/_prims/__init__.py:331(_elementwise_meta)
     1080/480    0.007    0.000    0.075    0.000 /torch/_dynamo/variables/base.py:95(__call__)
        70/50    0.005    0.000    0.074    0.001 {built-in method torch._ops.aten.}
            1    0.000    0.000    0.072    0.072 <eval_with_key>.490:4(forward)
            1    0.000    0.000    0.072    0.072 <eval_with_key>.546:4(forward)
           15    0.000    0.000    0.071    0.005 /torch/export/__init__.py:232(__init__)
          670    0.002    0.000    0.071    0.000 /torch/_dynamo/guards.py:128(_ast_unparse)
           15    0.000    0.000    0.071    0.005 /torch/_export/exported_program.py:230(_create_graph_module_for_export)
           10    0.001    0.000    0.070    0.007 /torch/_decomp/decompositions_for_rng.py:129(reset)
          110    0.000    0.000    0.070    0.001 /torch/_dynamo/guards.py:1169(replace)
          110    0.001    0.000    0.069    0.001 /torch/_dynamo/guards.py:862(replace)
          670    0.002    0.000    0.069    0.000 /usr/lib/python3.10/ast.py:1679(unparse)
           50    0.003    0.000    0.068    0.001 /torch/_subclasses/fake_tensor.py:653(conv)
     3685/310    0.027    0.000    0.068    0.000 /torch/_dynamo/variables/base.py:141(apply)
           30    0.000    0.000    0.068    0.002 /torch/_decomp/decompositions_for_rng.py:71(__init__)
           30    0.000    0.000    0.068    0.002 /torch/_decomp/decompositions_for_rng.py:74(reset)
           60    0.004    0.000    0.067    0.001 {built-in method torch.tensor}
     1080/480    0.002    0.000    0.067    0.000 /torch/_dynamo/variables/base.py:346(__post_init__)
          670    0.002    0.000    0.066    0.000 /usr/lib/python3.10/ast.py:811(visit)
    110035/109870    0.065    0.000    0.065    0.000 {built-in method builtins.len}




.. GENERATED FROM PYTHON SOURCE LINES 265-269

The following display helps to understand.
Most of the tiume added by the custom converter is used to
converter the initializer and build the onnx model once the conversion
is complete.

.. GENERATED FROM PYTHON SOURCE LINES 269-275

.. code-block:: Python


    # from onnx_array_api.profiling import profile2graph
    # root, nodes = profile2graph(ps, clean_text=clean_text)
    # text = root.to_text()
    # print(text)








.. GENERATED FROM PYTHON SOURCE LINES 276-278

Benchmark
+++++++++

.. GENERATED FROM PYTHON SOURCE LINES 278-355

.. code-block:: Python



    def benchmark():
        from onnxruntime import InferenceSession, SessionOptions, GraphOptimizationLevel

        shape = [1, 1, 128, 128]
        data = []
        confs = list(
            itertools.product(
                [_ for _ in os.listdir(".") if ".onnx" in _ and _.startswith("plot_torch")],
                [
                    ["CPUExecutionProvider"],
                    ["CUDAExecutionProvider", "CPUExecutionProvider"],
                ],
                ["0", "1"],
            )
        )
        loop = tqdm(confs)
        print(f"number of experiments: {len(loop)}")
        for name, ps, aot in loop:
            root = os.path.split(name)[-1]
            _, ext = os.path.splitext(root)
            if ext != ".onnx":
                continue

            obs = {}  # system_info()
            obs["name"] = name
            obs["providers"] = ",".join(ps)
            p = "CUDA" if "CUDA" in obs["providers"] else "CPU"
            obs["compute"] = p
            obs["aot"] = 1 if aot == "0" else 0
            obs["export"] = name.replace("plot_torch_export_", "").replace(".onnx", "")

            onx = onnx.load(name)
            obs["n_nodes"] = len(onx.graph.node)
            obs["n_function"] = len(onx.functions or [])
            obs["n_sub"] = len([n for n in onx.graph.node if n.op_type == "Sub"])

            opts = SessionOptions()
            opts.add_session_config_entry("session.disable_aot_function_inlining", aot)
            opts.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL
            opts.optimized_model_filepath = (
                f"ort-{name.replace('.onnx', '')}-{p.lower()}-aot{aot}.onnx"
            )

            try:
                sess = InferenceSession(name, opts, providers=ps)
            except Exception as e:
                loop.set_description(f"ERROR-load: {name} {e}")
                obs.update({"error": e, "step": "run"})
                data.append(obs)
                continue

            input_name = sess.get_inputs()[0].name
            feeds = {input_name: np.random.rand(*shape).astype(np.float32)}
            try:
                for i in range(0, 5):
                    sess.run(None, feeds)
            except Exception as e:
                loop.set_description(f"ERROR-run: {name} {e}")
                obs.update({"error": e, "step": "load"})
                data.append(obs)
                continue
            obs.update(measure_time(lambda: sess.run(None, feeds), max_time=1))

            loop.set_description(f"{obs['average']} {name} {ps}")
            data.append(obs)

        df = pandas.DataFrame(data)
        df.to_csv("benchmark.csv", index=False)
        df.to_excel("benchmark.xlsx", index=False)
        return df


    df = benchmark()
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/24 [00:00<?, ?it/s]number of experiments: 24
    0.012920297619053773 plot_torch_export_cus_p1.onnx ['CPUExecutionProvider']:   0%|          | 0/24 [00:01<?, ?it/s]    0.012920297619053773 plot_torch_export_cus_p1.onnx ['CPUExecutionProvider']:   4%|▍         | 1/24 [00:01<00:33,  1.47s/it]    0.013763085057470217 plot_torch_export_cus_p1.onnx ['CPUExecutionProvider']:   4%|▍         | 1/24 [00:03<00:33,  1.47s/it]    0.013763085057470217 plot_torch_export_cus_p1.onnx ['CPUExecutionProvider']:   8%|▊         | 2/24 [00:03<00:35,  1.62s/it]    0.001620969485291461 plot_torch_export_cus_p1.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:   8%|▊         | 2/24 [00:28<00:35,  1.62s/it]    0.001620969485291461 plot_torch_export_cus_p1.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  12%|█▎        | 3/24 [00:28<04:23, 12.56s/it]    0.001593664592593861 plot_torch_export_cus_p1.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  12%|█▎        | 3/24 [00:30<04:23, 12.56s/it]    0.001593664592593861 plot_torch_export_cus_p1.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  17%|█▋        | 4/24 [00:30<02:47,  8.37s/it]    0.014656697101441829 plot_torch_export_cus_p2.onnx ['CPUExecutionProvider']:  17%|█▋        | 4/24 [00:32<02:47,  8.37s/it]                             0.014656697101441829 plot_torch_export_cus_p2.onnx ['CPUExecutionProvider']:  21%|██        | 5/24 [00:32<01:51,  5.87s/it]    0.013667482666666425 plot_torch_export_cus_p2.onnx ['CPUExecutionProvider']:  21%|██        | 5/24 [00:33<01:51,  5.87s/it]    0.013667482666666425 plot_torch_export_cus_p2.onnx ['CPUExecutionProvider']:  25%|██▌       | 6/24 [00:33<01:18,  4.36s/it]    0.0017512387878813377 plot_torch_export_cus_p2.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  25%|██▌       | 6/24 [00:35<01:18,  4.36s/it]    0.0017512387878813377 plot_torch_export_cus_p2.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  29%|██▉       | 7/24 [00:35<00:58,  3.41s/it]    0.00160631751151929 plot_torch_export_cus_p2.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  29%|██▉       | 7/24 [00:36<00:58,  3.41s/it]      0.00160631751151929 plot_torch_export_cus_p2.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  33%|███▎      | 8/24 [00:36<00:44,  2.76s/it]    0.008168666433569222 plot_torch_export_script.onnx ['CPUExecutionProvider']:  33%|███▎      | 8/24 [00:38<00:44,  2.76s/it]                            0.008168666433569222 plot_torch_export_script.onnx ['CPUExecutionProvider']:  38%|███▊      | 9/24 [00:38<00:36,  2.42s/it]    0.009028141666673643 plot_torch_export_script.onnx ['CPUExecutionProvider']:  38%|███▊      | 9/24 [00:40<00:36,  2.42s/it]    0.009028141666673643 plot_torch_export_script.onnx ['CPUExecutionProvider']:  42%|████▏     | 10/24 [00:40<00:32,  2.35s/it]    0.0015296807017556714 plot_torch_export_script.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  42%|████▏     | 10/24 [00:41<00:32,  2.35s/it]    0.0015296807017556714 plot_torch_export_script.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  46%|████▌     | 11/24 [00:41<00:27,  2.15s/it]    0.001618982296651428 plot_torch_export_script.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  46%|████▌     | 11/24 [00:43<00:27,  2.15s/it]     0.001618982296651428 plot_torch_export_script.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  50%|█████     | 12/24 [00:43<00:23,  1.94s/it]    0.02053954901966077 plot_torch_export_cus_p0.onnx ['CPUExecutionProvider']:  50%|█████     | 12/24 [00:45<00:23,  1.94s/it]                              0.02053954901966077 plot_torch_export_cus_p0.onnx ['CPUExecutionProvider']:  54%|█████▍    | 13/24 [00:45<00:23,  2.11s/it]    0.019018220895576185 plot_torch_export_cus_p0.onnx ['CPUExecutionProvider']:  54%|█████▍    | 13/24 [00:48<00:23,  2.11s/it]    0.019018220895576185 plot_torch_export_cus_p0.onnx ['CPUExecutionProvider']:  58%|█████▊    | 14/24 [00:48<00:21,  2.15s/it]    0.002281675793653001 plot_torch_export_cus_p0.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  58%|█████▊    | 14/24 [00:49<00:21,  2.15s/it]    0.002281675793653001 plot_torch_export_cus_p0.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  62%|██████▎   | 15/24 [00:49<00:17,  1.96s/it]    0.0022854861261261215 plot_torch_export_cus_p0.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  62%|██████▎   | 15/24 [00:51<00:17,  1.96s/it]    0.0022854861261261215 plot_torch_export_cus_p0.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  67%|██████▋   | 16/24 [00:51<00:14,  1.85s/it]    0.025161007142872092 plot_torch_export_dynopt.onnx ['CPUExecutionProvider']:  67%|██████▋   | 16/24 [00:53<00:14,  1.85s/it]                              0.025161007142872092 plot_torch_export_dynopt.onnx ['CPUExecutionProvider']:  71%|███████   | 17/24 [00:53<00:13,  1.92s/it]    0.044606683333389206 plot_torch_export_dynopt.onnx ['CPUExecutionProvider']:  71%|███████   | 17/24 [00:55<00:13,  1.92s/it]    0.044606683333389206 plot_torch_export_dynopt.onnx ['CPUExecutionProvider']:  75%|███████▌  | 18/24 [00:55<00:12,  2.04s/it]    0.0023448135881035787 plot_torch_export_dynopt.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  75%|███████▌  | 18/24 [00:57<00:12,  2.04s/it]    0.0023448135881035787 plot_torch_export_dynopt.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  79%|███████▉  | 19/24 [00:57<00:09,  1.90s/it]    0.004259221544710205 plot_torch_export_dynopt.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  79%|███████▉  | 19/24 [00:58<00:09,  1.90s/it]     0.004259221544710205 plot_torch_export_dynopt.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  83%|████████▎ | 20/24 [00:58<00:07,  1.78s/it]    0.020721249206337868 plot_torch_export_dynamo.onnx ['CPUExecutionProvider']:  83%|████████▎ | 20/24 [01:01<00:07,  1.78s/it]                             0.020721249206337868 plot_torch_export_dynamo.onnx ['CPUExecutionProvider']:  88%|████████▊ | 21/24 [01:01<00:05,  1.95s/it]    0.04156973030297773 plot_torch_export_dynamo.onnx ['CPUExecutionProvider']:  88%|████████▊ | 21/24 [01:03<00:05,  1.95s/it]     0.04156973030297773 plot_torch_export_dynamo.onnx ['CPUExecutionProvider']:  92%|█████████▏| 22/24 [01:03<00:03,  1.93s/it]    0.0022504826235135514 plot_torch_export_dynamo.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  92%|█████████▏| 22/24 [01:04<00:03,  1.93s/it]    0.0022504826235135514 plot_torch_export_dynamo.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  96%|█████████▌| 23/24 [01:04<00:01,  1.95s/it]    0.005023675980398776 plot_torch_export_dynamo.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']:  96%|█████████▌| 23/24 [01:06<00:01,  1.95s/it]     0.005023675980398776 plot_torch_export_dynamo.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']: 100%|██████████| 24/24 [01:06<00:00,  1.79s/it]    0.005023675980398776 plot_torch_export_dynamo.onnx ['CUDAExecutionProvider', 'CPUExecutionProvider']: 100%|██████████| 24/24 [01:06<00:00,  2.77s/it]
                                 name                                   providers compute  aot  export  n_nodes  n_function  n_sub   average  deviation  min_exec  max_exec  repeat  number     ttime  context_size  warmup_time
    0   plot_torch_export_cus_p1.onnx                        CPUExecutionProvider     CPU    1  cus_p1       15           0      0  0.012920   0.000795  0.011773  0.013512       1    84.0  1.085305            64     0.011963
    1   plot_torch_export_cus_p1.onnx                        CPUExecutionProvider     CPU    0  cus_p1       15           0      0  0.013763   0.000693  0.012633  0.014831       1    87.0  1.197388            64     0.013315
    2   plot_torch_export_cus_p1.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    1  cus_p1       15           0      0  0.001621   0.000454  0.001512  0.008372       1   816.0  1.322711            64     0.008215
    3   plot_torch_export_cus_p1.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    0  cus_p1       15           0      0  0.001594   0.000011  0.001447  0.001599       1   675.0  1.075724            64     0.001626
    4   plot_torch_export_cus_p2.onnx                        CPUExecutionProvider     CPU    1  cus_p2       12           0      0  0.014657   0.000546  0.013585  0.015057       1    69.0  1.011312            64     0.013433
    5   plot_torch_export_cus_p2.onnx                        CPUExecutionProvider     CPU    0  cus_p2       12           0      0  0.013667   0.000541  0.012986  0.014449       1    75.0  1.025061            64     0.013617
    6   plot_torch_export_cus_p2.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    1  cus_p2       12           0      0  0.001751   0.000028  0.001479  0.001769       1   660.0  1.155818            64     0.001579
    7   plot_torch_export_cus_p2.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    0  cus_p2       12           0      0  0.001606   0.000011  0.001515  0.001620       1   651.0  1.045713            64     0.001455
    8   plot_torch_export_script.onnx                        CPUExecutionProvider     CPU    1  script       12           0      0  0.008169   0.000392  0.007682  0.010543       1   143.0  1.168119            64     0.007879
    9   plot_torch_export_script.onnx                        CPUExecutionProvider     CPU    0  script       12           0      0  0.009028   0.000598  0.008200  0.009886       1   120.0  1.083377            64     0.007946
    10  plot_torch_export_script.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    1  script       12           0      0  0.001530   0.000032  0.001487  0.001564       1   855.0  1.307877            64     0.001788
    11  plot_torch_export_script.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    0  script       12           0      0  0.001619   0.000071  0.001570  0.002383       1   627.0  1.015102            64     0.001484
    12  plot_torch_export_cus_p0.onnx                        CPUExecutionProvider     CPU    1  cus_p0       27           0      2  0.020540   0.001648  0.019366  0.022974       1    51.0  1.047517            64     0.020749
    13  plot_torch_export_cus_p0.onnx                        CPUExecutionProvider     CPU    0  cus_p0       27           0      2  0.019018   0.000640  0.018166  0.020004       1    67.0  1.274221            64     0.018788
    14  plot_torch_export_cus_p0.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    1  cus_p0       27           0      2  0.002282   0.000025  0.001874  0.002289       1   504.0  1.149965            64     0.001880
    15  plot_torch_export_cus_p0.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    0  cus_p0       27           0      2  0.002285   0.000028  0.002110  0.002538       1   555.0  1.268445            64     0.002031
    16  plot_torch_export_dynopt.onnx                        CPUExecutionProvider     CPU    1  dynopt       13          21      0  0.025161   0.002672  0.020189  0.027220       1    42.0  1.056762            64     0.022742
    17  plot_torch_export_dynopt.onnx                        CPUExecutionProvider     CPU    0  dynopt       13          21      0  0.044607   0.002668  0.038980  0.048468       1    24.0  1.070560            64     0.046328
    18  plot_torch_export_dynopt.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    1  dynopt       13          21      0  0.002345   0.000068  0.002119  0.002393       1   471.0  1.104407            64     0.002019
    19  plot_torch_export_dynopt.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    0  dynopt       13          21      0  0.004259   0.000037  0.003758  0.004286       1   246.0  1.047768            64     0.003568
    20  plot_torch_export_dynamo.onnx                        CPUExecutionProvider     CPU    1  dynamo       13          13      0  0.020721   0.001366  0.018397  0.021614       1    63.0  1.305439            64     0.019942
    21  plot_torch_export_dynamo.onnx                        CPUExecutionProvider     CPU    0  dynamo       13          13      0  0.041570   0.003044  0.039782  0.053197       1    33.0  1.371801            64     0.044708
    22  plot_torch_export_dynamo.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    1  dynamo       13          13      0  0.002250   0.000011  0.002240  0.002270       1   587.0  1.321033            64     0.002417
    23  plot_torch_export_dynamo.onnx  CUDAExecutionProvider,CPUExecutionProvider    CUDA    0  dynamo       13          13      0  0.005024   0.000028  0.004635  0.005029       1   204.0  1.024830            64     0.004803




.. GENERATED FROM PYTHON SOURCE LINES 356-357

Other view

.. GENERATED FROM PYTHON SOURCE LINES 357-369

.. code-block:: Python


    piv = pandas.pivot_table(
        df, index="export", columns=["compute", "aot"], values="average"
    )
    print(piv)

    fig, ax = plt.subplots()
    piv.plot.barh(ax=ax, title="Compares onnxruntime time on exported models")
    fig.tight_layout()
    fig.savefig("plot_torch_export_ort.png")





.. image-sg:: /auto_examples/images/sphx_glr_plot_torch_export_002.png
   :alt: Compares onnxruntime time on exported models
   :srcset: /auto_examples/images/sphx_glr_plot_torch_export_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    compute       CPU                CUDA          
    aot             0         1         0         1
    export                                         
    cus_p0   0.019018  0.020540  0.002285  0.002282
    cus_p1   0.013763  0.012920  0.001594  0.001621
    cus_p2   0.013667  0.014657  0.001606  0.001751
    dynamo   0.041570  0.020721  0.005024  0.002250
    dynopt   0.044607  0.025161  0.004259  0.002345
    script   0.009028  0.008169  0.001619  0.001530




.. GENERATED FROM PYTHON SOURCE LINES 370-372

Show the interesting models
+++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 372-400

.. code-block:: Python


    models = [
        _ for _ in os.listdir(".") if ".onnx" in _ and _.startswith("ort-plot_torch_export")
    ]
    for model in models:
        if (
            "cpu" not in model
            and "cuda" not in model
            or "aot" not in model
            or "cus_p0" in model
            or "cus_p1" in model
        ):
            print("skip1", model)
            continue
        if ("dynamo" in model or "dynopt" in model) and "aot0" in model:
            print("skip2", model)
            continue
        if "aot1" in model and ("dynamo" in model or "dynopt" in model):
            print("skip3", model)
            continue
        print()
        print("#################################################")
        print(model)
        print("#################################################")
        onx = onnx.load(model)
        print(onnx_simple_text_plot(onx))

    print("done.")




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    skip1 ort-plot_torch_export_cus_p0-cpu-aot1.onnx
    skip1 ort-plot_torch_export_cus_p0-cpu-aot0.onnx

    #################################################
    ort-plot_torch_export_cus_p2-cuda-aot1.onnx
    #################################################
    opset: domain='' version=18
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='arg0_1' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='arg1_1' type=dtype('float32') shape=(128,)
    init: name='arg2_1' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='arg3_1' type=dtype('float32') shape=(16,)
    init: name='arg5_1' type=dtype('float32') shape=(1024,)
    init: name='arg7_1' type=dtype('float32') shape=(128,)
    init: name='arg9_1' type=dtype('float32') shape=(10,)
    init: name='ortshared_7_1_2_0_token_8' type=dtype('int64') shape=(2,) -- array([    1, 13456])
    init: name='permute' type=dtype('float32') shape=(13456, 1024)
    init: name='permute_1' type=dtype('float32') shape=(1024, 128)
    init: name='permute_2' type=dtype('float32') shape=(128, 10)
    FusedConv[com.microsoft](input, arg0_1, arg1_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> relu
      MaxPool(relu, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool0, _onx_maxpool1
        FusedConv[com.microsoft](_onx_maxpool0, arg2_1, arg3_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> relu_1
          MaxPool(relu_1, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool03, _onx_maxpool13
            Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -> view
              Gemm(view, permute, arg5_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> addmm
                Relu(addmm) -> relu_2
                  Gemm(relu_2, permute_1, arg7_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> addmm_1
                    Relu(addmm_1) -> relu_3
                      Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> output
    output: name='output' type=dtype('float32') shape=[1, 10]
    skip1 ort-plot_torch_export_cus_p1-cpu-aot0.onnx

    #################################################
    ort-plot_torch_export_cus_p2-cpu-aot1.onnx
    #################################################
    opset: domain='' version=18
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='reorder' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='arg1_1' type=dtype('float32') shape=(128,)
    init: name='reorder_token_11' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='arg3_1' type=dtype('float32') shape=(16,)
    init: name='arg5_1' type=dtype('float32') shape=(1024,)
    init: name='arg7_1' type=dtype('float32') shape=(128,)
    init: name='arg9_1' type=dtype('float32') shape=(10,)
    init: name='ortshared_7_1_2_0_token_8' type=dtype('int64') shape=(2,) -- array([    1, 13456])
    init: name='permute' type=dtype('float32') shape=(13456, 1024)
    init: name='permute_1' type=dtype('float32') shape=(1024, 128)
    init: name='permute_2' type=dtype('float32') shape=(128, 10)
    Conv[com.microsoft.nchwc](input, reorder, arg1_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> reorder_token_10
      ReorderOutput[com.microsoft.nchwc](reorder_token_10, channels_last=0, channels=128) -> relu
        MaxPool(relu, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool0, _onx_maxpool1
          ReorderInput[com.microsoft.nchwc](_onx_maxpool0, channels_last=0) -> reorder_token_12
            Conv[com.microsoft.nchwc](reorder_token_12, reorder_token_11, arg3_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> reorder_token_13
              ReorderOutput[com.microsoft.nchwc](reorder_token_13, channels_last=0, channels=16) -> relu_1
                MaxPool(relu_1, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool03, _onx_maxpool13
                  Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -> view
                    FusedGemm[com.microsoft](view, permute, arg5_1, activation=b'Relu', transB=0, transA=0, alpha=1.00, beta=1.00) -> relu_2
                      FusedGemm[com.microsoft](relu_2, permute_1, arg7_1, activation=b'Relu', transB=0, transA=0, alpha=1.00, beta=1.00) -> relu_3
                        Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> output
    output: name='output' type=dtype('float32') shape=[1, 10]
    skip1 ort-plot_torch_export_cus_p1-cpu-aot1.onnx
    skip1 ort-plot_torch_export_cus_p1-cuda-aot0.onnx

    #################################################
    ort-plot_torch_export_script-cpu-aot0.onnx
    #################################################
    opset: domain='' version=17
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='reorder' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='conv1.bias' type=dtype('float32') shape=(128,)
    init: name='reorder_token_2' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='conv2.bias' type=dtype('float32') shape=(16,)
    init: name='fc1.weight' type=dtype('float32') shape=(1024, 13456)
    init: name='fc1.bias' type=dtype('float32') shape=(1024,)
    init: name='fc2.weight' type=dtype('float32') shape=(128, 1024)
    init: name='fc2.bias' type=dtype('float32') shape=(128,)
    init: name='fc3.weight' type=dtype('float32') shape=(10, 128)
    init: name='fc3.bias' type=dtype('float32') shape=(10,)
    Conv[com.microsoft.nchwc](input, reorder, conv1.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> reorder_token_0
      MaxPool[com.microsoft.nchwc](reorder_token_0, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> reorder_token_1
        Conv[com.microsoft.nchwc](reorder_token_1, reorder_token_2, conv2.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> reorder_token_3
          MaxPool[com.microsoft.nchwc](reorder_token_3, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> reorder_token_4
            ReorderOutput[com.microsoft.nchwc](reorder_token_4, channels_last=0, channels=16) -> /MaxPool_1_output_0
              Flatten(/MaxPool_1_output_0, axis=1) -> /Flatten_output_0
                FusedGemm[com.microsoft](/Flatten_output_0, fc1.weight, fc1.bias, activation=b'Relu', alpha=1.00, beta=1.00, transA=0, transB=1) -> /Relu_2_output_0
                  FusedGemm[com.microsoft](/Relu_2_output_0, fc2.weight, fc2.bias, activation=b'Relu', alpha=1.00, beta=1.00, transA=0, transB=1) -> /Relu_3_output_0
                    Gemm(/Relu_3_output_0, fc3.weight, fc3.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> 22
    output: name='22' type=dtype('float32') shape=[1, 10]
    skip1 ort-plot_torch_export_cus_p0-cuda-aot0.onnx
    skip3 ort-plot_torch_export_dynopt-cpu-aot1.onnx
    skip2 ort-plot_torch_export_dynamo-cpu-aot0.onnx

    #################################################
    ort-plot_torch_export_cus_p2-cpu-aot0.onnx
    #################################################
    opset: domain='' version=18
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='reorder' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='arg1_1' type=dtype('float32') shape=(128,)
    init: name='reorder_token_11' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='arg3_1' type=dtype('float32') shape=(16,)
    init: name='arg5_1' type=dtype('float32') shape=(1024,)
    init: name='arg7_1' type=dtype('float32') shape=(128,)
    init: name='arg9_1' type=dtype('float32') shape=(10,)
    init: name='ortshared_7_1_2_0_token_8' type=dtype('int64') shape=(2,) -- array([    1, 13456])
    init: name='permute' type=dtype('float32') shape=(13456, 1024)
    init: name='permute_1' type=dtype('float32') shape=(1024, 128)
    init: name='permute_2' type=dtype('float32') shape=(128, 10)
    Conv[com.microsoft.nchwc](input, reorder, arg1_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> reorder_token_10
      ReorderOutput[com.microsoft.nchwc](reorder_token_10, channels_last=0, channels=128) -> relu
        MaxPool(relu, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool0, _onx_maxpool1
          ReorderInput[com.microsoft.nchwc](_onx_maxpool0, channels_last=0) -> reorder_token_12
            Conv[com.microsoft.nchwc](reorder_token_12, reorder_token_11, arg3_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> reorder_token_13
              ReorderOutput[com.microsoft.nchwc](reorder_token_13, channels_last=0, channels=16) -> relu_1
                MaxPool(relu_1, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool03, _onx_maxpool13
                  Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -> view
                    FusedGemm[com.microsoft](view, permute, arg5_1, activation=b'Relu', transB=0, transA=0, alpha=1.00, beta=1.00) -> relu_2
                      FusedGemm[com.microsoft](relu_2, permute_1, arg7_1, activation=b'Relu', transB=0, transA=0, alpha=1.00, beta=1.00) -> relu_3
                        Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> output
    output: name='output' type=dtype('float32') shape=[1, 10]
    skip3 ort-plot_torch_export_dynamo-cuda-aot1.onnx
    skip1 ort-plot_torch_export_cus_p1-cuda-aot1.onnx
    skip3 ort-plot_torch_export_dynopt-cuda-aot1.onnx

    #################################################
    ort-plot_torch_export_script-cuda-aot0.onnx
    #################################################
    opset: domain='' version=17
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='conv1.weight' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='conv1.bias' type=dtype('float32') shape=(128,)
    init: name='conv2.weight' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='conv2.bias' type=dtype('float32') shape=(16,)
    init: name='fc1.weight' type=dtype('float32') shape=(1024, 13456)
    init: name='fc1.bias' type=dtype('float32') shape=(1024,)
    init: name='fc2.weight' type=dtype('float32') shape=(128, 1024)
    init: name='fc2.bias' type=dtype('float32') shape=(128,)
    init: name='fc3.weight' type=dtype('float32') shape=(10, 128)
    init: name='fc3.bias' type=dtype('float32') shape=(10,)
    FusedConv[com.microsoft](input, conv1.weight, conv1.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> /Relu_output_0
      MaxPool(/Relu_output_0, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> /MaxPool_output_0
        FusedConv[com.microsoft](/MaxPool_output_0, conv2.weight, conv2.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> /Relu_1_output_0
          MaxPool(/Relu_1_output_0, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> /MaxPool_1_output_0
            Flatten(/MaxPool_1_output_0, axis=1) -> /Flatten_output_0
              Gemm(/Flatten_output_0, fc1.weight, fc1.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> /fc1/Gemm_output_0
                Relu(/fc1/Gemm_output_0) -> /Relu_2_output_0
                  Gemm(/Relu_2_output_0, fc2.weight, fc2.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> /fc2/Gemm_output_0
                    Relu(/fc2/Gemm_output_0) -> /Relu_3_output_0
                      Gemm(/Relu_3_output_0, fc3.weight, fc3.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> 22
    output: name='22' type=dtype('float32') shape=[1, 10]
    skip2 ort-plot_torch_export_dynamo-cuda-aot0.onnx
    skip2 ort-plot_torch_export_dynopt-cpu-aot0.onnx

    #################################################
    ort-plot_torch_export_cus_p2-cuda-aot0.onnx
    #################################################
    opset: domain='' version=18
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='arg0_1' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='arg1_1' type=dtype('float32') shape=(128,)
    init: name='arg2_1' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='arg3_1' type=dtype('float32') shape=(16,)
    init: name='arg5_1' type=dtype('float32') shape=(1024,)
    init: name='arg7_1' type=dtype('float32') shape=(128,)
    init: name='arg9_1' type=dtype('float32') shape=(10,)
    init: name='ortshared_7_1_2_0_token_8' type=dtype('int64') shape=(2,) -- array([    1, 13456])
    init: name='permute' type=dtype('float32') shape=(13456, 1024)
    init: name='permute_1' type=dtype('float32') shape=(1024, 128)
    init: name='permute_2' type=dtype('float32') shape=(128, 10)
    FusedConv[com.microsoft](input, arg0_1, arg1_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> relu
      MaxPool(relu, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool0, _onx_maxpool1
        FusedConv[com.microsoft](_onx_maxpool0, arg2_1, arg3_1, activation=b'Relu', dilations=[1,1], group=1, strides=[1,1], pads=[0,0,0,0], auto_pad=b'NOTSET') -> relu_1
          MaxPool(relu_1, storage_order=0, auto_pad=b'NOTSET', ceil_mode=0, dilations=[1,1], kernel_shape=[2,2], pads=[0,0,0,0], strides=[2,2]) -> _onx_maxpool03, _onx_maxpool13
            Reshape(_onx_maxpool03, ortshared_7_1_2_0_token_8, allowzero=0) -> view
              Gemm(view, permute, arg5_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> addmm
                Relu(addmm) -> relu_2
                  Gemm(relu_2, permute_1, arg7_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> addmm_1
                    Relu(addmm_1) -> relu_3
                      Gemm(relu_3, permute_2, arg9_1, transB=0, transA=0, alpha=1.00, beta=1.00) -> output
    output: name='output' type=dtype('float32') shape=[1, 10]
    skip2 ort-plot_torch_export_dynopt-cuda-aot0.onnx
    skip3 ort-plot_torch_export_dynamo-cpu-aot1.onnx

    #################################################
    ort-plot_torch_export_script-cuda-aot1.onnx
    #################################################
    opset: domain='' version=17
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='conv1.weight' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='conv1.bias' type=dtype('float32') shape=(128,)
    init: name='conv2.weight' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='conv2.bias' type=dtype('float32') shape=(16,)
    init: name='fc1.weight' type=dtype('float32') shape=(1024, 13456)
    init: name='fc1.bias' type=dtype('float32') shape=(1024,)
    init: name='fc2.weight' type=dtype('float32') shape=(128, 1024)
    init: name='fc2.bias' type=dtype('float32') shape=(128,)
    init: name='fc3.weight' type=dtype('float32') shape=(10, 128)
    init: name='fc3.bias' type=dtype('float32') shape=(10,)
    FusedConv[com.microsoft](input, conv1.weight, conv1.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> /Relu_output_0
      MaxPool(/Relu_output_0, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> /MaxPool_output_0
        FusedConv[com.microsoft](/MaxPool_output_0, conv2.weight, conv2.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> /Relu_1_output_0
          MaxPool(/Relu_1_output_0, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> /MaxPool_1_output_0
            Flatten(/MaxPool_1_output_0, axis=1) -> /Flatten_output_0
              Gemm(/Flatten_output_0, fc1.weight, fc1.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> /fc1/Gemm_output_0
                Relu(/fc1/Gemm_output_0) -> /Relu_2_output_0
                  Gemm(/Relu_2_output_0, fc2.weight, fc2.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> /fc2/Gemm_output_0
                    Relu(/fc2/Gemm_output_0) -> /Relu_3_output_0
                      Gemm(/Relu_3_output_0, fc3.weight, fc3.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> 22
    output: name='22' type=dtype('float32') shape=[1, 10]
    skip1 ort-plot_torch_export_cus_p0-cuda-aot1.onnx

    #################################################
    ort-plot_torch_export_script-cpu-aot1.onnx
    #################################################
    opset: domain='' version=17
    opset: domain='ai.onnx.ml' version=4
    opset: domain='ai.onnx.training' version=1
    opset: domain='ai.onnx.preview.training' version=1
    opset: domain='com.microsoft' version=1
    opset: domain='com.microsoft.experimental' version=1
    opset: domain='com.microsoft.nchwc' version=1
    opset: domain='org.pytorch.aten' version=1
    input: name='input' type=dtype('float32') shape=[1, 1, 128, 128]
    init: name='reorder' type=dtype('float32') shape=(128, 1, 5, 5)
    init: name='conv1.bias' type=dtype('float32') shape=(128,)
    init: name='reorder_token_2' type=dtype('float32') shape=(16, 128, 5, 5)
    init: name='conv2.bias' type=dtype('float32') shape=(16,)
    init: name='fc1.weight' type=dtype('float32') shape=(1024, 13456)
    init: name='fc1.bias' type=dtype('float32') shape=(1024,)
    init: name='fc2.weight' type=dtype('float32') shape=(128, 1024)
    init: name='fc2.bias' type=dtype('float32') shape=(128,)
    init: name='fc3.weight' type=dtype('float32') shape=(10, 128)
    init: name='fc3.bias' type=dtype('float32') shape=(10,)
    Conv[com.microsoft.nchwc](input, reorder, conv1.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> reorder_token_0
      MaxPool[com.microsoft.nchwc](reorder_token_0, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> reorder_token_1
        Conv[com.microsoft.nchwc](reorder_token_1, reorder_token_2, conv2.bias, activation=b'Relu', auto_pad=b'NOTSET', dilations=[1,1], group=1, strides=[1,1], kernel_shape=[5,5], pads=[0,0,0,0]) -> reorder_token_3
          MaxPool[com.microsoft.nchwc](reorder_token_3, storage_order=0, ceil_mode=0, kernel_shape=[2,2], pads=[0,0,0,0], auto_pad=b'NOTSET', strides=[2,2]) -> reorder_token_4
            ReorderOutput[com.microsoft.nchwc](reorder_token_4, channels_last=0, channels=16) -> /MaxPool_1_output_0
              Flatten(/MaxPool_1_output_0, axis=1) -> /Flatten_output_0
                FusedGemm[com.microsoft](/Flatten_output_0, fc1.weight, fc1.bias, activation=b'Relu', alpha=1.00, beta=1.00, transA=0, transB=1) -> /Relu_2_output_0
                  FusedGemm[com.microsoft](/Relu_2_output_0, fc2.weight, fc2.bias, activation=b'Relu', alpha=1.00, beta=1.00, transA=0, transB=1) -> /Relu_3_output_0
                    Gemm(/Relu_3_output_0, fc3.weight, fc3.bias, alpha=1.00, beta=1.00, transA=0, transB=1) -> 22
    output: name='22' type=dtype('float32') shape=[1, 10]
    done.





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 45.965 seconds)


.. _sphx_glr_download_auto_examples_plot_torch_export.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_torch_export.ipynb <plot_torch_export.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_torch_export.py <plot_torch_export.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
