
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_torch_export_compile_102.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_torch_export_compile_102.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_torch_export_compile_102.py:


.. _l-plot-torch-export-compile-101:

======================
102: Tweak onnx export
======================

export, unflatten and compile
=============================

.. GENERATED FROM PYTHON SOURCE LINES 11-45

.. code-block:: Python


    import torch
    from experimental_experiment.helpers import pretty_onnx
    from experimental_experiment.torch_interpreter import to_onnx


    class SubNeuron(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x):
            z = self.linear(x)
            return torch.sigmoid(z)


    class Neuron(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.neuron = SubNeuron(n_dims, n_targets)

        def forward(self, x):
            z = self.neuron(x)
            return torch.relu(z)


    model = Neuron()
    inputs = (torch.randn(1, 5),)
    expected = model(*inputs)
    exported_program = torch.export.export(model, inputs)

    print("-- fx graph with torch.export.export")
    print(exported_program.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- fx graph with torch.export.export
    graph():
        %p_neuron_linear_weight : [num_users=1] = placeholder[target=p_neuron_linear_weight]
        %p_neuron_linear_bias : [num_users=1] = placeholder[target=p_neuron_linear_bias]
        %x : [num_users=1] = placeholder[target=x]
        %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_neuron_linear_weight, %p_neuron_linear_bias), kwargs = {})
        %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%linear,), kwargs = {})
        %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%sigmoid,), kwargs = {})
        return (relu,)




.. GENERATED FROM PYTHON SOURCE LINES 46-47

The export keeps track of the submodules calls.

.. GENERATED FROM PYTHON SOURCE LINES 47-51

.. code-block:: Python


    print("-- module_call_graph", type(exported_program.module_call_graph))
    print(exported_program.module_call_graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- module_call_graph <class 'list'>
    [ModuleCallEntry(fqn='', signature=ModuleCallSignature(inputs=[], outputs=[], in_spec=TreeSpec(tuple, None, [TreeSpec(tuple, None, [*]),
      TreeSpec(dict, [], [])]), out_spec=*, forward_arg_names=['x'])), ModuleCallEntry(fqn='neuron', signature=None), ModuleCallEntry(fqn='neuron.linear', signature=None)]




.. GENERATED FROM PYTHON SOURCE LINES 52-53

That information can be converted back into a exported program.

.. GENERATED FROM PYTHON SOURCE LINES 53-58

.. code-block:: Python


    ep = torch.export.unflatten(exported_program)
    print("-- unflatten", type(exported_program.graph))
    print(ep.graph)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- unflatten <class 'torch.fx.graph.Graph'>
    graph():
        %x : [num_users=1] = placeholder[target=x]
        %neuron : [num_users=1] = call_module[target=neuron](args = (%x,), kwargs = {})
        %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%neuron,), kwargs = {})
        return (relu,)




.. GENERATED FROM PYTHON SOURCE LINES 59-60

Another graph obtained with torch.compile.

.. GENERATED FROM PYTHON SOURCE LINES 60-71

.. code-block:: Python



    def my_compiler(gm, example_inputs):
        print("-- graph with torch.compile")
        print(gm.graph)
        return gm.forward


    optimized_mod = torch.compile(model, fullgraph=True, backend=my_compiler)
    optimized_mod(*inputs)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    -- graph with torch.compile
    graph():
        %l_self_modules_neuron_modules_linear_parameters_weight_ : torch.nn.parameter.Parameter [num_users=1] = placeholder[target=L_self_modules_neuron_modules_linear_parameters_weight_]
        %l_self_modules_neuron_modules_linear_parameters_bias_ : torch.nn.parameter.Parameter [num_users=1] = placeholder[target=L_self_modules_neuron_modules_linear_parameters_bias_]
        %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]
        %z : [num_users=1] = call_function[target=torch._C._nn.linear](args = (%l_x_, %l_self_modules_neuron_modules_linear_parameters_weight_, %l_self_modules_neuron_modules_linear_parameters_bias_), kwargs = {})
        %z_1 : [num_users=1] = call_function[target=torch.sigmoid](args = (%z,), kwargs = {})
        %relu : [num_users=1] = call_function[target=torch.relu](args = (%z_1,), kwargs = {})
        return (relu,)

    tensor([[0.2611, 0.5184, 0.7222]], grad_fn=<ReluBackward0>)



.. GENERATED FROM PYTHON SOURCE LINES 72-74

Unflattened
===========

.. GENERATED FROM PYTHON SOURCE LINES 74-103

.. code-block:: Python



    class SubNeuron2(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.linear = torch.nn.Linear(n_dims, n_targets)

        def forward(self, x):
            z = self.linear(x)
            return torch.sigmoid(z)


    class Neuron2(torch.nn.Module):
        def __init__(self, n_dims: int = 5, n_targets: int = 3):
            super().__init__()
            self.neuron = SubNeuron2(n_dims, n_targets)

        def forward(self, x):
            z = self.neuron(x)
            return torch.relu(z)


    model = Neuron2()
    inputs = (torch.randn(1, 5),)
    expected = model(*inputs)

    onx = to_onnx(model, inputs)
    print(pretty_onnx(onx))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    doc_string: large_model=False, inline=False, external_threshold=102...
    input: name='x' type=dtype('float32') shape=[1, 5]
    init: name='neuron.linear.weight' type=float32 shape=(3, 5)           -- DynamoInterpret.placeholder.1/P(neuron.linear.weight)
    init: name='neuron.linear.bias' type=float32 shape=(3,) -- array([-0.41757116,  0.08138233,  0.22417201], dtype=float32)-- DynamoInterpret.placeholder.1/P(neuron.linear.bias)
    Gemm(x, neuron.linear.weight, neuron.linear.bias, transB=1) -> linear
      Sigmoid(linear) -> sigmoid
        Relu(sigmoid) -> output_0
    output: name='output_0' type=dtype('float32') shape=[1, 3]




.. GENERATED FROM PYTHON SOURCE LINES 104-105

Let's preserve the module.

.. GENERATED FROM PYTHON SOURCE LINES 105-109

.. code-block:: Python



    onx = to_onnx(model, inputs, export_modules_as_functions=True)
    print(pretty_onnx(onx))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    opset: domain='aten_local_function' version=1
    doc_string: large_model=False, inline=False, external_threshold=102...
    input: name='x' type=dtype('float32') shape=[1, 5]
    __main__.SubNeuron2[aten_local_function](x) -> neuron
      Relu(neuron) -> output_0
    output: name='output_0' type=dtype('float32') shape=[1, 3]
    ----- function name=Linear domain=aten_local_function
    ----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
    opset: domain='' version=18
    input: 'x'
    Constant(value=[[-0.25066...) -> weight
      Transpose(weight, perm=[1,0]) -> _onx_transpose0
        Transpose(_onx_transpose0, perm=[1,0]) -> GemmTransposePattern--_onx_transpose0
    Constant(value=[-0.417571...) -> bias
      Gemm(x, GemmTransposePattern--_onx_transpose0, bias, transB=1) -> output
    Constant(value=[[-0.25066...) -> neuron.linear.weight
    Constant(value=[-0.417571...) -> neuron.linear.bias
    output: name='output' type=? shape=?
    ----- function name=__main__.SubNeuron2 domain=aten_local_function
    ----- doc_string: function_options=FunctionOptions(export_as_function=Tru...
    opset: domain='' version=18
    opset: domain='aten_local_function' version=1
    input: 'x'
    Linear[aten_local_function](x) -> linear
      Sigmoid(linear) -> output
    output: name='output' type=? shape=?





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.195 seconds)


.. _sphx_glr_download_auto_examples_plot_torch_export_compile_102.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_torch_export_compile_102.ipynb <plot_torch_export_compile_102.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_torch_export_compile_102.py <plot_torch_export_compile_102.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_torch_export_compile_102.zip <plot_torch_export_compile_102.zip>`


.. include:: plot_torch_export_compile_102.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
