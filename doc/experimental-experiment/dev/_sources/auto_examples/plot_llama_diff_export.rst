
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_llama_diff_export.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_llama_diff_export.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_llama_diff_export.py:


Compares LLAMA exporters
========================

The script compares the two exporters implemented in :epkg:`pytorch`
for a part of llama model. The model are compared after all optimizations
were made with :epkg:`onnx-rewriter` and :epkg:`onnxruntime`.

* `TorchScript-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchscript-based-onnx-exporter>`_,
  let's call it **script**
* `TorchDynamo-based ONNX Exporter
  <https://pytorch.org/docs/stable/onnx.html#torchdynamo-based-onnx-exporter>`_,
  let's call it **dynamo**

To run the script:

::

    python _doc/examples/plot_llama_diff_export --help

Some helpers
++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 25-64

.. code-block:: Python


    import contextlib
    import os
    import io
    import warnings
    import logging

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            import onnxruntime

            has_cuda = "CUDAExecutionProvider" in onnxruntime.get_available_providers()
    except ImportError:
        print("onnxruntime not available.")
        import sys

        sys.exit(0)

    import numpy as np
    import onnx
    from onnx_array_api.reference import compare_onnx_execution, ExtendedReferenceEvaluator
    import torch
    from experimental_experiment.ext_test_case import get_parsed_args, unit_test_going
    from experimental_experiment.convert.convert_helper import (
        optimize_model_proto,
        ort_optimize,
    )
    from experimental_experiment.torch_helper.llama_helper import (
        get_llama_model,
        get_llama_attention,
        get_llama_decoder,
    )

    has_cuda = has_cuda and torch.cuda.is_available()
    logging.disable(logging.ERROR)
    provider = "cuda" if has_cuda else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 65-67

The exporting functions
+++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 67-105

.. code-block:: Python



    script_args = get_parsed_args(
        "plot_llama_diff_export",
        description=__doc__,
        part=("attention", "one value among attention, decoder, model"),
        expose="part",
    )

    print(f"part={script_args.part}")


    def opt_filename(filename: str) -> str:
        name, ext = os.path.splitext(filename)
        return f"{name}.opt{ext}"


    def export_script(filename, model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                torch.onnx.export(model, args, filename, input_names=["input"])
        onx = onnx.load(filename)
        ort_optimize(onx, opt_filename(filename), providers=provider)


    def export_dynamo(filename, model, *args):
        with contextlib.redirect_stdout(io.StringIO()):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                export_output = torch.onnx.dynamo_export(model, *args)
                model = export_output.model_proto
        new_model = optimize_model_proto(model)
        with open(filename, "wb") as f:
            f.write(new_model.SerializeToString())
        ort_optimize(new_model, opt_filename(filename), providers=provider)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    part=attention




.. GENERATED FROM PYTHON SOURCE LINES 106-107

Model and data

.. GENERATED FROM PYTHON SOURCE LINES 107-136

.. code-block:: Python


    if unit_test_going():
        kwargs = dict(input_dims=[(2, 1024)] * 2)
    else:
        kwargs = dict(
            input_dims=[(2, 1024)] * 2,
            _attn_implementation="eager",
            num_hidden_layers=1,
            hidden_size=512,
            vocab_size=4000,
            intermediate_size=2000,
            max_position_embeddings=2048,
            num_attention_heads=8,
        )

    if script_args.part == "attention":
        model, inputs = get_llama_attention(**kwargs)
    elif script_args.part == "decoder":
        model, inputs = get_llama_decoder(**kwargs)
    elif script_args.part == "model":
        model, inputs = get_llama_model(**kwargs)
    else:
        raise RuntimeError(f"Unexpected value for part={script_args.part!r}")

    print(f"simple run with {len(inputs)} inputs")
    expected = model(*inputs[0])
    print(f"eager mode worked {expected.shape}, {expected.dtype}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    simple run with 2 inputs
    eager mode worked torch.Size([2, 1024, 512]), torch.float32




.. GENERATED FROM PYTHON SOURCE LINES 137-138

Export

.. GENERATED FROM PYTHON SOURCE LINES 138-148

.. code-block:: Python


    file1 = f"llama.{script_args.part}.script.onnx"
    file2 = f"llama.{script_args.part}.dynamo.onnx"

    print("torch script exporter")
    export_script(file1, model, *inputs[0])

    print("torch dynamo exporter")
    export_dynamo(file2, model, *inputs[0])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch script exporter
    torch dynamo exporter




.. GENERATED FROM PYTHON SOURCE LINES 149-150

Verification

.. GENERATED FROM PYTHON SOURCE LINES 150-181

.. code-block:: Python


    file1 = f"llama.{script_args.part}.script.opt.onnx"
    file2 = f"llama.{script_args.part}.dynamo.opt.onnx"


    providers = (
        ["CPUExecutionProvider"]
        if provider == "cpu"
        else ["CUDAExecutionProvider", "CPUExecutionProvider"]
    )
    sess1 = onnxruntime.InferenceSession(file1, providers=providers)
    sess2 = onnxruntime.InferenceSession(file2, providers=providers)


    model1 = onnx.load(file1)
    model2 = onnx.load(file2)

    feeds1, feeds2 = {}, {}
    for i in range(len(inputs[0])):
        x = inputs[0][i].detach().numpy()
        feeds1[sess1.get_inputs()[i].name] = x
        feeds2[sess2.get_inputs()[i].name] = x

    got1 = sess1.run(None, feeds1)
    got2 = sess2.run(None, feeds2)

    diff1 = np.abs(expected.detach().numpy() - got1[0]).max()
    diff2 = np.abs(expected.detach().numpy() - got2[0]).max()

    print(f"Error with the eager model: {diff1}, {diff2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Error with the eager model: 6.705522537231445e-08, 6.705522537231445e-08




.. GENERATED FROM PYTHON SOURCE LINES 182-183

With the reference evaluator

.. GENERATED FROM PYTHON SOURCE LINES 183-196

.. code-block:: Python


    sess1 = ExtendedReferenceEvaluator(file1)
    sess2 = ExtendedReferenceEvaluator(file2)


    got1 = sess1.run(None, feeds1)
    got2 = sess2.run(None, feeds2)

    diff1 = np.abs(expected.detach().numpy() - got1[0]).max()
    diff2 = np.abs(expected.detach().numpy() - got2[0]).max()

    print(f"Error with the eager model: {diff1}, {diff2}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Error with the eager model: 4.0978193283081055e-08, 4.0978193283081055e-08




.. GENERATED FROM PYTHON SOURCE LINES 197-199

Comparison and execution
++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 199-215

.. code-block:: Python



    def clean_name(name):
        return name.replace(
            "_inlfunc_transformers_models_llama_modeling_llama_LlamaAttention", ""
        ).replace("_inlfunc_torch_nn_modules_linear_Linear", "")


    np_inputs = [i.detach().numpy() for i in inputs[0]]
    res1, res2, align, dc = compare_onnx_execution(
        model1, model2, inputs=np_inputs, verbose=1
    )
    for r in res2:
        r.name = clean_name(r.name)
    text = dc.to_str(res1, res2, align, column_size=90)
    print(text)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [compare_onnx_execution] generate inputs
    [compare_onnx_execution] got 3 inputs
    [compare_onnx_execution] execute first model
    /home/xadupre/github/onnx-array-api/onnx_array_api/reference/evaluator_yield.py:92: RuntimeWarning: invalid value encountered in cast
      value4i = value4.astype(np.int64) % modulo
    [compare_onnx_execution] got 53 results
    [compare_onnx_execution] execute second model
    [compare_onnx_execution] got 83 results
    [compare_onnx_execution] compute edit distance
    [compare_onnx_execution] got 86 pairs
    [compare_onnx_execution] done
    ~ | INITIA float32  512x512         YAAL            onnx::MatMul_131                           | INITIA float32                  BAAA            ortshared_1_0_1_1_token_164               
    + |                                                                                            | INITIA int64    3               CKSA            ortshared_7_1_3_2_token_162                
    - | INITIA float32  512x512         WBPW            onnx::MatMul_132                           |                                                                                           
    - | INITIA float32  512x512         CZXP            onnx::MatMul_133                           |                                                                                           
    = | INITIA float32  512x512         DWEZ            onnx::MatMul_169                           | INITIA float32  512x512         DWEZ            _attention_o_proj_1_t_3                   
    + |                                                                                            | INITIA float32                  IAAA            ortshared_1_0_1_0_token_163                
    ~ | INITIA int64    4               CKIM            ortshared_7_1_4_0_token_76                 | INITIA int64    4               CIKM            ortshared_7_1_4_1_token_159               
    + |                                                                                            | INITIA int64    2               USAA            ortshared_7_1_2_1_token_167                
    ~ | INITIA int64    1               AAAA            ortshared_7_1_1_2_token_75                 | INITIA int64    4               CIKK            ortshared_7_1_4_0_token_154               
    ~ | INITIA int64    1               DAAA            ortshared_7_1_1_1_token_74                 | INITIA int64    4               CKIM            ortshared_7_1_4_2_token_165               
    + |                                                                                            | INITIA int64    3               QKMA            ortshared_7_1_3_1_token_158                
    = | INITIA float32  1024x64         CJYF            /attention/rotary_emb/Constant_output_0    | INITIA float32  1024x64         CJYF            _attention_1__val_22                      
    + |                                                                                            | INITIA int64                    ZAAA            ortshared_7_0_1_1_token_171                
    + |                                                                                            | INITIA int64                    BAAA            ortshared_7_0_1_0_token_156                
    = | INITIA float32  1024x64         GSEC            /attention/rotary_emb/Constant_1_output_0  | INITIA float32  1024x64         GSEC            _attention_1__val_32                      
    ~ | INITIA int64    1               SAAA            ortshared_7_1_1_0_token_73                 | INITIA int64    1               GAAA            ortshared_7_1_1_2_token_166               
    + |                                                                                            | INITIA float32  512x512         WBPW            _attention_k_proj_1_t_1                    
    + |                                                                                            | INITIA int64    1               AAAA            ortshared_7_1_1_0_token_155                
    + |                                                                                            | INITIA float32  512x512         CZXP            _attention_v_proj_1_t_2                    
    + |                                                                                            | INITIA float32  512x512         YAAL            _attention_q_proj_1_t                      
    + |                                                                                            | INITIA int64    1               DAAA            ortshared_7_1_1_4_token_170                
    = | INITIA int64    1               BAAA            ortshared_7_1_1_3_token_78                 | INITIA int64    1               BAAA            ortshared_7_1_1_3_token_169               
    + |                                                                                            | INITIA int64    1               SAAA            ortshared_7_1_1_1_token_160                
    ~ | INITIA int64    3               CKSA            ortshared_7_1_3_0_token_80                 | INITIA int64    3               QKKA            ortshared_7_1_3_0_token_157               
    + |                                                                                            | INITIA int64    3               QMKA            ortshared_7_1_3_3_token_168                
    ~ | INITIA int64    1               GAAA            ortshared_7_1_1_4_token_79                 | INITIA int64    2               BKAA            ortshared_7_1_2_0_token_161               
    = | INPUT  float32  2x1024x512      PLME            input                                      | INPUT  float32  2x1024x512      PLME            l_hidden_states_                          
    = | INPUT  float32  2x1x1024x1024   AAAA            onnx::Add_1                                | INPUT  float32  2x1x1024x1024   AAAA            l_attention_mask_                         
    = | INPUT  int64    1x1024          KAQG            position_ids                               | INPUT  int64    1x1024          KAQG            l_position_ids_                           
    + |                                                                                            | RESULT float32  2048x512        PLME Reshape    _attention_v_proj_1_view_4                 
    + |                                                                                            | RESULT float32  2048x512        JYYU MatMul     _attention_v_proj_1_mm_2                   
    + |                                                                                            | RESULT float32  2x1024x512      JYYU Reshape    _attention_1_attention_v_proj_1            
    + |                                                                                            | RESULT float32  2x1024x8x64     JYYU Reshape    _attention_1_view_8                        
    + |                                                                                            | RESULT float32  2x8x1024x64     RQVY Transpose  _attention_1_transpose_2                   
    + |                                                                                            | RESULT float32  16x1024x64      RQVY Reshape    _attention_1_view_13                       
    + |                                                                                            | RESULT float32  2x1x1024x1024   AAAA Mul        _inlfunc_aten_add|folded_2_other_1         
    + |                                                                                            | RESULT int64    1x1024          KAQG Expand     _attention_1__val_35                       
    + |                                                                                            | RESULT int64    1x1024x1        KAQG Unsqueeze  _attention_1__val_37                       
    + |                                                                                            | RESULT int64    1x1024x1        KAQG Concat     _attention_1__val_38                       
    ~ | RESULT float32  1x1024x64       GSEC Gather     /attention/Gather_1_output_0               | RESULT float32  1x1024x64       GSEC GatherND   _attention_1__val_39                      
    = | RESULT float32  1x1x1024x64     GSEC Unsqueeze  /attention/Unsqueeze_1_output_0            | RESULT float32  1x1x1024x64     GSEC Unsqueeze  _attention_1_aten_unsqueeze_65_n2         
    = | RESULT float32  1x1024x1x64     GSEC Transpose  Transpose_token_4_out0                     | RESULT float32  1x1024x1x64     GSEC Transpose  Transpose_token_5_out0                    
    + |                                                                                            | RESULT float32  2048x512        RSWD MatMul     _attention_k_proj_1_mm_1                   
    ~ | RESULT float32  2x1024x512      RSWD MatMul     /attention/k_proj/MatMul_output_0          | RESULT float32  2x1024x512      RSWD Reshape    _attention_1_attention_k_proj_1           
    = | RESULT float32  2x1024x8x64     RSWD Reshape    /attention/Reshape_1_output_0              | RESULT float32  2x1024x8x64     RSWD Reshape    _attention_1_view_7                       
    = | RESULT float32  2x1024x8x32     BRSK Slice      /attention/Slice_3                         | RESULT float32  2x1024x8x32     BRSK Slice      _attention_1_Slice_140                    
    = | RESULT float32  2x1024x8x32     ZJIQ Neg        /attention/Neg_1                           | RESULT float32  2x1024x8x32     ZJIQ Neg        _attention_1_aten_neg_141_n0              
    = | RESULT float32  2x1024x8x32     QBFT Slice      /attention/Slice_2                         | RESULT float32  2x1024x8x32     QBFT Slice      _attention_1_Slice_123                    
    = | RESULT float32  2x1024x8x64     PLNJ Concat     /attention/Concat_1                        | RESULT float32  2x1024x8x64     PLNJ Concat     _attention_1_aten_cat_143_n0              
    = | RESULT float32  2x1024x8x64     HHXU Mul        /attention/Mul_3                           | RESULT float32  2x1024x8x64     HHXU Mul        _attention_1_aten_mul_144_n0              
    ~ | RESULT float32  1x1024x64       CJYF Gather     /attention/Gather_output_0                 | RESULT float32  1x1024x64       CJYF GatherND   _attention_1__val_29                      
    = | RESULT float32  1x1x1024x64     CJYF Unsqueeze  /attention/Unsqueeze_output_0              | RESULT float32  1x1x1024x64     CJYF Unsqueeze  _attention_1_aten_unsqueeze_55_n2         
    = | RESULT float32  1x1024x1x64     CJYF Transpose  Transpose_token_6_out0                     | RESULT float32  1x1024x1x64     CJYF Transpose  Transpose_token_8_out0                    
    = | RESULT float32  2x1024x8x64     PMON Mul        /attention/Mul_2                           | RESULT float32  2x1024x8x64     PMON Mul        _attention_1_aten_mul_106_n0              
    = | RESULT float32  2x1024x8x64     VSLI Add        /attention/Add_1                           | RESULT float32  2x1024x8x64     VSLI Add        _inlfunc_aten_add|folded_1_n3             
    = | RESULT float32  2x8x64x1024     RVJK Transpose  /attention/Transpose_3_output_0            | RESULT float32  2x8x64x1024     RVJK Transpose  _attention_1_transpose_3                  
    + |                                                                                            | RESULT float32  16x64x1024      RVJK Reshape    _attention_1_view_10                       
    + |                                                                                            | RESULT float32  1x1x1024x64     GSEC Transpose  _attention_1_unsqueeze_1                   
    + |                                                                                            | RESULT float32  2048x512        PWWA MatMul     _attention_q_proj_1_mm                     
    ~ | RESULT float32  2x1024x512      PWWA MatMul     /attention/q_proj/MatMul_output_0          | RESULT float32  2x1024x512      PWWA Reshape    _attention_1_attention_q_proj_1           
    = | RESULT float32  2x1024x8x64     PWWA Reshape    /attention/Reshape_output_0                | RESULT float32  2x1024x8x64     PWWA Reshape    _attention_1_view_6                       
    = | RESULT float32  2x8x1024x64     URLK Transpose  /attention/Transpose_output_0              | RESULT float32  2x8x1024x64     URLK Transpose  _attention_1_transpose                    
    = | RESULT float32  2x8x1024x32     UQCB Slice      /attention/Slice_1_output_0                | RESULT float32  2x8x1024x32     UQCB Slice      _attention_1_slice_4                      
    = | RESULT float32  2x8x1024x32     GKYZ Neg        /attention/Neg_output_0                    | RESULT float32  2x8x1024x32     GKYZ Neg        _attention_1_neg                          
    = | RESULT float32  2x8x1024x32     ACJJ Slice      /attention/Slice_output_0                  | RESULT float32  2x8x1024x32     ACJJ Slice      _attention_1_slice_3                      
    = | RESULT float32  2x8x1024x64     GNII Concat     /attention/Concat_output_0                 | RESULT float32  2x8x1024x64     GNII Concat     _attention_1_cat                          
    = | RESULT float32  2x8x1024x64     VUGZ Mul        /attention/Mul_1_output_0                  | RESULT float32  2x8x1024x64     VUGZ Mul        _attention_1_mul_1                        
    + |                                                                                            | RESULT float32  1x1x1024x64     CJYF Transpose  _attention_1_unsqueeze                     
    = | RESULT float32  2x8x1024x64     EDAD Mul        /attention/Mul_output_0                    | RESULT float32  2x8x1024x64     EDAD Mul        _attention_1_mul                          
    = | RESULT float32  2x8x1024x64     ZXHC Add        /attention/Add_output_0                    | RESULT float32  2x8x1024x64     ZXHC Add        _attention_1_add                          
    + |                                                                                            | RESULT float32  16x1024x64      ZXHC Reshape    _attention_1_view_9                        
    + |                                                                                            | RESULT float32  16x1024x1024    ZHSF MatMul     _attention_1_bmm                           
    + |                                                                                            | RESULT float32  2x8x1024x1024   ZHSF Reshape    _attention_1_view_11                       
    ~ | RESULT float32  2x8x1024x1024   MIIL FusedMatMu /attention/Div_output_0                    | RESULT float32  2x8x1024x1024   MIIL Div        _attention_1_div                          
    = | RESULT float32  2x8x1024x1024   MIIL Add        /attention/Add_2_output_0                  | RESULT float32  2x8x1024x1024   MIIL Add        _attention_1_add_2                        
    = | RESULT float32  2x8x1024x1024   NNOO Softmax    /attention/Softmax_output_0                | RESULT float32  2x8x1024x1024   NNOO Softmax    _attention_1_aten_softmax_no_dtype_163_res
    + |                                                                                            | RESULT float32  16x1024x1024    NNOO Reshape    _attention_1_view_12                       
    - | RESULT float32  2x1024x512      JYYU MatMul     /attention/v_proj/MatMul_output_0          |                                                                                           
    ~ | RESULT float32  2x1024x8x64     JYYU Reshape    /attention/Reshape_2_output_0              | RESULT float32  16x1024x64      UOUY MatMul     _attention_1_bmm_1                        
    ~ | RESULT float32  2x8x1024x64     RQVY Transpose  /attention/Transpose_2_output_0            | RESULT float32  2x8x1024x64     UOUY Reshape    _attention_1_view_14                      
    + |                                                                                            | RESULT float32  2x1024x8x64     RSHL Transpose  _attention_1_transpose_4                   
    ~ | RESULT float32  2x8x1024x64     UOUY MatMul     /attention/MatMul_1_output_0               | RESULT float32  2x1024x512      RSHL Reshape    _attention_1_view_15                      
    ~ | RESULT float32  2x1024x8x64     RSHL Transpose  /attention/Transpose_4_output_0            | RESULT float32  2048x512        RSHL Reshape    _attention_o_proj_1_view_16               
    ~ | RESULT float32  2x1024x512      RSHL Reshape    /attention/Reshape_3_output_0              | RESULT float32  2048x512        EFIK MatMul     _attention_o_proj_1_mm_3                  
    ~ | RESULT float32  2x1024x512      EFIK MatMul     130                                        | RESULT float32  2x1024x512      EFIK Reshape    attention_1                               
    = | OUTPUT float32  2x1024x512      EFIK            130                                        | OUTPUT float32  2x1024x512      EFIK            attention_1                               





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 9.699 seconds)


.. _sphx_glr_download_auto_examples_plot_llama_diff_export.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_llama_diff_export.ipynb <plot_llama_diff_export.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_llama_diff_export.py <plot_llama_diff_export.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
