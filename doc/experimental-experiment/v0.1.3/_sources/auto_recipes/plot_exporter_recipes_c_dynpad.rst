
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_recipes/plot_exporter_recipes_c_dynpad.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_recipes_plot_exporter_recipes_c_dynpad.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_recipes_plot_exporter_recipes_c_dynpad.py:


.. _l-plot-exporter-recipes-custom-dynpad:

to_onnx and padding one dimension to a mulitple of a constant
=============================================================

This is a frequent task which does not play well with dynamic shapes.
Let's see how to avoid using :func:`torch.cond`.

A model with a test
+++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 13-22

.. code-block:: Python


    import onnx
    from onnx_array_api.plotting.graphviz_helper import plot_dot
    import torch
    from onnx_diagnostic.helpers import max_diff
    from onnx_diagnostic.helpers.onnx_helper import pretty_onnx
    from experimental_experiment.reference import ExtendedReferenceEvaluator
    from experimental_experiment.torch_interpreter import to_onnx








.. GENERATED FROM PYTHON SOURCE LINES 23-24

We define a model padding to a multiple of a constant.

.. GENERATED FROM PYTHON SOURCE LINES 24-49

.. code-block:: Python



    class PadToMultiple(torch.nn.Module):
        def __init__(
            self,
            multiple: int,
            dim: int = 0,
        ):
            super().__init__()
            self.dim_to_pad = dim
            self.multiple = multiple

        def forward(self, x):
            shape = x.shape
            dim = x.shape[self.dim_to_pad]
            next_dim = ((dim + self.multiple - 1) // self.multiple) * self.multiple
            to_pad = next_dim - dim
            pad = torch.zeros(
                (*shape[: self.dim_to_pad], to_pad, *shape[self.dim_to_pad + 1 :]), dtype=x.dtype
            )
            return torch.cat([x, pad], dim=self.dim_to_pad)


    model = PadToMultiple(4, dim=1)








.. GENERATED FROM PYTHON SOURCE LINES 50-51

Let's check it runs.

.. GENERATED FROM PYTHON SOURCE LINES 51-60

.. code-block:: Python

    x = torch.randn((6, 7, 8))
    y = model(x)
    print(f"x.shape={x.shape}, y.shape={y.shape}")

    # Let's check it runs on another example.
    x2 = torch.randn((6, 8, 8))
    y2 = model(x2)
    print(f"x2.shape={x2.shape}, y2.shape={y2.shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    x.shape=torch.Size([6, 7, 8]), y.shape=torch.Size([6, 8, 8])
    x2.shape=torch.Size([6, 8, 8]), y2.shape=torch.Size([6, 8, 8])




.. GENERATED FROM PYTHON SOURCE LINES 61-65

Export
++++++

Let's defined the dynamic shapes and checks it exports.

.. GENERATED FROM PYTHON SOURCE LINES 65-72

.. code-block:: Python


    DYNAMIC = torch.export.Dim.DYNAMIC
    ep = torch.export.export(
        model, (x,), dynamic_shapes=({0: DYNAMIC, 1: DYNAMIC, 2: DYNAMIC},), strict=False
    )
    print(ep)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, x: "f32[s77, s27, s53]"):
                # No stacktrace found for following nodes
                sym_size_int_3: "Sym(s77)" = torch.ops.aten.sym_size.int(x, 0)
                sym_size_int_4: "Sym(s27)" = torch.ops.aten.sym_size.int(x, 1)
                sym_size_int_5: "Sym(s53)" = torch.ops.aten.sym_size.int(x, 2)
            
                # File: /home/xadupre/github/experimental-experiment/_doc/recipes/plot_exporter_recipes_c_dynpad.py:39 in forward, code: next_dim = ((dim + self.multiple - 1) // self.multiple) * self.multiple
                add: "Sym(s27 + 4)" = sym_size_int_4 + 4
                sub: "Sym(s27 + 3)" = add - 1;  add = None
                floordiv: "Sym(((s27 + 3)//4))" = sub // 4;  sub = None
                mul: "Sym(4*(((s27 + 3)//4)))" = floordiv * 4;  floordiv = None
            
                # File: /home/xadupre/github/experimental-experiment/_doc/recipes/plot_exporter_recipes_c_dynpad.py:40 in forward, code: to_pad = next_dim - dim
                sub_1: "Sym(-s27 + 4*(((s27 + 3)//4)))" = mul - sym_size_int_4;  mul = sym_size_int_4 = None
            
                # File: /home/xadupre/github/experimental-experiment/_doc/recipes/plot_exporter_recipes_c_dynpad.py:41 in forward, code: pad = torch.zeros(
                zeros: "f32[s77, -s27 + 4*(((s27 + 3)//4)), s53]" = torch.ops.aten.zeros.default([sym_size_int_3, sub_1, sym_size_int_5], dtype = torch.float32, device = device(type='cpu'), pin_memory = False);  sym_size_int_3 = sub_1 = sym_size_int_5 = None
            
                # File: /home/xadupre/github/experimental-experiment/_doc/recipes/plot_exporter_recipes_c_dynpad.py:44 in forward, code: return torch.cat([x, pad], dim=self.dim_to_pad)
                cat: "f32[s77, 4*(((s27 + 3)//4)), s53]" = torch.ops.aten.cat.default([x, zeros], 1);  x = zeros = None
                return (cat,)
            
    Graph signature: 
        # inputs
        x: USER_INPUT
    
        # outputs
        cat: USER_OUTPUT
    
    Range constraints: {s77: VR[2, int_oo], s27: VR[2, int_oo], s53: VR[2, int_oo]}





.. GENERATED FROM PYTHON SOURCE LINES 73-74

We can also inline the local function.

.. GENERATED FROM PYTHON SOURCE LINES 74-78

.. code-block:: Python


    onx = to_onnx(model, (x,), dynamic_shapes=({0: "batch", 1: "seq_len", 2: "num_frames"},))
    print(pretty_onnx(onx))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opset: domain='' version=18
    input: name='x' type=dtype('float32') shape=['batch', 'seq_len', 'num_frames']
    init: name='init7_s_3' type=int64 shape=() -- array([3])              -- shape_type_compute._cast_inputs.1(add)
    init: name='init7_s_4' type=int64 shape=() -- array([4])              -- Opset.make_node.1/Shape##shape_type_compute._cast_inputs.1(mul)
    init: name='init7_s_-1' type=int64 shape=() -- array([-1])            -- shape_type_compute._cast_inputs.1(mul)
    init: name='init7_s1_0' type=int64 shape=(1,) -- array([0])           -- Opset.make_node.1/Shape##Opset.make_node.1/Shape##Opset.make_node.1/Shape
    Shape(x, end=1, start=0) -> x::Shape:1
    Shape(x, end=2, start=1) -> x::Shape1:2
      Squeeze(x::Shape1:2) -> sym_size_int_4
        Add(init7_s_3, sym_size_int_4) -> _onx_add_init7_s_3
          Div(_onx_add_init7_s_3, init7_s_4) -> _onx_div_add_1
            Mul(init7_s_4, _onx_div_add_1) -> _onx_mul_init7_s_4
    Shape(x, end=3, start=2) -> x::Shape2:3
    Mul(init7_s_-1, sym_size_int_4) -> _onx_mul_init7_s_-1
      Add(_onx_mul_init7_s_-1, _onx_mul_init7_s_4) -> add_2
        Unsqueeze(add_2, init7_s1_0) -> add_2::UnSq0
      Concat(x::Shape:1, add_2::UnSq0, x::Shape2:3, axis=0) -> _onx_concat_sym_size_int_3::UnSq0
        ConstantOfShape(_onx_concat_sym_size_int_3::UnSq0, value=[0.0]) -> zeros
          Concat(x, zeros, axis=1) -> output_0
    output: name='output_0' type=dtype('float32') shape=['batch', 'seq_len+add_2', 'num_frames']




.. GENERATED FROM PYTHON SOURCE LINES 79-80

We save it.

.. GENERATED FROM PYTHON SOURCE LINES 80-82

.. code-block:: Python

    onnx.save(onx, "plot_exporter_recipes_c_dynpad.onnx")








.. GENERATED FROM PYTHON SOURCE LINES 83-87

Validation
++++++++++

Let's validate the exported model a set of inputs.

.. GENERATED FROM PYTHON SOURCE LINES 87-101

.. code-block:: Python

    ref = ExtendedReferenceEvaluator(onx)
    inputs = [
        torch.randn((6, 8, 8)),
        torch.randn((6, 7, 8)),
        torch.randn((5, 8, 17)),
        torch.randn((1, 24, 4)),
        torch.randn((3, 9, 11)),
    ]
    for inp in inputs:
        expected = model(inp)
        got = ref.run(None, {"x": inp.numpy()})
        diff = max_diff(expected, got[0])
        print(f"diff with shape={inp.shape} -> {expected.shape}: discrepancies={diff['abs']}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    diff with shape=torch.Size([6, 8, 8]) -> torch.Size([6, 8, 8]): discrepancies=0.0
    diff with shape=torch.Size([6, 7, 8]) -> torch.Size([6, 8, 8]): discrepancies=0.0
    diff with shape=torch.Size([5, 8, 17]) -> torch.Size([5, 8, 17]): discrepancies=0.0
    diff with shape=torch.Size([1, 24, 4]) -> torch.Size([1, 24, 4]): discrepancies=0.0
    diff with shape=torch.Size([3, 9, 11]) -> torch.Size([3, 12, 11]): discrepancies=0.0




.. GENERATED FROM PYTHON SOURCE LINES 102-103

And visually.

.. GENERATED FROM PYTHON SOURCE LINES 103-105

.. code-block:: Python


    plot_dot(onx, figsize=(10, 12))



.. image-sg:: /auto_recipes/images/sphx_glr_plot_exporter_recipes_c_dynpad_001.png
   :alt: plot exporter recipes c dynpad
   :srcset: /auto_recipes/images/sphx_glr_plot_exporter_recipes_c_dynpad_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.490 seconds)


.. _sphx_glr_download_auto_recipes_plot_exporter_recipes_c_dynpad.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_exporter_recipes_c_dynpad.ipynb <plot_exporter_recipes_c_dynpad.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_exporter_recipes_c_dynpad.py <plot_exporter_recipes_c_dynpad.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_exporter_recipes_c_dynpad.zip <plot_exporter_recipes_c_dynpad.zip>`


.. include:: plot_exporter_recipes_c_dynpad.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
