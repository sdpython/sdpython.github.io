{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# InputObserver with Transformers Cache\n\n:class:`InputObserver <yobx.torch.input_observer.InputObserver>` is a context manager\nthat **steals** a model's forward method during inference to record every set of inputs\nand outputs.  After the context exits, the collected data can be used to:\n\n* infer which tensor dimensions are **dynamic** across the observed calls, and\n* build a representative set of **export arguments** (with empty tensors for optional\n  inputs that were missing in some calls).\n\nThese two pieces of information are exactly what :func:`torch.export.export` and\n:func:`torch.onnx.export` need.\n\nThe example below shows three progressively richer scenarios:\n\n1. **Simple model** \u2014 two plain-tensor inputs with varying batch and sequence lengths.\n2. **LLM-like model** \u2014 inputs that include a\n   :class:`transformers.cache_utils.DynamicCache` (key-value cache), which requires\n   registering custom pytree flattening rules via\n   :func:`register_flattening_functions\n   <yobx.torch.flatten_helper.register_flattening_functions>`.\n3. **Multimodal model** \u2014 a model that receives ``pixel_values`` only on the very first\n   call (the *prefill* step).  The ``value_if_missing`` argument tells the observer what\n   to substitute when the input is absent, so that the dynamic shape analysis remains\n   possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom yobx.helpers import string_type\nfrom yobx.torch import register_flattening_functions\nfrom yobx.torch.input_observer import InputObserver\nfrom yobx.torch.transformers.cache_helper import make_dynamic_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Simple model - two tensor inputs\n\nWe start with the most basic case: a model that takes two float tensors and\nreturns their sum.  We run it with three different shapes so that the observer\ncan detect that both the batch and the sequence dimension are dynamic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AddModel(torch.nn.Module):\n    \"\"\"Adds two tensors element-wise (broadcasting on the batch dimension).\"\"\"\n\n    def forward(self, x, y):\n        return x + y\n\n\nmodel_add = AddModel()\n\ninputs_add = [\n    (torch.randn(2, 6), torch.randn(1, 6)),\n    (torch.randn(3, 7), torch.randn(1, 7)),\n    (torch.randn(4, 8), torch.randn(1, 8)),\n]\n\nobserver_add = InputObserver()\nwith observer_add(model_add):\n    for x, y in inputs_add:\n        model_add(x, y)\n\n# InputObserver captures at most 3 calls by default (store_n_calls=3).\nprint(\"Observations stored:\", observer_add.num_obs())\nassert observer_add.num_obs() == 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``infer_dynamic_shapes`` returns a tuple of per-argument shape specs, using\n``torch.export.Dim.DYNAMIC`` as a placeholder wherever a dimension varies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dyn_add = observer_add.infer_dynamic_shapes()\nprint(\"Dynamic shapes (add model):\", dyn_add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``infer_arguments`` returns one representative set of inputs with empty tensors\nsubstituted for any optional argument that was missing in some calls.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "args_add = observer_add.infer_arguments()\nprint(\"Inferred arguments:\", string_type(args_add, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LLM-like model - inputs with a DynamicCache\n\nTransformer language models store previously computed key/value pairs in a\n:class:`transformers.cache_utils.DynamicCache`.  Because ``DynamicCache`` is a custom\nclass (not a plain Python container), we must register it as a *pytree node* before\n``torch.utils._pytree.tree_flatten`` can decompose it.\n\n:func:`register_flattening_functions` is a context manager that registers all\nsupported cache types (``DynamicCache``, ``EncoderDecoderCache`` \u2026) on entry and\nunregisters them on exit.\n\nWe simulate two decoding steps: one with a short sequence in the cache and\none with a slightly longer sequence.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LLMLikeModel(torch.nn.Module):\n    \"\"\"Minimal stand-in for a causal LM forward pass.\"\"\"\n\n    def forward(self, input_ids, attention_mask=None, past_key_values=None):\n        # A real model would compute hidden states here.\n        # We just return the inputs unchanged so the example is self-contained.\n        return input_ids, past_key_values\n\n\nn_layers = 2\nn_heads = 4\nhead_dim = 32\n\n# Prefill step: the KV-cache holds 10 tokens.\ncache_prefill = make_dynamic_cache(\n    [\n        (torch.rand(1, n_heads, 10, head_dim), torch.rand(1, n_heads, 10, head_dim))\n        for _ in range(n_layers)\n    ]\n)\n\n# First decode step: the KV-cache now holds 11 tokens.\ncache_decode = make_dynamic_cache(\n    [\n        (torch.rand(1, n_heads, 11, head_dim), torch.rand(1, n_heads, 11, head_dim))\n        for _ in range(n_layers)\n    ]\n)\n\nllm_inputs = [\n    dict(\n        input_ids=torch.randint(0, 1000, (1, 10)),\n        attention_mask=torch.ones(1, 10, dtype=torch.int64),\n        past_key_values=cache_prefill,\n    ),\n    dict(\n        input_ids=torch.randint(0, 1000, (1, 1)),\n        attention_mask=torch.ones(1, 11, dtype=torch.int64),\n        past_key_values=cache_decode,\n    ),\n]\n\nmodel_llm = LLMLikeModel()\nobserver_llm = InputObserver()\n\n# The `register_flattening_functions` context manager must wrap *both* the\n# inference calls and the subsequent shape / argument inference.\nwith (\n    register_flattening_functions(patch_transformers=True),\n    observer_llm(model_llm),\n):\n    for kwargs in llm_inputs:\n        model_llm(**kwargs)\n\nprint(\"\\nObservations stored (LLM):\", observer_llm.num_obs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve dynamic shapes.  We pass ``set_batch_dimension_for=True`` to mark the\nfirst dimension of every tensor as dynamic even though both calls used batch=1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with register_flattening_functions(patch_transformers=True):\n    dyn_llm = observer_llm.infer_dynamic_shapes(set_batch_dimension_for=True)\n    kwargs_llm = observer_llm.infer_arguments()\n\nprint(\"Dynamic shapes (LLM):\", dyn_llm)\nprint(\"Inferred kwargs:\", string_type(kwargs_llm, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The shapes for ``past_key_values`` are expressed as a flat list (one entry per\nkey or value tensor across all layers).  Both dimension 0 (batch) and dimension 2\n(sequence length) are marked dynamic, while dimension 1 (heads) and dimension 3\n(head dimension) are static.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Multimodal model - pixel_values present only on the first call\n\nVision-language models like Gemma3 or LLaVA receive ``pixel_values`` only during\nthe prefill step. Subsequent decode steps omit that argument and introduce\n``past_key_values`` instead.\n\nWithout extra information the observer cannot infer an empty tensor for\n``pixel_values`` (it was never seen as an empty tensor). The ``value_if_missing``\nargument provides this information explicitly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MultimodalModel(torch.nn.Module):\n    \"\"\"Minimal stand-in for a vision-language model forward pass.\"\"\"\n\n    def forward(\n        self,\n        input_ids,\n        pixel_values=None,\n        attention_mask=None,\n        past_key_values=None,\n    ):\n        return input_ids, past_key_values\n\n\nimage_h, image_w = 224, 224\n\ncache_mm_step1 = make_dynamic_cache(\n    [\n        (torch.rand(1, n_heads, 20, head_dim), torch.rand(1, n_heads, 20, head_dim))\n        for _ in range(n_layers)\n    ]\n)\ncache_mm_step2 = make_dynamic_cache(\n    [\n        (torch.rand(1, n_heads, 21, head_dim), torch.rand(1, n_heads, 21, head_dim))\n        for _ in range(n_layers)\n    ]\n)\n\nmm_inputs = [\n    # Prefill: image + text, no past cache yet.\n    dict(\n        input_ids=torch.randint(0, 1000, (1, 20)),\n        pixel_values=torch.rand(1, 3, image_h, image_w),\n        attention_mask=torch.ones(1, 20, dtype=torch.int64),\n    ),\n    # Decode step 1: no image, but a growing KV-cache.\n    dict(\n        input_ids=torch.randint(0, 1000, (1, 1)),\n        attention_mask=torch.ones(1, 21, dtype=torch.int64),\n        past_key_values=cache_mm_step1,\n    ),\n    # Decode step 2.\n    dict(\n        input_ids=torch.randint(0, 1000, (1, 1)),\n        attention_mask=torch.ones(1, 22, dtype=torch.int64),\n        past_key_values=cache_mm_step2,\n    ),\n]\n\nmodel_mm = MultimodalModel()\n\n# Provide an empty tensor (batch=0) for pixel_values so the observer knows its\n# shape and dtype when it is absent.  The zero batch dimension signals\n# \"optional but with this shape when present\".\nobserver_mm = InputObserver(\n    value_if_missing=dict(pixel_values=torch.empty((0, 3, image_h, image_w), dtype=torch.float32))\n)\n\nwith (\n    register_flattening_functions(patch_transformers=True),\n    observer_mm(model_mm),\n):\n    for kwargs in mm_inputs:\n        model_mm(**kwargs)\n\nprint(\"\\nObservations stored (multimodal):\", observer_mm.num_obs())\n\nwith register_flattening_functions(patch_transformers=True):\n    dyn_mm = observer_mm.infer_dynamic_shapes(set_batch_dimension_for=True)\n    kwargs_mm = observer_mm.infer_arguments()\n\nprint(\"Dynamic shapes (multimodal):\", dyn_mm)\nprint(\"Inferred kwargs:\", string_type(kwargs_mm, with_shape=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that ``pixel_values`` now appears in the inferred arguments with an empty\nfirst dimension (batch=0) even though it was absent in two of the three calls.\nThe spatial dimensions 2 and 3 (height and width) are not dynamic because they\nwere always 224x224.\n\nThese shapes and arguments can be passed directly to\n:func:`torch.export.export` or :func:`torch.onnx.export`:\n\n```python\nimport torch\n\nep = torch.export.export(\n    model_mm,\n    (),\n    kwargs=kwargs_mm,\n    dynamic_shapes=dyn_mm,\n)\n```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}