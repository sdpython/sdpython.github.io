{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# MiniOnnxBuilder: serialize tensors to an ONNX model\n\n:class:`MiniOnnxBuilder <yobx.helpers.mini_onnx_builder.MiniOnnxBuilder>`\ncreates minimal ONNX models whose only purpose is to store tensors as\ninitializers and return them when the model is executed.  The model has\n**no inputs** \u2014 running it simply replays the stored values.\n\nThis is useful for:\n\n* capturing intermediate activations or model weights for debugging,\n* persisting arbitrary nested Python structures (dicts, tuples, lists,\n  torch tensors, ``DynamicCache`` \u2026) in a standard, portable format,\n* sharing small test fixtures without committing raw binary files.\n\nThe module also provides two higher-level helpers built on top of\n:class:`MiniOnnxBuilder`:\n\n* :func:`create_onnx_model_from_input_tensors\n  <yobx.helpers.mini_onnx_builder.create_onnx_model_from_input_tensors>`\n  \u2014 serialize any nested structure to an ``onnx.ModelProto``.\n* :func:`create_input_tensors_from_onnx_model\n  <yobx.helpers.mini_onnx_builder.create_input_tensors_from_onnx_model>`\n  \u2014 deserialize the model back to the original Python structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport torch\nfrom yobx.helpers.mini_onnx_builder import (\n    MiniOnnxBuilder,\n    create_onnx_model_from_input_tensors,\n    create_input_tensors_from_onnx_model,\n)\nfrom yobx.reference import ExtendedReferenceEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Store a single numpy array\n\nThe simplest use-case: add one initializer as an output and recover it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "builder = MiniOnnxBuilder()\nweights = np.array([1.0, 2.0, 3.0], dtype=np.float32)\nbuilder.append_output_initializer(\"weights\", weights)\n\nmodel = builder.to_onnx()\nref = ExtendedReferenceEvaluator(model)\n(recovered,) = ref.run(None, {})\n\nprint(\"original :\", weights)\nprint(\"recovered:\", recovered)\nassert np.array_equal(weights, recovered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Store multiple tensors (numpy + torch)\n\nSeveral calls to :meth:`append_output_initializer\n<yobx.helpers.mini_onnx_builder.MiniOnnxBuilder.append_output_initializer>`\nadd more outputs to the same model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "builder2 = MiniOnnxBuilder()\nx_np = np.arange(6, dtype=np.int64).reshape(2, 3)\nx_torch = torch.tensor([[0.1, 0.2], [0.3, 0.4]], dtype=torch.float32)\n\nbuilder2.append_output_initializer(\"x_np\", x_np)\nbuilder2.append_output_initializer(\"x_torch\", x_torch.numpy())\n\nmodel2 = builder2.to_onnx()\nref2 = ExtendedReferenceEvaluator(model2)\ngot_np, got_torch = ref2.run(None, {})\n\nprint(\"x_np   :\", got_np)\nprint(\"x_torch:\", got_torch)\nassert np.array_equal(x_np, got_np)\nassert np.allclose(x_torch.numpy(), got_torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Store a sequence of tensors\n\n:meth:`append_output_sequence\n<yobx.helpers.mini_onnx_builder.MiniOnnxBuilder.append_output_sequence>`\nwraps multiple tensors into an ONNX ``Sequence``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "builder3 = MiniOnnxBuilder()\nseq = [np.array([10, 20], dtype=np.int64), np.array([30, 40], dtype=np.int64)]\nbuilder3.append_output_sequence(\"my_seq\", seq)\n\nmodel3 = builder3.to_onnx()\nref3 = ExtendedReferenceEvaluator(model3)\n(got_seq,) = ref3.run(None, {})\n\nprint(\"sequence:\", got_seq)\nfor original, restored in zip(seq, got_seq):\n    assert np.array_equal(original, restored)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Round-trip a nested Python structure\n\nThe higher-level helpers handle arbitrary nesting of dicts, tuples,\nlists, numpy arrays and torch tensors automatically.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs = {\n    \"ids\": np.array([1, 2, 3], dtype=np.int64),\n    \"mask\": np.array([1, 1, 0], dtype=np.bool_),\n    \"hidden\": torch.randn(2, 4, dtype=torch.float32),\n}\n\nproto = create_onnx_model_from_input_tensors(inputs)\nrestored = create_input_tensors_from_onnx_model(proto)\n\nprint(\"keys:\", list(restored.keys()))\nfor k in inputs:\n    print(f\"  {k}: {inputs[k].shape} -> {restored[k].shape}\")\n    if isinstance(inputs[k], np.ndarray):\n        assert np.array_equal(inputs[k], restored[k]), f\"mismatch for {k}\"\n    else:\n        assert torch.equal(inputs[k], restored[k]), f\"mismatch for {k}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Randomize float tensors to save space\n\nWhen ``randomize=True`` the actual weight values are replaced by a\nrandom-number generator node, keeping the shape and dtype but\ndiscarding the original values.  This drastically reduces model size\nfor large weight tensors when exact values are not needed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "big = np.random.randn(128, 256).astype(np.float32)\nproto_rand = create_onnx_model_from_input_tensors(big, randomize=True)\nproto_exact = create_onnx_model_from_input_tensors(big)\n\nprint(f\"randomized model size : {proto_rand.ByteSize():>8} bytes\")\nprint(f\"exact      model size : {proto_exact.ByteSize():>8} bytes\")\nassert proto_rand.ByteSize() < proto_exact.ByteSize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot: model size comparison\n\nThe bar chart below illustrates the difference in serialized model size\nbetween a model that stores the actual weight values (``exact``) and one\nthat replaces them with a random-number generator node (``randomized``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt  # noqa: E402\n\nsizes = [proto_exact.ByteSize(), proto_rand.ByteSize()]\nlabels = [\"exact\", \"randomized\"]\n\nfig, ax = plt.subplots(figsize=(5, 4))\nbars = ax.bar(labels, sizes, color=[\"#4c72b0\", \"#dd8452\"])\nax.set_ylabel(\"Serialized size (bytes)\")\nax.set_title(\"ONNX model size: exact weights vs randomized\")\nfor bar, size in zip(bars, sizes):\n    ax.text(\n        bar.get_x() + bar.get_width() / 2,\n        bar.get_height() * 1.01,\n        f\"{size:,}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9,\n    )\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}