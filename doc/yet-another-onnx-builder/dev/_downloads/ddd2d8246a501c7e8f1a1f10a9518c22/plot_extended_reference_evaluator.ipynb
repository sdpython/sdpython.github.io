{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# ExtendedReferenceEvaluator: running models with contrib operators\n\n:class:`ExtendedReferenceEvaluator\n<yobx.reference.evaluator.ExtendedReferenceEvaluator>` extends\n:class:`onnx.reference.ReferenceEvaluator` with additional operator kernels for\nnon-standard domains such as ``com.microsoft``.\n\nThis makes it possible to execute and test ONNX models that contain ONNX\nRuntime contrib operators (e.g. ``FusedMatMul``, ``QuickGelu``) without\nneeding a full ONNX Runtime installation just for unit-testing an\noptimization pattern.\n\nThis example shows:\n\n1. Running a model with standard ONNX operators.\n2. Running a model that uses the ``FusedMatMul`` contrib operator.\n3. Running a model that uses the ``QuickGelu`` contrib operator.\n4. Adding a custom operator implementation via ``new_ops``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport onnx\nimport onnx.helper as oh\nfrom yobx.reference import ExtendedReferenceEvaluator\n\nTFLOAT = onnx.TensorProto.FLOAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Standard ONNX operators\n\n:class:`ExtendedReferenceEvaluator` is a drop-in replacement for\n:class:`onnx.reference.ReferenceEvaluator`.  Any model that runs\nwith the standard evaluator also runs here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_add = oh.make_model(\n    oh.make_graph(\n        [oh.make_node(\"Add\", [\"X\", \"Y\"], [\"Z\"])],\n        \"add_graph\",\n        [\n            oh.make_tensor_value_info(\"X\", TFLOAT, [None, None]),\n            oh.make_tensor_value_info(\"Y\", TFLOAT, [None, None]),\n        ],\n        [oh.make_tensor_value_info(\"Z\", TFLOAT, [None, None])],\n    ),\n    opset_imports=[oh.make_opsetid(\"\", 18)],\n    ir_version=10,\n)\n\nx = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\nref = ExtendedReferenceEvaluator(model_add)\n(result,) = ref.run(None, {\"X\": x, \"Y\": x})\nprint(\"Add result:\\n\", result)\nassert np.allclose(result, x + x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. FusedMatMul (com.microsoft contrib operator)\n\n``FusedMatMul`` is an ONNX Runtime contrib operator that fuses a matrix\nmultiplication with optional transpositions.  The standard\n:class:`onnx.reference.ReferenceEvaluator` does not know about it, but\n:class:`ExtendedReferenceEvaluator` does.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_fmm = oh.make_model(\n    oh.make_graph(\n        [oh.make_node(\"FusedMatMul\", [\"X\", \"Y\"], [\"Z\"], domain=\"com.microsoft\")],\n        \"fused_matmul_graph\",\n        [\n            oh.make_tensor_value_info(\"X\", TFLOAT, None),\n            oh.make_tensor_value_info(\"Y\", TFLOAT, None),\n        ],\n        [oh.make_tensor_value_info(\"Z\", TFLOAT, None)],\n    ),\n    opset_imports=[oh.make_opsetid(\"\", 18), oh.make_opsetid(\"com.microsoft\", 1)],\n)\n\na = np.arange(4, dtype=np.float32).reshape(2, 2)\nref_fmm = ExtendedReferenceEvaluator(model_fmm)\n(z,) = ref_fmm.run(None, {\"X\": a, \"Y\": a})\nprint(\"FusedMatMul result:\\n\", z)\nassert np.allclose(z, a @ a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With ``transA=1`` the first operand is transposed before the multiplication.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_fmm_t = oh.make_model(\n    oh.make_graph(\n        [oh.make_node(\"FusedMatMul\", [\"X\", \"Y\"], [\"Z\"], domain=\"com.microsoft\", transA=1)],\n        \"fused_matmul_transA_graph\",\n        [\n            oh.make_tensor_value_info(\"X\", TFLOAT, None),\n            oh.make_tensor_value_info(\"Y\", TFLOAT, None),\n        ],\n        [oh.make_tensor_value_info(\"Z\", TFLOAT, None)],\n    ),\n    opset_imports=[oh.make_opsetid(\"\", 18), oh.make_opsetid(\"com.microsoft\", 1)],\n)\n\nref_fmm_t = ExtendedReferenceEvaluator(model_fmm_t)\n(z_t,) = ref_fmm_t.run(None, {\"X\": a, \"Y\": a})\nprint(\"FusedMatMul(transA=1) result:\\n\", z_t)\nassert np.allclose(z_t, a.T @ a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. QuickGelu (com.microsoft contrib operator)\n\n``QuickGelu`` applies the gated sigmoid activation\n``x * sigmoid(alpha * x)`` element-wise.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_gelu = oh.make_model(\n    oh.make_graph(\n        [oh.make_node(\"QuickGelu\", [\"X\"], [\"Z\"], domain=\"com.microsoft\", alpha=1.702)],\n        \"quick_gelu_graph\",\n        [oh.make_tensor_value_info(\"X\", TFLOAT, None)],\n        [oh.make_tensor_value_info(\"Z\", TFLOAT, None)],\n    ),\n    opset_imports=[oh.make_opsetid(\"\", 18), oh.make_opsetid(\"com.microsoft\", 1)],\n)\n\nx_gelu = np.array([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=np.float32)\nref_gelu = ExtendedReferenceEvaluator(model_gelu)\n(z_gelu,) = ref_gelu.run(None, {\"X\": x_gelu})\nprint(\"QuickGelu result:\", z_gelu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Adding a custom operator via ``new_ops``\n\nAny :class:`OpRun <onnx.reference.op_run.OpRun>` subclass can be passed\nthrough the ``new_ops`` argument.  The built-in :attr:`default_ops\n<yobx.reference.evaluator.ExtendedReferenceEvaluator.default_ops>` are\nalways merged in automatically, so you only need to list your additions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnx.reference.op_run import OpRun  # noqa: E402\n\n\nclass Scale(OpRun):\n    \"\"\"Multiplies every element of X by a constant *factor*.\"\"\"\n\n    op_domain = \"my.domain\"\n\n    def _run(self, X, factor=2.0):  # type: ignore[override]\n        return (X * np.float32(factor),)\n\n\nmodel_custom = oh.make_model(\n    oh.make_graph(\n        [oh.make_node(\"Scale\", [\"X\"], [\"Z\"], domain=\"my.domain\", factor=3.0)],\n        \"scale_graph\",\n        [oh.make_tensor_value_info(\"X\", TFLOAT, [None])],\n        [oh.make_tensor_value_info(\"Z\", TFLOAT, [None])],\n    ),\n    opset_imports=[oh.make_opsetid(\"\", 18), oh.make_opsetid(\"my.domain\", 1)],\n    ir_version=10,\n)\n\nx_s = np.array([1.0, 2.0, 3.0], dtype=np.float32)\nref_custom = ExtendedReferenceEvaluator(model_custom, new_ops=[Scale])\n(z_s,) = ref_custom.run(None, {\"X\": x_s})\nprint(\"Scale(factor=3) result:\", z_s)\nassert np.allclose(z_s, x_s * 3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Listing the default operators\n\n:attr:`default_ops` shows all operator implementations that are\nregistered automatically.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pprint  # noqa: E402\n\npprint.pprint(ExtendedReferenceEvaluator.default_ops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plot: QuickGelu activation curve\n\nThe ``QuickGelu`` function ``x * sigmoid(alpha * x)`` is plotted below\nalongside a plain sigmoid and the identity line for comparison.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt  # noqa: E402\n\nalpha = 1.702\nx_plot = np.linspace(-4, 4, 200).astype(np.float32)\nsigmoid = 1.0 / (1.0 + np.exp(-alpha * x_plot))\nquick_gelu_vals = x_plot * sigmoid\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(x_plot, quick_gelu_vals, label=\"QuickGelu (\u03b1=1.702)\")\nax.plot(x_plot, sigmoid, linestyle=\"--\", label=\"sigmoid(\u03b1\u00b7x)\")\nax.axhline(0, color=\"k\", linewidth=0.5)\nax.axvline(0, color=\"k\", linewidth=0.5)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"QuickGelu activation function\")\nax.legend()\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}