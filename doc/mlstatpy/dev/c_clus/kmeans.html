
<!DOCTYPE html>


<html lang="fr" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>k-means &#8212; Documentation mlstatpy 0.4.0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/translations.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'c_clus/kmeans';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Recherche" href="../search.html" />
    <link rel="next" title="Mélange de lois normales" href="gauss_mixture.html" />
    <link rel="prev" title="Clustering" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/project_ico.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/project_ico.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_ml/index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_ml/index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_ml/index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../license.html">
                        License
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_ml/index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_ml/index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_ml/index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../license.html">
                        License
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Navigation de la section">
  <p class="bd-links__title" role="heading" aria-level="1">Navigation de la section</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">k-means</a></li>
<li class="toctree-l1"><a class="reference internal" href="gauss_mixture.html">Mélange de lois normales</a></li>
<li class="toctree-l1"><a class="reference internal" href="kohonen.html">Carte de Kohonen</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Fils d'Ariane">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Fil d'Ariane">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Acceuil">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Clustering</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">k-means</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="k-means">
<span id="l-k-means"></span><h1>k-means<a class="headerlink" href="#k-means" title="Lien permanent vers cette rubrique">#</a></h1>
<nav class="contents local" id="sommaire">
<ul class="simple">
<li><p><a class="reference internal" href="#principe" id="id23">Principe</a></p>
<ul>
<li><p><a class="reference internal" href="#homogeneite-des-dimensions" id="id24">Homogénéité des dimensions</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#ameliorations-de-l-initialisation" id="id25">Améliorations de l’initialisation</a></p>
<ul>
<li><p><a class="reference internal" href="#l-kmeanspp" id="id26">K-means++</a></p></li>
<li><p><a class="reference internal" href="#id4" id="id27">K-means||</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#estimation-de-probabilites" id="id28">Estimation de probabilités</a></p></li>
<li><p><a class="reference internal" href="#selection-du-nombre-de-classes" id="id29">Sélection du nombre de classes</a></p>
<ul>
<li><p><a class="reference internal" href="#critere-de-qualite" id="id30">Critère de qualité</a></p></li>
<li><p><a class="reference internal" href="#maxima-de-la-fonction-densite" id="id31">Maxima de la fonction densité</a></p></li>
<li><p><a class="reference internal" href="#decroissance-du-nombre-de-classes" id="id32">Décroissance du nombre de classes</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#extension-des-nuees-dynamiques" id="id33">Extension des nuées dynamiques</a></p>
<ul>
<li><p><a class="reference internal" href="#classes-elliptiques" id="id34">Classes elliptiques</a></p></li>
<li><p><a class="reference internal" href="#rival-penalized-competitive-learning-rpcl" id="id35">Rival Penalized Competitive Learning (RPCL)</a></p></li>
<li><p><a class="reference internal" href="#rpcl-based-local-pca" id="id36">RPCL-based local PCA</a></p></li>
<li><p><a class="reference internal" href="#frequency-sensitive-competitive-learning-fscl" id="id37">Frequency Sensitive Competitive Learning (FSCL)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#k-means-norme-l1" id="id38">k-means norme L1</a></p></li>
<li><p><a class="reference internal" href="#bibliographie" id="id39">Bibliographie</a></p></li>
</ul>
</nav>
<p><em>Dénomination française : algorithme des centres mobiles.</em></p>
<section id="principe">
<span id="index-0"></span><h2><a class="toc-backref" href="#id23" role="doc-backlink">Principe</a><a class="headerlink" href="#principe" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Les centres mobiles ou nuées dynamiques sont un algorithme de classification
<em>non supervisée</em>. A partir d’un ensemble de points, il détermine pour un
nombre de classes fixé, une répartition des points qui minimise un
critère appelé <em>inertie</em> ou variance <em>intra-classe</em>.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme0">
<div class="docutils container">
</div>
<p class="admonition-title" id="kmeans-def-algo">Algorithme A1 : centre mobile, k-means</p>
<p>On considère un ensemble de points :</p>
<div class="math">
<p><img src="../_images/math/4bc17ca2dc85199245723f33d15ebd8a4a6c3a91.svg" alt="\left(X_i\right)_{1\leqslant i\leqslant P}\in\left(\R^N\right)^P"/></p>
</div><p>A chaque point est associée une classe :
<img class="math" src="../_images/math/8fbb04a7fd604cf3d5ab4f74d7a34f443fee16b0.svg" alt="\left(c_i\right)_{1\leqslant i\leqslant P}\in\left\{1,...,C\right\}^P"/>.
On définit les barycentres des classes :
<img class="math" src="../_images/math/e02fb0b4af5767f50daa4aeb3db241fefaebac5f.svg" alt="\left( G_i\right)_{1\leqslant i\leqslant C}\in\left(\R^N\right)^C"/>.</p>
<p><em>Initialisation</em></p>
<p>L’initialisation consiste à choisir pour chaque point une classe aléatoirement dans
<img class="math" src="../_images/math/2ab98387c03ab0e650306cbd442f1f9a60ff4218.svg" alt="\left\{1,...,C\right\}"/>. On pose <img class="math" src="../_images/math/e75ee36648418afabd4cbd3434fb5e3f14b2dd35.svg" alt="t = 0"/>.</p>
<p id="hmm-cm-step-bary"><em>Calcul des barycentres</em></p>
<div class="line-block">
<div class="line">for k in <img class="math" src="../_images/math/f963c7631cb87c320d1687686f34c5e48b508b2d.svg" alt="1..C"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/8a3db512bbb25da3c950ee29e091b8bbd5386d18.svg" alt="G_k^t \longleftarrow \sum_{i=1}^P X_i \, \mathbf{1}_{\left\{c_i^t=k\right\}} \sum_{i=1}^P \mathbf{1}_{\left\{c_i^t=k\right\}}"/></div>
</div>
</div>
<p><em>Calcul de l’inertie</em></p>
<div class="math">
<p><img src="../_images/math/28fa5741d138e33445bca8bd990c592a523aa6b9.svg" alt="\begin{array}{lll}
I^t &amp;\longleftarrow&amp; \sum_{i=1}^P \; d^2\left(X_i, G_{c_i^t}^t\right) \\
t   &amp;\longleftarrow&amp; t+1
\end{array}"/></p>
</div><div class="line-block">
<div class="line">if <img class="math" src="../_images/math/00d07f71cd210dbc4d6982774175b3b746d6330e.svg" alt="t &gt; 0"/> et <img class="math" src="../_images/math/620d5f81891c433e5f48def7a7191a56b1d5dfe6.svg" alt="I_t \sim I_{t-1}"/></div>
<div class="line-block">
<div class="line">arrêt de l’algorithme</div>
</div>
</div>
<p id="hmm-cm-step-attr"><em>Attribution des classes</em></p>
<div class="line-block">
<div class="line">for in <img class="math" src="../_images/math/df2683ce64cbd8aa455227bc05a9dff56f80cd46.svg" alt="1..P"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/cf85f2c54f14763d61530fad52f5f476aeb71ed1.svg" alt="c_i^{t+1} \longleftarrow \underset{k}{\arg\min} \; d\left(  X_{i},G_{k}^{t}\right)"/></div>
<div class="line">où <img class="math" src="../_images/math/c2cc82176a4ab3caca55fe95f563d4e0657fbf8f.svg" alt="d\left(X_i,G_k^t\right)"/> est la distance entre <img class="math" src="../_images/math/c15b262677ad7177c2e37298a2eb382d712b3a52.svg" alt="X_i"/> et <img class="math" src="../_images/math/75014aeefe371ad56993a725fb07f0591c0c8820.svg" alt="G_k^t"/></div>
</div>
</div>
<p>Retour à l’étape du calcul des barycentres jusqu’à convergence de l’inertie <img class="math" src="../_images/math/8384668a714bfd2a900009fc5988cdcae0e5679a.svg" alt="I^t"/>.</p>
</div>
<div class="admonition-mathdef admonition" id="indexmathe-Théorème0">
<div class="docutils container">
</div>
<p class="admonition-title" id="theoreme-inertie-1">Théorème T1 : convergence des k-means</p>
<p>Quelque soit l’initialisation choisie, la suite <img class="math" src="../_images/math/dc6e79a6b925c6370d9f1098066ee6268c69fb4d.svg" alt="\pa{I_t}_{t\supegal 0}"/>
construite par l’algorithme des <a class="reference internal" href="#kmeans-def-algo"><span class="std std-ref">k-means</span></a>
converge.</p>
</div>
<p>La démonstration du théorème nécessite le lemme suivant.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Lemme0">
<div class="docutils container">
</div>
<p class="admonition-title" id="lemme-inertie-minimum">Lemme L1 : inertie minimum</p>
<p>Soit <img class="math" src="../_images/math/41ef498bf4da2bfbc9a4ce88342a5716ba0e1228.svg" alt="\vecteur{X_1}{X_P} \in \pa{\R^N}^P"/>,
<img class="math" src="../_images/math/2f2a105c289b01c226ad6e0f506d96386c48d440.svg" alt="P"/> points de <img class="math" src="../_images/math/cd812dcc80d48ccbacc00d59d47335f43339484f.svg" alt="\R^N"/>, le minimum de la quantité
<img class="math" src="../_images/math/537a053863f4779bed0748e02cc50fae3397c018.svg" alt="Q\pa{Y \in \R^N}"/> :</p>
<div class="math">
<p><img src="../_images/math/24c842009c8496b5cfa1ba182a599e7f574954f6.svg" alt="\begin{eqnarray}
Q\pa{Y} &amp;=&amp; \sum_{i=1}^P \; d^2\pa{X_i,Y}
\end{eqnarray}"/></p>
</div><p>est atteint pour <img class="math" src="../_images/math/9640141c96e289d1cc695d1777f52505fcf46649.svg" alt="Y=G=\dfrac{1}{P} \sum_{i=1}^{P} X_i"/>
le barycentre des points <img class="math" src="../_images/math/cf168a3f3647fa1f5fa81a83378c60fc279eecf3.svg" alt="\vecteur{X_1}{X_P}"/>.</p>
</div>
<p>Soit <img class="math" src="../_images/math/41ef498bf4da2bfbc9a4ce88342a5716ba0e1228.svg" alt="\vecteur{X_1}{X_P} \in \pa{\R^N}^P"/>,
<img class="math" src="../_images/math/2f2a105c289b01c226ad6e0f506d96386c48d440.svg" alt="P"/> points de <img class="math" src="../_images/math/cd812dcc80d48ccbacc00d59d47335f43339484f.svg" alt="\R^N"/>.</p>
<div class="math">
<p><img src="../_images/math/56d2e8de01fe4f0d78ee2fcadf0e72b7965fc34b.svg" alt="\begin{eqnarray*}
                    \sum_{i=1}^{P} \overrightarrow{GX_{i}} = \overrightarrow{0}
&amp;\Longrightarrow&amp;      \sum_{i=1}^{P} d^2\pa{X_i,Y} = \sum_{i=1}^{P} d^2\pa{X_i,G}+ P \, d^2\pa{G,Y} \\
&amp;\Longrightarrow&amp;     \underset{Y\in\R^N}{\arg\min} \; \sum_{i=1}^{P} d^2\pa{X_i,Y} = \acc{G}
\end{eqnarray*}"/></p>
</div><p>On peut maintenant démontrer le théorème.
L’étape d’attribution des classes consiste à attribuer à chaque
point le barycentre le plus proche. On définit <img class="math" src="../_images/math/3b1d41c0394f1d0df53dcd8a361ce1a3c2fc5160.svg" alt="J_t"/> par :</p>
<div class="math">
<p><img src="../_images/math/543a741e1011b1ec47fda7a2979b2b21b386ae0d.svg" alt="\begin{eqnarray}
J^{t+1} &amp;=&amp; \sum_{i=1}^{P} \; d^2\pa{ X_i, G_{c_i^{t+1}}^t}
\end{eqnarray}"/></p>
</div><p>On en déduit que :</p>
<div class="math">
<p><img src="../_images/math/e51293c212d0e06e0bb1fd38dedd893bbfa954c3.svg" alt="\begin{eqnarray}
J^{t+1}    &amp;=&amp; \sum_{i, c_i^t \neq c_i^{t+1}} \; d^2\pa{ X_i, G_{c_i^{t+1}}^t} + J^{t+1} \sum_{i, c_i^t = c_i^{t+1}} \; d^2\pa{ X_i, G_{c_i^{t+1}}^t}  \\
J^{t+1}    &amp;\infegal&amp;  \sum_{i, c_i^t \neq c_i^{t+1}} \; d^2\pa{ X_i, G_{c_i^{t}}^t} + \sum_{i, c_i^t = c_i^{t+1}} \; d^2\pa{ X_i, G_{c_i^{t}}^t} \\
J^{t+1}    &amp;\infegal&amp;  I^t
\end{eqnarray}"/></p>
</div><p>Le lemme précédent appliqué à chacune des classes <img class="math" src="../_images/math/79c96636ca69f9cb49bd9df5dab7ef30f8d5ba3e.svg" alt="\ensemble{1}{C}"/>,
permet d’affirmer que <img class="math" src="../_images/math/e54903941d087b31e6c61363d1272531014718ef.svg" alt="I^{t+1} \infegal J^{t+1}"/>.
Par conséquent, la suite <img class="math" src="../_images/math/dc6e79a6b925c6370d9f1098066ee6268c69fb4d.svg" alt="\pa{I_t}_{t\supegal 0}"/> est décroissante et minorée par
0, elle est donc convergente.</p>
<p id="index-1">L’algorithme des centres mobiles cherche à attribuer à chaque
point de l’ensemble une classe parmi les <img class="math" src="../_images/math/2c0e77bc741b692ecdf6e85966263a89422af4c8.svg" alt="C"/> disponibles.
La solution trouvée dépend de l’initialisation et n’est pas forcément
celle qui minimise l’inertie intra-classe : l’inertie finale est
un minimum local. Néanmoins, elle assure que la partition est formée
de classes convexes : soit <img class="math" src="../_images/math/3950d3def11e9f0784eacec55d2b79c3a0bf19df.svg" alt="c_1"/> et <img class="math" src="../_images/math/c50924711ece3aac359df4fd02db9d6f74bb0fb1.svg" alt="c_2"/> deux classes différentes,
on note <img class="math" src="../_images/math/0247a196052dd9baf0366fbf8f6a1d87e64c7c5f.svg" alt="C_1"/> et <img class="math" src="../_images/math/5838b45439d93f4669eab86d0796eeb6e7195cba.svg" alt="C_2"/> les enveloppes convexes des points qui
constituent ces deux classes, alors
<img class="math" src="../_images/math/d985fae5e74d3e4571004f70e62cd235c3007e27.svg" alt="\overset{o}{C_1} \cap \overset{o}{C_2} = \emptyset"/>.
La figure suivante présente un exemple d’utilisation de l’algorithme
des centres mobiles. Des points sont générés aléatoirement
dans le plan et répartis en quatre groupes.</p>
<img alt="../_images/cm.png" src="../_images/cm.png" />
<p>C’est une application des centres mobiles avec une classification en quatre classes
d’un ensemble aléatoire de points plus dense sur la partie droite du graphe. Les quatre classes
ainsi formées sont convexes.</p>
<section id="homogeneite-des-dimensions">
<span id="hmm-classification-obs-deux"></span><h3><a class="toc-backref" href="#id24" role="doc-backlink">Homogénéité des dimensions</a><a class="headerlink" href="#homogeneite-des-dimensions" title="Lien permanent vers cette rubrique">#</a></h3>
<p>Les coordonnées des points
<img class="math" src="../_images/math/dd086705964b88aa3e74471351d44ca6915c1c66.svg" alt="\left(X_i\right) \in \R^N"/> sont généralement non homogènes :
les ordres de grandeurs de chaque dimension sont différents.
C’est pourquoi il est conseillé de centrer et normaliser chaque dimension.
On note : <img class="math" src="../_images/math/0ebe5f3361de03c441c9b2cbf93bb1339755661c.svg" alt="\forall i \in \intervalle{1}{P}, \; X_i = \vecteur{X_{i,1}}{X_{i,N}}"/> :</p>
<div class="math">
<p><img src="../_images/math/2d27398f90cd778fc8219befb5f4969fccdc1f11.svg" alt="\begin{eqnarray*}
g_k &amp;=&amp; \pa{EX}_k = \frac{1}{P} \sum_{i=1}^P X_{i,k} \\
v_{kk} &amp;=&amp; \pa{E\left(X-EX\right)^2}_{kk}=\pa{EX^2}_{kk} - g_k^2
\end{eqnarray*}"/></p>
</div><p>Les points centrés et normalisés sont :</p>
<div class="math">
<p><img src="../_images/math/2e5a10fc46f97a6baa43b43deddb13bf0349f460.svg" alt="\forall i \in \intervalle{1}{P}, \;
X_i^{\prime}=\left(\dfrac{x_{i,1}-g_{1}}{\sqrt{v_{11}}},...,\dfrac{x_{i,N}-g_{N}}{\sqrt{v_{NN}}}\right)"/></p>
</div><p id="index-2">L’algorithme des centres mobiles est appliqué sur l’ensemble
<img class="math" src="../_images/math/b2057ecde740acd964bd1160b7789a09dfb3c71c.svg" alt="\left( X_{i}^{\prime}\right)_{1\leqslant i\leqslant P}"/>.
Il est possible ensuite de décorréler les variables ou d’utiliser
une distance dite de <a class="reference external" href="https://fr.wikipedia.org/wiki/Distance_de_Mahalanobis">Malahanobis</a> définie par
<img class="math" src="../_images/math/cac45acdfd9138ea17baa41d675fcda9e107bc4d.svg" alt="d_M\pa{X, Y} = X \, M \, Y'"/> où <img class="math" src="../_images/math/2f38be5157e7f5029042c49957a29cf7c2f29ee0.svg" alt="Y'"/>
désigne la transposée de <img class="math" src="../_images/math/b59453388a752e340d555df9064bfb27e7112e68.svg" alt="Y"/> et <img class="math" src="../_images/math/e5c619f0600e251cabb3318b03870bc6f2c4870f.svg" alt="M"/>
est une matrice symétrique définie positive.
Dans le cas de variables corrélées, la matrice
<img class="math" src="../_images/math/8f2e404988e46f58a6e37c86071bc82d8322d3de.svg" alt="M = \Sigma^{-1}"/> où <img class="math" src="../_images/math/101365d041befcd5108977fd092dbd8bc9785dbc.svg" alt="\Sigma^{-1}"/> est la matrice
de variance-covariance des variables aléatoires <img class="math" src="../_images/math/72643a45ac37dbe64e26a084f8d7bf0c765e522b.svg" alt="\pa{X_i}_i"/>.</p>
</section>
</section>
<section id="ameliorations-de-l-initialisation">
<h2><a class="toc-backref" href="#id25" role="doc-backlink">Améliorations de l’initialisation</a><a class="headerlink" href="#ameliorations-de-l-initialisation" title="Lien permanent vers cette rubrique">#</a></h2>
<section id="l-kmeanspp">
<span id="id1"></span><h3><a class="toc-backref" href="#id26" role="doc-backlink">K-means++</a><a class="headerlink" href="#l-kmeanspp" title="Lien permanent vers cette rubrique">#</a></h3>
<p id="index-3">L’article <a class="reference internal" href="#arthur2007" id="id2"><span>[Arthur2007]</span></a> montre que l’initialisation aléatoire n’est pas efficace et
est sensible aux outliers ou points aberrants. L’étape d’initialisation est remplacée
par la suivante :</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme1">
<div class="docutils container">
</div>
<p class="admonition-title" id="init-kmeanspp">Algorithme A2 : initialisation k-means++</p>
<p>Cette étape d’initialisation viendra remplacer celle
définie dans l’algorithme
<a class="reference internal" href="#kmeans-def-algo"><span class="std std-ref">k-means</span></a>.
On considère un ensemble de points :</p>
<div class="math">
<p><img src="../_images/math/7572d5ba79ddbb47b4abc01312e9688ed3a8d365.svg" alt="X=\left(X_i\right)_{1\leqslant i\leqslant P}\in\left(\R^N\right)^P"/></p>
</div><p>A chaque point est associée une classe :
<img class="math" src="../_images/math/8fbb04a7fd604cf3d5ab4f74d7a34f443fee16b0.svg" alt="\left(c_i\right)_{1\leqslant i\leqslant P}\in\left\{1,...,C\right\}^P"/>.</p>
<p>Pour <img class="math" src="../_images/math/312028c07e271534bd0dbde5434e49e76880744f.svg" alt="k"/> centres, on choisit <img class="math" src="../_images/math/0247a196052dd9baf0366fbf8f6a1d87e64c7c5f.svg" alt="C_1"/>
au hasard dans l’ensemble <img class="math" src="../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/>.
Pour les suivants :</p>
<ol class="arabic simple">
<li><p><img class="math" src="../_images/math/f3426cfc16493940b22f7321730b3e3c6e4aa4bb.svg" alt="k \leftarrow 2"/></p></li>
<li><p>On choisit aléatoirement <img class="math" src="../_images/math/e005910ee0a428539915444c2ea62c0b8c6bba74.svg" alt="G_k \in X"/> avec la probabilité
<img class="math" src="../_images/math/b217ea0d59e42a46339694765455b3f276d18b0d.svg" alt="P(x) = \frac{D_{k-1}(x)^2}{\sum_{x\in X}D_{k-1}(x)^2}"/></p></li>
<li><p><img class="math" src="../_images/math/26f65fceab6599facc16133f496cd1c24b9073f8.svg" alt="k \leftarrow k+1"/></p></li>
<li><p>On revient à l’étape 2 jusqu’à ce que <img class="math" src="../_images/math/2142171ad258df7aa0ffd1bc4abe7f785f080ecb.svg" alt="k=C"/>.</p></li>
</ol>
<p>La fonction <img class="math" src="../_images/math/96d648b9ba49faf544f18bffe162bff2e293634f.svg" alt="D_k"/> est définie par la distance du point <img class="math" src="../_images/math/cdd160a57af92571d6838c69c7bde600843a23bf.svg" alt="x"/>
au centre <img class="math" src="../_images/math/36b42e357e2e138a2e83f331c2467495293aac6d.svg" alt="G_l"/> choisi parmi les <img class="math" src="../_images/math/312028c07e271534bd0dbde5434e49e76880744f.svg" alt="k"/> premiers centres.
<img class="math" src="../_images/math/560def0284f129eff8f708e4149e8c97bab18d6a.svg" alt="D_k(x) = \min_{1 \infegal l \infegal k} d(x - G_l)"/>.</p>
<p>La suite de l’algorithme <em>k-means++</em> reprend les mêmes étapes que
<a class="reference internal" href="#kmeans-def-algo"><span class="std std-ref">k-means</span></a>.</p>
</div>
<p>Cette initilisation éloigne le prochain centre le plus possibles des
centres déjà choisis. L’article montre que :</p>
<div class="admonition-mathdef admonition" id="indexmathe-Théorème1">
<p class="admonition-title">Théorème T2 : Borne supérieure de l’erreur produite par k-means++</p>
<p>On définit l’inertie par
<img class="math" src="../_images/math/1326dc7c9643c65fdf963b33d46e022fc14bbac9.svg" alt="J_(X) = \sum_{i=1}^{P} \; \min_G d^2(X_i, G)"/>.
Si <img class="math" src="../_images/math/83176a4f55c394ebf2e1fdf290bd87eb5a99a345.svg" alt="J_{OPT}"/> définit l’inertie optimale alors
<img class="math" src="../_images/math/29f958dfb0c32e2a3520d49880144e2f76a926f3.svg" alt="\esp{J(X)} \infegal 8 (\ln C + 2) J_{OPT}(X)"/>.</p>
</div>
<p>La démonstration est disponible dans l’article <a class="reference internal" href="#arthur2007" id="id3"><span>[Arthur2007]</span></a>.</p>
</section>
<section id="id4">
<h3><a class="toc-backref" href="#id27" role="doc-backlink">K-means||</a><a class="headerlink" href="#id4" title="Lien permanent vers cette rubrique">#</a></h3>
<p>L’article <a class="reference internal" href="#bahmani2012" id="id5"><span>[Bahmani2012]</span></a> propose une autre initialisation
que <a class="reference internal" href="#l-kmeanspp"><span class="std std-ref">K-means++</span></a> mais plus rapide et parallélisable.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme2">
<div class="docutils container">
</div>
<p class="admonition-title" id="init-kmeansppll">Algorithme A3 : initialisation k-means||</p>
<p>Cette étape d’initialisation viendra remplacer celle
définie dans l’algorithme
<a class="reference internal" href="#kmeans-def-algo"><span class="std std-ref">k-means</span></a>.
On considère un ensemble de points :</p>
<div class="math">
<p><img src="../_images/math/7572d5ba79ddbb47b4abc01312e9688ed3a8d365.svg" alt="X=\left(X_i\right)_{1\leqslant i\leqslant P}\in\left(\R^N\right)^P"/></p>
</div><p>A chaque point est associée une classe :
<img class="math" src="../_images/math/8fbb04a7fd604cf3d5ab4f74d7a34f443fee16b0.svg" alt="\left(c_i\right)_{1\leqslant i\leqslant P}\in\left\{1,...,C\right\}^P"/>.</p>
<p>Pour <img class="math" src="../_images/math/312028c07e271534bd0dbde5434e49e76880744f.svg" alt="k"/> centres, on choisit <img class="math" src="../_images/math/67ff5828e87602fb558f801a96396559e04e0dd9.svg" alt="G = \{G_1\}"/>
au hasard dans l’ensemble <img class="math" src="../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/>.</p>
<div class="line-block">
<div class="line">on répète <img class="math" src="../_images/math/0a185a031e88ca1155325a6e0ba9c06642d3fab1.svg" alt="O(\ln D(G, X))"/> fois :</div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/e193c978ec2415600c7ea2810f794ff2959db4c2.svg" alt="G' \leftarrow"/> échantillon aléatoire issue de <img class="math" src="../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/> de probabilité <img class="math" src="../_images/math/4eea0ad742152d6108fde2a6a484470903d03702.svg" alt="p(x) = l \frac{D(G,x)^2}{\sum_x D(G,x)^2}"/></div>
<div class="line"><img class="math" src="../_images/math/6261544c59180ee36cd1108bd4e15069bd94c12f.svg" alt="G \leftarrow G \cup G'"/></div>
</div>
</div>
<p>La fonction <img class="math" src="../_images/math/6f299d138def92cb9b8de6e24011d19bbbfb2e76.svg" alt="D(G,x)"/> est définie par la distance du point <img class="math" src="../_images/math/cdd160a57af92571d6838c69c7bde600843a23bf.svg" alt="x"/>
au plus proche centre <img class="math" src="../_images/math/d3a5ece8d7ec78151cf34c4ac504aa41edd1f660.svg" alt="g \in G"/> :
<img class="math" src="../_images/math/3a7b447089b20dc22e9407de72af078c6c025584.svg" alt="D(g,x) = \min_{g \in G} d(x - g)"/>.
Cette étape ajoute à l’ensemble des centres <img class="math" src="../_images/math/86a30eae2899d36dcee14ab62c5e4c8a68feed4d.svg" alt="G"/>
un nombre aléatoire de centres à chaque étape.
L’ensemble <img class="math" src="../_images/math/86a30eae2899d36dcee14ab62c5e4c8a68feed4d.svg" alt="G"/> contiendra plus de <img class="math" src="../_images/math/2c0e77bc741b692ecdf6e85966263a89422af4c8.svg" alt="C"/> centres.</p>
<ol class="arabic simple">
<li><p>Pour tout <img class="math" src="../_images/math/d3a5ece8d7ec78151cf34c4ac504aa41edd1f660.svg" alt="g \in G"/>, on assigne le poids <img class="math" src="../_images/math/7a384ce2009fcbc54c587a963141c4ba88347609.svg" alt="w_g = card \acc{ y | d(x, y) &lt; \min_{h \in G} d(x, h)}"/></p></li>
<li><p>On clusterise l’ensemble des points <img class="math" src="../_images/math/86a30eae2899d36dcee14ab62c5e4c8a68feed4d.svg" alt="G"/> en <img class="math" src="../_images/math/2c0e77bc741b692ecdf6e85966263a89422af4c8.svg" alt="C"/> clusters
(avec un k-means classique par exemple)</p></li>
</ol>
</div>
<p>Au lieu d’ajouter les centres un par un comme dans l’algorithme
<a class="reference internal" href="#init-kmeanspp"><span class="std std-ref">k-means++</span></a>, plusieurs sont ajoutés à chaque fois,
plus <img class="math" src="../_images/math/b40d7de3530507b378090edb54198bdc06e33be9.svg" alt="l"/> est grand, plus ce nombre est grand. Le tirage d’un échantillon
aléatoire consiste à inclure chaque point <img class="math" src="../_images/math/cdd160a57af92571d6838c69c7bde600843a23bf.svg" alt="x"/> avec la probabilité
<img class="math" src="../_images/math/4eea0ad742152d6108fde2a6a484470903d03702.svg" alt="p(x) = l \frac{D(G,x)^2}{\sum_x D(G,x)^2}"/>.</p>
</section>
</section>
<section id="estimation-de-probabilites">
<span id="hmm-classification-obs-trois"></span><h2><a class="toc-backref" href="#id28" role="doc-backlink">Estimation de probabilités</a><a class="headerlink" href="#estimation-de-probabilites" title="Lien permanent vers cette rubrique">#</a></h2>
<p>A partir de cette classification en <img class="math" src="../_images/math/2c0e77bc741b692ecdf6e85966263a89422af4c8.svg" alt="C"/> classes, on construit un
vecteur de probabilités pour chaque point <img class="math" src="../_images/math/112a6e7af6c6a5a3440b0fea3f5d5503c1ff9336.svg" alt="\pa{X_{i}}_{1 \infegal i \infegal P}"/>
en supposant que la loi de <img class="math" src="../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/> sachant sa classe <img class="math" src="../_images/math/f69ba531fba42e37e610be4c5e36f06fa00e78f4.svg" alt="c_X"/> est une loi
normale multidimensionnelle. La classe de <img class="math" src="../_images/math/c15b262677ad7177c2e37298a2eb382d712b3a52.svg" alt="X_i"/> est
notée <img class="math" src="../_images/math/fa4ae29716395f8ebb2444ec26b1c6da2f3b53db.svg" alt="c_i"/>. On peut alors écrire :</p>
<div class="math">
<p><img src="../_images/math/9a5fb39737956619460c225d56792438543f79bb.svg" alt="\begin{eqnarray*}
\forall i \in \intervalle{1}{C}, \; &amp; &amp; \\
G_i &amp;=&amp; E\pa{X \indicatrice{c_X = i}} = \dfrac{\sum_{k=1}^{P} X_k \indicatrice {c_k = i}} {\sum_{k=1}^{P} \indicatrice {c_k = i}} \\
V_i &amp;=&amp; E\pa{XX' \indicatrice{c_X = i}} = \dfrac{\sum_{k=1}^{P} X_k X_k' \indicatrice {c_k = i}} {\sum_{k=1}^{P} \indicatrice {c_k = i}} \\
\pr{c_X = i} &amp;=&amp; \sum_{k=1}^{P} \indicatrice {c_k = i} \\
f\pa{X | c_X = i} &amp;=&amp; \dfrac{1}{\pa{2\pi}^{\frac{N}{2}} \sqrt{\det \pa{V_i}}} \; e^{ - \frac{1}{2} \pa{X - G_i}' \; V_i^{-1} \; \pa{X - G_i} } \\
f\pa{X} &amp;=&amp; \sum_{k=1}^{P}  f\pa{X | c_X = i} \pr{c_X = i}
\end{eqnarray*}"/></p>
</div><p>On en déduit que :</p>
<div class="math">
<p><img src="../_images/math/8f1131ba92f299ab42d7e3925c303eab93f02633.svg" alt="\pr{c_X = i |X } = \dfrac{f\pa{X | c_X = i}\pr{c_X = i}} {f\pa{X} }"/></p>
</div><p>La densité des obervations est alors modélisée par une mélange de
lois normales, chacune centrée au barycentre de chaque classe.
Ces probabilités peuvent également être apprises par un réseau de neurones
classifieur où servir d’initialisation à un
<a class="reference external" href="https://fr.wikipedia.org/wiki/Algorithme_esp%C3%A9rance-maximisation">algorithme EM</a>.</p>
</section>
<section id="selection-du-nombre-de-classes">
<h2><a class="toc-backref" href="#id29" role="doc-backlink">Sélection du nombre de classes</a><a class="headerlink" href="#selection-du-nombre-de-classes" title="Lien permanent vers cette rubrique">#</a></h2>
<section id="critere-de-qualite">
<span id="classification-selection-nb-classe-bouldin"></span><h3><a class="toc-backref" href="#id30" role="doc-backlink">Critère de qualité</a><a class="headerlink" href="#critere-de-qualite" title="Lien permanent vers cette rubrique">#</a></h3>
<p>L’algorithme des centres mobiles effectue une classification non supervisée
à condition de connaître au préalable le nombre de classes et
cette information est rarement disponible. Une alternative consiste à
estimer la pertinence des classifications obtenues pour différents
nombres de classes, le nombre de classes optimal est celui
qui correspond à la classification la plus pertinente.
Cette pertinence ne peut être estimée de manière unique, elle dépend des
hypothèses faites sur les éléments à classer, notamment sur la forme
des classes qui peuvent être convexes ou pas, être modélisées par des
lois normales multidimensionnelles, à matrice de covariances diagonales, …
Les deux critères qui suivent sont adaptés à l’algorithme des centres mobiles.
Le critère de <a class="reference external" href="https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index">Davies-Bouldin</a>
(voir <a class="reference internal" href="#davies1979" id="id6"><span>[Davies1979]</span></a>)
est minimum lorsque le nombre de classes est optimal.</p>
<div class="math" id="index-4">
<p><img src="../_images/math/1fc5f1924491aa845b7d3b4aca6781b95d97cf7f.svg" alt="\begin{eqnarray}
DB &amp;=&amp; \dfrac{1}{C} \;     \sum_{i=1}^{C} \; \max_{i \neq j} \; \dfrac{\sigma_i + \sigma_j}{ d\pa{C_i,C_j}}
\end{eqnarray}"/></p>
</div><p>Avec :</p>
<table class="table">
<colgroup>
<col style="width: 33.3%" />
<col style="width: 66.7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><img class="math" src="../_images/math/2c0e77bc741b692ecdf6e85966263a89422af4c8.svg" alt="C"/></p></th>
<th class="head"><p>nombre de classes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img class="math" src="../_images/math/918edf1163affce55968c2b270f7ebe1865389aa.svg" alt="\sigma_i"/></p></td>
<td><p>écart-type des distances des observations de la classe <img class="math" src="../_images/math/6c9c36eeb0679ad71efe34ded19d79168983fb38.svg" alt="i"/></p></td>
</tr>
<tr class="row-odd"><td><p><img class="math" src="../_images/math/028f0a9b50db11bb7b06a1941fbc0c441259482c.svg" alt="C_i"/></p></td>
<td><p>centre de la classe <img class="math" src="../_images/math/6c9c36eeb0679ad71efe34ded19d79168983fb38.svg" alt="i"/></p></td>
</tr>
</tbody>
</table>
<p>Le critère de <a class="reference external" href="https://en.wikipedia.org/wiki/Goodman_and_Kruskal%27s_gamma">Goodman-Kruskal</a>
(voir <a class="reference internal" href="#goodman1954" id="id7"><span>[Goodman1954]</span></a>) est quant à lui maximum lorsque le nombre de classes est optimal.
Il est toutefois plus coûteux à calculer.</p>
<div class="math" id="index-5">
<p><img src="../_images/math/0768685dee2700f421deb6400b093439d6cbd03e.svg" alt="\begin{eqnarray}
GK &amp;=&amp; \dfrac{S^+ - S^-} { S^+ + S^-}
\end{eqnarray}"/></p>
</div><p>Avec :</p>
<div class="math">
<p><img src="../_images/math/3970264331ba20df051df7bb3fd64d7e416e1b4c.svg" alt="\begin{eqnarray*}
S^+ &amp;=&amp; \acc{ \pa{q,r,s,t} \sac d\pa{q,r} &lt; d\pa{s,t} } \\
S^- &amp;=&amp; \acc{ \pa{q,r,s,t} \sac d\pa{q,r} &lt; d\pa{s,t} }
\end{eqnarray*}"/></p>
</div><p>Où <img class="math" src="../_images/math/347189a1a98974758d795362c319c74abba1aca3.svg" alt="\pa{q,r}"/> sont dans la même classe et <img class="math" src="../_images/math/c00163f11b47dc018c3149112d68ee2c77359f1a.svg" alt="\pa{s,t}"/> sont dans des classes différentes.</p>
<table class="table">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="../_images/class_4.png" src="../_images/class_4.png" />
</td>
<td><img alt="../_images/class_4_db.png" src="../_images/class_4_db.png" />
</td>
</tr>
</tbody>
</table>
<p>Classification en quatre classes : nombre de classes sélectionnées par le critère
de Davies-Bouldin dont les valeurs sont illustrées par le graphe apposé à droite.</p>
</section>
<section id="maxima-de-la-fonction-densite">
<h3><a class="toc-backref" href="#id31" role="doc-backlink">Maxima de la fonction densité</a><a class="headerlink" href="#maxima-de-la-fonction-densite" title="Lien permanent vers cette rubrique">#</a></h3>
<p>L’article <a class="reference internal" href="#herbin2001" id="id8"><span>[Herbin2001]</span></a> propose une méthode différente pour estimer
le nombre de classes, il s’agit tout d’abord d’estimer la fonction
densité du nuage de points qui est une fonction de
<img class="math" src="../_images/math/31cac255ed81f7f22aa9652e1f8d97eb62f62cd6.svg" alt="\R^n \longrightarrow \R"/>. Cette estimation est effectuée au moyen
d’une méthode non paramètrique telle que les estimateurs à noyau
(voir <a class="reference internal" href="#silverman1986" id="id9"><span>[Silverman1986]</span></a>)
Soit <img class="math" src="../_images/math/26f924af3662c85fdae93b78379cf709640d2119.svg" alt="\vecteur{X_1}{X_N}"/> un nuage de points inclus dans une image,
on cherche à estimer la densité <img class="math" src="../_images/math/6f28fd7539a56974612f2213cb591ec0a3a84dfa.svg" alt="f_H\pa{x}"/> au pixel <img class="math" src="../_images/math/cdd160a57af92571d6838c69c7bde600843a23bf.svg" alt="x"/> :</p>
<div class="math">
<p><img src="../_images/math/89da891f3b8e707b7d971bcae862881797a35bf1.svg" alt="\hat{f}_H\pa{x} = \dfrac{1}{N} \; \sum_{i=1}^{N} \; \dfrac{1}{\det H} \; K\pa{ H^{-1} \pa{x - X_i}}"/></p>
</div><p>Où :</p>
<div class="math">
<p><img src="../_images/math/6bbe174691ee6154a0430ed381cf37c64c380bf4.svg" alt="K\pa{x} = \dfrac{1}{ \pa{2 \pi}^{ \frac{d}{2}} } \; e^{ - \frac{ \norme{x}^2 } {2} }"/></p>
</div><p><img class="math" src="../_images/math/dd8a8ece0b1684a9537eb7947dc6b8fdc1652abc.svg" alt="H"/> est un paramètre estimée avec la règle de Silverman.
L’exemple utilisé dans cet article est un problème de segmentation
d’image qui ne peut pas être résolu par la méthode des nuées
dynamiques puisque la forme des classes n’est pas convexe,
ainsi que le montre la figure suivante. La fonction de densité
<img class="math" src="../_images/math/bd1e0600a66dbe0f8deddbd57953774419ba4e4f.svg" alt="f"/> est seuillée de manière à obtenir une fonction
<img class="math" src="../_images/math/5c98e0e3a3ed82bf15fb2ed51543d1956c2b6288.svg" alt="g : \R^n \longrightarrow \acc{0,1}"/> définie par :</p>
<div class="math">
<p><img src="../_images/math/93c93cd7cae65ced8e18c991415b215492d994bc.svg" alt="g \pa{x} = \indicatrice{f\pa{x} \supegal s}"/></p>
</div><p id="index-6">L’ensemble <img class="math" src="../_images/math/bbcdc8c2d36c1abe68f95af51f4179598b8c64e8.svg" alt="g^{-1}\pa{\acc{1}} \subset \R^n"/>
est composée de <img class="math" src="../_images/math/bceb9186b5004313ecccd0d22d07ea9617b62f98.svg" alt="N"/> composantes connexes notées
<img class="math" src="../_images/math/a20f10afa060e7ffe17e890961a8c5c514e607aa.svg" alt="\vecteur{C_1}{C_N}"/>, la classe d’un point <img class="math" src="../_images/math/cdd160a57af92571d6838c69c7bde600843a23bf.svg" alt="x"/>
est alors l’indice de la composante connexe à la
laquelle il appartient ou la plus proche le cas échéant.</p>
<table class="table">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="../_images/herbin1.png" src="../_images/herbin1.png" />
</td>
<td><img alt="../_images/herbin2.png" src="../_images/herbin2.png" />
</td>
</tr>
</tbody>
</table>
<p>Exemple de classification non supervisée appliquée à un problème
de segmentation d’image, la première figure montre la densité obtenue,
la seconde figure illustre la classification obtenue, figure extraite de <a class="reference internal" href="#herbin2001" id="id10"><span>[Herbin2001]</span></a>.
Cette méthode paraît néanmoins difficilement applicable lorsque la
dimension de l’espace vectoriel atteint de grande valeur. L’exemple de l’image
est pratique, elle est déjà découpée en région représentées par les pixels,
l’ensemble <img class="math" src="../_images/math/e86e25f9bcdebece2cacba7f08ce89fff5255162.svg" alt="g^{-1}\pa{\acc{1}}"/> correspond à
l’ensemble des pixels <img class="math" src="../_images/math/cdd160a57af92571d6838c69c7bde600843a23bf.svg" alt="x"/> pour lesquels <img class="math" src="../_images/math/f4fe0782fbd5a03d4b57fb70f83e9517bca8ed6b.svg" alt="f\pa{x} \supegal s"/>.</p>
</section>
<section id="decroissance-du-nombre-de-classes">
<h3><a class="toc-backref" href="#id32" role="doc-backlink">Décroissance du nombre de classes</a><a class="headerlink" href="#decroissance-du-nombre-de-classes" title="Lien permanent vers cette rubrique">#</a></h3>
<p>L’article <a class="reference internal" href="#kothari1999" id="id11"><span>[Kothari1999]</span></a> propose une méthode permettant de
faire décroître le nombre de classes afin de choisir le nombre
approprié. L’algorithme des centres mobiles
proposent de faire décroître l’inertie notée <img class="math" src="../_images/math/49d95c6215d3875f7504526ed066025568d9ebd7.svg" alt="I"/>
définie pour un ensemble de points noté <img class="math" src="../_images/math/c892e2c9602fd4163676454715101e9a50ce09f3.svg" alt="X = \vecteur{x_1}{x_N}"/>
et <img class="math" src="../_images/math/2c1a946cef5763c39269198f722873a239852f50.svg" alt="K"/> classes. La classe d’un élément <img class="math" src="../_images/math/cdd160a57af92571d6838c69c7bde600843a23bf.svg" alt="x"/>
est notée <img class="math" src="../_images/math/95bf46381c090f66d5e98e0d3be6e029f6548726.svg" alt="C\pa{x}"/>. Les centres des classes sont notés
<img class="math" src="../_images/math/074ee5adf2cbdff1b8d7b0ead440d12636e4aabe.svg" alt="Y = \vecteur{y_1}{y_K}"/>.
L’inertie de ce nuage de points est définie par :</p>
<div class="math">
<p><img src="../_images/math/31be75e9671bc62b1758455dbe298cb2252619fe.svg" alt="I  =  \sum_{x \in X} \; \norme{ x - y_{C\pa{x} }}^2"/></p>
</div><p>On définit tout d’abord une distance
<img class="math" src="../_images/math/ea5dd5178d29172fe56eacd1c12149a2b8955455.svg" alt="\alpha \in \R^+"/>, puis l’ensemble
<img class="math" src="../_images/math/ccd1e4d7f6aa47c9c967189327462351135ceca6.svg" alt="V\pa{y,\alpha} = \acc{ z \in Y \sac d\pa{y,z} \infegal \alpha }"/>,
<img class="math" src="../_images/math/28a053cbc22f90a9759293ff092729de4c8f1ba3.svg" alt="V\pa{y,\alpha}"/> est donc l’ensemble des voisins des
centres dont la distance avec <img class="math" src="../_images/math/bd7dd5ee003cda2294a46fd47aa8a1ecf28b0418.svg" alt="y"/> est inférieur à <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/>.
L’article <a class="reference internal" href="#kothari1999" id="id12"><span>[Kothari1999]</span></a> propose de minimiser le coût <img class="math" src="../_images/math/f3e6140f911171400bdaceb5d083d9209c5edcd3.svg" alt="J\pa{\alpha}"/>
suivant :</p>
<div class="math">
<p><img src="../_images/math/90f2a125b6b5f85477afd3470a19e16b3101e243.svg" alt="J\pa{\alpha} = \sum_{x \in X} \; \norme{ x - y_{C\pa{x} }}^2 + \sum_{x \in X} \;
\sum_{y \in V\pa{y_{C\pa{x}}, \alpha} } \; \lambda\pa{y} \, \norme{ y -  y_{C\pa{x}}}^2"/></p>
</div><p>Lorsque <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/> est nul, ce facteur est égal à l’inertie :
<img class="math" src="../_images/math/4610cbefa0b0e891dffd33740d3ed6da052b0960.svg" alt="I = J\pa{0}"/> et ce terme est minimal lorsqu’il y a autant de
classes que d’éléments dans <img class="math" src="../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/>. Lorsque <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/>
tend vers l’infini, <img class="math" src="../_images/math/addb662507d13d8b52795a48d310f8ac82d303aa.svg" alt="J\pa{\alpha} \rightarrow J\pa{\infty}"/> où :</p>
<div class="math">
<p><img src="../_images/math/d2ac8602353aef93ea590fe08b5d7744eedd8850.svg" alt="J\pa{\infty} = \sum_{x \in X} \; \norme{ x - y_{C\pa{x} }}^2 + \sum_{x \in X} \; \sum_{y \in Y} \;
\lambda\pa{y} \, \norme{ y -  y_{C\pa{x}}} ^2"/></p>
</div><p>Ici encore, il est possible de montrer que ce terme
<img class="math" src="../_images/math/7ac7481f42895412479c377e02d6ca83a3bb6eef.svg" alt="J\pa{\infty}"/> est minimal lorsqu’il n’existe plus qu’une
seule classe. Le principe de cette méthode consiste à faire varier
le paramètre <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/>, plus le paramètre <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/> augmente,
plus le nombre de classes devra être réduit. Néanmoins, il existe
des intervalles pour lequel ce nombre de classes est stable,
le véritable nombre de classes de l’ensemble <img class="math" src="../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/>
sera considéré comme celui correspondant au plus grand intervalle
stable.</p>
<table class="table">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="../_images/koth1.png" src="../_images/koth1.png" />
</td>
<td><img alt="../_images/koth2.png" src="../_images/koth2.png" />
</td>
</tr>
<tr class="row-even"><td><p><em>(a)</em></p></td>
<td><p><em>(b)</em></p></td>
</tr>
</tbody>
</table>
<p>Evolutation du nombre de classes en fonction du paramètre <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/> lors de la
minimisation du critère <img class="math" src="../_images/math/f3e6140f911171400bdaceb5d083d9209c5edcd3.svg" alt="J\pa{\alpha}"/>, figure extraite de <a class="reference internal" href="#kothari1999" id="id13"><span>[Kothari1999]</span></a>.
La première image représente le nuage de points illustrant quatre classes sans recouvrement.
La seconde image montre que quatre classes est l’état le plus longtemps stable
lorsque <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/> croît.</p>
<p id="index-7">Le coût <img class="math" src="../_images/math/f3e6140f911171400bdaceb5d083d9209c5edcd3.svg" alt="J\pa{\alpha}"/> est une somme de coût dont
l’importance de l’un par rapport à l’autre est contrôle
par les paramètres <img class="math" src="../_images/math/80afa851b8fd175808926b7332be30d5049ac387.svg" alt="\lambda\pa{y}"/>. Le problème de
minimisation de <img class="math" src="../_images/math/f3e6140f911171400bdaceb5d083d9209c5edcd3.svg" alt="J\pa{\alpha}"/> est résolu par l’algorithme qui suit.
Il s’appuie sur la méthode des multiplicateurs de Lagrange.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme3">
<div class="docutils container">
</div>
<p class="admonition-title" id="classification-kothari-1999">Algorithme A4 : sélection du nombre de classes</p>
<p>(voir  <a class="reference internal" href="#kothari1999" id="id14"><span>[Kothari1999]</span></a>)
Les notations sont celles utilisés dans les paragraphes précédents. On suppose que le
paramètre <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/> évolue dans l’intervalle <img class="math" src="../_images/math/07a162d4ebb87d9aff1af094d5e7a401193212fb.svg" alt="\cro{\alpha_1, \alpha_2}"/>
à intervalle régulier <img class="math" src="../_images/math/b16448ae42ff1802844bc83136c27e139a1b4be6.svg" alt="\alpha_t"/>.
Le nombre initial de classes est noté <img class="math" src="../_images/math/2c1a946cef5763c39269198f722873a239852f50.svg" alt="K"/> et il est supposé surestimer le véritable
nombre de classes. Soit <img class="math" src="../_images/math/0af93cb0b8d164e9534df19c3e33895aca887fe8.svg" alt="\eta \in \left]0,1\right["/>,
ce paramètre doit être choisi de telle sorte que dans
l’algorithme qui suit, l’évolution des centres <img class="math" src="../_images/math/785ba36534ec0d91ce856196476629c1d1f2715d.svg" alt="y_k"/>
soit autant assurée par le premier de la fonction de coût que par le second.</p>
<p><em>initialisation</em></p>
<div class="math">
<p><img src="../_images/math/f73d5167f2b49ca17745dfed32ea3daa88001439.svg" alt="\alpha \longleftarrow \alpha_1"/></p>
</div><p>On tire aléatoirement les centres des <img class="math" src="../_images/math/2c1a946cef5763c39269198f722873a239852f50.svg" alt="K"/> classes <img class="math" src="../_images/math/7a56d6c75a92645253c6ef84dd286840abdbf0fd.svg" alt="\vecteur{y_1}{y_K}"/>.</p>
<p><em>préparation</em></p>
<p>On définit les deux suites entières <img class="math" src="../_images/math/bf703c0d5c7e771b938eb3394d199ff195365c09.svg" alt="\vecteur{c^1_1}{c^1_K}"/>, <img class="math" src="../_images/math/b0512fd7cd213cc45a3ff646ddf9b126de778c6e.svg" alt="\vecteur{c^2_1}{c^2_K}"/>,
et les deux suites de vecteur <img class="math" src="../_images/math/ab6bdf44db868b8f626a24676119e1fad6c8d884.svg" alt="\vecteur{z^1_1}{z^1_K}"/>,
<img class="math" src="../_images/math/433c8f9af7a82dd5c6bf2e59101ef85ff36c60c9.svg" alt="\vecteur{z^2_1}{z^2_K}"/>.</p>
<div class="math">
<p><img src="../_images/math/7162f87c88eebed2bf2ae5ed53d0248e9ac17a32.svg" alt="\begin{array}{rlll}
\forall k, &amp;  c^1_k &amp;=&amp; 0 \\
\forall k, &amp;  c^2_k &amp;=&amp; 0 \\
\forall k, &amp;  z^1_k &amp;=&amp; 0 \\
\forall k, &amp;  z^2_k &amp;=&amp; 0
\end{array}"/></p>
</div><p><em>calcul des mises à jour</em></p>
<div class="line-block">
<div class="line">for i in <img class="math" src="../_images/math/e4d60e95747872a941338b80fe2b8a00b4b8429e.svg" alt="1..N"/></div>
<div class="line-block">
<div class="line">Mise à jour d’après le premier terme de la fonction de coût <img class="math" src="../_images/math/f3e6140f911171400bdaceb5d083d9209c5edcd3.svg" alt="J\pa{\alpha}"/>.</div>
<div class="line"><img class="math" src="../_images/math/26b9a8cb1af1a39e1bd23e64230a6045979c8622.svg" alt="w \longleftarrow \underset{1 \infegal l \infegal K}{\arg \min} \; \norme{x_i - y_l}^2"/></div>
<div class="line"><img class="math" src="../_images/math/c62cc04a2a9e8462d3730bebf0d7a71c5fcffc4d.svg" alt="z^1_w \longleftarrow z^1_w + \eta \pa{ x_i - y_w}"/></div>
<div class="line"><img class="math" src="../_images/math/e0a900b1f252eca04a439fc8ba280471b0fe287f.svg" alt="c^1_w \longleftarrow c^1_w + 1"/></div>
<div class="line"><br /></div>
<div class="line">Mise à jour d’après le second terme de la fonction de coût <img class="math" src="../_images/math/f3e6140f911171400bdaceb5d083d9209c5edcd3.svg" alt="J\pa{\alpha}"/></div>
<div class="line"><br /></div>
<div class="line">for v in <img class="math" src="../_images/math/addc0166f989e29d1c798c0c5f3e67c87458d71e.svg" alt="1..k"/></div>
<div class="line-block">
<div class="line">if <img class="math" src="../_images/math/2ede1329c159e73f44a8ae9d888d916f15715dd1.svg" alt="\norme{y_v - y_w} &lt; \alpha"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/067f9f7794c24f4895d70674b56365d2f24bc3bf.svg" alt="z^2_v \longleftarrow z^2_v - \pa{ y_v - y_w}"/></div>
<div class="line"><img class="math" src="../_images/math/1dcc26f65780391461c4266fb214fc5bd5be7558.svg" alt="c^2_v \longleftarrow c^2_v + 1"/></div>
<div class="line"><br /></div>
</div>
</div>
<div class="line">for v in <img class="math" src="../_images/math/addc0166f989e29d1c798c0c5f3e67c87458d71e.svg" alt="1..k"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/7ec218064d0ff287b9e40bbecdc922096e4bdb2b.svg" alt="\lambda_v \longleftarrow \frac{ c^2_v \norme{z^1_v} } { c^1_v \norme{z^2_v} }"/></div>
<div class="line"><img class="math" src="../_images/math/b5a6888df5ed8b61a742f85268ce721f7e49e923.svg" alt="y_v \longleftarrow y_v + z^1_v + \lambda_v z^2_v"/></div>
</div>
</div>
</div>
<p><em>convergence</em></p>
<p>Tant que l’étape précédente n’a pas convergé vers une version stable des centres,
<img class="math" src="../_images/math/785ba36534ec0d91ce856196476629c1d1f2715d.svg" alt="y_k"/>, retour à l’étape précédente. Sinon, tous les couples de classes <img class="math" src="../_images/math/2fb092f228ae912a8b75d8fc2ad7929b84aa9483.svg" alt="\pa{i,j}"/>
vérifiant <img class="math" src="../_images/math/b98226fd74e5a8f077f4051c2c6afca781680b6d.svg" alt="\norme{y_i - y_j} &gt; \alpha"/> sont fusionnés :
<img class="math" src="../_images/math/49fd77d5a18fbf5909d5b71ab19a89b7fe6cceaa.svg" alt="\alpha \longleftarrow \alpha + \alpha_t"/>.
Si <img class="math" src="../_images/math/4b7e1147e0d853196279289e16d3132115b35211.svg" alt="\alpha \infegal \alpha2"/>, retour à l’étape de préparation.</p>
<p><em>terminaison</em></p>
<p>Le nombre de classes est celui ayant prévalu pour le plus grand nombre de valeur de <img class="math" src="../_images/math/a1e7b66a55fc77cebd91d9a1ebfa93a29c1736fb.svg" alt="\alpha"/>.</p>
</div>
</section>
</section>
<section id="extension-des-nuees-dynamiques">
<h2><a class="toc-backref" href="#id33" role="doc-backlink">Extension des nuées dynamiques</a><a class="headerlink" href="#extension-des-nuees-dynamiques" title="Lien permanent vers cette rubrique">#</a></h2>
<section id="classes-elliptiques">
<span id="classification-nuees-dynamique-extension"></span><h3><a class="toc-backref" href="#id34" role="doc-backlink">Classes elliptiques</a><a class="headerlink" href="#classes-elliptiques" title="Lien permanent vers cette rubrique">#</a></h3>
<p id="index-8">La version de l’algorithme des nuées dynamique proposée dans l’article
<a class="reference internal" href="#cheung2003" id="id15"><span>[Cheung2003]</span></a> suppose que les classes ne sont plus de forme circulaire
mais suivent une loi normale quelconque. La loi de l’échantillon
constituant le nuage de points est de la forme :</p>
<div class="math">
<p><img src="../_images/math/37adf413e36e8b14c77a742988f64916af67c86c.svg" alt="f\pa{x} =  \sum_{i=1}^{N} \; p_i \; \dfrac{1}{\pa{2 \pi}^{\frac{d}{2}}\sqrt{\det \Sigma_i}} \; exp \pa{-\frac{1}{2}  \pa{x-\mu_i}' \Sigma_i^{-1} \pa{x-\mu_i} }"/></p>
</div><p>Avec <img class="math" src="../_images/math/f3e0f7f4cefff66c9c97898243d62b1862457ed8.svg" alt="sum_{i=1}^{N} \; p_i = 1"/>. On définit :</p>
<div class="math">
<p><img src="../_images/math/46b7df49a10e3ed7d44f05ed384524a85f13806f.svg" alt="G\pa{x, \mu, \Sigma} = \dfrac{1}{\pa{2 \pi}^{\frac{d}{2}}\sqrt{\det \Sigma}} \; exp \pa{-\frac{1}{2}  \pa{x-\mu}' \Sigma^{-1} \pa{x-\mu} }"/></p>
</div><p>L’algorithme qui suit a pour objectif de minimiser la quantité pour un échantillon <img class="math" src="../_images/math/7ccf5fe9b0a4a7c90908c020c8f6660232583105.svg" alt="\vecteur{X_1}{X_K}"/> :</p>
<div class="math">
<p><img src="../_images/math/73e2bf77e19594999e2934785a3229d253db06dc.svg" alt="I = \sum_{i=1}^{N}\sum_{k=1}^{K} \indicatrice{ i = \underset{1 \infegal j \infegal N}{\arg \max}
G\pa{X_k, \mu_j,\Sigma_j} } \; \ln \cro{ p_i G\pa{ X_k, \mu_i, \Sigma_i } }"/></p>
</div><div class="admonition-mathdef admonition" id="indexmathe-Algorithme4">
<p class="admonition-title">Algorithme A5 : nuées dynamiques généralisées</p>
<p>Les notations sont celles utilisées dans ce paragraphe. Soient <img class="math" src="../_images/math/b3b84fabe7c42a7e9b77253ea0d56f2812634ba8.svg" alt="\eta"/>,
<img class="math" src="../_images/math/17d10a5a2c9e2139752a8ebb8c733ccc29293c20.svg" alt="\eta_s"/> deux réels tels que <img class="math" src="../_images/math/b864139223cc8bcb51ff3c36ddcaadc00d1949af.svg" alt="\eta &gt; \eta_s"/>.
La règle préconisée par l’article <a class="reference internal" href="#cheung2003" id="id16"><span>[Cheung2003]</span></a> est <img class="math" src="../_images/math/277952fbe124fd0ff6a3174af75c663c6d97301b.svg" alt="\eta_s \sim \frac{\eta}{10}"/>.</p>
<p><em>initialisation</em></p>
<p><img class="math" src="../_images/math/b00a80936656de39ddcaf3e556b63da3145c2258.svg" alt="t \longleftarrow 0"/>.
Les paramètres <img class="math" src="../_images/math/3f26d0cc23c001fe7da9984a1b66e040dd9a2b39.svg" alt="\acc{p_i^0, \mu_i^0, \Sigma_i^0 \sac 1 \infegal i \infegal N}"/> sont initialisés
grâce à un algorithme des <a class="reference internal" href="#kmeans-def-algo"><span class="std std-ref">k-means</span></a> ou <a class="reference internal" href="#label-kmeans-fscl"><span class="std std-ref">FSCL</span></a>.
<img class="math" src="../_images/math/5efb9b8bac01ce9912371bc225e335d9061c92bd.svg" alt="\forall i, \; p_i^0 = \frac{1}{N}"/> et <img class="math" src="../_images/math/c57eeb0a88c2d9caceb75b59901b834fae2d15c5.svg" alt="\beta_i^0 = 0"/>.</p>
<p><em>récurrence</em></p>
<p>Soit <img class="math" src="../_images/math/a64fc33c15255329b8e9309a9a44e1e0f012f72f.svg" alt="X_k"/> choisi aléatoirement dans <img class="math" src="../_images/math/7ccf5fe9b0a4a7c90908c020c8f6660232583105.svg" alt="\vecteur{X_1}{X_K}"/>.</p>
<div class="math">
<p><img src="../_images/math/8dc79cf046593e7278151da34c5b503e25b42702.svg" alt="i = \underset{1 \infegal i \infegal N}{\arg \min} \; G\pa{X_k, \mu_i^t, \Sigma_i^t}"/></p>
</div><div class="line-block">
<div class="line">for i in <img class="math" src="../_images/math/e4d60e95747872a941338b80fe2b8a00b4b8429e.svg" alt="1..N"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/adb57abc9603e0235b766603a17c83819d695994.svg" alt="\mu_i^{t+1} = \mu_i^t + \eta \, \pa{\Sigma_i^t}^{-1} \, \pa{ X_k - \mu_i^t}"/></div>
<div class="line"><img class="math" src="../_images/math/8f35e78c82f2656e410a45a49e6d57c2dd541fc7.svg" alt="\beta_i^{t+1} = \beta_i^t + \eta \, \pa{1 - \alpha_i^t}"/></div>
<div class="line"><img class="math" src="../_images/math/fa5968afec024ee9b3edd0a37ca365633ea6ee75.svg" alt="\Sigma^{t+1}_i = \pa{1 - \eta_s} \, \Sigma_i^t + \eta_s \, \pa{ X_k - \mu_i^t} \pa{ X_k - \mu_i^t}'"/></div>
<div class="line"><br /></div>
</div>
<div class="line">for i in <img class="math" src="../_images/math/e4d60e95747872a941338b80fe2b8a00b4b8429e.svg" alt="1..N"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/d7f1de0bc9afbff6a3bcf3d3bb37151cd1affc46.svg" alt="p^{t+1}_i = \frac{ e^{ \beta_i^{t+1} } } { \sum_{j=1}^{N} e^{ \beta_j^{t+1} } }"/></div>
<div class="line"><br /></div>
</div>
<div class="line"><img class="math" src="../_images/math/c38192a0bf7fc25d58110b59ec881f89dae27bb7.svg" alt="t \longleftarrow t + 1"/></div>
</div>
<p><em>terminaison</em></p>
<p>Tant que <img class="math" src="../_images/math/560911f6447159e4f8056751f37b91b40c852090.svg" alt="\underset{1 \infegal i \infegal N}{\arg \min} \; G\pa{X_k, \mu_i^t, \Sigma_i^t}"/>
change pour au moins un des points <img class="math" src="../_images/math/a64fc33c15255329b8e9309a9a44e1e0f012f72f.svg" alt="X_k"/>.</p>
</div>
<p>Lors de la mise à jour de <img class="math" src="../_images/math/101365d041befcd5108977fd092dbd8bc9785dbc.svg" alt="\Sigma^{-1}"/>,
l’algorithme précédent propose la mise à jour de <img class="math" src="../_images/math/7db6957ff4e3716bec2334044990cfc332851bb9.svg" alt="\Sigma_i"/>
alors que le calcul de <img class="math" src="../_images/math/74fe222dfce78c1d224c9499b08b7c11c849db18.svg" alt="G\pa{., \mu_i, \Sigma_i}"/>
implique <img class="math" src="../_images/math/83b454fbaf2d869cc101169851dfa6d12333aad5.svg" alt="\Sigma_i^{-1}"/>,
par conséquent, il est préférable de mettre à jour directement la matrice
<img class="math" src="../_images/math/101365d041befcd5108977fd092dbd8bc9785dbc.svg" alt="\Sigma^{-1}"/> :</p>
<div class="math">
<p><img src="../_images/math/e7c86991000483459e58075e1e136f6ed27ba59f.svg" alt="\pa{\Sigma^{t+1}_i}^{-1} = \frac{ \pa{\Sigma_i^t}^{-1} } {1 - \eta_s}
\cro{I - \frac{ \eta_s  \pa{ X_k - \mu_i^t} \pa{ X_k - \mu_i^t}' \pa{\Sigma_i^t}^{-1} }
{1 - \eta_s + \eta_s \pa{ X_k - \mu_i^t}' \, \pa{\Sigma_i^t}^{-1}\pa{ X_k - \mu_i^t} } }"/></p>
</div></section>
<section id="rival-penalized-competitive-learning-rpcl">
<span id="class-rpcl"></span><h3><a class="toc-backref" href="#id35" role="doc-backlink">Rival Penalized Competitive Learning (RPCL)</a><a class="headerlink" href="#rival-penalized-competitive-learning-rpcl" title="Lien permanent vers cette rubrique">#</a></h3>
<p id="index-9">L’algorithme suivant développé dans <a class="reference internal" href="#xu1993" id="id17"><span>[Xu1993]</span></a>, est une variante de celui des centres mobiles.
Il entreprend à la fois la classification et la sélection du nombre optimal de classes à condition
qu’il soit inférieur à une valeur maximale à déterminer au départ de l’algorithme.
Un mécanisme permet d’éloigner les centres des classes peu pertinentes
de sorte qu’aucun point ne leur sera affecté.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme5">
<div class="docutils container">
</div>
<p class="admonition-title" id="classif-algo-rpcl">Algorithme A6 : RPCL</p>
<p>Soient <img class="math" src="../_images/math/26f924af3662c85fdae93b78379cf709640d2119.svg" alt="\vecteur{X_1}{X_N}"/>, <img class="math" src="../_images/math/bceb9186b5004313ecccd0d22d07ea9617b62f98.svg" alt="N"/> vecteurs à classer en au
plus <img class="math" src="../_images/math/6cf1fdb1c46f5cefb926a2b20ec9dd4a481dd11d.svg" alt="T"/> classes de centres <img class="math" src="../_images/math/fb3d517c4eeb65cce6c34dfc9b7dc7089292995d.svg" alt="\vecteur{C_1}{C_T}"/>.
Soient deux réels <img class="math" src="../_images/math/03b31e7dea339fc978bc9c23c1cae7daa6bd2615.svg" alt="\alpha_r"/> et <img class="math" src="../_images/math/5f6da0c05eb3c25ba2397c23d2086b3f9dc39501.svg" alt="\alpha_c"/>
tels que <img class="math" src="../_images/math/fd93c6698c67f35e2a22cb1c7a6803db2576461a.svg" alt="0 &lt; \alpha_r \ll \alpha_c &lt; 1"/>.</p>
<p><em>initialisation</em></p>
<p>Tirer aléatoirement les centres <img class="math" src="../_images/math/fb3d517c4eeb65cce6c34dfc9b7dc7089292995d.svg" alt="\vecteur{C_1}{C_T}"/>.</p>
<div class="line-block">
<div class="line">for j in <img class="math" src="../_images/math/f963c7631cb87c320d1687686f34c5e48b508b2d.svg" alt="1..C"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/625febfb4c1fa41b81a2a7ca2f2786fd0752f4ee.svg" alt="n_j^0 \longleftarrow 1"/></div>
</div>
</div>
<p><em>calcul de poids</em></p>
<p>Choisir aléatoirement un point <img class="math" src="../_images/math/c15b262677ad7177c2e37298a2eb382d712b3a52.svg" alt="X_i"/>.</p>
<div class="line-block">
<div class="line">for j in <img class="math" src="../_images/math/f963c7631cb87c320d1687686f34c5e48b508b2d.svg" alt="1..C"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/819208f885e89ffcd0fd93ab5fd1ef848907e5c7.svg" alt="\gamma_j = \dfrac{n_j}{ \sum_{k=1}^{C} n_k}"/></div>
<div class="line"><br /></div>
</div>
<div class="line">for j in <img class="math" src="../_images/math/f963c7631cb87c320d1687686f34c5e48b508b2d.svg" alt="1..C"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/1b39f5dac1c02fc3e0eae94c98dfb253f39bf2d7.svg" alt="u_j ="/></div>
<div class="line-block">
<div class="line">1 si <img class="math" src="../_images/math/fbf3eb686a733e9b53bcd0d32bc83cef97f55e13.svg" alt="j \in \underset{k}{\arg \min} \; \cro {\gamma_k \; d\pa{X_i,C_k} }"/></div>
<div class="line">-1 si <img class="math" src="../_images/math/d01fb1b2ebcf432016cb3bd7de5fb0fe0a7a6107.svg" alt="j \in \underset{j \neq k}{\arg \min} \; \cro {\gamma_k \; d\pa{X_i,C_k} }"/></div>
<div class="line">0 sinon</div>
</div>
</div>
</div>
<p><em>mise à jour</em></p>
<div class="line-block">
<div class="line">for j in <img class="math" src="../_images/math/f963c7631cb87c320d1687686f34c5e48b508b2d.svg" alt="1..C"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/09dd9c63a737d007f778fad69a9eefc45afbf503.svg" alt="C_j^{t+1} \longleftarrow  C_j^t +  \left \{ \begin{array}{ll} \alpha_c \pa{X_i - C_j} &amp; \text{si } u_j = 1 \\ - \alpha_r \pa{X_i - C_j} &amp; \text{si } u_j = -1 \\ 0 &amp; \text{sinon} \end{array} \right."/></div>
<div class="line"><img class="math" src="../_images/math/71536267e1787bbc8916f6f2124324fce9b3b850.svg" alt="n_j^t +  \left \{ \begin{array}{ll} 1 &amp; \text{si } u_j = 1 \\ 0 &amp; \text{sinon} \end{array} \right."/></div>
<div class="line"><br /></div>
</div>
<div class="line"><img class="math" src="../_images/math/02dd178e61d85f10a2824b1cd98c269269370b59.svg" alt="t \longleftarrow t+1"/></div>
</div>
<p><em>terminaison</em></p>
<p>S’il existe un indice <img class="math" src="../_images/math/c2ca7da683b1e0aa549bb4675efbd6008c4ffa6e.svg" alt="j"/> pour lequel <img class="math" src="../_images/math/c4d5551f26826df6ec17ca781fc2a10021174733.svg" alt="C^{t+1}_j \neq C^t_j"/>
alors retourner à  l’étape de calcul de poids ou que les centres des classes jugées inutiles
ont été repoussés vers l’infini.</p>
</div>
<p>Pour chaque point, le centre de la classe la plus proche en est rapproché
tandis que le centre de la seconde classe la plus proche en est éloigné
mais d’une façon moins importante (condition <img class="math" src="../_images/math/eeaffc9e6b3b9543f3086f2664cb223ac3fba57a.svg" alt="\alpha_r \ll \alpha_c"/>).
Après convergence, les centres des classes inutiles ou non pertinentes
seront repoussés vers l’infini. Par conséquent, aucun point n’y sera rattaché.</p>
<p>L’algorithme doit être lancé plusieurs fois. L’algorithme RPCL peut terminer
sur un résultat comme celui de la figure suivante où un centre reste coincé
entre plusieurs autres. Ce problème est moins important
lorsque la dimension de l’espace est plus grande.</p>
<img alt="../_images/class6.png" src="../_images/class6.png" />
<p>Application de l’algorithme <a class="reference internal" href="#classif-algo-rpcl"><span class="std std-ref">RPCL</span></a> : la classe 0 est incrusté entre les quatre autres
et son centre ne peut se « faufiler » vers l’infini.</p>
</section>
<section id="rpcl-based-local-pca">
<span id="classification-rpcl-local-pca"></span><h3><a class="toc-backref" href="#id36" role="doc-backlink">RPCL-based local PCA</a><a class="headerlink" href="#rpcl-based-local-pca" title="Lien permanent vers cette rubrique">#</a></h3>
<p id="index-10">L’article <a class="reference internal" href="#liu2003" id="id18"><span>[Liu2003]</span></a> propose une extension de l’algorithme <a class="reference internal" href="#classif-algo-rpcl"><span class="std std-ref">RPCL</span></a>
et suppose que les classes ne sont plus de forme circulaire mais
suivent une loi normale quelconque. Cette méthode est utilisée pour
la détection de ligne considérées ici comme des lois normales dégénérées
en deux dimensions, la matrice de covariance définit une ellipse dont le
grand axe est très supérieur au petit axe, ce que montre la figure suivante.
Cette méthode est aussi présentée comme un possible algorithme de squelettisation.</p>
<img alt="../_images/liu3.png" src="../_images/liu3.png" />
<p>Figure extraite de <a class="reference internal" href="#liu2003" id="id19"><span>[Liu2003]</span></a>, l’algorithme est utilisé pour la détection de lignes
considérées ici comme des lois normales dont la matrice de covariance définit une ellipse
dégénérée dont le petit axe est très inférieur au grand axe. Les traits fin grisés correspondent aux
classes isolées par l’algorithme RPCL-based local PCA.</p>
<p>On modélise le nuage de points par une mélange de lois normales :</p>
<div class="math">
<p><img src="../_images/math/09d3deb3f6ba3f5d925b5e79e6ff9e9b53c81ff6.svg" alt="f\pa{x} =  \sum_{i=1}^{N} \; p_i \; \dfrac{1}{\pa{2 \pi}^{\frac{d}{2}}\sqrt{\det \Sigma_i}} \;
exp \pa{-\frac{1}{2}  \pa{x-\mu_i}' \Sigma_i^{-1} \pa{x-\mu_i} }"/></p>
</div><p>Avec <img class="math" src="../_images/math/4818fb2d6e00bee968d65982434c0e7c0bfbe18d.svg" alt="\sum_{i=1}^{N} \; p_i = 1"/>.</p>
<p>On suppose que le nombre de classes initiales <img class="math" src="../_images/math/bceb9186b5004313ecccd0d22d07ea9617b62f98.svg" alt="N"/> surestime le
véritable nombre de classes. L’article <a class="reference internal" href="#liu2003" id="id20"><span>[Liu2003]</span></a> s’intéresse
au cas particulier où les matrices de covariances vérifient
<img class="math" src="../_images/math/09871a20d40f17cc98971443d3a19c487934fddc.svg" alt="\Sigma_i = \zeta_i \, I + \sigma_i \, \phi_i \phi_i'"/>
avec <img class="math" src="../_images/math/198b07e64fd2fa4750aa3695b2ea2b3fb5dae3a1.svg" alt="\zeta_i &gt; 0, \; \sigma_i &gt; 0, \; \phi_i' \phi_i = 1"/>.</p>
<p>On définit également :</p>
<div class="math">
<p><img src="../_images/math/6eb7b1bb68207926c1af7e42158c8d6a1ebc03f6.svg" alt="G\pa{x, \mu, \Sigma} = \dfrac{1}{\pa{2 \pi}^{\frac{d}{2}}\sqrt{\det \Sigma}} \;
exp \pa{-\frac{1}{2}  \pa{x-\mu}' \Sigma^{-1} \pa{x-\mu} }"/></p>
</div><p>L’algorithme utilisé est similaire à l’algortihme <a class="reference internal" href="#classif-algo-rpcl"><span class="std std-ref">RPCL</span></a>.
La distance <img class="math" src="../_images/math/2160a217243173398700954f681412f83f781a6e.svg" alt="d"/> utilisée lors de l’étape de calcul des poids
afin de trouver la classe la plus probable pour un point
donné <img class="math" src="../_images/math/a64fc33c15255329b8e9309a9a44e1e0f012f72f.svg" alt="X_k"/> est remplacée par l’expression :</p>
<div class="math">
<p><img src="../_images/math/c0d6806ed4c76aea403828810c79d39ef322687a.svg" alt="d\pa{X_k, classe \, i} = - \ln { p_i^t \, G\pa{X_k, \, \mu_i^t, \, \Sigma^t_i } }"/></p>
</div><p>L’étape de mise à jour des coefficients est remplacée par :</p>
<div class="math">
<p><img src="../_images/math/f952370525f7703ca7254c74428d36b69e5dd027.svg" alt="x^{t+1} \longleftarrow  x^t +  \left \{ \begin{array}{ll}
\alpha_c \nabla x^t &amp; \text{si } u_j = 1 \\
- \alpha_r \nabla x^t &amp; \text{si } u_j = -1 \\
0 &amp; \text{sinon}
\end{array} \right."/></p>
</div><p>Où <img class="math" src="../_images/math/4171408fcfbe97daea34f08aef4aac37520717b1.svg" alt="x^t"/> joue le rôle d’un paramètre et est remplacé
successivement par <img class="math" src="../_images/math/77d7f28937206f8e34b8b96b0d0d264c9facca66.svg" alt="p_i^t"/>, <img class="math" src="../_images/math/6ed63b7beea15877e6ba7cec114276350657dae1.svg" alt="\mu_i^t"/>, <img class="math" src="../_images/math/e9a905eb88b274112b4100f00439e1cc973868d8.svg" alt="\zeta_i^t"/>, <img class="math" src="../_images/math/4863314ea214002f4ff7fd2f783a5e672ebe6239.svg" alt="\sigma^t_i"/>, <img class="math" src="../_images/math/99f29dadea043e5b60520a183519551ba589eb98.svg" alt="\phi^t_i"/> :</p>
<div class="math">
<p><img src="../_images/math/bb1785385e15b2920550c06d3826dba61d478d70.svg" alt="\begin{array}{lll}
\nabla p_i^t &amp;=&amp; - \frac{1}{p_i^t} \\
\nabla \mu_i^t &amp;=&amp; - \pa{ X_k - \mu_i^t} \\
\nabla \zeta_i^t  &amp;=&amp; \frac{1}{2} \; tr\cro{ \pa{\Sigma_i^t}^{-1} \,
\pa{ I - \pa{ X_k - \mu_i^t} \pa{ X_k - \mu_i^t}' \pa{\Sigma_i^t}^{-1} } } \\
\nabla \sigma_i^t &amp;=&amp;    \frac{1}{2} \; \pa{\phi_i^t}' \pa{\Sigma_i^t}^{-1}
\pa{ I - \pa{ X_k - \mu_i^t} \pa{ X_k - \mu_i^t}' \pa{\Sigma_i^t}^{-1} } \phi_i^t \\
\nabla \phi_i^t     &amp;=&amp;    \sigma_i^t \pa{\Sigma_i^t}^{-1}
\pa{ I - \pa{ X_k - \mu_i^t} \pa{ X_k - \mu_i^t}' \pa{\Sigma_i^t}^{-1} } \phi_i^t \\
\end{array}"/></p>
</div></section>
<section id="frequency-sensitive-competitive-learning-fscl">
<span id="label-kmeans-fscl"></span><h3><a class="toc-backref" href="#id37" role="doc-backlink">Frequency Sensitive Competitive Learning (FSCL)</a><a class="headerlink" href="#frequency-sensitive-competitive-learning-fscl" title="Lien permanent vers cette rubrique">#</a></h3>
<p id="index-11">L’algorithme Frequency Sensitive Competitive Learning est présenté dans
<a class="reference internal" href="#balakrishnan1996" id="id21"><span>[Balakrishnan1996]</span></a>. Par rapport à l’algorithme des centres mobiles classique,
lors de l’estimation des centres des classes, l’algorithme évite la formation de classes sous-représentées.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme6">
<div class="docutils container">
</div>
<p class="admonition-title" id="classification-fscl">Algorithme A7 : FSCL</p>
<p>Soit un nuage de points <img class="math" src="../_images/math/26f924af3662c85fdae93b78379cf709640d2119.svg" alt="\vecteur{X_1}{X_N}"/>,
soit <img class="math" src="../_images/math/2c0e77bc741b692ecdf6e85966263a89422af4c8.svg" alt="C"/> vecteurs <img class="math" src="../_images/math/006dd3f24de01449dca0eca6514bdde697fc8cb3.svg" alt="\vecteur{\omega_1}{\omega_C}"/>
initialisés de manière aléatoires.
Soit <img class="math" src="../_images/math/6c47182110344e6f2497b17b0b20b8f48cb61be9.svg" alt="F : \pa{u,t} \in \R^2 \longrightarrow \R^+"/>
croissante par rapport à <img class="math" src="../_images/math/3ded7c5accc3db646e5a061facda1aef616d548f.svg" alt="u"/>.
Soit une suite de réels <img class="math" src="../_images/math/aff557df7578eefb5dca5b59e3147960acf07f7c.svg" alt="\vecteur{u_1}{u_C}"/>,
soit une suite <img class="math" src="../_images/math/712de58615936cdf87cfd5b1c5f017bccc7ea9f8.svg" alt="\epsilon\pa{t} \in \cro{0,1}"/> décroissante où <img class="math" src="../_images/math/09c7628f51842c683db31bd6826cff8cc447ece3.svg" alt="t"/>
représente le nombre d’itérations.
Au début <img class="math" src="../_images/math/06589e14407f2f6b63d16767dd96125304b98dd7.svg" alt="t \leftarrow 0"/>.</p>
<p><em>meilleur candidat</em></p>
<p>Pour un vecteur <img class="math" src="../_images/math/a64fc33c15255329b8e9309a9a44e1e0f012f72f.svg" alt="X_k"/> choisi aléatoirement dans
l’ensemble <img class="math" src="../_images/math/26f924af3662c85fdae93b78379cf709640d2119.svg" alt="\vecteur{X_1}{X_N}"/>, on détermine :</p>
<div class="math">
<p><img src="../_images/math/be7749778fe4c36c60c7283bc5a0089c4938a94c.svg" alt="i^* \in \arg \min \acc{ D_i = F\pa{u_i,t} \, d\pa{X_k, \omega_i} }"/></p>
</div><p><em>mise à jour</em></p>
<div class="line-block">
<div class="line"><img class="math" src="../_images/math/71df14eb8e9a48aa4aaa710d7ca820442aa71819.svg" alt="\omega_{i^*} \pa{t+1}  \longleftarrow \omega_{i^*} \pa{t} + \epsilon\pa{t} \pa { X_k - \omega_{i^*} \pa{t} }"/></div>
<div class="line"><img class="math" src="../_images/math/02dd178e61d85f10a2824b1cd98c269269370b59.svg" alt="t \longleftarrow t+1"/></div>
<div class="line"><img class="math" src="../_images/math/eb010d3f7364d6bab102b761d4b112de022f29e6.svg" alt="u_{i^*} \longleftarrow u_{i^*} + 1"/></div>
</div>
<p>Retour à l’étape précédente jusqu’à ce que les nombres
<img class="math" src="../_images/math/dda6a76e26fd07cf6c368a3365438c65050f34a5.svg" alt="\frac{u_i}{\sum_{i}u_i}"/> convergent.</p>
</div>
<p>Exemple de fonctions pour <img class="math" src="../_images/math/d3df781b97caf23fd84697ec8aac41efdf4f6793.svg" alt="F"/>, <img class="math" src="../_images/math/5d07e66e6e9d55b1dd504ca14a3d870dfe30fb29.svg" alt="\epsilon"/> (voir <a class="reference internal" href="#balakrishnan1996" id="id22"><span>[Balakrishnan1996]</span></a>) :</p>
<div class="math">
<p><img src="../_images/math/c97dd7bb1f54357b2f271b937b03dfe3bd886d4e.svg" alt="\begin{eqnarray*}
F\pa{u,t} &amp;=&amp; u \, \beta e^{-t/T} \text{ avec } \beta = 0,06 \text{ et } 1/T = 0,00005 \\
\epsilon\pa{t} &amp;=&amp; \beta \, e^{ - \gamma t } \text{ avec } \gamma = 0,05
\end{eqnarray*}"/></p>
</div><p>Cet algorithme ressemble à celui des cartes topographiques de Kohonen
sans toutefois utiliser un maillage entre les neurones
(ici les vecteurs <img class="math" src="../_images/math/ce439c3bf6e8d427634dd59f969b64429819b464.svg" alt="\omega_i"/>). Contrairement à l’algorithme RPCL,
les neurones ne sont pas repoussés s’ils ne sont pas choisis mais la fonction
croissante <img class="math" src="../_images/math/d6ee29fc5b0000217692b3fe96efde422a6c371a.svg" alt="F\pa{u,t}"/> par rapport à <img class="math" src="../_images/math/3ded7c5accc3db646e5a061facda1aef616d548f.svg" alt="u"/> assure que plus un neurone
est sélectionné, moins il a de chance de l’être,
bien que cet avantage disparaisse au fur et à mesure des itérations.</p>
</section>
</section>
<section id="k-means-norme-l1">
<h2><a class="toc-backref" href="#id38" role="doc-backlink">k-means norme L1</a><a class="headerlink" href="#k-means-norme-l1" title="Lien permanent vers cette rubrique">#</a></h2>
<p>L’algorithme dans sa version la plus courante optimise l’inertie définie
par <img class="math" src="../_images/math/40bbc182fe555c7e5da694e0535ddc2b0adb23ee.svg" alt="\sum_{i=1}^P \; d^2\left(X_i, G_{c_i^t}^t\right)"/>, qui est
en quelque sorte une inertie <em>L2</em>. Que devriendrait l’algorithme
si la norme choisie était une norme <em>L1</em>, il faudrait alors choisir
à chaque itération <em>t</em> des <em>points</em> qui minimise la quantité :
<img class="math" src="../_images/math/ba04609461dc2d5ca073ac714d021b17b621180b.svg" alt="\sum_{i=1}^P \; d_1\left(X_i, G_{c_i^t}^t\right)"/> où
<img class="math" src="../_images/math/dba54403216e84bb9f17c788e3602a282932af16.svg" alt="d_1"/> est la norme <em>L1</em> entre deux points <em>X,Y</em> :
<img class="math" src="../_images/math/e2762704c8e87d7119880487281a3baf9559b381.svg" alt="d_1(X, Y) = \sum_i |X_i - Y_i|"/>. Avant de continuer,
on rappelle un théorème :</p>
<div class="admonition-mathdef admonition" id="indexmathe-propriété0">
<div class="docutils container">
</div>
<p class="admonition-title" id="mediane-l1">propriété P1 : Médiane et valeur absolue</p>
<p>Soit <img class="math" src="../_images/math/b403da094d28e0106b06efe11a8771d61757252e.svg" alt="A=(x_1, ..., x_n)"/> un ensembl de <em>n</em> réels quelconque.
On note <img class="math" src="../_images/math/414fe48e1d982fcdb0d024ae5192bfa6a5553663.svg" alt="m=med(x_1, ..., x_n)"/> la médiane
de l’ensemble de points <em>A</em>. Alors la médiane <em>m</em>
minimise la quantité <img class="math" src="../_images/math/ed2739077602d2045deb89d56b9de19f197c0e14.svg" alt="\sum_{i=1}^n |m-x_i|"/>.</p>
</div>
<p>C’est cette propriété qui est utilisée pour définir ce qu’est
la <a class="reference internal" href="../c_ml/regression_quantile.html#l-reg-quantile"><span class="std std-ref">régression quantile</span></a> et sa démonstration
est présentée à la page <a class="reference internal" href="../c_ml/regression_quantile.html#l-reg-quantile-demo"><span class="std std-ref">Médiane et valeur absolue</span></a>. Il ne reste
plus qu’à se servir de ce résultat pour mettre à jour l’algorithme
<a class="reference internal" href="#kmeans-def-algo"><span class="std std-ref">centre mobile, k-means</span></a>. L’étape qui
consiste à affecter un point à un cluster représenté par un point
ne pose pas de problème si on utilise cette nouvelle norme. Il ne reste
plus qu’à déterminer le point qui représente un cluster sachant
les points qui le constituent. Autrement dit, il faut déterminer
le point qui minimiser la pseudo-inertie définie comme suit
pour un ensemble de points <img class="math" src="../_images/math/bfe5330a47312509984b1f7abf71fd221d3d8dc1.svg" alt="(X_1, ..., X_n)"/> appartenant à un
espace vectoriel de dimension <em>k</em>.</p>
<div class="math">
<p><img src="../_images/math/33584f87f2dfdf574e2906c428e43546e5672afc.svg" alt="I(G,X_1,...,X_n) = \norm{G - X_i}_1 = \sum_{i=1}^n \sum_{k=1}^d \abs{G_k - X_{ik}}"/></p>
</div><p>On cherche le point <em>G</em> qui minimise la quantité <img class="math" src="../_images/math/c484af6489df0496107d8da599f9d84c3931aebf.svg" alt="I(G,X_1,...,X_n)"/>.
Comme <img class="math" src="../_images/math/60d3d7234cef59ab3e1d4c4caa7c88c629e43cab.svg" alt="\sum_{i=1}^n \sum_{k=1}^d \abs{G_k - X_{ik}} = \sum_{k=1}^d \sum_{i=1}^n  \abs{G_k - X_{ik}}"/>,
on en déduit qu’on peut chercher la coordonnée <img class="math" src="../_images/math/5b586d8ac2fcc343ef09f4c37723e90b948ae179.svg" alt="G_k"/> indépendemment
les unes des autres. On en déduit
que le barycentre de norme L1 d’un ensemble de points dans un
espace vectoriel de dimension <em>d</em> a pour coordonnées les <em>d</em>
médianes extraites sur chacune des dimensions.
L’algorithme est implémenté dans le module <a class="reference external" href="http://www.xavierdupre.fr/app/mlinsights/helpsphinx/index.html">mlinsights</a>
en s’inspirant du code <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">KMeans</a>.</p>
</section>
<section id="bibliographie">
<h2><a class="toc-backref" href="#id39" role="doc-backlink">Bibliographie</a><a class="headerlink" href="#bibliographie" title="Lien permanent vers cette rubrique">#</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="arthur2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Arthur2007<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>k-means++: the advantages of careful seeding (2007),
<em>Arthur, D.; Vassilvitskii, S.</em>,
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms.
Society for Industrial and Applied Mathematics Philadelphia, PA, USA. pp. 1027–1035.
<a class="reference external" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">2006-13.pdf</a>.</p>
</div>
<div class="citation" id="balakrishnan1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Balakrishnan1996<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id21">1</a>,<a role="doc-backlink" href="#id22">2</a>)</span>
<p>Comparative performance of the FSCL neural net and K-means algorithm for market segmentation (1996),
P. V. Sundar Balakrishnan, Martha Cooper, Varghese S. Jacob, Phillip A. Lewis,
<em>European Journal of Operation Research</em>, volume 93, pages 346-357</p>
</div>
<div class="citation" id="bahmani2012" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Bahmani2012</a><span class="fn-bracket">]</span></span>
<p>Scalable K-Means++ (2012),
<em>Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, Sergei Vassilvitskii</em>,
Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp. 622-633 (2012)
<a class="reference external" href="http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf">vldb12-kmpar.pdf</a>,
<a class="reference external" href="https://arxiv.org/abs/1203.6402">arXiv.1203.6402</a></p>
</div>
<div class="citation" id="cheung2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cheung2003<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id15">1</a>,<a role="doc-backlink" href="#id16">2</a>)</span>
<p><img class="math" src="../_images/math/fc90b3f44afc581dfadde98aeb786a78889e3a4c.svg" alt="k^*"/>-Means: A new generalized k-means clustering algorithm (2003),
Yiu-Ming Cheung,
<em>Pattern Recognition Letters</em>, volume 24, 2883-2893</p>
</div>
<div class="citation" id="davies1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Davies1979</a><span class="fn-bracket">]</span></span>
<p>A cluster Separation Measure (1979),
D. L. Davies, D. W. Bouldin,
<em>IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI)</em>, volume 1(2)</p>
</div>
<div class="citation" id="goodman1954" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">Goodman1954</a><span class="fn-bracket">]</span></span>
<p>Measures of associations for cross-validations (1954),
L. Goodman, W. Kruskal,
<em>J. Am. Stat. Assoc.</em>, volume 49, pages 732-764</p>
</div>
<div class="citation" id="herbin2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Herbin2001<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>Estimation of the number of clusters and influence zones (2001),
M. Herbin, N. Bonnet, P. Vautrot,
<em>Pattern Recognition Letters</em>, volume 22, pages 1557-1568</p>
</div>
<div class="citation" id="kothari1999" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kothari1999<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id12">2</a>,<a role="doc-backlink" href="#id13">3</a>,<a role="doc-backlink" href="#id14">4</a>)</span>
<p>On finding the number of clusters (1999),
Ravi Kothari, Dax Pitts,
<em>Pattern Recognition Letters</em>, volume 20, pages 405-416</p>
</div>
<div class="citation" id="liu2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Liu2003<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id18">1</a>,<a role="doc-backlink" href="#id19">2</a>,<a role="doc-backlink" href="#id20">3</a>)</span>
<p>Strip line detection and thinning by RPCL-based local PCA (2003),
Zhi-Yong Liu, Kai-Chun Chiu, Lei Xu,
<em>Pattern Recognition Letters</em> volume 24, pages 2335-2344</p>
</div>
<div class="citation" id="silverman1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">Silverman1986</a><span class="fn-bracket">]</span></span>
<p>Density Estimation for Statistics and Data Analysis (1986),
B. W. Silverman,
<em>Monographs on Statistics and Applied Probability, Chapman and Hall, London</em>, volume 26</p>
</div>
<div class="citation" id="xu1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">Xu1993</a><span class="fn-bracket">]</span></span>
<p>Rival penalized competitive learning for clustering analysis, rbf net and curve detection (1993),
L. Xu, A. Krzyzak, E. Oja,
<em>IEEE Trans. Neural Networks</em>, volume (4), pages 636-649</p>
</div>
</div>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Clustering</p>
      </div>
    </a>
    <a class="right-next"
       href="gauss_mixture.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Mélange de lois normales</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Sur cette page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principe">Principe</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homogeneite-des-dimensions">Homogénéité des dimensions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ameliorations-de-l-initialisation">Améliorations de l’initialisation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l-kmeanspp">K-means++</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">K-means||</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-de-probabilites">Estimation de probabilités</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-du-nombre-de-classes">Sélection du nombre de classes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#critere-de-qualite">Critère de qualité</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxima-de-la-fonction-densite">Maxima de la fonction densité</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decroissance-du-nombre-de-classes">Décroissance du nombre de classes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-des-nuees-dynamiques">Extension des nuées dynamiques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classes-elliptiques">Classes elliptiques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rival-penalized-competitive-learning-rpcl">Rival Penalized Competitive Learning (RPCL)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rpcl-based-local-pca">RPCL-based local PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequency-sensitive-competitive-learning-fscl">Frequency Sensitive Competitive Learning (FSCL)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-norme-l1">k-means norme L1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliographie">Bibliographie</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../_sources/c_clus/kmeans.rst">
      <i class="fa-solid fa-file-lines"></i> Montrer le code source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2016-2023, Xavier Dupré.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Créé en utilisant <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.0.1.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Construit avec le <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">Thème PyData Sphinx</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>