
<!DOCTYPE html>


<html lang="fr" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Régression logistique par morceaux, arbres de décision &#8212; Documentation mlstatpy 0.4.0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=72ff47bc"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/translations.js?v=d99ca74e"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'c_ml/lr_trees';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Recherche" href="../search.html" />
    <link rel="next" title="Réseaux de neurones" href="../notebooks/ml/reseau_neurones.html" />
    <link rel="prev" title="Voronoï et régression logistique" href="../notebooks/ml/logreg_voronoi.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/project_ico.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/project_ico.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_clus/index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../license.html">
                        License
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_clus/index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../license.html">
                        License
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Navigation de la section">
  <p class="bd-links__title" role="heading" aria-level="1">Navigation de la section</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="lr_voronoi.html">Régression logistique, diagramme de Voronoï, k-Means</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/ml/logreg_voronoi.html">Voronoï et régression logistique</a></li>
</ul>
</li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Régression logistique par morceaux, arbres de décision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/ml/reseau_neurones.html">Réseaux de neurones</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="survival_analysis.html">Analyse de survie</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/ml/survival.html">Analyse de survie en pratique</a></li>
</ul>
</li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Fils d'Ariane">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Fil d'Ariane">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Acceuil">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index_reg_log.html" class="nav-link">Régression logistique</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Régression logistique par morceaux, arbres de décision</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="regression-logistique-par-morceaux-arbres-de-decision">
<span id="l-lr-trees-nn"></span><h1>Régression logistique par morceaux, arbres de décision<a class="headerlink" href="#regression-logistique-par-morceaux-arbres-de-decision" title="Lien permanent vers cette rubrique">#</a></h1>
<p id="index-0">Ce qui suit explore une façon fantaisiste de construire des régressions
logistiques à mi-chemin entre les arbres de décisions
et les réseaux de neurones. Dans un premier temps, on s’intéresse
uniquement à une classification binaire.</p>
<nav class="contents local" id="sommaire">
<ul class="simple">
<li><p><a class="reference internal" href="#parallele-entre-un-neurone-et-une-regression-logistique" id="id3">Parallèle entre un neurone et une régression logistique</a></p></li>
<li><p><a class="reference internal" href="#principe-d-un-arbre-de-decision" id="id4">Principe d’un arbre de décision</a></p></li>
<li><p><a class="reference internal" href="#construction-d-un-pseudo-arbre" id="id5">Construction d’un pseudo arbre</a></p></li>
<li><p><a class="reference internal" href="#aparte-mathematique" id="id6">Aparté mathématique</a></p></li>
<li><p><a class="reference internal" href="#approche-em-et-regression-logistique" id="id7">Approche EM et régression logistique</a></p></li>
<li><p><a class="reference internal" href="#lien-vers-les-reseaux-de-neurones" id="id8">Lien vers les réseaux de neurones</a></p></li>
<li><p><a class="reference internal" href="#plan-orthogonal" id="id9">Plan orthogonal</a></p></li>
<li><p><a class="reference internal" href="#interpretabilite" id="id10">Interprétabilité</a></p></li>
<li><p><a class="reference internal" href="#bibliographie" id="id11">Bibliographie</a></p></li>
</ul>
</nav>
<section id="parallele-entre-un-neurone-et-une-regression-logistique">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Parallèle entre un neurone et une régression logistique</a><a class="headerlink" href="#parallele-entre-un-neurone-et-une-regression-logistique" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Les paragraphes <a class="reference internal" href="rn/rn_7_clas2.html#rn-classification"><span class="std std-ref">Classification</span></a> et
<a class="reference internal" href="rn/rn_3_clas.html#nn-classification"><span class="std std-ref">La classification</span></a> présente le problème de la classification
qui consiste à trouver une fonction <em>f</em> qui maximise la vraisemblance
du nuage de points <img class="math" src="../_images/math/b9fcbdf90099e952b01c231bdafa14b368bd8c2d.svg" alt="(X_i, y_i)_i"/> où <img class="math" src="../_images/math/02a0e4eeaebf353bfe8785b985e826009c9a5181.svg" alt="X_i \in \R^d"/>
et <img class="math" src="../_images/math/3e94f1f2fcb78ea4aea0bc5768428865c3e92b34.svg" alt="y_i \in \acc{0, 1}"/>.</p>
<div class="math">
<p><img src="../_images/math/ad4e65ea47878df03a30525ab4b51f4cf8ab92a4.svg" alt="\ln L(\Theta, X, y) = \sum_{i=1}^n y_i \ln f(\Theta, X_i) + (1-y_i) \ln (1-f(\Theta, X_i))"/></p>
</div><p>Dans le cas de la régression logistique, la fonction <em>f</em> est définie comme suit :</p>
<div class="math">
<p><img src="../_images/math/288d0a79ad2cb5d6649a1d20a395fb4c2c4b0478.svg" alt="f(\Theta, X_i) = \frac{1}{1 + e^{-\sum_{k=1}^d \theta_k x_{ik}}}"/></p>
</div><p>Cela ressemble beaucoup à la définition d’un <a class="reference internal" href="rn/rn_1_def.html#l-rn-neurone"><span class="std std-ref">neurone</span></a>
où la fonction d’activation <img class="math" src="../_images/math/901c318c883eb3d7f98117bca8a8aa39aa976640.svg" alt="f(x) = \frac{1}{1 + e^{-x}}"/> est une
fonction sigmoïde.</p>
</section>
<section id="principe-d-un-arbre-de-decision">
<span id="l-lr-log-likelihood"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">Principe d’un arbre de décision</a><a class="headerlink" href="#principe-d-un-arbre-de-decision" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Un arbre de décision se construit peu à peu en répétant toujours
la même optimisation sur des sous-ensemble de plus en plus petit.
Il faut d’abord un critère qui permette d’évaluer la pertinence
de la division effectuée par un noeud de l’arbre.
Pour un ensemble <img class="math" src="../_images/math/d679a76a5f375a1b9e47ddb8b8621af141d1d9e5.svg" alt="(X_i, y_i)_{1 \infegal i \infegal n}"/>, on
peut estimer la probabilité
<img class="math" src="../_images/math/5a0a34953eda219d1de293e247077c260bba8c52.svg" alt="p(y_1, ..., y_n) = p(Y) = \frac{1}{n}\sum{i=1}^n y_i"/>.
Le critère de Gini <em>G</em> qui évalue la pertinence d’une classification est
défini par <img class="math" src="../_images/math/e804294146fa87e476bb17dd3470a5e1704a490f.svg" alt="G(Y) = p(Y) (1 - p(Y))"/>.
Un autre critère est le gain d’information ou entropie <em>H</em> :
<img class="math" src="../_images/math/ad93996a4e215efad6477fee9935d0b0dcf7fd42.svg" alt="H(Y) = - p(Y) \ln p(Y) - (1-p(Y)) \ln (1 - p(Y))"/>.</p>
<p>On note <img class="math" src="../_images/math/05a71410aaa870b65b552aab8416211bda12649d.svg" alt="Y_S"/> l’ensemble des <img class="math" src="../_images/math/824f8c1cf4d933e5a0952972e52248f815bf5b46.svg" alt="\acc{y_i | i \in S}"/>
où <em>S</em> est un sous-ensemble. <img class="math" src="../_images/math/c6d2fe4ce4543952ec313467aad41d6822c3eda4.svg" alt="S^C"/> est noté le complémentaire.</p>
<p>Pour le premier noeud de l’arbre de décision, on calcule pour
toutes les variables et toutes les observations la diminution
du critère choisi :</p>
<div class="math">
<p><img src="../_images/math/fc8a5937aeab36c00335345bba6fffa8ff435a85.svg" alt="\begin{array}{rcl}
S_{ik} &amp;=&amp; \acc{ m | x_{mk} \infegal x_{ik}} \\
\Delta_{ik} &amp;=&amp; H(Y) - ( H(Y_{S_{ik}}) + H(Y_{S_{ik}^C} )
\end{array}"/></p>
</div><p>On choisit alors la variable <em>k</em> et le seuil <img class="math" src="../_images/math/b510bf05496cee5a8dded0e87278911b3341ed9c.svg" alt="x_{ik}"/> qui
maximise le gain. Dans le cas d’une régression logistique,
la vraisemblance correspond à :</p>
<div class="math">
<p><img src="../_images/math/ad4e65ea47878df03a30525ab4b51f4cf8ab92a4.svg" alt="\ln L(\Theta, X, y) = \sum_{i=1}^n y_i \ln f(\Theta, X_i) + (1-y_i) \ln (1-f(\Theta, X_i))"/></p>
</div><p>Si on suppose que la fonction <em>f</em> retourne une constante <em>c</em>,
cette expression devient :</p>
<div class="math">
<p><img src="../_images/math/32403573678a2aed9a40034e09bd533edeed0a97.svg" alt="\ln L(\Theta, X, y) = \sum_{i=1}^n y_i \ln c + (1-y_i) \ln (1-c) = p(Y) \ln c + (1-p(Y)) \ln (1-c)"/></p>
</div><p>Or cette expression admet un maximum pour <img class="math" src="../_images/math/59ee1df05f7b961a0dadebb8190b275e26b9277c.svg" alt="c=p(Y)"/> puisque la dérivée
s’annule de façon évidente pour cette valeur :</p>
<div class="math">
<p><img src="../_images/math/97f377cbc21d6572cf554d82f8f99e9e5041f4b0.svg" alt="\frac{\partial \ln L(\Theta, X, y)}{\partial c} = \frac{p(Y)}{c} - \frac{1-p(Y)}{1-c}"/></p>
</div><p>On remarque que l’optimisation d’un noeud d’un arbre de décision
correspond à l’optimisation de la vraisemblance par une
fonction constante. Une régression logistique calculée sur une
seule variable est en quelque sorte une généralisation de ce modèle.
On apprend un arbre de décision qu’on exporte au format <a class="reference external" href="https://fr.wikipedia.org/wiki/DOT_(langage)">dot</a>.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">export_graphviz</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">target</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">%</span> <span class="mi">2</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">)</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
<span class="c1"># export_graphviz(dt)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Ce qui donne :</p>
<div class="graphviz"><img src="../_images/graphviz-77252c69fe458f4b693e321577d953cb2aa0422f.png" alt="digraph Tree {
    node [shape=box] ;
    0 [label=&quot;X[3] &lt;= 0.8\nentropy = 0.918\nsamples = 150\nvalue = [100, 50]&quot;] ;
    1 [label=&quot;entropy = 0.0\nsamples = 50\nvalue = [50, 0]&quot;] ;
    0 -&gt; 1 [labeldistance=2.5, labelangle=45, headlabel=&quot;True&quot;] ;
    2 [label=&quot;X[3] &lt;= 1.75\nentropy = 1.0\nsamples = 100\nvalue = [50, 50]&quot;] ;
    0 -&gt; 2 [labeldistance=2.5, labelangle=-45, headlabel=&quot;False&quot;] ;
    3 [label=&quot;X[2] &lt;= 4.95\nentropy = 0.445\nsamples = 54\nvalue = [5, 49]&quot;] ;
    2 -&gt; 3 ;
    4 [label=&quot;entropy = 0.146\nsamples = 48\nvalue = [1, 47]&quot;] ;
    3 -&gt; 4 ;
    5 [label=&quot;entropy = 0.918\nsamples = 6\nvalue = [4, 2]&quot;] ;
    3 -&gt; 5 ;
    6 [label=&quot;X[2] &lt;= 4.85\nentropy = 0.151\nsamples = 46\nvalue = [45, 1]&quot;] ;
    2 -&gt; 6 ;
    7 [label=&quot;entropy = 0.918\nsamples = 3\nvalue = [2, 1]&quot;] ;
    6 -&gt; 7 ;
    8 [label=&quot;entropy = 0.0\nsamples = 43\nvalue = [43, 0]&quot;] ;
    6 -&gt; 8 ;
}" class="graphviz" /></div>
</section>
<section id="construction-d-un-pseudo-arbre">
<span id="l-criteria-reg-log"></span><h2><a class="toc-backref" href="#id5" role="doc-backlink">Construction d’un pseudo arbre</a><a class="headerlink" href="#construction-d-un-pseudo-arbre" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Et si on remplaçait chaque noeud par une régression logistique
appris sur les exemples passant par ce noeud… Plutôt que de prendre
une décision basée sur une variable donnée et de retourner une probabilité
constante, on estime une régression logistique et on retourne
la probabilité retournée par la régression.</p>
<p>S’il n’y a théoriquement aucun obstacle, en pratique, certains cas
posent quelques problèmes comme le montre l’exemple
<a class="reference internal" href="../auto_examples/plot_logistic_decision.html#l-example-logistic-decision"><span class="std std-ref">Arbre d’indécision</span></a> et repris ci-dessous.
La fonction <a class="reference internal" href="../api/modules/logreg.html#mlstatpy.ml.logreg.criteria" title="mlstatpy.ml.logreg.criteria"><code class="xref py py-func docutils literal notranslate"><span class="pre">criteria</span></code></a>
calcule les différents gains selon les points de coupure.</p>
<p>(<a class="reference download internal" download="" href="../_downloads/531af7b80f366303f390485c8cd1ee16/lr_trees-1.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="../_downloads/a2c0745acab7d3c94fac2245153cdb03/lr_trees-1.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/62374232df2a743ea34be18c892cae77/lr_trees-1.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/868238c99784e257ca069fbf1a8025e8/lr_trees-1.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default">
<img alt="../_images/lr_trees-1.png" class="plot-directive" src="../_images/lr_trees-1.png" />
</figure>
<p>Le seuil de coupure est évident dans le premier cas et
quasiment impossible à trouver de façon numérique dans le second
avec les algorithmes tels qu’ils sont implémentés.
Les arbres de décision contournent
ce problème en imposant que le seuil de coupure laisse au moins
quelques exemples de chaque côté ce que la régression logistique
ne fait pas. On peut réflechir à d’autres critères.
Le suivant explore la log-vraisemblance.</p>
<p>(<a class="reference download internal" download="" href="../_downloads/4e0344eee99db31604908633cd37a711/lr_trees-2.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="../_downloads/31f57bf26430f0072efb381f4353b705/lr_trees-2.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/70994af43ed566f2a3001c3e99d0fa94/lr_trees-2.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="../_downloads/447af61da46bb961bfdb5f44996ab02f/lr_trees-2.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default">
<img alt="../_images/lr_trees-2.png" class="plot-directive" src="../_images/lr_trees-2.png" />
</figure>
<p>La log-vraisemblance dans ce problème à une dimension
est assez simple à écrire. Pour avoir une expression qui
ne change pas en invertissant les classes, on considère
le maxiimum des vraisemblance en considérant deux classifieurs
opposés. Le graphe précédent fait varier <img class="math" src="../_images/math/97c4568f4c81437317fcedb2bfafe4ef3dcc5c33.svg" alt="x_0"/> avec
différents <img class="math" src="../_images/math/38b8dfb3d1a10a5197333bf027a6a9c326563ed9.svg" alt="\theta"/>.</p>
<div class="math">
<p><img src="../_images/math/c682ccf995442aff91b8d04782d1cf774a09b52b.svg" alt="LL(x_0, \theta) = \max \left\{ \begin{array}{ll}
\frac{1}{1 + \exp{\left(\frac{x-x_0}{\theta}\right)}} \\
\frac{1}{1 + \exp{\left(-\frac{x-x_0}{\theta}\right)}}
\end{array}\right."/></p>
</div></section>
<section id="aparte-mathematique">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Aparté mathématique</a><a class="headerlink" href="#aparte-mathematique" title="Lien permanent vers cette rubrique">#</a></h2>
<p>La log-vraisemblance d’une régression logistique pour
un jeu de données <img class="math" src="../_images/math/3190ad9816294c28609eb295103dfe16eb2f1af9.svg" alt="(X_i, y_i)"/> s’exprime comme
suit pour une régression logistique de paramètre
<img class="math" src="../_images/math/bd898b02c226d1ce791865c51d30012165adf53d.svg" alt="\beta"/>.</p>
<div class="math">
<p><img src="../_images/math/9c53b6cc877178acfdc5c914708536fe3620afc1.svg" alt="\begin{array}{rcl}
L(\beta, X, y) &amp;=&amp; \sum_{i=1}^n y_i \ln f(\beta, X_i) + (1-y_i) \ln (1-f(\beta, X_i)) \\
\text{avec } f(\beta, X_i) &amp;=&amp; \frac{1}{1 + \exp(- (\beta_0 + \sum_{k=1}^d x_{ik} \beta_k))}
\end{array}"/></p>
</div><p>On remarque que :</p>
<div class="math">
<p><img src="../_images/math/e6cd4eda5832dc6c0adc942ae5fd9fbe9ac6d0b3.svg" alt="\begin{array}{rcl}
f(x) &amp;=&amp; \frac{1}{1 + e^{-x}} \\
\Rightarrow f(-x) &amp;=&amp; \frac{1}{1 + e^{x}} = \frac{e^{-x}}{1 + e^{-x}} \\
\Rightarrow f(x) + f(-x) &amp;=&amp; \frac{1}{1 + e^{-x}} + \frac{e^{-x}}{1 + e^{-x}} = 1
\end{array}"/></p>
</div><p>Cela explique pour on utilise souvent cette fonction pour transformer
une distance en probabilité pour un classifieur binaire.
L’apprentissage d’un arbre de décision
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="(disponible dans scikit-learn v1.3)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeClassifier</span></code></a> propose le
paramètre <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>. On se propose dans le cadre
de la régression logistique de chercher le paramètre
<img class="math" src="../_images/math/187f6a9980782a0dca16d03f610fda1f27f6ef66.svg" alt="\beta_0"/> qui permet de vérifier la contrainte
fixée par <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>. Cela revient à trounver
un classifieur linéaire parallèle au premier qui vérifie
les contraintes.</p>
</section>
<section id="approche-em-et-regression-logistique">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Approche EM et régression logistique</a><a class="headerlink" href="#approche-em-et-regression-logistique" title="Lien permanent vers cette rubrique">#</a></h2>
<p>L’article <a class="reference internal" href="#scott2013" id="id1"><span>[Scott2013]</span></a> explicite un algorithme d’apprentissage EM
pour une régression logistique.</p>
<img alt="../_images/bayes.png" src="../_images/bayes.png" />
<p>Il faudrait adapter cet agorithme pour apprendre deux régressions
logistiques dont la combinaison sur deux parties disjointes
serait meilleure qu’une seule régression logistique sur
la réunion des deux parties. Cet algorithme devrait trouver à
la fois les modèles et la séparation entre les deux parties.</p>
</section>
<section id="lien-vers-les-reseaux-de-neurones">
<span id="l-decnntrees"></span><h2><a class="toc-backref" href="#id8" role="doc-backlink">Lien vers les réseaux de neurones</a><a class="headerlink" href="#lien-vers-les-reseaux-de-neurones" title="Lien permanent vers cette rubrique">#</a></h2>
<p>En remplaçant chaque noeud par une régression logistique,
l’arbre de décision deviendrait un réseau de neurones,
avec une structure particulière certes mais un réseau de
neurones tout de même.
Chaque noeud du graphe serait transformé en un neurone
avec une régression logistique impliquant toutes les variables.
Il ne resterait plus qu’à continuer l’apprentissage avec des
algorithmes à base de gradient stochastique. Cela reviendrait
à changer l’initialisation du réseau de neurones.
On considère le petit arbre décision suivant,
trois features, trois noeuds, deux classes.</p>
<div class="graphviz"><img src="../_images/graphviz-7edaaeb8b7bea2c7c1a58b600614e5271da95ffc.png" alt="digraph tree {
    A [label=&quot;X1 &amp;lt; 5&quot;,shape=record];
    B [label=&quot;X2 &amp;lt; 3&quot;,shape=record];
    C [label=&quot;X3 &amp;lt; 2&quot;,shape=record];
    A -&gt; B;
    A -&gt; C;
    D [label=&quot;&lt;c0&gt; 0|&lt;c1&gt; 1&quot;,shape=record];
    E [label=&quot;&lt;c0&gt; 0|&lt;c1&gt; 1&quot;,shape=record];
    B -&gt; D:c0;
    B -&gt; D:c1;
    C -&gt; E:c0;
    C -&gt; E:c1;
}" class="graphviz" /></div>
<p>On souhaite le transformer en réseau de neurones avec une
structure qui serait celle qui suit. On note tout d’abord
la fonction sigmoïde <img class="math" src="../_images/math/f1dcbd117beeb2ca73e7c169cd263599663a1abb.svg" alt="f(x, s, h)=\frac{1}{1 + e^{-h(x - s)}}"/>.
Elle vaut <em>1/2</em> lorsque <em>x</em> vaut <em>s</em>, vaut 1 lorsque <em>x</em>
est très grand, et 0 lorsque <em>x</em> est très petit.
C’est équivalent à la fonction
<img class="math" src="../_images/math/c1711779e3ce0601bc4ddefdb0501c3cb6f43192.svg" alt="f(x, s, h)=g(X, S, v_0, h)\frac{1}{1 + e^{h(&lt;X,V&gt; + v_0)}}"/>
où <img class="math" src="../_images/math/b27eff5d2e3f215726775131fd8e71b5e0d7ec15.svg" alt="X=(x_1, x_2, x_3)"/>, <img class="math" src="../_images/math/2ee382df89009496bd7b32148d8d7274ac09d1eb.svg" alt="V=(-1, 0, 0)"/> et <img class="math" src="../_images/math/e4a6748bb8cc947a84af5a7f1f3077cad094c0ac.svg" alt="v_0=s"/>.</p>
<div class="graphviz"><img src="../_images/graphviz-ea73b40fb594b0ad7c6f920ced380bdde5c53a62.png" alt="digraph tree {
    A [label=&quot;y1=g(X, (-1, 0, 0), 5, h)&quot;,shape=record];
    B [label=&quot;y2=g(X, (0, -1, 0), 3, h)&quot;,shape=record];
    C [label=&quot;y3=g(X, (0, 0, -1), 2, h)&quot;,shape=record];
    D [label=&quot;y4=g((y1, y2), (-1, -1), 1, h)&quot;,shape=record];
    E [label=&quot;y5=g((y1, y3), (-1, -1), 1, h)&quot;,shape=record];
    A -&gt; D;
    A -&gt; E;
    B -&gt; D;
    C -&gt; E;

    F [label=&quot;y6=g((y4, y5), (-1, -1), 1, h)&quot;,shape=record];
    CL3 [label=&quot;&lt;c0&gt; 0|&lt;c1&gt; 1&quot;];
    D -&gt; F;
    E -&gt; F;
    F -&gt; CL3:c0;
    F -&gt; CL3:c1;
}" class="graphviz" /></div>
<p>Le problème avec la structure proposée est que chaque noeud
final retourne toujours une classe alors que dans un arbre de
décision, seule une feuille répond. Un noeud final fait la somme
de toutes les feuilles, deux dans cet exemple. L’implémentation
de <a class="reference external" href="https://scikit-learn.org/stable/index.html">scikit-learn</a> n’est pas la plus facile à manipuler
dans le sens où chaque couche ne peut prendre comme entrée
que les sorties de la précédente et la fonction d’activation
est la même pour tous les neurones. On ne peut pas non plus
geler certains coefficients lors de l’apprentissage.
C’est à ce moment-là qu’on se demande si ça vaut le coup
de se lancer dans une implémentation à la rigueur jolie mais
sans doute pas porteuse d’une innovation majeure. Et ce n’est
pas la première fois que quelqu’un se lance dans la conversion
d’un arbre en réseaux de neurones.</p>
<p>J’ai quand même essayé avec le notebook <a class="reference internal" href="../notebooks/ml/neural_tree.html"><span class="std std-ref">Un arbre de décision en réseaux de neurones</span></a>
et les classes <a class="reference internal" href="../api/modules/neural_tree.html#mlstatpy.ml._neural_tree_node.NeuralTreeNode" title="mlstatpy.ml._neural_tree_node.NeuralTreeNode"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeuralTreeNode</span></code></a>,
<a class="reference internal" href="../api/modules/neural_tree.html#mlstatpy.ml.neural_tree.NeuralTreeNet" title="mlstatpy.ml.neural_tree.NeuralTreeNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeuralTreeNet</span></code></a>.
Si l’idée de départ est séduisante, elle requiert une contrainte
supplémentaire qui est de créer un réseau de neurones qui ne soit
pas un minimum local de la fonction d’erreur auquel cas
un apprentissage avec un algorithme à base de gradient ne pourra
pas améliorer les performances du réseau de neurones.</p>
<a class="reference internal image-reference" href="../_images/mloc.png"><img alt="../_images/mloc.png" src="../_images/mloc.png" style="width: 200px;" /></a>
<p>La structure proposée n’est cependant pas la meilleure et elle
pourrait être simplifiée. D’autres projets s’appuie des librairies
existantes :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/kimhc6028/soft-decision-tree">Soft-Decision-Tree</a></p></li>
<li><p><a class="reference external" href="https://github.com/jingxil/Neural-Decision-Forests">Neural-Decision-Forests</a></p></li>
<li><p><a class="reference external" href="https://github.com/microsoft/hummingbird">hummingbird</a></p></li>
</ul>
<p>Ce dernier package convertit un réseau de neurones en autant de couches
que la profondeur de l’arbre. L’image qui suit est tiré de l’article
<a class="reference internal" href="#nakandalam2020" id="id2"><span>[Nakandalam2020]</span></a> et qui résume leur idée.</p>
<img alt="../_images/hb.png" src="../_images/hb.png" />
</section>
<section id="plan-orthogonal">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Plan orthogonal</a><a class="headerlink" href="#plan-orthogonal" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Dans un espace à plusieurs dimensions, la régression logistique
divise l’espace à l’aide d’un hyperplan. La fonction de décision
reste similaire puisque la probabilité de classification dépend de la
distance à cet hyperplan. On suppose qu’il existe une
régression logistique binaire apprise sur un nuage de points
<img class="math" src="../_images/math/3190ad9816294c28609eb295103dfe16eb2f1af9.svg" alt="(X_i, y_i)"/>. La probabilité de bonne classification est
définie par :</p>
<div class="math">
<p><img src="../_images/math/060fc5a6ee1327ff34efe1cf787b1b8eecba4f1c.svg" alt="f(\Theta, X_i) = \frac{1}{1 + e^{-\theta_0 + \sum_{k=1}^d \theta_k x_{ik}}}"/></p>
</div><p>Le vecteur <img class="math" src="../_images/math/cedeedd03cef3c883a44d87e1b30d11b4096832b.svg" alt="\Theta"/> définit un hyperplan. On choisit un vecteur
<img class="math" src="../_images/math/c3489f3eb27b77220c9fac4658290e92e2c5e21c.svg" alt="\Theta'"/> de telle sorte que <img class="math" src="../_images/math/69122eb9ebc5c57f39ac6a78ec836460a16a2957.svg" alt="&lt;\Theta,\Theta'&gt; = 0"/>. Les deux
vecteurs sont orthogonaux. On définit maintenant deux
autres vecteurs <img class="math" src="../_images/math/1077f7e30552bd2046af1f5c68844e8d8c2e624c.svg" alt="\Theta_1, \Theta_2"/> pour deux autres régressions
logistiques. Pour classer un point <img class="math" src="../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/>, on procède comme suit :</p>
<ul class="simple">
<li><p>si <img class="math" src="../_images/math/424dd88e1ca83843c41a5cedab10f5067539684a.svg" alt="&lt;\Theta',X&gt; &lt; 0"/>, on classe le point en appliquant
la régression logistique définie par <img class="math" src="../_images/math/a42f69ae1f1c27fa56d58c8973cb47ff483f5286.svg" alt="Theta_1"/>,</p></li>
<li><p>si <img class="math" src="../_images/math/48b69add5053cec3fa8d97de200ae16d62f08cf4.svg" alt="&lt;\Theta',X&gt; \leqslant 0"/>, on classe le point en appliquant
la régression logistique définie par <img class="math" src="../_images/math/3d3a5758c2ee818a6841358ed871715b3f32e4f0.svg" alt="Theta_2"/>.</p></li>
</ul>
<p>De manière évidente, les performances en classification sont les mêmes
que la première régression logistique. On peut ensuite réestimer les
vecteurs <img class="math" src="../_images/math/1077f7e30552bd2046af1f5c68844e8d8c2e624c.svg" alt="\Theta_1, \Theta_2"/> pour maximiser la vraisemblance
sur chacune des parties. Il ne reste plus qu’à montrer que la vraisemblance
globale sera supérieur à celle obtenue par la première régression logistique.</p>
<p>On pourrait implémenter l’algorithme suivant
(Arbre de régressions logistiques en cascade orthogonale) :</p>
<ul class="simple">
<li><p>Apprendre une régression logistique</p></li>
<li><dl class="simple">
<dt>Choisir un hyperplan perpendiculaire en optimisation</dt><dd><p>un critère <a class="reference internal" href="#l-criteria-reg-log"><span class="std std-ref">Construction d’un pseudo arbre</span></a></p>
</dd>
</dl>
</li>
<li><p>Apprendre une régression logistique sur chacune des parties.</p></li>
<li><p>Continuer jusqu’à ce l’amélioration soit négligeable</p></li>
</ul>
</section>
<section id="interpretabilite">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Interprétabilité</a><a class="headerlink" href="#interpretabilite" title="Lien permanent vers cette rubrique">#</a></h2>
</section>
<section id="bibliographie">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Bibliographie</a><a class="headerlink" href="#bibliographie" title="Lien permanent vers cette rubrique">#</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="scott2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Scott2013</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/pdf/1306.0040.pdf">Expectation-maximization for logistic regression</a>,
James G. Scott, Liang Sun</p>
</div>
<div class="citation" id="nakandalam2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Nakandalam2020</a><span class="fn-bracket">]</span></span>
<p>A Tensor-based Approach for One-size-fits-all ML Prediction Serving.
Supun Nakandalam, Karla Saur, Gyeong-In Yu, Konstantinos Karanasos, Carlo Curino,
Markus Weimer, Matteo Interlandi. To appear at <a class="reference external" href="https://www.usenix.org/conference/osdi20">OSDI 2020</a>.</p>
</div>
</div>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../notebooks/ml/logreg_voronoi.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Voronoï et régression logistique</p>
      </div>
    </a>
    <a class="right-next"
       href="../notebooks/ml/reseau_neurones.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Réseaux de neurones</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Sur cette page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallele-entre-un-neurone-et-une-regression-logistique">Parallèle entre un neurone et une régression logistique</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principe-d-un-arbre-de-decision">Principe d’un arbre de décision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construction-d-un-pseudo-arbre">Construction d’un pseudo arbre</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aparte-mathematique">Aparté mathématique</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approche-em-et-regression-logistique">Approche EM et régression logistique</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lien-vers-les-reseaux-de-neurones">Lien vers les réseaux de neurones</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plan-orthogonal">Plan orthogonal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretabilite">Interprétabilité</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliographie">Bibliographie</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../_sources/c_ml/lr_trees.rst">
      <i class="fa-solid fa-file-lines"></i> Montrer le code source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2016-2023, Xavier Dupré.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Créé en utilisant <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.1.2.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Construit avec le <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">Thème PyData Sphinx</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>