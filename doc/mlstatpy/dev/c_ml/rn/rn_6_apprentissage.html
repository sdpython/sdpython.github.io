
<!DOCTYPE html>


<html lang="fr" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Apprentissage d’un réseau de neurones &#8212; Documentation mlstatpy 0.4.0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css?v=7f9a90b1" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=72ff47bc"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../_static/translations.js?v=d99ca74e"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'c_ml/rn/rn_6_apprentissage';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Recherche" href="../../search.html" />
    <link rel="next" title="Classification" href="rn_7_clas2.html" />
    <link rel="prev" title="Descente de gradient" href="rn_5_newton.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/project_ico.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/project_ico.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_clus/index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    More
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../license.html">
                        License
                      </a>
                    </li>
                
                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_clus/index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    More
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link dropdown-item nav-internal" href="../../license.html">
                        License
                      </a>
                    </li>
                
                </ul>
            </li>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Navigation de la section">
  <p class="bd-links__title" role="heading" aria-level="1">Navigation de la section</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="rn.html">Réseaux de neurones</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="rn_1_def.html">Définition des réseaux de neurones multi-couches</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_2_reg.html">La régression</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_3_clas.html">La classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_4_densite.html">Démonstration du théorème de la densité des réseaux de neurones</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_5_newton.html">Descente de gradient</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Apprentissage d’un réseau de neurones</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_7_clas2.html">Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_8_prol.html">Prolongements</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_9_auto.html">Analyse en composantes principales (ACP) et Auto Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_biblio.html">Bibliographie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../kppv.html">Classification à l’aide des plus proches voisins</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../missing_values_mf.html">Liens entre factorisation de matrices, ACP, k-means</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/ml/mf_acp.html">Factorisation et matrice et ACP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/ml/valeurs_manquantes_mf.html">Valeurs manquantes et factorisation de matrices</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/ml/neural_tree.html">Un arbre de décision en réseaux de neurones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/ml/neural_tree_onnx.html">NeuralTreeNet et ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/ml/neural_tree_cost.html">NeuralTreeNet et coût</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Fil d'Ariane">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Accueil">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Non linéaire</a></li>
    
    
    <li class="breadcrumb-item"><a href="rn.html" class="nav-link">Réseaux de neurones</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Apprentissag...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="apprentissage-d-un-reseau-de-neurones">
<h1>Apprentissage d’un réseau de neurones<a class="headerlink" href="#apprentissage-d-un-reseau-de-neurones" title="Lien permanent vers cette rubrique">#</a></h1>
<nav class="contents local" id="sommaire">
<ul class="simple">
<li><p><a class="reference internal" href="#apprentissage-avec-gradient-global" id="id8">Apprentissage avec gradient global</a></p>
<ul>
<li><p><a class="reference internal" href="#methodes-du-premier-ordre" id="id9">Méthodes du premier ordre</a></p></li>
<li><p><a class="reference internal" href="#methodes-du-second-ordre" id="id10">Méthodes du second ordre</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#apprentissage-avec-gradient-stochastique" id="id11">Apprentissage avec gradient stochastique</a></p></li>
</ul>
</nav>
<p>Le terme apprentissage est encore inspiré de la biologie et se traduit
par la minimisation de la fonction <a class="reference internal" href="rn_5_newton.html#equation-equation-fonction-erreur-g">(2)</a> où
<img class="math" src="../../_images/math/bd1e0600a66dbe0f8deddbd57953774419ba4e4f.svg" alt="f"/> est un réseau de neurone défini par un <a class="reference internal" href="rn_1_def.html#rn-definition-perpception-1"><span class="std std-ref">perceptron</span></a>.
Il existe plusieurs méthodes pour effectuer celle-ci.
Chacune d’elles vise à minimiser la fonction d’erreur :</p>
<div class="math">
<p><img src="../../_images/math/ab0f3aab1b6c085dce399157adfa87c341cd01ad.svg" alt="E\pa{W}   = G \pa{W}  =   \sum_{i=1}^{N} e\pa {Y_{i} - \widehat{Y_{i}^W}}
                                    =   \sum_{i=1}^{N} e\pa {Y_{i} - f \pa{W,X_{i}}}"/></p>
</div><p>Dans tous les cas, les différents apprentissages utilisent la suite
suivante <img class="math" src="../../_images/math/88c60f67e721d340b6249b8e809044ced8ec128a.svg" alt="\pa{ \epsilon_{t}}"/> vérifiant <a class="reference internal" href="#equation-rn-suite-epsilon-train">(1)</a>
et proposent une convergence vers un minimum local.</p>
<div class="math" id="equation-rn-suite-epsilon-train">
<p><span class="eqno">(1)<a class="headerlink" href="#equation-rn-suite-epsilon-train" title="Lien permanent vers cette équation">#</a></span><img src="../../_images/math/f5d45ae3feb7ccb915206408af9e26ca37ae95a6.svg" alt="\forall t&gt;0,\quad\varepsilon_{t}\in \R_{+}^{\ast} \text{ et }
\sum_{t\geqslant0}\varepsilon_{t}=+\infty,\quad
\sum_{t\geqslant0}\varepsilon_{t}^{2}&lt;+\infty"/></p>
</div><p>Il est souhaitable d’apprendre plusieurs fois la même fonction en modifiant
les conditions initiales de ces méthodes de manière à améliorer la robustesse de la solution.</p>
<section id="apprentissage-avec-gradient-global">
<span id="rn-apprentissage-global"></span><h2><a class="toc-backref" href="#id8" role="doc-backlink">Apprentissage avec gradient global</a><a class="headerlink" href="#apprentissage-avec-gradient-global" title="Lien permanent vers cette rubrique">#</a></h2>
<p>L’algorithme de <a class="reference internal" href="rn_5_newton.html#algo-retropropagation"><span class="std std-ref">rétropropagation</span></a> permet d’obtenir
la dérivée de l’erreur <img class="math" src="../../_images/math/a52a5cbf4e378c1f5b4401b55b3ed7cd1a281e1d.svg" alt="e"/> pour un vecteur d’entrée <img class="math" src="../../_images/math/b66c7d6c52c7b12bee7fb24d76c38fc70cfc0a2f.svg" alt="X"/>. Or l’erreur
<img class="math" src="../../_images/math/dd1ae49ba41cce059d20a1b88a25aac2f5f49ddc.svg" alt="E\pa{W}"/> à minimiser est la somme des erreurs pour chaque exemple
<img class="math" src="../../_images/math/c15b262677ad7177c2e37298a2eb382d712b3a52.svg" alt="X_i"/>, le gradient global <img class="math" src="../../_images/math/016fd3067731d48565711ad8c1a265ee4ddbb750.svg" alt="\partialfrac{E\pa{W}}{W}"/> de cette erreur
globale est la somme des gradients pour chaque exemple
(voir équation <a class="reference internal" href="rn_5_newton.html#equation-algo-retro-1">(3)</a>).
Parmi les méthodes d’optimisation basées sur le gradient global, on distingue deux catégories :</p>
<ul class="simple">
<li><p>Les méthodes du premier ordre, elles sont calquées sur la
<a class="reference external" href="https://fr.wikipedia.org/wiki/M%C3%A9thode_de_Newton">méthode de Newton</a>
et n’utilisent que le gradient.</p></li>
<li><p>Les méthodes du second ordre ou méthodes utilisant un
<a class="reference external" href="https://fr.wikipedia.org/wiki/M%C3%A9thode_du_gradient_conjugu%C3%A9">gradient conjugué</a>
elles sont plus coûteuses en calcul mais plus performantes
puisque elles utilisent la dérivée seconde ou une valeur approchée.</p></li>
</ul>
<section id="methodes-du-premier-ordre">
<span id="rn-optim-premier-ordre"></span><h3><a class="toc-backref" href="#id9" role="doc-backlink">Méthodes du premier ordre</a><a class="headerlink" href="#methodes-du-premier-ordre" title="Lien permanent vers cette rubrique">#</a></h3>
<p>Les méthodes du premier ordre sont rarement utilisées.
Elles sont toutes basées sur le principe
de la descente de gradient de Newton présentée dans
la section <a class="reference internal" href="rn_5_newton.html#optimisation-newton"><span class="std std-ref">Algorithme et convergence</span></a> :</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme0">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-algorithme-apprentissage-1">Algorithme A1 : optimisation du premier ordre</p>
<p><em>Initialiation</em></p>
<p>Le premier jeu de coefficients <img class="math" src="../../_images/math/bc75580554dcbb669594f87f31230f72104e48c7.svg" alt="W_0"/> du réseau
de neurones est choisi aléatoirement.</p>
<div class="math">
<p><img src="../../_images/math/5388ef68de96930b793a48328dff1504867571e5.svg" alt="\begin{array}{rcl}
t   &amp;\longleftarrow&amp;    0 \\
E_0 &amp;\longleftarrow&amp;    \sum_{i=1}^{N} e\pa {Y_{i} - f \pa{W_0,X_{i}}}
\end{array}"/></p>
</div><p><em>Calcul du gradient</em></p>
<p><img class="math" src="../../_images/math/aeed0fb547a698da1ca6d0363ab80b896841cd08.svg" alt="g_t \longleftarrow \partialfrac{E_t}{W} \pa {W_t} = \sum_{i=1}^{N}
e'\pa {Y_{i} - f \pa{W_t,X_{i}}}"/></p>
<p><em>Mise à jour</em></p>
<div class="math">
<p><img src="../../_images/math/a312431bf0192fab0852451b6e48f1159ba5f63b.svg" alt="\begin{array}{rcl}
W_{t+1} &amp;\longleftarrow&amp; W_t - \epsilon_t g_t \\
E_{t+1} &amp;\longleftarrow&amp; \sum_{i=1}^{N} e\pa {Y_i - f \pa{W_{t+1},X_i}} \\
t       &amp;\longleftarrow&amp; t+1
\end{array}"/></p>
</div><p><em>Terminaison</em></p>
<p>Si <img class="math" src="../../_images/math/3b6716971f87450506016adebd159bd9d428fb33.svg" alt="\frac{E_t}{E_{t-1}} \approx 1"/> (ou <img class="math" src="../../_images/math/01f3e82099e7da9fc961e88b4160ff0bcb2bd37a.svg" alt="\norm{g_t} \approx 0"/>)
alors l’apprentissage a convergé sinon retour au calcul du gradient.</p>
</div>
<p>La condition d’arrêt peut-être plus ou moins stricte selon les besoins du problème.
Cet algorithme converge vers un minimum local de la fonction d’erreur
(d’après le théorème de <a class="reference internal" href="rn_5_newton.html#theoreme-convergence"><span class="std std-ref">convergence</span></a>
mais la vitesse de convergence est inconnue.</p>
</section>
<section id="methodes-du-second-ordre">
<span id="rn-optim-second-ordre"></span><h3><a class="toc-backref" href="#id10" role="doc-backlink">Méthodes du second ordre</a><a class="headerlink" href="#methodes-du-second-ordre" title="Lien permanent vers cette rubrique">#</a></h3>
<p>L’algorithme <a class="reference internal" href="#rn-apprentissage-global"><span class="std std-ref">apprentissage global</span></a> fournit le canevas des
méthodes d’optimisation du second ordre. La mise à jour des coefficients est différente car
elle prend en compte les dernières valeurs des coefficients ainsi que les
derniers gradients calculés. Ce passé va être utilisé pour estimer une
direction de recherche pour le minimum différente de celle du gradient,
cette direction est appelée gradient conjugué (voir <a class="reference internal" href="rn_biblio.html#more1977" id="id1"><span>[Moré1977]</span></a>).</p>
<p>Ces techniques sont basées sur une approximation du second degré de la fonction à minimiser.
On note <img class="math" src="../../_images/math/e5c619f0600e251cabb3318b03870bc6f2c4870f.svg" alt="M"/> le nombre de coefficients du réseau de neurones (biais compris).
Soit <img class="math" src="../../_images/math/f3fa871b290645456b54760bd7a2e2d6f2554115.svg" alt="h: \R^{M} \dans \R"/> la fonction d’erreur associée au réseau de neurones :
<img class="math" src="../../_images/math/b5dca968eb586406aa09b85d022fc4a41382fe28.svg" alt="h \pa {W} = \sum_{i} e \pa{Y_i,f \pa{ W,X_i} }"/>.
Au voisinage de <img class="math" src="../../_images/math/8b51d45c8d4a3a12e6d7fdfbaedc54c592cb6b03.svg" alt="W_{0}"/>, un développement limité donne :</p>
<div class="math">
<p><img src="../../_images/math/27cd274752f6c17d97a3c357fc3a92cc49b6070f.svg" alt="h \pa {W}     =   h\pa {W_0}  + \frac{\partial h\left( W_{0}\right)  }{\partial W}\left( W-W_{0}\right) +\left(
W-W_{0}\right) ^{\prime}\frac{\partial^{2}h\left(  W_{0}\right)  }{\partial W^{2}}\left( W-W_{0}\right) +o\left\|
W-W_{0}\right\|  ^{2}"/></p>
</div><p>Par conséquent, sur un voisinage de <img class="math" src="../../_images/math/8b51d45c8d4a3a12e6d7fdfbaedc54c592cb6b03.svg" alt="W_{0}"/>, la fonction <img class="math" src="../../_images/math/f2a2cbec18360093e12a24d5658016c0669c3216.svg" alt="h\left( W\right)"/>
admet un minimum local si <img class="math" src="../../_images/math/4d8c8b6f93eaa2f51c19b94bffd23bd4af00868d.svg" alt="\frac{\partial^{2}h\left( W_{0}\right) }{\partial W^{2}}"/>
est définie positive strictement.</p>
<p><em>Rappel :</em> <img class="math" src="../../_images/math/c23baac04e89eda1a09673a2d72750acc500ac49.svg" alt="\dfrac{\partial^{2}h\left(  W_{0}\right)  }{\partial W^{2}}"/>
est définie positive strictement <img class="math" src="../../_images/math/e655abeb79e9e353aec7e446314ae8f651ae00e5.svg" alt="\Longleftrightarrow\forall Z\in\R^{N},\; Z\neq0\Longrightarrow
Z^{\prime}\dfrac{\partial ^{2}h\left( W_{0}\right)  }{\partial W^{2}}Z&gt;0"/>.</p>
<p>Une matrice symétrique définie strictement positive est inversible,
et le minimum est atteint pour la valeur :</p>
<div class="math" id="equation-rn-hessien">
<p><span class="eqno">(2)<a class="headerlink" href="#equation-rn-hessien" title="Lien permanent vers cette équation">#</a></span><img src="../../_images/math/06cec53733bd1fac736fa67e71a42d1a5a7b43e7.svg" alt="\begin{eqnarray}
W_{\min}= W_0 + \frac{1}{2}\left[  \dfrac{\partial^{2}h\left(  W_{0}\right) }
        {\partial W^{2}}\right]  ^{-1}\left[  \frac{\partial h\left(  W_{0}\right)
}{\partial W}\right] \nonumber
\end{eqnarray}"/></p>
</div><p>Néanmoins, pour un réseau de neurones, le calcul de la dérivée seconde est coûteux,
son inversion également. C’est pourquoi les dernières valeurs des coefficients
et du gradient sont utilisées afin d’approcher cette dérivée seconde ou directement
son inverse. Deux méthodes d’approximation sont présentées :</p>
<ul class="simple">
<li><p>L’algorithme <a class="reference external" href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">BFGS (Broyden-Fletcher-Goldfarb-Shano)</a>
(<a class="reference internal" href="rn_biblio.html#broyden1967" id="id2"><span>[Broyden1967]</span></a>, <a class="reference internal" href="rn_biblio.html#fletcher1993" id="id3"><span>[Fletcher1993]</span></a>), voir aussi les versions <a class="reference external" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a>.</p></li>
<li><p>L’algoritmhe <a class="reference external" href="https://en.wikipedia.org/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula">DFP  (Davidon-Fletcher-Powell)</a>
(<a class="reference internal" href="rn_biblio.html#davidon1959" id="id4"><span>[Davidon1959]</span></a>, <a class="reference internal" href="rn_biblio.html#fletcher1963" id="id5"><span>[Fletcher1963]</span></a>).</p></li>
</ul>
<p>La figure du <a class="reference internal" href="#figure-gradient-conjugue"><span class="std std-ref">gradient conjugué</span></a> est couramment employée
pour illustrer l’intérêt des méthodes de gradient conjugué.
Le problème consiste à trouver le minimum d’une fonction quadratique,
par exemple, <img class="math" src="../../_images/math/df55abf7d78ed004fee5c47137dcbf9f96665ad8.svg" alt="G\pa{x,y} = 3x^2 + y^2"/>. Tandis que le gradient est orthogonal
aux lignes de niveaux de la fonction <img class="math" src="../../_images/math/86a30eae2899d36dcee14ab62c5e4c8a68feed4d.svg" alt="G"/>, le gradient conjugué se dirige plus
sûrement vers le minimum global.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Figure0">
<div class="docutils container">
</div>
<p class="admonition-title" id="figure-gradient-conjugue">Figure F1 : Gradient conjugué</p>
<img alt="Wikipedia" src="../../_images/Conjugate_gradient_illustration.png" />
<p>Gradient et gradient conjugué sur une ligne de niveau de la fonction <img class="math" src="../../_images/math/df55abf7d78ed004fee5c47137dcbf9f96665ad8.svg" alt="G\pa{x,y} = 3x^2 + y^2"/>,
le gradient est orthogonal aux lignes de niveaux de la fonction <img class="math" src="../../_images/math/86a30eae2899d36dcee14ab62c5e4c8a68feed4d.svg" alt="G"/>,
mais cette direction est rarement la bonne à moins que le point
<img class="math" src="../../_images/math/6b06a232d5ddbeef16cc8e18544872a82ea7ce8b.svg" alt="\pa{x,y}"/> se situe sur un des axes des ellipses,
le gradient conjugué agrège les derniers déplacements et propose une direction
de recherche plus plausible pour le minimum de la fonction.
Voir <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">Conjugate Gradient Method</a>.</p>
</div>
<p>Ces méthodes proposent une estimation de la dérivée seconde
(ou de son inverse) utilisée en <a class="reference internal" href="#equation-rn-hessien">(2)</a>.
Dans les méthodes du premier ordre, une itération permet de calculer les
poids <img class="math" src="../../_images/math/c22f92a3a7715cbda63093d0b91e1f5901d695a9.svg" alt="W_{t+1}"/> à partir des poids <img class="math" src="../../_images/math/bddf7a6848121731125f5226c8406dc99bab1b4b.svg" alt="W_t"/> et du
gradient <img class="math" src="../../_images/math/960be04fe11ab2149364585d376337ecc81ce7d0.svg" alt="G_t"/>. Si ce gradient est petit, on peut supposer
que <img class="math" src="../../_images/math/c3cca538a050f4f0f173f692d5185f3695bd2394.svg" alt="G_{t+1}"/> est presque égal au produit de la dérivée seconde par
<img class="math" src="../../_images/math/960be04fe11ab2149364585d376337ecc81ce7d0.svg" alt="G_t"/>. Cette relation est mise à profit pour construire une estimation
de la dérivée seconde. Cette matrice notée <img class="math" src="../../_images/math/739004e0316eecb56f6f6a20192d7308ead48c54.svg" alt="B_t"/> dans
l’algorithme <a class="reference internal" href="#rn-algo-bfgs"><span class="std std-ref">BFGS</span></a>
est d’abord supposée égale à l’identité puis actualisée à chaque
itération en tenant de l’information apportée par chaque déplacement.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme1">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-algo-bfgs">Algorithme A2 : BFGS</p>
<p>Le nombre de paramètres de la fonction <img class="math" src="../../_images/math/bd1e0600a66dbe0f8deddbd57953774419ba4e4f.svg" alt="f"/> est <img class="math" src="../../_images/math/e5c619f0600e251cabb3318b03870bc6f2c4870f.svg" alt="M"/>.</p>
<p><em>Initialisation</em></p>
<p>Le premier jeu de coefficients <img class="math" src="../../_images/math/bc75580554dcbb669594f87f31230f72104e48c7.svg" alt="W_0"/> du réseau de neurones est
choisi aléatoirement.</p>
<div class="math">
<p><img src="../../_images/math/804c33ea5bcf5b8bacc5e8fa7a961ce9653d9a7b.svg" alt="\begin{array}{lcl}
t   &amp;\longleftarrow&amp;    0 \\
E_0 &amp;\longleftarrow&amp;    \sum_{i=1}^{N} e\pa {Y_{i} - f \pa{W_0,X_{i}}} \\
B_0 &amp;\longleftarrow&amp;    I_M \\
i   &amp;\longleftarrow&amp;    0
\end{array}"/></p>
</div><p><em>Calcul du gradient</em></p>
<div class="math">
<p><img src="../../_images/math/a7ecc026d9ab8a9a72c088ef011ae6405de4a317.svg" alt="\begin{array}{lcl}
g_t &amp;\longleftarrow&amp; \partialfrac{E_t}{W} \pa {W_t}= \sum_{i=1}^{N} e'\pa {Y_{i} - f \pa{W_t,X_{i}}} \\
c_t &amp;\longleftarrow&amp; B_t g_t
\end{array}"/></p>
</div><p><em>Mise à jour des coefficients</em></p>
<div class="math">
<p><img src="../../_images/math/76d9807a61c38275b95502c8d60559473cb32d88.svg" alt="\begin{array}{lcl}
\epsilon^*  &amp;\longleftarrow&amp;    \underset{\epsilon}{\arg \inf} \; \sum_{i=1}^{N}
         e\pa {Y_i - f \pa{W_t - \epsilon c_t,X_i}}  \\
W_{t+1}     &amp;\longleftarrow&amp;    W_t - \epsilon^* c_t \\
E_{t+1}     &amp;\longleftarrow&amp;    \sum_{i=1}^{N} e\pa {Y_i - f \pa{W_{t+1},X_i}} \\
t           &amp;\longleftarrow&amp;    t+1
\end{array}"/></p>
</div><p><em>Mise à jour de la marice :math:`B_t`</em></p>
<div class="line-block">
<div class="line">si <img class="math" src="../../_images/math/f3e52d8e225c30d827b35e29dd145dec90aba17e.svg" alt="t - i \supegal M"/> ou <img class="math" src="../../_images/math/afe436c823e7ca0426a943ab32e8babe63521bbb.svg" alt="g'_{t-1} B_{t-1} g_{t-1} \infegal 0"/> ou <img class="math" src="../../_images/math/9b4c8ed648bc9ab197724b92a6866ab0452d697e.svg" alt="g'_{t-1} B_{t-1} \pa {g_t - g_{t-1}} \infegal 0"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/1ba32dc8d96e50cbc4af248f7e833f1f573dcab9.svg" alt="B_{t} \longleftarrow I_M"/></div>
<div class="line"><img class="math" src="../../_images/math/b83d14d4cbb48b16e40aea2cbeef640eef199e61.svg" alt="i \longleftarrow  t"/></div>
</div>
<div class="line">sinon</div>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/c3f33b1ac6c3a0812d73921e1c22938636591089.svg" alt="s_t \longleftarrow    W_t - W_{t-1}"/></div>
<div class="line"><img class="math" src="../../_images/math/ad6fdd3eae29399f3641eea97fb925caf5237ba6.svg" alt="d_t    \longleftarrow    g_t - g_{t-1}"/></div>
<div class="line"><img class="math" src="../../_images/math/789736e77349e0a5712132a79f9eb70e1edc2c8d.svg" alt="B_{t}  \longleftarrow    B_{t-1} +   \pa{1 + \dfrac{ d'_t B_{t-1} d_t}{d'_t s_t}}\dfrac{s_t s'_t} {s'_t d_t}- \dfrac{s_t d'_t B_{t-1} +  B_{t-1} d_t s'_t } { d'_t s_t }"/></div>
</div>
</div>
<p><em>Terminaison</em></p>
<p>Si <img class="math" src="../../_images/math/3b6716971f87450506016adebd159bd9d428fb33.svg" alt="\frac{E_t}{E_{t-1}} \approx 1"/> alors l’apprentissage a convergé sinon retour au calcul
du gradient.</p>
</div>
<p>Lorsque la matrice <img class="math" src="../../_images/math/739004e0316eecb56f6f6a20192d7308ead48c54.svg" alt="B_t"/> est égale à l’identité,
le gradient conjugué est égal au gradient. Au fur et
à mesure des itérations, cette matrice toujours
symétrique évolue en améliorant la convergence de l’optimisation.
Néanmoins, la matrice <img class="math" src="../../_images/math/739004e0316eecb56f6f6a20192d7308ead48c54.svg" alt="B_t"/> doit être « nettoyée »
(égale à l’identité) fréquemment afin d’éviter qu’elle
n’agrège un passé trop lointain. Elle est aussi nettoyée lorsque
le gradient conjugué semble trop s’éloigner du véritable gradient
et devient plus proche d’une direction perpendiculaire.</p>
<p>La convergence de cet algorithme dans le cas des réseaux de
neurones est plus rapide qu’un algorithme du premier ordre,
une preuve en est donnée dans <a class="reference internal" href="rn_biblio.html#driancourt1996" id="id6"><span>[Driancourt1996]</span></a>.</p>
<p>En pratique, la recherche de <img class="math" src="../../_images/math/86639fcd9d046a06aedfa8d313c7db83bb87ea6a.svg" alt="\epsilon^*"/> est réduite car
le calcul de l’erreur est souvent coûteux, il peut être effectué
sur un grand nombre d’exemples. C’est pourquoi on remplace
l’étape de mise à jour de l’algorithme <a class="reference internal" href="#rn-algo-bfgs"><span class="std std-ref">BFGS</span></a>
par celle-ci :</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme2">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-algo-bfgs-prime">Algorithme A3 : BFGS”</p>
<p>Le nombre de paramètre de la fonction <img class="math" src="../../_images/math/bd1e0600a66dbe0f8deddbd57953774419ba4e4f.svg" alt="f"/> est <img class="math" src="../../_images/math/e5c619f0600e251cabb3318b03870bc6f2c4870f.svg" alt="M"/>.</p>
<p><em>Initialisation, calcul du gradient</em></p>
<p>Voir <a class="reference internal" href="#rn-algo-bfgs"><span class="std std-ref">BFGS</span></a>.</p>
<p><em>Recherche de :math:`epsilon^*`</em></p>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/c2f4787ef82d766dea8c28c3e4cbac971fc1dc62.svg" alt="\epsilon^*  \longleftarrow    \epsilon_0"/></div>
<div class="line">while <img class="math" src="../../_images/math/11597c815a54a4ddb3661b9b20f3c1ebb1e359a9.svg" alt="E_{t+1} \supegal E_t"/> et <img class="math" src="../../_images/math/d53e61f7f95e1f232f3c89173cb1faa81c976ee1.svg" alt="\epsilon^* \gg 0"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/f4a4f67b7bdcdeb007c58b06750772e00b6e5b34.svg" alt="\epsilon^*  \longleftarrow   \frac{\epsilon^*}{2}"/></div>
<div class="line"><img class="math" src="../../_images/math/4bf26dbaa478f61a2210c669b127a0165e8097d1.svg" alt="W_{t+1}     \longleftarrow    W_t - \epsilon^* c_t"/></div>
<div class="line"><img class="math" src="../../_images/math/acbc9d1e717a03726b906d62eab8743bdd0fa457.svg" alt="E_{t+1}     \longleftarrow    \sum_{i=1}^{N} e\pa {Y_i - f \pa{W_{t+1},X_i}}"/></div>
<div class="line"><br /></div>
</div>
<div class="line">if <img class="math" src="../../_images/math/e17cb35f4573720c3e638f5daf5a38c21f3a0ff7.svg" alt="\epsilon_* \approx 0"/> et <img class="math" src="../../_images/math/5e51e64fddb431273f3f79871c42877e4757e8a9.svg" alt="B_t \neq I_M"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/cf9e9f0f427389eccf91cbf5ef61cb29697bbf0f.svg" alt="B_{t}       \longleftarrow   I_M"/></div>
<div class="line"><img class="math" src="../../_images/math/39a5c4292337b8d76d9482579c3e647be3c90710.svg" alt="i           \longleftarrow    t"/></div>
<div class="line">Retour au calcul du gradient.</div>
</div>
</div>
<p><em>Mise à jour des coefficients</em></p>
<div class="math">
<p><img src="../../_images/math/88eab5209c4e6d7bbc92a2efc2f52e2cd41e7d5c.svg" alt="\begin{array}{lcl}
W_{t+1}     &amp;\longleftarrow&amp;    W_t - \epsilon^* c_t \\
E_{t+1}     &amp;\longleftarrow&amp;    \sum_{i=1}^{N} e\pa {Y_i - f \pa{W_{t+1},X_i}} \\
t           &amp;\longleftarrow&amp;    t+1
\end{array}"/></p>
</div><p><em>Mise à jour de la matrice :math:`B_t`, temrinaison</em></p>
<p>Voir <a class="reference internal" href="#rn-algo-bfgs"><span class="std std-ref">BFGS</span></a>.</p>
</div>
<p>L’algorithme DFP est aussi un algorithme de gradient conjugué
qui propose une approximation différente de l’inverse de la dérivée seconde.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme3">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-algo-dfp">Algorithme A4 : DFP</p>
<p>Le nombre de paramètre de la fonction <img class="math" src="../../_images/math/bd1e0600a66dbe0f8deddbd57953774419ba4e4f.svg" alt="f"/> est <img class="math" src="../../_images/math/e5c619f0600e251cabb3318b03870bc6f2c4870f.svg" alt="M"/>.</p>
<p><em>Initialisation</em></p>
<p>Le premier jeu de coefficients <img class="math" src="../../_images/math/bc75580554dcbb669594f87f31230f72104e48c7.svg" alt="W_0"/>
du réseau de neurones est choisi aléatoirement.</p>
<div class="math">
<p><img src="../../_images/math/804c33ea5bcf5b8bacc5e8fa7a961ce9653d9a7b.svg" alt="\begin{array}{lcl}
t   &amp;\longleftarrow&amp;    0 \\
E_0 &amp;\longleftarrow&amp;    \sum_{i=1}^{N} e\pa {Y_{i} - f \pa{W_0,X_{i}}} \\
B_0 &amp;\longleftarrow&amp;    I_M \\
i   &amp;\longleftarrow&amp;    0
\end{array}"/></p>
</div><p><em>Calcul du gradient</em></p>
<div class="math">
<p><img src="../../_images/math/a7ecc026d9ab8a9a72c088ef011ae6405de4a317.svg" alt="\begin{array}{lcl}
g_t &amp;\longleftarrow&amp; \partialfrac{E_t}{W} \pa {W_t}= \sum_{i=1}^{N} e'\pa {Y_{i} - f \pa{W_t,X_{i}}} \\
c_t &amp;\longleftarrow&amp; B_t g_t
\end{array}"/></p>
</div><p><em>Mise à jour des coefficients</em></p>
<div class="math">
<p><img src="../../_images/math/0015ebaa71bdf20549057303b634591859387414.svg" alt="\begin{array}{lcl}
\epsilon^*  &amp;\longleftarrow&amp;    \underset{\epsilon}{\arg \inf} \;
                             \sum_{i=1}^{N} e\pa {Y_i - f \pa{W_t - \epsilon c_t,X_i}}  \\
W_{t+1}     &amp;\longleftarrow&amp;    W_t - \epsilon^* c_t \\
E_{t+1}     &amp;\longleftarrow&amp;    \sum_{i=1}^{N} e\pa {Y_i - f \pa{W_{t+1},X_i}} \\
t           &amp;\longleftarrow&amp;    t+1
\end{array}"/></p>
</div><p><em>Mise à jour de la matrice :math:`B_t`</em></p>
<div class="line-block">
<div class="line">si <img class="math" src="../../_images/math/f3e52d8e225c30d827b35e29dd145dec90aba17e.svg" alt="t - i \supegal M"/> ou <img class="math" src="../../_images/math/afe436c823e7ca0426a943ab32e8babe63521bbb.svg" alt="g'_{t-1} B_{t-1} g_{t-1} \infegal 0"/> ou <img class="math" src="../../_images/math/9b4c8ed648bc9ab197724b92a6866ab0452d697e.svg" alt="g'_{t-1} B_{t-1} \pa {g_t - g_{t-1}} \infegal 0"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/807869dc278eadfef93c21dde16f86514cbf9a0b.svg" alt="B_{t}       \longleftarrow    I_M"/></div>
<div class="line"><img class="math" src="../../_images/math/39a5c4292337b8d76d9482579c3e647be3c90710.svg" alt="i           \longleftarrow    t"/></div>
</div>
<div class="line">sinon</div>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/83a63df4d67019872cef3e6df6179d431d594cba.svg" alt="d_t         \longleftarrow    W_t - W_{t-1}"/></div>
<div class="line"><img class="math" src="../../_images/math/6c98162b604899d81a0c52f3f4e6d2f1ed676e55.svg" alt="s_t         \longleftarrow    g_t - g_{t-1}"/></div>
<div class="line"><img class="math" src="../../_images/math/3aa698b1dd0f06d7f324112666625ff2bc0a1f5f.svg" alt="B_{t}       \longleftarrow"/>    B_{t-1} +     dfrac{d_t d’_t} {d’_t s_t} - dfrac{B_{t-1} s_t s’_t B_{t-1} } { s’_t B_{t-1} s_t }`</div>
</div>
</div>
<p><em>Terminaison</em></p>
<p>Si <img class="math" src="../../_images/math/3b6716971f87450506016adebd159bd9d428fb33.svg" alt="\frac{E_t}{E_{t-1}} \approx 1"/> alors l’apprentissage a convergé sinon retour à
du calcul du gradient.</p>
</div>
<p>Seule l’étape de mise à jour <img class="math" src="../../_images/math/739004e0316eecb56f6f6a20192d7308ead48c54.svg" alt="B_t"/> diffère dans les
algorithmes <a class="reference internal" href="#rn-algo-bfgs"><span class="std std-ref">BFGS</span></a> et <a class="reference internal" href="#rn-algo-dfp"><span class="std std-ref">DFP</span></a>.
Comme l’algorithme <a class="reference internal" href="#rn-algo-bfgs"><span class="std std-ref">BFGS</span></a>,
on peut construire une version <a class="reference internal" href="#rn-algo-dfp"><span class="std std-ref">DFP</span></a>”
inspirée de l’algorithme <a class="reference internal" href="#rn-algo-bfgs-prime"><span class="std std-ref">BFGS”</span></a>.</p>
</section>
</section>
<section id="apprentissage-avec-gradient-stochastique">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Apprentissage avec gradient stochastique</a><a class="headerlink" href="#apprentissage-avec-gradient-stochastique" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Compte tenu des courbes d’erreurs très <a class="reference internal" href="#figure-courbe-accident"><span class="std std-ref">accidentées</span></a>
dessinées par les réseaux de neurones, il existe une multitude de minima
locaux. De ce fait, l’apprentissage global converge rarement vers le
minimum global de la fonction d’erreur lorsqu’on applique les algorithmes
basés sur le gradient global. L’apprentissage avec gradient stochastique
est une solution permettant de mieux explorer ces courbes d’erreurs.
De plus, les méthodes de gradient conjugué nécessite le stockage d’une
matrice trop grande parfois pour des fonctions ayant quelques milliers
de paramètres. C’est pourquoi l’apprentissage avec gradient stochastique
est souvent préféré à l’apprentissage global pour de grands réseaux de
neurones alors que les méthodes du second ordre trop coûteuses en
calcul sont cantonnées à de petits réseaux. En contrepartie, la
convergence est plus lente. La démonstration de cette convergence nécessite
l’utilisation de quasi-martingales et est une convergence presque sûre <a class="reference internal" href="rn_biblio.html#bottou1991" id="id7"><span>[Bottou1991]</span></a>.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Figure1">
<div class="docutils container">
</div>
<p class="admonition-title" id="figure-courbe-accident">Figure F2 : Exemple de minimal locaux</p>
<img alt="../../_images/errminloc.png" src="../../_images/errminloc.png" />
</div>
<div class="admonition-mathdef admonition" id="indexmathe-Algprithme0">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-algorithme-apprentissage-2">Algprithme A1 : apprentissage stochastique</p>
<p><em>Initialisation</em></p>
<p>Le premier jeu de coefficients <img class="math" src="../../_images/math/bc75580554dcbb669594f87f31230f72104e48c7.svg" alt="W_0"/>
du réseau de neurones est choisi aléatoirement.</p>
<div class="math">
<p><img src="../../_images/math/991a0fbe8fd698858c4f61e05f1bfc6e10cefacb.svg" alt="\begin{array}{lcl}
t       &amp;\longleftarrow&amp;    0 \\
E_0 &amp;\longleftarrow&amp;    \sum_{i=1}^{N} e\pa {Y_{i} - f \pa{W_0,X_{i}}}
\end{array}"/></p>
</div><p><em>Récurrence</em></p>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/fda32587146b31ad06e47a16a6ae551a5417c920.svg" alt="W_{t,0} \longleftarrow    W_0"/></div>
<div class="line">for <img class="math" src="../../_images/math/3f43cb1046b6339c2199c6ab241d6fb272ca7a82.svg" alt="t'"/> in <img class="math" src="../../_images/math/b8884bfe4e80e0c9234f8806465397209a2d4395.svg" alt="0..N-1"/></div>
<div class="line-block">
<div class="line"><img class="math" src="../../_images/math/375d56054bff1fbb9621c9c64afe7a514bc4d35f.svg" alt="i \longleftarrow"/> nombre aléatoire dans <img class="math" src="../../_images/math/aa9ee889bdbcfbaa910718c537b7ed7dd93f5123.svg" alt="\ensemble{1}{N}"/></div>
<div class="line"><img class="math" src="../../_images/math/a25ed9f8afe8bb6d796cfc93081217a717b88328.svg" alt="g \longleftarrow \partialfrac{E}{W} \pa {W_{t,t'}}=  e'\pa {Y_{i} - f\pa{W_{t,t'},X_{i}}}"/></div>
<div class="line"><img class="math" src="../../_images/math/9953cf1952d3da30c599da7fee5afab03d274696.svg" alt="W_{t,t'+1} \longleftarrow    W_{t,t'} - \epsilon_t g"/></div>
</div>
<div class="line"><img class="math" src="../../_images/math/0935c9d88dedb7635588ccd0f1ec14453f3c6468.svg" alt="W_{t+1} \longleftarrow W_{t,N}"/></div>
<div class="line"><img class="math" src="../../_images/math/45148d8b094084ca0e9ae5ac503192904467e958.svg" alt="E_{t+1} \longleftarrow \sum_{i=1}^{N} e\pa {Y_{i} - f \pa{W_{t+1},X_{i}}}"/></div>
<div class="line"><img class="math" src="../../_images/math/02dd178e61d85f10a2824b1cd98c269269370b59.svg" alt="t \longleftarrow t+1"/></div>
</div>
<p><em>Terminaison</em></p>
<p>Si <img class="math" src="../../_images/math/3b6716971f87450506016adebd159bd9d428fb33.svg" alt="\frac{E_t}{E_{t-1}} \approx 1"/>
alors l’apprentissage a convergé sinon retour au
calcul du gradient.</p>
</div>
<p>En pratique, il est utile de converser le meilleur jeu de
coefficients : <img class="math" src="../../_images/math/dca43c0fbab92989b5f823e604dd34ee28bd2c93.svg" alt="W^* = \underset{u \supegal 0}{\arg \min} \; E_{u}"/>
car la suite <img class="math" src="../../_images/math/a3ad3c031439712b7229bcab40dbcd8b8ec547ff.svg" alt="\pa {E_u}_{u \supegal 0}"/> n’est pas une suite décroissante.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="rn_5_newton.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Descente de gradient</p>
      </div>
    </a>
    <a class="right-next"
       href="rn_7_clas2.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Sur cette page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apprentissage-avec-gradient-global">Apprentissage avec gradient global</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methodes-du-premier-ordre">Méthodes du premier ordre</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methodes-du-second-ordre">Méthodes du second ordre</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apprentissage-avec-gradient-stochastique">Apprentissage avec gradient stochastique</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/c_ml/rn/rn_6_apprentissage.rst">
      <i class="fa-solid fa-file-lines"></i> Montrer le code source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2016-2023, Xavier Dupré.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Créé en utilisant <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.1.2.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Construit avec le <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">Thème PyData Sphinx</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>