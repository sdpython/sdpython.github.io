<!doctype html>
<html class="no-js" lang="fr" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Recherche" href="../search.html"><link rel="next" title="Régression linéaire par morceaux" href="../notebooks/ml/piecewise_linear_regression.html"><link rel="prev" title="Régression quantile illustrée" href="../notebooks/dsgarden/quantile_regression_example.html">
        <link rel="prefetch" href="../_static/project_ico.png" as="image">

    <!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>Régression linéaire par morceaux - Documentation mlstatpy 0.5.0</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Documentation mlstatpy 0.5.0</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/project_ico.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Documentation mlstatpy 0.5.0</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Recherche" name="q" aria-label="Recherche">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Mathematics</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../c_clus/index.html">Clustering</a><input aria-label="Toggle navigation of Clustering" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../c_clus/kmeans.html">k-means</a></li>
<li class="toctree-l2"><a class="reference internal" href="../c_clus/gauss_mixture.html">Mélange de lois normales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../c_clus/kohonen.html">Carte de Kohonen</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="index.html">Non linéaire</a><input aria-label="Toggle navigation of Non linéaire" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="rn/rn.html">Réseaux de neurones</a><input aria-label="Toggle navigation of Réseaux de neurones" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_1_def.html">Définition des réseaux de neurones multi-couches</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_2_reg.html">La régression</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_3_clas.html">La classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_4_densite.html">Démonstration du théorème de la densité des réseaux de neurones</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_5_newton.html">Descente de gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_6_apprentissage.html">Apprentissage d’un réseau de neurones</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_7_clas2.html">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_8_prol.html">Prolongements</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_9_auto.html">Analyse en composantes principales (ACP) et Auto Encoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="rn/rn_biblio.html">Bibliographie</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="kppv.html">Classification à l’aide des plus proches voisins</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="missing_values_mf.html">Liens entre factorisation de matrices, ACP, k-means</a><input aria-label="Toggle navigation of Liens entre factorisation de matrices, ACP, k-means" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/mf_acp.html">Factorisation et matrice et ACP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/valeurs_manquantes_mf.html">Valeurs manquantes et factorisation de matrices</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/ml/neural_tree.html">Un arbre de décision en réseaux de neurones</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/ml/neural_tree_onnx.html">NeuralTreeNet et ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/ml/neural_tree_cost.html">NeuralTreeNet et coût</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index_reg_lin.html">Régression linéaire</a><input aria-label="Toggle navigation of Régression linéaire" checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../notebooks/dsgarden/regression_lineaire.html">Régression linéaire</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="regression_quantile.html">Régression quantile ou régression L1</a><input aria-label="Toggle navigation of Régression quantile ou régression L1" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/quantile_regression_example.html">Régression quantile illustrée</a></li>
</ul>
</li>
<li class="toctree-l2 current has-children current-page"><a class="current reference internal" href="#">Régression linéaire par morceaux</a><input aria-label="Toggle navigation of Régression linéaire par morceaux" checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/piecewise_linear_regression.html">Régression linéaire par morceaux</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/regression_no_inversion.html">Régression sans inversion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="l1l2.html">Normalisation des coefficients</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="index_reg_log.html">Régression logistique</a><input aria-label="Toggle navigation of Régression logistique" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="lr_voronoi.html">Régression logistique, diagramme de Voronoï, k-Means</a><input aria-label="Toggle navigation of Régression logistique, diagramme de Voronoï, k-Means" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/logreg_voronoi.html">Voronoï et régression logistique</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lr_trees.html">Régression logistique par morceaux, arbres de décision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/ml/reseau_neurones.html">Réseaux de neurones</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="survival_analysis.html">Analyse de survie</a><input aria-label="Toggle navigation of Analyse de survie" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/survival.html">Analyse de survie en pratique</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../c_nlp/index.html">NLP</a><input aria-label="Toggle navigation of NLP" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../c_nlp/completion.html">Complétion</a><input aria-label="Toggle navigation of Complétion" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../c_nlp/completion_formalisation.html">Formalisation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../c_nlp/completion_fausse.html">Fausses idées reçues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../c_nlp/completion_metrique.html">Nouvelle métrique</a></li>
<li class="toctree-l3"><a class="reference internal" href="../c_nlp/completion_propriete.html">Propriétés mathématiques</a></li>
<li class="toctree-l3"><a class="reference internal" href="../c_nlp/completion_optimisation.html">Problème d’optimisation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../c_nlp/completion_implementation.html">Implémentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../c_nlp/completion_digression.html">Digressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_trie.html">Complétion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_profiling.html">Completion profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_trie_long.html">Completion Trie and metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_simple.html">Complétion Simple</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../c_metric/index.html">Métriques</a><input aria-label="Toggle navigation of Métriques" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../c_metric/roc.html">Courbe ROC</a><input aria-label="Toggle navigation of Courbe ROC" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/metric/roc_example.html">ROC</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../c_metric/pvalues.html">Confidence Interval and p-Value</a><input aria-label="Toggle navigation of Confidence Interval and p-Value" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/metric/pvalues_examples.html">p-values</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../c_algo/index.html">Algorithmes</a><input aria-label="Toggle navigation of Algorithmes" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../c_algo/edit_distance.html">Distance d’édition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../c_algo/graph_distance.html">Distance between two graphs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../c_algo/gest.html">Détection de segments</a><input aria-label="Toggle navigation of Détection de segments" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/image/segment_detection.html">Détection de segments dans une image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../c_garden/index.html">Pérégrinations</a><input aria-label="Toggle navigation of Pérégrinations" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/dsgarden/split_train_test.html">Répartir en base d’apprentissage et de test</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/dsgarden/correlation_non_lineaire.html">Corrélations non linéaires</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../c_garden/file_dattente.html">File d’attente, un petit exemple</a><input aria-label="Toggle navigation of File d’attente, un petit exemple" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/file_dattente_ex.html">File d’attente, un exemple simple</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../c_garden/strategie_avec_alea.html">Optimisation avec données aléatoires</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/dsgarden/discret_gradient.html">Le gradient et le discret</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../c_garden/quantization.html">Quantization</a><input aria-label="Toggle navigation of Quantization" class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/quantization_f8.html">Quantization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/dsgarden/classification_multiple.html">Classification multiple</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API</a><input aria-label="Toggle navigation of API" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/ml.html">Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/optim.html">Optimisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/text.html">Traitement du langage naturel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/data.html">Source de données</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/graph.html">Graphes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/image.html">Image</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../api/modules/index.html">Modules</a><input aria-label="Toggle navigation of Modules" class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/poulet.html">mlstatpy.garden.poulet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/graph_distance.html">mlstatpy.graph.graph_distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/kppv.html">mlstatpy.ml.kppv</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/kppv_laesa.html">mlstatpy.ml.kppv_laesa</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/logreg.html">mlstatpy.ml.logreg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/neural_tree.html">mlstatpy.ml.neural_tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/roc.html">mlstatpy.ml.roc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/completion.html">mlstatpy.nlp.completion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/completion_simple.html">mlstatpy.nlp.completion_simple</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/modules/sgd.html">mlstatpy.optim.sgd</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../i_ex.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../defthe_index.html">Listes des définitions et théorèmes</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_examples/index.html">Gallery of examples</a><input aria-label="Toggle navigation of Gallery of examples" class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" role="switch" type="checkbox"/><label for="toctree-checkbox-23"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/plot_logistic_decision.html">Arbre d’indécision</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../notebooks/index.html">Galleries de notebooks</a><input aria-label="Toggle navigation of Galleries de notebooks" class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" role="switch" type="checkbox"/><label for="toctree-checkbox-24"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../notebooks/dsgarden/index.html">Le petit coin des data scientists</a><input aria-label="Toggle navigation of Le petit coin des data scientists" class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" role="switch" type="checkbox"/><label for="toctree-checkbox-25"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/classification_multiple.html">Classification multiple</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/correlation_non_lineaire.html">Corrélations non linéaires</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/discret_gradient.html">Le gradient et le discret</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/file_dattente_ex.html">File d’attente, un exemple simple</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/quantile_regression_example.html">Régression quantile illustrée</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/quantization_f8.html">Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/regression_lineaire.html">Régression linéaire</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/dsgarden/split_train_test.html">Répartir en base d’apprentissage et de test</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../notebooks/image/index.html">Images</a><input aria-label="Toggle navigation of Images" class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" role="switch" type="checkbox"/><label for="toctree-checkbox-26"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/image/segment_detection.html">Détection de segments dans une image</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../notebooks/metric/index.html">Métriques</a><input aria-label="Toggle navigation of Métriques" class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" role="switch" type="checkbox"/><label for="toctree-checkbox-27"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/metric/pvalues_examples.html">p-values</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/metric/roc_example.html">ROC</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../notebooks/ml/index.html">Machine Learning</a><input aria-label="Toggle navigation of Machine Learning" class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" role="switch" type="checkbox"/><label for="toctree-checkbox-28"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/logreg_voronoi.html">Voronoï et régression logistique</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/mf_acp.html">Factorisation et matrice et ACP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/neural_tree.html">Un arbre de décision en réseaux de neurones</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/neural_tree_cost.html">NeuralTreeNet et coût</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/neural_tree_onnx.html">NeuralTreeNet et ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/piecewise_linear_regression.html">Régression linéaire par morceaux</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/regression_no_inversion.html">Régression sans inversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/reseau_neurones.html">Réseaux de neurones</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/survival.html">Analyse de survie en pratique</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/ml/valeurs_manquantes_mf.html">Valeurs manquantes et factorisation de matrices</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../notebooks/nlp/index.html">NLP - Natural Language Processing</a><input aria-label="Toggle navigation of NLP - Natural Language Processing" class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" role="switch" type="checkbox"/><label for="toctree-checkbox-29"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_profiling.html">Completion profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_simple.html">Complétion Simple</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_trie.html">Complétion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/nlp/completion_trie_long.html">Completion Trie and metrics</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CHANGELOGS.html">Change Logs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Index du module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../search.html">Page de recherche</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/c_ml/piecewise.rst" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="regression-lineaire-par-morceaux">
<span id="l-reglin-piecewise-streaming"></span><h1>Régression linéaire par morceaux<a class="headerlink" href="#regression-lineaire-par-morceaux" title="Lien vers cette rubrique">¶</a></h1>
<p>Le paragraphe <a class="reference internal" href="../notebooks/dsgarden/regression_lineaire.html"><span class="std std-ref">Régression linéaire</span></a>
étudie le lien entre le coefficient <img class="math" src="../_images/math/a8daffb4a324830d6527f2af583a712b1d98e24b.svg" alt="R^2"/>
et la corrélation pour finalement illustrer
une façon de réaliser une régression linéaire par
morceaux. L’algorithme s’appuie sur un arbre
de régression pour découper en morceaux ce qui
n’est pas le plus satisfaisant car l’arbre
cherche à découper en segment en approximant
la variable à régresser <em>Y</em> par une constante sur chaque
morceaux et non une droite.
On peut se poser la question de comment faire
pour construire un algorithme qui découpe en approximant
<em>Y</em> par une droite et non une constante. Le plus dur
n’est pas de le faire mais de le faire efficacement.
Et pour comprendre là où je veux vous emmener, il faudra
un peu de mathématiques.</p>
<p>Une implémentation de ce type de méthode est proposée
dans la pull request <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/issues/13106">Model trees (M5P and co)</a>
qui répond à au problème posée dans
<a class="reference external" href="https://github.com/scikit-learn/scikit-learn/pull/13732">Model trees (M5P)</a>
et originellement implémentée dans
<a class="reference external" href="https://github.com/ankonzoid/LearningX/tree/main/advanced_ML/model_tree">Building Model Trees</a>.
Cette dernière implémentation réestime les modèles comme l’implémentation
décrite au paragraphe <a class="reference internal" href="#l-decisiontree-reglin-piecewise-naive"><span class="std std-ref">Implémentation naïve d’une régression linéaire par morceaux</span></a>
mais étendue à tout type de modèle.</p>
<section id="exploration">
<h2>Exploration<a class="headerlink" href="#exploration" title="Lien vers cette rubrique">¶</a></h2>
<section id="probleme-et-regression-lineaire-dans-un-espace-a-une-dimension">
<h3>Problème et regréssion linéaire dans un espace à une dimension<a class="headerlink" href="#probleme-et-regression-lineaire-dans-un-espace-a-une-dimension" title="Lien vers cette rubrique">¶</a></h3>
<p>Tout d’abord, une petite
illustration du problème avec la classe <a class="reference external" href="https://sdpython.github.io/doc/mlinsights/dev/auto_examples/plot_piecewise_linear_regression.html">PiecewiseRegression</a>
implémentée selon l’API de <a class="reference external" href="https://scikit-learn.org/stable/index.html">scikit-learn</a>.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/ml/piecewise_linear_regression.html">Régression linéaire par morceaux</a></li>
</ul>
</div>
<a class="reference internal image-reference" href="../_images/piecenaive.png"><img alt="../_images/piecenaive.png" src="../_images/piecenaive.png" style="width: 250px;" />
</a>
<p>Cette régression par morceaux est obtenue grâce à un arbre
de décision. Celui-ci trie le nuage de points <img class="math" src="../_images/math/1694ea2dd2b8ef2b567244ec4cf39e7192b1b7c7.svg" alt="(X_i, Y_i)"/>
par ordre croissant selon les <em>X</em>, soit <img class="math" src="../_images/math/80aac0c0a3a79c497b125b9349e93f7ddafdb8ea.svg" alt="X_i \leqslant X_{i+1}"/>.
L’arbre coupe en deux lorsque la différence des erreurs quadratiques est
maximale, erreur quadratique obtenue en approximant <em>Y</em> par sa moyenne
sur l’intervalle considéré. On note l’erreur quadratique :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/c0dbbe5922b80f19b549722b28143388a13d302b.svg" alt="\begin{array}{rcl}
C_(i,j) &amp;=&amp; \frac{1}{j - i + 1} \sum_{i \leqslant k \leqslant j} Y_i \\
D_(i,j) &amp;=&amp; \frac{1}{j - i + 1} \sum_{i \leqslant k \leqslant j} Y^2_i \\
E_(i,j) &amp;=&amp; \frac{1}{j - i + 1} \sum_{i \leqslant k \leqslant j} ( Y_i - C(i,j))^2 =
\frac{1}{j - i + 1} \sum_{i \leqslant k \leqslant j} Y_i^2 - C(i,j)^2 = D(i,j) - C(i,j)^2
\end{array}"/></p>
</div></div>
<p>La dernière ligne applique la formule <img class="math" src="../_images/math/3ec7ba9791a9755d03bc6290e612d960d55a8d42.svg" alt="\var{X} = \esp{X^2} - \esp{X}^2"/>
qui est facile à redémontrer.
L’algorithme de l’arbre de décision coupe un intervalle en
deux et détermine l’indice <em>k</em> qui minimise la différence :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/f5cb835ee6610f896bebc4e26a344237ba0c5195.svg" alt="\Delta_k = E(1, n) - (E(1, k) + E(k+1, n))"/></p>
</div></div>
<p>L’arbre de décision optimise la construction d’une fonction
en escalier qui représente au mieux le nuage de points,
les traits verts sur le graphe suivant, alors qu’il faudrait
choisir une erreur quadratique qui corresponde aux traits
oranges.</p>
<a class="reference internal image-reference" href="../_images/piecenaive2.png"><img alt="../_images/piecenaive2.png" src="../_images/piecenaive2.png" style="width: 250px;" />
</a>
<p>Il suffirait donc de remplacer l’erreur <em>E</em> par celle obtenue
par une régression linéaire. Mais si c’était aussi simple,
l’implémentation de <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" title="(disponible dans scikit-learn v1.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code></a>
la proposerait. Alors pourquoi ?
La raison principale est que cela coûte trop cher en
temps de calcul. Pour trouver l’indice <em>k</em>, il faut calculer
toutes les erreurs <img class="math" src="../_images/math/658d808845aa1029116e11b53bbc364d306f9ad1.svg" alt="E(1,k)"/> <img class="math" src="../_images/math/8ba8d4da615f311fe7116f5a56a72bd4c56c4ffc.svg" alt="E(k+1,n)"/>, ce qui
coûte très cher lorsque cette erreur est celle d’une régression
linéaire parce qu’il est difficile de simplifier la différence :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/172ed5b5f52ef052c3b239cbd94cb2846d2d49a9.svg" alt="\begin{array}{rcl}
\Delta_{k} - \Delta_{k-1} &amp;=&amp;  - (E(1, k) + E(k+1, n)) + (E(1, k-1) + E(k, n)) \\
&amp;=&amp;  E(1, k-1) - E(1, k) + E(k, n) - E(k+1, n)
\end{array}"/></p>
</div></div>
<p><strong>Arbre de régression constante</strong></p>
<p>On s’intéresse au terme <img class="math" src="../_images/math/26b2c5fb803dae6f99049f089811f9f2a7db2fec.svg" alt="E(1, k-1) - E(1, k)"/> dans le cas
le nuage de points est représenté par une constante sur chaque segment.
C’est l’hypothèse faite par l’algorithme classique de construction
d’un arbre de régression (segments verts sur le premier dessin) :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/71e4e06330fde0816ce29df3c62bc8d2a52c6205.svg" alt="\begin{array}{rcl}
C_(1,k-1) - C_(1,k) &amp;=&amp; \frac{1}{k-1} \sum_{1 \leqslant i \leqslant k-1} Y_i
- \frac{1}{k} \sum_{1 \leqslant i \leqslant k} Y_i \\
&amp;=&amp; (\frac{1}{k-1} - \frac{1}{k}) \sum_{1 \leqslant i \leqslant k-1} Y_i - \frac{Y_k}{k} \\
&amp;=&amp; \frac{1}{k(k-1)} \sum_{1 \leqslant i \leqslant k-1} Y_i- \frac{Y_k}{k} \\
&amp;=&amp; \frac{1}{k} C(1,k-1) - \frac{Y_k}{k}
\end{array}"/></p>
</div></div>
<p>On en déduit que :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/5d6714e74c99aece3d6fe3e491d4e655fbb8e0fd.svg" alt="\begin{array}{rcl}
E(1, k-1) - E(1, k) &amp;=&amp; \frac{1}{k} D(1,k-1) - \frac{Y_k^2}{k} +
(C_(1,k-1) - C_(1,k))(C_(1,k-1) + C_(1,k)) \\
&amp;=&amp; \frac{1}{k} D(1,k-1) - \frac{Y_k^2}{k} + \pa{\frac{1}{k} C(1,k-1) - \frac{Y_k}{k}}
\pa{\frac{Y_k}{k} - \frac{1}{k} C(1,k-1) + 2 C(1,k-1)}
\end{array}"/></p>
</div></div>
<p>On voit que cette formule ne fait intervenir que <img class="math" src="../_images/math/eb34a0af52ec5549d9e0f643877f6a2b5ec39f8f.svg" alt="C(1,k-1), D(1,k-1), Y_k"/>,
elle est donc très rapide à calculer et c’est pour cela qu’apprendre un arbre
de décision peut s’apprendre en un temps raisonnable. Cela repose sur la possibilité
de calculer le critère optimisé par récurrence. On voit également que ces formules
ne font pas intervenir <em>X</em>, elles sont donc généralisables au cas
multidimensionnel. Il suffira de trier les couples <img class="math" src="../_images/math/1694ea2dd2b8ef2b567244ec4cf39e7192b1b7c7.svg" alt="(X_i, Y_i)"/>
selon chaque dimension et déterminer le meilleur seuil de coupure
d’abord sur chacune des dimensions puis de prendre le meilleur
de ces seuils sur toutes les dimensions. Le problème est résolu.</p>
<p>Le notebook <a class="reference external" href="https://sdpython.github.io/doc/mlinsights/dev/auto_examples/plot_piecewise_linear_regression_criterion.html">Custom Criterion for DecisionTreeRegressor</a>
implémente une version pas efficace du critère
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html">MSE</a>
et compare la vitesse d’exécution avec l’implémentation de <a class="reference external" href="https://scikit-learn.org/stable/index.html">scikit-learn</a>.
Il implémente ensuite le calcul rapide de <em>scikit-learn</em> pour
montrer qu’on obtient un temps comparable.
Le résultat est sans équivoque. La version rapide n’implémente pas
<img class="math" src="../_images/math/cf538c492b8662ac5b95fb96ea2e4ff69aa124f7.svg" alt="\Delta_{k} - \Delta_{k-1}"/> mais plutôt les sommes
<img class="math" src="../_images/math/9fa2d7ea9cb02f83e71b11fa5b2fd7ac6a9fe6a4.svg" alt="\sum_1^k w_i Y_i"/>, <img class="math" src="../_images/math/f6e959d6d545898c8d87aedfe83e0f01da643180.svg" alt="\sum_1^k w_i Y_i^2"/> dans un sens
et dans l’autre. En gros,
le code stocke les séries des numérateurs et des dénominateurs
pour les diviser au dernier moment.</p>
<p><strong>Arbre de régression linéaire</strong></p>
<p>Le cas d’une régression est plus complexe. Prenons d’abord le cas
où il n’y a qu’un seule dimension,
il faut d’abord optimiser le problème :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/c37312238907df1cd378017669c970bb43d18db5.svg" alt="E(1, n) = \min_{a,b} = \sum_{k=1}^n (a X_k + b - Y_k)^2"/></p>
</div></div>
<p>On dérive pour aboutir au système d’équations suivant :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/413af9dbb7608c510a00cc75d4c3f8ef6cec10fc.svg" alt="\begin{array}{rcl}
\frac{\partial E(1,n)}{\partial a} &amp;=&amp; 0 = \sum_{k=1}^n X_k(a X_k + b - Y_k) \\
\frac{\partial E(1,n)}{\partial b} &amp;=&amp; 0 = \sum_{k=1}^n a X_k + b - Y_k
\end{array}"/></p>
</div></div>
<p>Ce qui aboutit à :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/0022aea42c8943591783338a0e929a7affe29fac.svg" alt="\begin{array}{rcl}
a(1, n) &amp;=&amp; \frac{\sum_{k=1}^n X_kY_k - \pa{\sum_{k=1}^n X_k}\pa{\sum_{k=1}^n Y_k} }
{\sum_{k=1}^n X_k^2 -\pa{\sum_{k=1}^n X_k}^2 } \\
b(1, n) &amp;=&amp; \sum_{k=1}^n Y_k - a \pa{\sum_{k=1}^n X_k}
\end{array}"/></p>
</div></div>
<p>Pour construire un algorithme rapide pour apprendre un arbre de décision
avec cette fonction de coût, il faut pouvoir calculer
<img class="math" src="../_images/math/0608f7efc2c362e1487489c1e132cf634be4e910.svg" alt="a(1, k)"/> en fonction de <img class="math" src="../_images/math/1559d93d1f6666214ae3d31f3ee76896eb793a16.svg" alt="a(1, k-1), b(1, k-1), X_k, Y_k"/>
ou d’autres quantités intermédiaires qui ne font pas intervenir
les valeurs <img class="math" src="../_images/math/0b9d5de2f033904a0cd6dc46ab7cc5352d1112a0.svg" alt="X_{i&lt;k} &lt; Y_{i&lt;k}"/>. D’après ce qui précède,
cela paraît tout-à-fait possible. Mais dans le
<a class="reference external" href="https://fr.wikipedia.org/wiki/R%C3%A9gression_lin%C3%A9aire#Estimateur_des_moindres_carr%C3%A9s">cas multidimensionnel</a>,
il faut déterminer le vecteur <em>A</em> qui minimise <img class="math" src="../_images/math/8c79fbf1d7c6d7b7a96f1e4f34fba6a2c50e1376.svg" alt="\sum_{k=1}^n \norme{Y - XA}^2"/>
ce qui donne <img class="math" src="../_images/math/f449a41aa7237dba7242313b7dedebf33ca4bc58.svg" alt="A = (X'X)^{-1} X' Y"/>. Si on note <img class="math" src="../_images/math/55494098942c9c6553dcd9946cdeba3469c04319.svg" alt="M_{1..k}"/> la matrice
<em>M</em> tronquée pour ne garder que ses <em>k</em> premières lignes, il faudrait pouvoir
calculer rapidement :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/99b34a44bcec286c76aeb85dcfdbe9e5b5cda4af.svg" alt="A_{k-1} - A_k = (X_{1..k-1}'X_{1..k-1})^{-1} X'_{1..k-1} Y_{1..k-1} -
(X_{1..k}'X_{1..k})^{-1} X'_{1..k} Y_{1..k}"/></p>
</div></div>
<p>La documentation de <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" title="(disponible dans scikit-learn v1.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code></a>
ne mentionne que deux critères pour apprendre un arbre de décision
de régression, <em>MSE</em> pour
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="(disponible dans scikit-learn v1.7)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.mean_squared_error()</span></code></a> et <em>MAE</em> pour
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="(disponible dans scikit-learn v1.7)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.mean_absolute_error()</span></code></a>. Les autres critères n’ont
probablement pas été envisagés. L’article <a class="reference internal" href="#acharya2016" id="id1"><span>[Acharya2016]</span></a> étudie la possibilité
de ne pas calculer la matrice <img class="math" src="../_images/math/faddbcdb1cedc000285cd02f2b6592b66b2a6e4d.svg" alt="A_k"/> pour tous les <em>k</em>.
Le paragraphe <a class="reference internal" href="#l-piecewise-linear-regression"><span class="std std-ref">Streaming Linear Regression</span></a> utilise
le fait que la matrice <em>A</em> est la solution d’un problème d’optimisation
quadratique et propose un algorithme de mise à jour de la matrice <em>A</em>
(cas unidimensionnel). Cet exposé va un peu plus loin
pour proposer une version qui ne calcule pas de matrices inverses.</p>
</section>
<section id="implementation-naive-d-une-regression-lineaire-par-morceaux">
<span id="l-decisiontree-reglin-piecewise-naive"></span><h3>Implémentation naïve d’une régression linéaire par morceaux<a class="headerlink" href="#implementation-naive-d-une-regression-lineaire-par-morceaux" title="Lien vers cette rubrique">¶</a></h3>
<p>On part du cas général qui écrit la solution d’une régression
linéaire comme étant la matrice <img class="math" src="../_images/math/f449a41aa7237dba7242313b7dedebf33ca4bc58.svg" alt="A = (X'X)^{-1} X' Y"/>
et on adapte l’implémentation de <a class="reference external" href="https://scikit-learn.org/stable/index.html">scikit-learn</a> pour
optimiser l’erreur quadratique obtenue. Ce n’est pas simple mais
pas impossible. Il faut entrer dans du code <a class="reference external" href="https://cython.org/">cython</a> et, pour
éviter de réécrire une fonction qui multiplie et inverse une matrice,
on peut utiliser la librairie <a class="reference external" href="http://www.netlib.org/lapack/">LAPACK</a>. Je ne vais pas plus loin
ici car cela serait un peu hors sujet mais ce n’était pas une partie
de plaisir. Cela donne :
<a class="reference external" href="https://github.com/sdpython/mlinsights/blob/main/mlinsights/mlmodel/piecewise_tree_regression_criterion_linear.pyx">piecewise_tree_regression_criterion_linear.pyx</a>
C’est illustré toujours par le notebook
<a class="reference external" href="https://sdpython.github.io/doc/mlinsights/dev/auto_examples/plot_piecewise_linear_regression_criterion.html">DecisionTreeRegressor optimized for Linear Regression</a>.</p>
</section>
<section id="aparte-sur-la-continuite-de-la-regression-lineaire-par-morceaux">
<h3>Aparté sur la continuité de la régression linéaire par morceaux<a class="headerlink" href="#aparte-sur-la-continuite-de-la-regression-lineaire-par-morceaux" title="Lien vers cette rubrique">¶</a></h3>
<p id="index-0">Approcher la fonction <img class="math" src="../_images/math/aa0348f3df30a93683ef38e881ca7cb946248c92.svg" alt="y=f(x) + \epsilon"/> quand <em>x</em> et <em>y</em>
sont réels est un problème facile, trop facile… A voir le dessin,
précédent, il est naturel de vouloir recoller les morceaux lorsqu’on
passe d’un segment à l’autre. Il s’agit d’une optimisation sous contrainte.
Il est possible également d’ajouter une contrainte de régularisation
qui tient compte de cela. On exprime cela comme suit avec une régression
linéaire à deux morceaux.</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/9ca153f5cd3fea640a9618a1af77f488f78f31db.svg" alt="E = \sum_{X_i \leqslant t} (a_1 X_i + b_1 - y)^2 +
\sum_{X_i \geqslant t} (a_2 X_i + b_2 - y)^2 +
\lambda (a_1 t + b_1 - a_2 t - b)^2"/></p>
</div></div>
<p>Le cas multidimensionnel est loin d’être aussi simple. Avec une
dimension, chaque zone a deux voisines. En deux dimensions,
chaque zone peut en avoir plus de deux. La figure suivante
montre une division de l’espace dans laquelle la zone centrale
a cinq voisins.</p>
<a class="reference internal image-reference" href="../_images/voisin.png"><img alt="../_images/voisin.png" src="../_images/voisin.png" style="width: 200px;" />
</a>
<p>Peut-on facilement approcher une fonction <img class="math" src="../_images/math/c4637fb53c04074df92efab45afa88fd72c24298.svg" alt="z = f(x,y) + \epsilon"/>
par un plan en trois dimensions ? A moins que tous les sommets soient
déjà dans le même plan, c’est impossible. La zone en question n’est
peut-être même pas convexe. Une régression linéaire par morceaux
et continue en plusieurs dimensions n’est pas un problème facile.
Cela n’empêche pas pour autant d’influencer la détermination de chaque
morceaux avec une contrainte du type de celle évoquée plus haut
mais pour écrire la contrainte lorsque les zones sont construites
à partir des feuilles d’un arbre de décision, il faut déterminer
quelles sont les feuilles voisines.
Et ça c’est un problème intéressant !</p>
</section>
<section id="regression-lineaire-et-correlation">
<span id="l-reglin-nocoreel-solution"></span><h3>Régression linéaire et corrélation<a class="headerlink" href="#regression-lineaire-et-correlation" title="Lien vers cette rubrique">¶</a></h3>
<p>On reprend le calcul multidimensionnel mais on s’intéresse au
cas où la matrice <img class="math" src="../_images/math/a4a91a4fe99964abc306490321f26a321eeac5c8.svg" alt="X'X"/> est diagonale qui correspond au cas
où les variables <img class="math" src="../_images/math/9c0d15f488dd82383f4e8b8e5228855160de0a99.svg" alt="X_1, ..., X_C"/> ne sont pas corrélées.
Si <img class="math" src="../_images/math/463381d7b89c81a6d10821f9eea2f541161bc82a.svg" alt="X'X = diag(\lambda_1, ..., \lambda_C) = diag(\sum_{k=1}^n X^2_{k1}, ..., \sum_{k=1}^n X^2_{kC})"/>,
la matrice <img class="math" src="../_images/math/5fdb7ef407cf673109e4443028c7e8898ee8c89e.svg" alt="A"/> s’exprime plus simplement <img class="math" src="../_images/math/359f4112b2ef3c5677bc09af5ad921f402e464f1.svg" alt="A = D^{-1} X' Y"/>.
On en déduit que :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/f380dc8cbada34b26796b278a51ac66e829f63f1.svg" alt="a_c = \frac{\sum_{k=1}^n X_{kc} Y_k}{\sum_{k=1}^n X^2_{kc}} =
\frac{\sum_{k=1}^n X_{kc} Y_k}{\lambda_c}"/></p>
</div></div>
<p>Cette expression donne un indice sur la résolution d’une régression linéaire
pour laquelle les variables sont corrélées. Il suffit d’appliquer d’abord une
<a class="reference external" href="https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales">ACP</a>
(Analyse en Composantes Principales) et de calculer les coefficients
<img class="math" src="../_images/math/167cb0aaeedc8023300fe0fc135378363fd9d686.svg" alt="a_c"/> associés à des valeurs propres non nulles. On écrit alors
<img class="math" src="../_images/math/a13fee46f20b78fa6888da98ade61c1c7bead3f2.svg" alt="X'X = P'DP"/> où la matrice <em>P</em> vérifie <img class="math" src="../_images/math/0b14464e38fd8042d17db9235e3738066c5f6921.svg" alt="P'P = I"/>.</p>
</section>
<section id="idee-de-l-algorithme">
<h3>Idée de l’algorithme<a class="headerlink" href="#idee-de-l-algorithme" title="Lien vers cette rubrique">¶</a></h3>
<p>On s’intéresser d’abord à la recherche d’un meilleur point de coupure.
Pour ce faire, les éléments <img class="math" src="../_images/math/3190ad9816294c28609eb295103dfe16eb2f1af9.svg" alt="(X_i, y_i)"/> sont triés le plus souvent
selon l’ordre défini par une dimension. On note <em>E</em> l’erreur de prédiction
sur cette échantillon <img class="math" src="../_images/math/ee8ac6bed719f2a9169811c18deb651678c40d9f.svg" alt="E = \min_\beta \sum_k (X_k \beta - y_k)^2"/>.
On définit ensuite <img class="math" src="../_images/math/60d03d481e939a550d514faeecf31d3f9588c569.svg" alt="E(i, j) = \min_\beta \sum_{k=i}^j (X_k \beta - y_k)^2"/>.
D’après cette notation, <img class="math" src="../_images/math/30be0391d08a68eeb255483750ddb13964c0b359.svg" alt="E = E(1,n)"/>. La construction de l’arbre
de décision passe par la détermination de <img class="math" src="../_images/math/fc90b3f44afc581dfadde98aeb786a78889e3a4c.svg" alt="k^*"/> qui vérifie :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/1447416645ca81ab2e0011e4f9b57bf57cfe4299.svg" alt="\begin{array}{rcl}
E(1,k^*) + E(k^*+1, n) &amp;=&amp; \min_k E(1,k) + E(k+1, n) \\
&amp;=&amp; \min_k \pa{ \min_{\beta_1} \sum_{l=1}^k (X_l \beta_1 - y_l)^2 +
\min_{\beta_2} \sum_{l=k+1}^n (X_l \beta_2 - y_l)^2}
\end{array}"/></p>
</div></div>
<p>Autrement dit, on cherche le point de coupure qui maximise la différence
entre la prédiction obtenue avec deux régressions linéaires plutôt qu’une.
On sait qu’il existe une matrice <em>P</em> qui vérifie :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/026f8d8927a9938835854ee476679e47f2a7a24d.svg" alt="PP' = 1 \text{ et } (XP)'(XP) = P'X'XP = D = Z'Z"/></p>
</div></div>
<p>Où <img class="math" src="../_images/math/b81bcd71dc255ed8f164e21ded62afea801e2ecf.svg" alt="D=diag(d_1, ..., d_C)"/> est une matrice
diagonale. On a posé <img class="math" src="../_images/math/06ce22e281fe92cdc1b9875a910603411a96d157.svg" alt="Z = XP"/>,
donc <img class="math" src="../_images/math/76281a90bb6470abd656b8fbb654cfbc66d6627d.svg" alt="d_a = &lt;Z_a, Z_a&gt;"/>.
On peut réécrire le problème
de régression comme ceci :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/30485a5a82496c83b4b819e08bd57c1777bc8237.svg" alt="\beta^* = \arg\min_\beta \sum_i \norm{ y_i - X_i\beta} =
\arg\min_\beta \norm{Y - X\beta}"/></p>
</div></div>
<p>Comme <img class="math" src="../_images/math/7896f030b42a84bb6abd71bb2697ed8da76024bb.svg" alt="X = ZP'"/> :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/8a4466c42323fb980b26884e611d41afd2faa650.svg" alt="\norm{Y - X\beta} = \norm{Y - X\beta} = \norm{Y - ZP'\beta} =
\norm{Y - Z\gamma}"/></p>
</div></div>
<p>Avec <img class="math" src="../_images/math/7b86c8cad43690a38d1605e3316312db8fcc4608.svg" alt="\gamma = P'\beta"/>. C’est la même régression
après un changement de repère et on la résoud de la même manière :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/ecaadd2a048acd89ce262c67e1451990304f421b.svg" alt="\gamma^* = (Z'Z)^{-1}Z'Y = D^{-1}Z'Y"/></p>
</div></div>
<p>La notation <img class="math" src="../_images/math/3f0b680b6a1c224ae13cb630ebdfecfef9987c9a.svg" alt="M_i"/> désigne la ligne <em>i</em> et
<img class="math" src="../_images/math/2d8171bb45a9a4c9ad469b8edc60c5056e0f52d6.svg" alt="M_{[k]}"/> désigne la colonne.
On en déduit que le coefficient de la régression
<img class="math" src="../_images/math/69f57d69a6e4aec68fb613b82a2724444285e59a.svg" alt="\gamma_k"/> est égal à :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/d6f80bc601bab9bfc4c7233bf827d723bc5b9614.svg" alt="\gamma_k = \frac{&lt;Z_{[k]},Y&gt;}{&lt;Z_{[k]},Z_{[k]}&gt;} =
\frac{&lt;(XP')_{[k]},Y&gt;}{&lt;(XP')_{[k]},(XP')_{[k]}&gt;}"/></p>
</div></div>
<p>On en déduit que :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/9d389c9d55b577739c7756ddc7c51c63af7ea6a6.svg" alt="\norm{Y - X\beta} = \norm{Y - \sum_{k=1}^{C}Z_{[k]}\frac{&lt;Z_{[k]},Y&gt;}{&lt;Z_{[k]},Z_{[k]}&gt;}} =
\norm{Y - \sum_{k=1}^{C}(XP')_{[k]}\frac{&lt;(XP')_{[k]},Y&gt;}{&lt;(XP')_{[k]},(XP')_{[k]}&gt;}}"/></p>
</div></div>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme0">
<div class="docutils container">
</div>
<p class="admonition-title" id="algo-decision-tree-mselin">Algorithme A1 : Arbre de décision optimisé pour les régressions linéaires</p>
<p>On dipose qu’un nuage de points <img class="math" src="../_images/math/3190ad9816294c28609eb295103dfe16eb2f1af9.svg" alt="(X_i, y_i)"/> avec
<img class="math" src="../_images/math/ffe8cf50d7747c488a36fbc58242a460fb6fe345.svg" alt="X_i \in \mathbb{R}^d"/> et <img class="math" src="../_images/math/201b1c795c6043eb115e648432b1cea22adb5771.svg" alt="y_i \in \mathbb{R}"/>. Les points sont
triés selon une dimension. On note <em>X</em> la matrice composée
des lignes <img class="math" src="../_images/math/315f302d548ef8fffc27408bb48c30f9595fff6e.svg" alt="X_1, ..., X_n"/> et le vecteur colonne
<img class="math" src="../_images/math/0ef44c1d0bb48b94dfdb8badde13b61795657545.svg" alt="y=(y_1, ..., y_n)"/>.
Il existe une matrice <img class="math" src="../_images/math/2f2a105c289b01c226ad6e0f506d96386c48d440.svg" alt="P"/> telle que <img class="math" src="../_images/math/0b14464e38fd8042d17db9235e3738066c5f6921.svg" alt="P'P = I"/>
et <img class="math" src="../_images/math/a13fee46f20b78fa6888da98ade61c1c7bead3f2.svg" alt="X'X = P'DP"/> avec <em>D</em> une matrice diagonale.
On note <img class="math" src="../_images/math/89ece0b87057d485bbef78d1f26d0fe7b2d96723.svg" alt="X_{a..b}"/> la matrice constituée des lignes
<em>a</em> à <em>b</em>. On calcule :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/bb12da92609d30869b483a4a788969ff4ecff98b.svg" alt="MSE(X, y, a, b) = \norm{Y - \sum_{k=1}^{C}(X_{a..b}P')_{[k]}
\frac{&lt;(X_{a..b}P')_{[k]},Y&gt;}{&lt;(X_{a..b}P')_{[k]},(X_{a..b}P')_{[k]}&gt;}}^2"/></p>
</div></div>
<p>Un noeud de l’arbre est construit en choisissant le point
de coupure qui minimise :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/f3ce1a0d6de34c6d855421624a09d4ef02093f4e.svg" alt="MSE(X, y, 1, t) + MSE(X, y, t+1, n)"/></p>
</div></div>
</div>
<p>Par la suite on verra que le fait que la matrice soit diagonale est l’élément
principal mais la matrice <em>P</em> ne doit pas nécessairement
vérifier <img class="math" src="../_images/math/2d52c454a507ddbc882d1d8d3b61d240dcb181be.svg" alt="P'P=I"/>.</p>
</section>
<section id="un-peu-plus-en-detail-dans-l-algorithme">
<h3>Un peu plus en détail dans l’algorithme<a class="headerlink" href="#un-peu-plus-en-detail-dans-l-algorithme" title="Lien vers cette rubrique">¶</a></h3>
<p>J’ai pensé à plein de choses pour aller plus loin car l’idée
est de quantifier à peu près combien on pert en précision en utilisant
des vecteurs propres estimés avec l’ensemble des données sur une partie
seulement. Je me suis demandé si les vecteurs propres d’une matrice
pouvait être construit à partir d’une fonction continue de la matrice
symétrique de départ. A peu près vrai mais je ne voyais pas une façon
de majorer cette continuité. Ensuite, je me suis dit que les vecteurs
propres de <img class="math" src="../_images/math/a4a91a4fe99964abc306490321f26a321eeac5c8.svg" alt="X'X"/> ne devaient pas être loin de ceux de <img class="math" src="../_images/math/ca56d8fe347379c939d037f6df5ddf47dfa913a3.svg" alt="X_\sigma'X_\sigma"/>
où <img class="math" src="../_images/math/4baabecff72a0674c9a5d1c8aee4cdecf96e884d.svg" alt="\sigma"/> est un sous-échantillon aléatoire de l’ensemble
de départ. Donc comme il faut juste avoir une base de vecteurs
orthogonaux, je suis passé à l”<a class="reference external" href="https://fr.wikipedia.org/wiki/Algorithme_de_Gram-Schmidt">orthonormalisation de Gram-Schmidt</a>.
Il n’a pas non plus ce défaut de permuter les dimensions ce qui rend
l’observation de la continuité a little bit more complicated comme
le max dans l”<a class="reference external" href="https://en.wikipedia.org/wiki/Jacobi_eigenvalue_algorithm">algorithme de Jacobi</a>.
L’idée est se servir cette orthonormalisation pour construire
la matrice <em>P</em> de l’algortihme.</p>
<p>La matrice <img class="math" src="../_images/math/9242cdfe946ac0542ce54d351c51b5955af25c3d.svg" alt="P \in \mathcal{M}_{CC}"/> est constituée de
<em>C</em> vecteurs ortonormaux <img class="math" src="../_images/math/b37a5f174519b7d784c3b0c7a235fa181b01d28d.svg" alt="(P_{[1]}, ..., P_{[C]})"/>.
Avec les notations que
j’ai utilisées jusqu’à présent :
<img class="math" src="../_images/math/a7117f1ead7754a60fba4fd8f97568bc60d6a3cf.svg" alt="X_{[k]} = (X_{1k}, ..., X_{nk})"/>.
On note la matrice identité <img class="math" src="../_images/math/0bfb769467e0030772bae588c3fca7ec8ac34a24.svg" alt="I_C=I"/>.</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/cbb272848eb51ed3365e135fa48895fce06f241d.svg" alt="\begin{array}{rcl}
T_{[1]} &amp;=&amp; \frac{ X_{[1]} }{ \norme{X_{[1]}} } \\
P_{[1]} &amp;=&amp; \frac{ I_{[1]} }{ \norme{X_{[1]}} } \\
T_{[2]} &amp;=&amp; \frac{ X_{[2]} - &lt;X_{[2]}, T_{[1]}&gt; T_{[1]} }
{ \norme{X_{[2]} - &lt;X_{[2]}, T_{[1]}&gt; T_{[1]}} } \\
P_{[2]} &amp;=&amp; \frac{ I_{[2]} - &lt;X_{[2]}, T_{[1]}&gt; T_{[1]} }
{ \norme{X_{[2]} - &lt;X_{[2]}, T_{[1]}&gt; T_{[1]}} } \\
... &amp;&amp; \\
T_{[k]} &amp;=&amp; \frac{ X_{[k]} - \sum_{i=1}^{k-1} &lt;X_{[k]}, T_{[i]}&gt; T_{[i]} }
{ \norme{ X_{[2]} - \sum_{i=1}^{k-1} &lt;X_{[k]}, T_{[i]}&gt; T_{[i]} } } \\
P_{[k]} &amp;=&amp; \frac{ I_{[k]} - \sum_{i=1}^{k-1} &lt;X_{[k]}, T_{[i]}&gt; T_{[i]} }
{ \norme{ X_{[2]} - \sum_{i=1}^{k-1} &lt;X_{[k]}, T_{[i]}&gt; T_{[i]} } } \\
\end{array}"/></p>
</div></div>
<p>La matrice <em>T</em> vérifie <img class="math" src="../_images/math/c7f5e6448064d8f0153830891e9612ff7ac09c51.svg" alt="T'T=I"/> puisque les vecteurs sont
construits de façon à être orthonormés. Et on vérifie que
<img class="math" src="../_images/math/5d9e9938f029419341ff821ac8d8224605a1a077.svg" alt="XP = T"/> et donc <img class="math" src="../_images/math/fc9f545c431271e8ad38177baa738f844c1455e9.svg" alt="PXX'P' = I"/>.
C’est implémenté par la fonction
<a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.gram_schmidt" title="mlstatpy.ml.matrices.gram_schmidt"><code class="xref py py-func docutils literal notranslate"><span class="pre">gram_schmidt</span></code></a>.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlstatpy.ml.matrices</span><span class="w"> </span><span class="kn">import</span> <span class="n">gram_schmidt</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">U</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="n">gram_schmidt</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">change</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">T</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">P</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">m</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="p">[[</span><span class="mf">1.000e+00</span> <span class="mf">1.155e-17</span><span class="p">]</span>
     <span class="p">[</span><span class="mf">1.155e-17</span> <span class="mf">1.000e+00</span><span class="p">]]</span>
</pre></div>
</div>
<p>Cela débouche sur une autre formulation du calcul
d’une régression linéaire à partir d’une orthornormalisation
de Gram-Schmidt qui est implémentée dans la fonction
<a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.linear_regression" title="mlstatpy.ml.matrices.linear_regression"><code class="xref py py-func docutils literal notranslate"><span class="pre">linear_regression</span></code></a>.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlstatpy.ml.matrices</span><span class="w"> </span><span class="kn">import</span> <span class="n">linear_regression</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">])</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">algo</span><span class="o">=</span><span class="s2">&quot;gram&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="p">[</span><span class="mf">1.008</span> <span class="mf">1.952</span><span class="p">]</span>
</pre></div>
</div>
<p>L’avantage est que cette formulation s’exprime
uniquement à partir de produits scalaires.
Voir le notebook svuiant <a class="reference internal" href="../notebooks/ml/regression_no_inversion.html"><span class="std std-ref">Régression sans inversion</span></a>.</p>
</section>
</section>
<section id="synthese-mathematique">
<span id="l-reglin-acp-svd"></span><h2>Synthèse mathématique<a class="headerlink" href="#synthese-mathematique" title="Lien vers cette rubrique">¶</a></h2>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme1">
<div class="docutils container">
</div>
<p class="admonition-title" id="algo-gram-schmidt">Algorithme A2 : Orthonormalisation de Gram-Schmidt</p>
<p>Soit une matrice <img class="math" src="../_images/math/51a179a97eb9f74e2584f4b3059feda71eca47c7.svg" alt="X \in \mathcal{M}_{nd}"/> avec
<img class="math" src="../_images/math/bb1448a7a46809eda5df92f8ec7bcbcb8c6f78fc.svg" alt="n \supegal d"/>. Il existe deux matrices telles que
<img class="math" src="../_images/math/7868d32e6034cc94d96c07fc23b92dacfe5b5d60.svg" alt="X P = T"/> ou <img class="math" src="../_images/math/ca96612eebcb1bb3f1700846450d0e78c005d279.svg" alt="P' X' = T'"/>.
<img class="math" src="../_images/math/d1ad1da4eb95950ce2563640b98a5d28411bcee5.svg" alt="P \in \mathcal{M}_{dd}"/> et <img class="math" src="../_images/math/6501ecb04936e603572aee6435fe2144a334604e.svg" alt="T \in \mathcal{M}_{nd}"/>.
La matrice <em>T</em> est triangulaire supérieure
et vérifie <img class="math" src="../_images/math/07fd2966f8938ac9b73cc9c6edb0f529bdbc4114.svg" alt="T'T = I_d"/> (<img class="math" src="../_images/math/2909f22a0101729b833311de248b52ab4ab59f14.svg" alt="I_d"/>
est la matrice identité). L’algorithme se décrit
comme suit :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/57674cf50bc692aa64f63c43dd6d5c07cd8f4e44.svg" alt="\begin{array}{ll}
\forall i \in &amp; range(1, d) \\
&amp; x_i = X_{[i]} - \sum_{j &lt; i} &lt;T_{[j]}, X_{[i]}&gt; T_{[j]} \\
&amp; p_i = P_{[i]} - \sum_{j &lt; i} &lt;T_{[j]}, X_{[i]}&gt; P_{[j]} \\
&amp; T_{[i]} = \frac{x_i}{\norme{x_i}} \\
&amp; P_{[i]} = \frac{p_i}{\norme{p_i}}
\end{array}"/></p>
</div></div>
</div>
<div class="admonition-mathdef admonition" id="indexmathe-Théorème0">
<div class="docutils container">
</div>
<p class="admonition-title" id="algo-gram-schmidt-reglin">Théorème T1 : Régression linéaire après Gram-Schmidt</p>
<p>Soit une matrice <img class="math" src="../_images/math/51a179a97eb9f74e2584f4b3059feda71eca47c7.svg" alt="X \in \mathcal{M}_{nd}"/> avec
<img class="math" src="../_images/math/bb1448a7a46809eda5df92f8ec7bcbcb8c6f78fc.svg" alt="n \supegal d"/>. Et un vecteur <img class="math" src="../_images/math/01b26d2f14bc53f24db5a7322d316f7257d97e3c.svg" alt="y \in \mathbb{R}^n"/>.
D’après l”<a class="reference internal" href="#algo-gram-schmidt"><span class="std std-ref">algorithme de Gram-Schmidt</span></a>,
il existe deux matrices telles que
<img class="math" src="../_images/math/7868d32e6034cc94d96c07fc23b92dacfe5b5d60.svg" alt="X P = T"/> ou <img class="math" src="../_images/math/ca96612eebcb1bb3f1700846450d0e78c005d279.svg" alt="P' X' = T'"/>.
<img class="math" src="../_images/math/d1ad1da4eb95950ce2563640b98a5d28411bcee5.svg" alt="P \in \mathcal{M}_{dd}"/> et <img class="math" src="../_images/math/6501ecb04936e603572aee6435fe2144a334604e.svg" alt="T \in \mathcal{M}_{nd}"/>.
La matrice <em>T</em> est triangulaire supérieure
et vérifie <img class="math" src="../_images/math/07fd2966f8938ac9b73cc9c6edb0f529bdbc4114.svg" alt="T'T = I_d"/> (<img class="math" src="../_images/math/2909f22a0101729b833311de248b52ab4ab59f14.svg" alt="I_d"/>
est la matrice identité). Alors
<img class="math" src="../_images/math/742fba2842222fef64ccdd4e85c643c07e3c870d.svg" alt="\beta = T' y P' = P' X' y P' = (X'X)^{-1}X'y"/>.
<img class="math" src="../_images/math/bd898b02c226d1ce791865c51d30012165adf53d.svg" alt="\beta"/> est la solution du problème d’optimisation
<img class="math" src="../_images/math/136e12f20fd35ea5e2598ea2bc2a229b62135892.svg" alt="\min_\beta \norme{y - X\beta}^2"/>.</p>
</div>
<p>La démonstration est géométrique et reprend l’idée
du paragraphe précédent. La solution de la régression
peut être vu comme la projection du vecteur <em>y</em>
sur l’espace vectoriel engendré par les vecteurs
<img class="math" src="../_images/math/e461db174618ec31492afbc505c1cc235728eca1.svg" alt="X_{[1]}, ..., X_{[d]}"/>.
Par construction, cet espace est le même que celui
engendré par <img class="math" src="../_images/math/b5cb7540f097d620e2940127a83aae1e3bc99c9f.svg" alt="T_{[1]}, ..., T_{[d]}"/>. Dans cette base,
la projection de <em>y</em> a pour coordoonées
<img class="math" src="../_images/math/86f3952d5a84f9c9a7493e0992dd82f29600f1b5.svg" alt="&lt;y, T_{[1]}&gt;, ..., &lt;y, T_{[d]}&gt; = T' y"/>.
On en déduit que la projection de <em>y</em> s’exprimer comme :</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/903bf75fcb235e47ea633f639410ff18c34fbc16.svg" alt="\hat{y} = \sum_{k=1}^d &lt;y, T_{[k]}&gt; T_{[k]}"/></p>
</div></div>
<p>Il ne reste plus qu’à expremier cette projection
dans la base initial <em>X</em>. On sait que
<img class="math" src="../_images/math/129b55bbeb248ab2218e50ef1135af3b728573e8.svg" alt="T_{[k]} = X P_{[k]}"/>. On en déduit que ;</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/b7203395e6eeecf528550b799f5bc276d6d06407.svg" alt="\begin{array}{rcl}
\hat{y} &amp;=&amp; \sum_{k=1}^d &lt;y, T_{[k]}&gt; X P_{[k]} \\
&amp;=&amp; \sum_{k=1}^d &lt;y, T_{[k]}&gt; \sum_{l=1}^d X_{[l]} P_{lk} \\
&amp;=&amp; \sum_{l=1}^d X_{[l]} \sum_{k=1}^d &lt;y, T_{[k]}&gt;  P_{lk} \\
&amp;=&amp; \sum_{l=1}^d X_{[l]} (T' y P_l) \\
&amp;=&amp; \sum_{l=1}^d X_{[l]} \beta_l
\end{array}"/></p>
</div></div>
<p>D’où <img class="math" src="../_images/math/4ee74e6ef441473103081eb9d48b7531b44b6a59.svg" alt="\beta = T' y P'"/>.
L’implémentation suit :</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="n">Xt</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">Tt</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">Xt</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">Pt</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">Tt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">Xt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Tt</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Xt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">Tt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">Tt</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">d</span>
        <span class="n">Pt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">Pt</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">d</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Tt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Tt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">**=</span> <span class="mf">0.5</span>
        <span class="n">Tt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/=</span> <span class="n">d</span>
        <span class="n">Pt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/=</span> <span class="n">d</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Tt</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X P&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">Pt</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;T T&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Tt</span> <span class="o">@</span> <span class="n">Tt</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.19</span><span class="p">,</span> <span class="mf">0.29</span><span class="p">])</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Xt</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">Xt</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">beta2</span> <span class="o">=</span> <span class="n">Tt</span> <span class="o">@</span> <span class="n">y</span> <span class="o">@</span> <span class="n">Pt</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;beta1&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;beta2&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">X</span>
    <span class="p">[[</span><span class="mf">1.</span> <span class="mf">5.</span> <span class="mf">5.</span><span class="p">]</span>
     <span class="p">[</span><span class="mf">2.</span> <span class="mf">6.</span> <span class="mf">6.</span><span class="p">]</span>
     <span class="p">[</span><span class="mf">3.</span> <span class="mf">6.</span> <span class="mf">7.</span><span class="p">]</span>
     <span class="p">[</span><span class="mf">4.</span> <span class="mf">6.</span> <span class="mf">8.</span><span class="p">]]</span>
    <span class="n">T</span>
    <span class="p">[[</span> <span class="mf">0.183</span>  <span class="mf">0.736</span>  <span class="mf">0.651</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.365</span>  <span class="mf">0.502</span> <span class="o">-</span><span class="mf">0.67</span> <span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.548</span>  <span class="mf">0.024</span> <span class="o">-</span><span class="mf">0.181</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.73</span>  <span class="o">-</span><span class="mf">0.453</span>  <span class="mf">0.308</span><span class="p">]]</span>
    <span class="n">X</span> <span class="n">P</span>
    <span class="p">[[</span> <span class="mf">0.183</span>  <span class="mf">0.736</span>  <span class="mf">0.651</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.365</span>  <span class="mf">0.502</span> <span class="o">-</span><span class="mf">0.67</span> <span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.548</span>  <span class="mf">0.024</span> <span class="o">-</span><span class="mf">0.181</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">0.73</span>  <span class="o">-</span><span class="mf">0.453</span>  <span class="mf">0.308</span><span class="p">]]</span>
    <span class="n">T</span> <span class="n">T</span><span class="s1">&#39;</span>
    <span class="p">[[</span> <span class="mf">1.000e+00</span>  <span class="mf">4.014e-16</span>  <span class="mf">9.293e-16</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">4.014e-16</span>  <span class="mf">1.000e+00</span> <span class="o">-</span><span class="mf">1.115e-14</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">9.293e-16</span> <span class="o">-</span><span class="mf">1.115e-14</span>  <span class="mf">1.000e+00</span><span class="p">]]</span>
    <span class="n">beta1</span>
    <span class="p">[</span> <span class="mf">0.077</span>  <span class="mf">0.037</span> <span class="o">-</span><span class="mf">0.032</span><span class="p">]</span>
    <span class="n">beta2</span>
    <span class="p">[</span> <span class="mf">0.077</span>  <span class="mf">0.037</span> <span class="o">-</span><span class="mf">0.032</span><span class="p">]</span>
</pre></div>
</div>
<p>La librairie implémente ces deux algorithmes de manière un peu
plus efficace dans les fonctions
<a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.gram_schmidt" title="mlstatpy.ml.matrices.gram_schmidt"><code class="xref py py-func docutils literal notranslate"><span class="pre">gram_schmidt</span></code></a> et
<a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.linear_regression" title="mlstatpy.ml.matrices.linear_regression"><code class="xref py py-func docutils literal notranslate"><span class="pre">linear_regression</span></code></a>.</p>
</section>
<section id="streaming">
<h2>Streaming<a class="headerlink" href="#streaming" title="Lien vers cette rubrique">¶</a></h2>
<section id="streaming-gram-schmidt">
<span id="l-stream-gram-schmidt"></span><h3>Streaming Gram-Schmidt<a class="headerlink" href="#streaming-gram-schmidt" title="Lien vers cette rubrique">¶</a></h3>
<p>Je ne sais pas vraiment comment le dire en français,
peut-être <em>régression linéaire mouvante</em>. Même Google ou Bing
garde le mot <em>streaming</em> dans leur traduction…
C’est néanmoins l’idée qu’il faut
réussir à mettre en place d’une façon ou d’une autre car pour
choisir le bon point de coupure pour un arbre de décision.
On note <img class="math" src="../_images/math/3311481402b873ece48de71c465c2ffa1a3488a8.svg" alt="X_{1..k}"/> la matrice composée
des lignes <img class="math" src="../_images/math/e5f4f326071976c6166439f2963fc322d320efc5.svg" alt="X_1, ..., X_k"/> et le vecteur colonne
<img class="math" src="../_images/math/b5caf110fa62de408d6f9f17a11a30cd24551e1e.svg" alt="y_{1..k}=(y_1, ..., y_k)"/>.
L’apprentissage de l’arbre de décision
faut calculer des régressions pour les problèmes
<img class="math" src="../_images/math/1dc82f50711a3641358ae06fa260fb6f83324035.svg" alt="(X_{1..k}, y_{1..k}), (X_{1..k+1}, y_{1..k+1})..."/>.
L’idée que je propose n’est pas parfaite mais elle fonctionne
pour l’idée de l’algorithme avec <a class="reference internal" href="#algo-decision-tree-mselin"><span class="std std-ref">Gram-Schmidt</span></a>.</p>
<p>Tout d’abord, il faut imaginer un algorithme
de Gram-Schmidt version streaming. Pour la matrice
<img class="math" src="../_images/math/d59bad1b5327b4fc60461b5ce1eb810e61fcabd6.svg" alt="X'_{1..k}"/>, celui-ci produit deux matrices
<img class="math" src="../_images/math/6f1d48046b5dc75e12203c75f5b869a609405008.svg" alt="T_{1..k}"/> et <img class="math" src="../_images/math/5305da6ba6785bd9b4109be420760b1b280f6af0.svg" alt="P_{1..k}"/> telles que :
<img class="math" src="../_images/math/bda6f102f92db77dae8fad28b059dace61d10cb4.svg" alt="X'_{1..k}P_{1..k}=T_{1..k}"/>. On note <em>d</em> la dimension
des observations. Comment faire pour ajouter une observation
<img class="math" src="../_images/math/711bcae2ea49f7b1c2f8ae7822236b98c212b57f.svg" alt="(X_{k+1}, y_{k+1})"/> ? L’idée d’un algorithme au format streaming
est que le coût de la mise à jour pour l’itération <em>k+1</em>
ne dépend pas de <em>k</em>.</p>
<p>On suppose donc que <img class="math" src="../_images/math/7c287f91ba9cdaaa19d4084a13a5d332b5c9943a.svg" alt="(T_k, P_k)"/> sont les deux matrices
retournées par l’algorithme de <a class="reference internal" href="#algo-gram-schmidt"><span class="std std-ref">Gram-Schmidt</span></a>.
On construit la matrice <img class="math" src="../_images/math/e6005751a5251521bfae67216a4016543bda2ae3.svg" alt="V_{k+1} = [ T_k, X_{k+1} P_k ]"/> :
on ajoute une ligne à la matrice <img class="math" src="../_images/math/f0ac41956a8931819e3dafb195b81310d30e3514.svg" alt="T_k"/>. On applique
une itération de algorithme de <a class="reference internal" href="#algo-gram-schmidt"><span class="std std-ref">Gram-Schmidt</span></a>
pour obtenir <img class="math" src="../_images/math/8b43d88863a0da334d82cd92bbb73d6f24faa18c.svg" alt="(T_{k+1}, P)"/>. On en déduit que
<img class="math" src="../_images/math/6ab8c6c8c3030480c453fc1fe45940e340a122b5.svg" alt="(T_{k+1}, P_{k+1}) = (T_{k+1}, P_k P)"/>. L’expression
de la régression ne change pas mais il reste à l’expression
de telle sorte que les expressions ne dépendent pas de <em>k</em>.
Comme <img class="math" src="../_images/math/0aab38f7097e24c35662b59a1e17eb2f874a6e1b.svg" alt="T_k = X_{[1..k]} P_k"/>, la seule matrice qui nous intéresse
véritablement est <img class="math" src="../_images/math/418d904082dad412ef14ebd94cfc5080e4cb35d5.svg" alt="P_k"/>.</p>
<p>Maintenant, on considère la matrice <img class="math" src="../_images/math/ee5f38a558413718beb275f82f20b2f50a53a439.svg" alt="T_{[1..k]}"/> qui vérifie
<img class="math" src="../_images/math/88485808930fdc604a3e282dfa471a642064fd8d.svg" alt="T_k'T_k = I_d"/> et on ajoute une ligne
<img class="math" src="../_images/math/4ab181f175b02f8a1b2c16da052be49820eee940.svg" alt="X_{k+1} P_k"/> pour former
<img class="math" src="../_images/math/e878f34cfce5c03e871ecd49ba3db8d8cf01d64b.svg" alt="[ [T_k] [X_{k+1} P_k] ] = [ [X_{[1..k]} P_k] [X_{k+1} P_k] ]"/>.
La fonction <a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.streaming_gram_schmidt_update" title="mlstatpy.ml.matrices.streaming_gram_schmidt_update"><code class="xref py py-func docutils literal notranslate"><span class="pre">streaming_gram_schmidt_update</span></code></a>
implémente la mise à jour. Le coût de la fonction est en
<img class="math" src="../_images/math/4a0bcedee2e85875719e472142fc91c7ca8511fd.svg" alt="O(d^2)"/>.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlstatpy.ml.matrices</span><span class="w"> </span><span class="kn">import</span> <span class="n">streaming_gram_schmidt_update</span><span class="p">,</span> <span class="n">gram_schmidt</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span>
    <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">Xt</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>

<span class="n">Tk</span><span class="p">,</span> <span class="n">Pk</span> <span class="o">=</span> <span class="n">gram_schmidt</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">change</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;k=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Pk</span><span class="p">)</span>
<span class="n">Tk</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">Pk</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Tk</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Tk</span><span class="p">)</span>

<span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">while</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">streaming_gram_schmidt_update</span><span class="p">(</span><span class="n">Xt</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span> <span class="n">Pk</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;k=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">Pk</span><span class="p">)</span>
    <span class="n">Tk</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="n">Pk</span><span class="o">.</span><span class="n">T</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">Tk</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Tk</span><span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">k</span><span class="o">=</span><span class="mi">3</span>
    <span class="p">[[</span> <span class="mf">0.099</span>  <span class="mf">0.</span>     <span class="mf">0.</span>   <span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.953</span>  <span class="mf">0.482</span>  <span class="mf">0.</span>   <span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.287</span> <span class="o">-</span><span class="mf">3.338</span>  <span class="mf">3.481</span><span class="p">]]</span>
    <span class="p">[[</span> <span class="mf">1.000e+00</span> <span class="o">-</span><span class="mf">1.310e-15</span> <span class="o">-</span><span class="mf">2.238e-15</span><span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">1.310e-15</span>  <span class="mf">1.000e+00</span>  <span class="mf">1.390e-14</span><span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">2.238e-15</span>  <span class="mf">1.390e-14</span>  <span class="mf">1.000e+00</span><span class="p">]]</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">4</span>
    <span class="p">[[</span> <span class="mf">0.089</span>  <span class="mf">0.</span>     <span class="mf">0.</span>   <span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.308</span>  <span class="mf">0.177</span>  <span class="mf">0.</span>   <span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.03</span>  <span class="o">-</span><span class="mf">3.334</span>  <span class="mf">3.348</span><span class="p">]]</span>
    <span class="p">[[</span> <span class="mf">1.000e+00</span> <span class="o">-</span><span class="mf">3.570e-16</span> <span class="o">-</span><span class="mf">1.808e-15</span><span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">3.570e-16</span>  <span class="mf">1.000e+00</span>  <span class="mf">2.423e-15</span><span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">1.808e-15</span>  <span class="mf">2.423e-15</span>  <span class="mf">1.000e+00</span><span class="p">]]</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">[[</span> <span class="mf">0.088</span>  <span class="mf">0.</span>     <span class="mf">0.</span>   <span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.212</span>  <span class="mf">0.128</span>  <span class="mf">0.</span>   <span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.016</span> <span class="o">-</span><span class="mf">3.335</span>  <span class="mf">3.342</span><span class="p">]]</span>
    <span class="p">[[</span> <span class="mf">1.000e+00</span>  <span class="mf">1.756e-17</span> <span class="o">-</span><span class="mf">4.660e-15</span><span class="p">]</span>
     <span class="p">[</span> <span class="mf">1.756e-17</span>  <span class="mf">1.000e+00</span>  <span class="mf">9.833e-16</span><span class="p">]</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">4.660e-15</span>  <span class="mf">9.833e-16</span>  <span class="mf">1.000e+00</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="streaming-linear-regression">
<span id="l-piecewise-linear-regression"></span><h3>Streaming Linear Regression<a class="headerlink" href="#streaming-linear-regression" title="Lien vers cette rubrique">¶</a></h3>
<p>Je reprends l’idée introduite dans l’article
<a class="reference external" href="https://stats.stackexchange.com/questions/6920/efficient-online-linear-regression">Efficient online linear regression</a>.
On cherche à minimiser <img class="math" src="../_images/math/ed9315b3df375af94e88425001e4b7f0dbfb0a17.svg" alt="L(\beta)=\norme{y - X\beta}^2"/> et le vecteur
solution annuler le gradient : <img class="math" src="../_images/math/ef14f6ce55446332314a46682b416079ffc0fa78.svg" alt="\nabla(\beta) = -2X'(y - X\beta) = 0"/>.
On note le vecteur <img class="math" src="../_images/math/4931209ead3ea6429a213a578ce800ac1f1b91e8.svg" alt="\beta_k"/> qui vérifie
<img class="math" src="../_images/math/b9917cf17bcfe3f9152b16d93089b4b51957327c.svg" alt="\nabla(\beta_k) = -2X_{1..k}'(y_{1..k} - X_{1..k}\beta_k) = 0"/>.
Qu’en est-il de <img class="math" src="../_images/math/b6f069785b4558ed872bddb4d74d250f607b696f.svg" alt="\beta_{k+1}"/> ?
On note <img class="math" src="../_images/math/563c7befdec3f23e25f856deb913f24e4191b090.svg" alt="\beta_{k+1} = \beta_k + d\beta"/>.</p>
<div class="math-wrapper docutils container">
<div class="math">
<p><img src="../_images/math/e09f9839874543ddd91206a2a8ebcf6833eebce8.svg" alt="\begin{array}{rcl}
\nabla(\beta_{k+1}) &amp;=&amp; -2X_{1..k+1}'(y_{1..k+1} - X_{1..k+1}(\beta_k + d\beta)) \\
&amp;=&amp; -2 [ X_{1..k}' X_{k+1}' ] ( [ y_{1..k} y_{k+1} ] - [ X_{1..k} X_{k+1} ]'(\beta_k + d\beta)) \\
&amp;=&amp; -2 X_{1..k}' ( y_{1..k} - X_{1..k} (\beta_k + d\beta))
-2 X_{k+1}' ( y_{k+1} - X_{k+1} (\beta_k + d\beta)) \\
&amp;=&amp; 2 X_{1..k}' X_{1..k} d\beta -2 X_{k+1}' ( y_{k+1} - X_{k+1} (\beta_k + d\beta)) \\
&amp;=&amp; 2 (X_{1..k}' X_{1..k} + X_{k+1}' X_{k+1}) d\beta - 2 X_{k+1}' (y_{k+1} - X_{k+1} \beta_k)
\end{array}"/></p>
</div></div>
<p>On en déduit la valeur <img class="math" src="../_images/math/3673c91de80304f16e82d1b7f05ead48f514c6fd.svg" alt="d\beta"/> qui annule le gradient.
On peut décliner cette formule en version streaming.
C’est ce qu’implémente la fonction
<a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.streaming_linear_regression_update" title="mlstatpy.ml.matrices.streaming_linear_regression_update"><code class="xref py py-func docutils literal notranslate"><span class="pre">streaming_linear_regression_update</span></code></a>.
Le coût de l’algorithme est en <img class="math" src="../_images/math/49c0b1afd7854cfe11d312bb61d956fbe50e3271.svg" alt="O(d^3)"/>.
L’inconvénient de cet algorithme est qu’il requiert des
matrices inversibles. C’est souvent le cas et la probabilité
que cela ne le soit pas décroît avec <em>k</em>. C’est un petit
inconvénient compte tenu de la simplicité de l’implémentation.
On vérifie que tout fonction bien sur un exemple.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>


<span class="k">def</span><span class="w"> </span><span class="nf">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inv</span> <span class="o">@</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">streaming_linear_regression_update</span><span class="p">(</span><span class="n">Xk</span><span class="p">,</span> <span class="n">yk</span><span class="p">,</span> <span class="n">XkXk</span><span class="p">,</span> <span class="n">bk</span><span class="p">):</span>
    <span class="n">Xk</span> <span class="o">=</span> <span class="n">Xk</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">XkXk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">xxk</span> <span class="o">=</span> <span class="n">Xk</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Xk</span>
    <span class="n">XkXk</span> <span class="o">+=</span> <span class="n">xxk</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">Xk</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">yk</span> <span class="o">-</span> <span class="n">Xk</span> <span class="o">@</span> <span class="n">bk</span><span class="p">)</span>
    <span class="n">bk</span><span class="p">[:]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XkXk</span><span class="p">)</span> <span class="o">@</span> <span class="n">err</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">streaming_linear_regression</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">start</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">Xk</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[:</span><span class="n">start</span><span class="p">]</span>
    <span class="n">XkXk</span> <span class="o">=</span> <span class="n">Xk</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Xk</span>
    <span class="n">bk</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XkXk</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">Xk</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">[:</span><span class="n">start</span><span class="p">])</span>
    <span class="k">yield</span> <span class="n">bk</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">start</span>
    <span class="k">while</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">streaming_linear_regression_update</span><span class="p">(</span><span class="n">mat</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span> <span class="p">:</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">XkXk</span><span class="p">,</span> <span class="n">bk</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">bk</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]],</span>
    <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">streaming_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
    <span class="n">bk0</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">3</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">3</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">bk</span><span class="p">,</span> <span class="n">bk0</span><span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">iteration</span> <span class="mi">0</span> <span class="p">[</span> <span class="mf">1.</span>     <span class="mf">0.667</span> <span class="o">-</span><span class="mf">0.667</span><span class="p">]</span> <span class="p">[</span> <span class="mf">1.</span>     <span class="mf">0.667</span> <span class="o">-</span><span class="mf">0.667</span><span class="p">]</span>
    <span class="n">iteration</span> <span class="mi">1</span> <span class="p">[</span> <span class="mf">1.03</span>   <span class="mf">0.682</span> <span class="o">-</span><span class="mf">0.697</span><span class="p">]</span> <span class="p">[</span> <span class="mf">1.03</span>   <span class="mf">0.682</span> <span class="o">-</span><span class="mf">0.697</span><span class="p">]</span>
    <span class="n">iteration</span> <span class="mi">2</span> <span class="p">[</span> <span class="mf">1.036</span>  <span class="mf">0.857</span> <span class="o">-</span><span class="mf">0.875</span><span class="p">]</span> <span class="p">[</span> <span class="mf">1.036</span>  <span class="mf">0.857</span> <span class="o">-</span><span class="mf">0.875</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="streaming-linear-regression-version-gram-schmidt">
<span id="l-piecewise-linear-regression-gram-schmidt"></span><h3>Streaming Linear Regression version Gram-Schmidt<a class="headerlink" href="#streaming-linear-regression-version-gram-schmidt" title="Lien vers cette rubrique">¶</a></h3>
<p>L’algorithme reprend le théorème
<a class="reference internal" href="#algo-gram-schmidt-reglin"><span class="std std-ref">Régression linéaire après Gram-Schmidt</span></a>
et l’algorithme <a class="reference internal" href="#l-stream-gram-schmidt"><span class="std std-ref">Streaming Gram-Schmidt</span></a>. Tout tient dans cette formule :
<img class="math" src="../_images/math/5b91bdc1b0b517c811d56665be93181c7ba31dd0.svg" alt="\beta_k = P_k' X_{1..k}' y_{1..k} P_k'"/> qu’on écrit différemment
en considérent l’associativité de la multiplication des matrices :
<img class="math" src="../_images/math/d69cfa13036676883e5272c2b90578fdd78e14e5.svg" alt="\beta_k = P_k' (X_{1..k}' y_{1..k}) P_k'"/>. La matrice centrale
a pour dimension <em>d</em>. L’exemple suivant implémente cette idée.
Il s’appuie sur les fonctions <a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.streaming_gram_schmidt_update" title="mlstatpy.ml.matrices.streaming_gram_schmidt_update"><code class="xref py py-func docutils literal notranslate"><span class="pre">streaming_gram_schmidt_update</span></code></a> et
<a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.gram_schmidt" title="mlstatpy.ml.matrices.gram_schmidt"><code class="xref py py-func docutils literal notranslate"><span class="pre">gram_schmidt</span></code></a>.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlstatpy.ml.matrices</span><span class="w"> </span><span class="kn">import</span> <span class="n">gram_schmidt</span><span class="p">,</span> <span class="n">streaming_gram_schmidt_update</span>


<span class="k">def</span><span class="w"> </span><span class="nf">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inv</span> <span class="o">@</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">streaming_linear_regression_gram_schmidt_update</span><span class="p">(</span><span class="n">Xk</span><span class="p">,</span> <span class="n">yk</span><span class="p">,</span> <span class="n">Xkyk</span><span class="p">,</span> <span class="n">Pk</span><span class="p">,</span> <span class="n">bk</span><span class="p">):</span>
    <span class="n">Xk</span> <span class="o">=</span> <span class="n">Xk</span><span class="o">.</span><span class="n">T</span>
    <span class="n">streaming_gram_schmidt_update</span><span class="p">(</span><span class="n">Xk</span><span class="p">,</span> <span class="n">Pk</span><span class="p">)</span>
    <span class="n">Xkyk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">Xk</span> <span class="o">*</span> <span class="n">yk</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xkyk</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">bk</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">Pk</span> <span class="o">@</span> <span class="n">Xkyk</span> <span class="o">@</span> <span class="n">Pk</span>


<span class="k">def</span><span class="w"> </span><span class="nf">streaming_linear_regression_gram_schmidt</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">start</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">Xk</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[:</span><span class="n">start</span><span class="p">]</span>
    <span class="n">xyk</span> <span class="o">=</span> <span class="n">Xk</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">[:</span><span class="n">start</span><span class="p">]</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">Pk</span> <span class="o">=</span> <span class="n">gram_schmidt</span><span class="p">(</span><span class="n">Xk</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">change</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bk</span> <span class="o">=</span> <span class="n">Pk</span> <span class="o">@</span> <span class="n">xyk</span> <span class="o">@</span> <span class="n">Pk</span>
    <span class="k">yield</span> <span class="n">bk</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">start</span>
    <span class="k">while</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">streaming_linear_regression_gram_schmidt_update</span><span class="p">(</span><span class="n">mat</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">xyk</span><span class="p">,</span> <span class="n">Pk</span><span class="p">,</span> <span class="n">bk</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">bk</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]],</span>
    <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">streaming_linear_regression_gram_schmidt</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
    <span class="n">bk0</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">3</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">3</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">bk</span><span class="p">,</span> <span class="n">bk0</span><span class="p">)</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">iteration</span> <span class="mi">0</span> <span class="p">[</span> <span class="mf">1.</span>     <span class="mf">0.667</span> <span class="o">-</span><span class="mf">0.667</span><span class="p">]</span> <span class="p">[</span> <span class="mf">1.</span>     <span class="mf">0.667</span> <span class="o">-</span><span class="mf">0.667</span><span class="p">]</span>
    <span class="n">iteration</span> <span class="mi">1</span> <span class="p">[</span> <span class="mf">1.03</span>   <span class="mf">0.682</span> <span class="o">-</span><span class="mf">0.697</span><span class="p">]</span> <span class="p">[</span> <span class="mf">1.03</span>   <span class="mf">0.682</span> <span class="o">-</span><span class="mf">0.697</span><span class="p">]</span>
    <span class="n">iteration</span> <span class="mi">2</span> <span class="p">[</span> <span class="mf">1.036</span>  <span class="mf">0.857</span> <span class="o">-</span><span class="mf">0.875</span><span class="p">]</span> <span class="p">[</span> <span class="mf">1.036</span>  <span class="mf">0.857</span> <span class="o">-</span><span class="mf">0.875</span><span class="p">]</span>
</pre></div>
</div>
<p>Ces deux fonctions sont implémentées dans le module par
<a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.streaming_linear_regression_gram_schmidt_update" title="mlstatpy.ml.matrices.streaming_linear_regression_gram_schmidt_update"><code class="xref py py-func docutils literal notranslate"><span class="pre">streaming_linear_regression_gram_schmidt_update</span></code></a>
et <a class="reference internal" href="../api/ml.html#mlstatpy.ml.matrices.streaming_linear_regression_gram_schmidt" title="mlstatpy.ml.matrices.streaming_linear_regression_gram_schmidt"><code class="xref py py-func docutils literal notranslate"><span class="pre">streaming_linear_regression_gram_schmidt</span></code></a>.
Le coût de l’algorithme est en <img class="math" src="../_images/math/49c0b1afd7854cfe11d312bb61d956fbe50e3271.svg" alt="O(d^3)"/> mais n’inclut pas
d’inversion de matrices.</p>
</section>
</section>
<section id="digressions">
<h2>Digressions<a class="headerlink" href="#digressions" title="Lien vers cette rubrique">¶</a></h2>
<p>L’article <a class="reference external" href="http://jmlr.org/papers/volume20/18-460/18-460.pdf">An Efficient Two Step Algorithm for High DimensionalChange Point Regression Models Without Grid Search</a> propose un cadre
théorique pour déterminer une frontière dans un nuage de données
qui délimite un changement de modèle linéaire.
Le suivant étudie des changements de paramètres
<a class="reference external" href="http://jmlr.org/papers/volume20/17-352/17-352.pdf">Change Surfaces for Expressive MultidimensionalChangepoints and Counterfactual Prediction</a> d’une façon
plus générique.</p>
</section>
<section id="notebooks">
<h2>Notebooks<a class="headerlink" href="#notebooks" title="Lien vers cette rubrique">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/ml/regression_no_inversion.html">Régression sans inversion</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/ml/regression_no_inversion.html#Streaming-versions">Streaming versions</a></li>
</ul>
</li>
</ul>
</div>
<p>Voir aussi <a class="reference internal" href="#cai2020" id="id2"><span>[Cai2020]</span></a>, <a class="reference internal" href="#nie2016" id="id3"><span>[Nie2016]</span></a>, <a class="reference internal" href="#preda2010" id="id4"><span>[Preda2010]</span></a>.</p>
</section>
<section id="implementations">
<h2>Implémentations<a class="headerlink" href="#implementations" title="Lien vers cette rubrique">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://sdpython.github.io/doc/mlinsights/dev/api/mlmodel_tree.html#piecewisetreeregressor">PiecewiseTreeRegressor</a></p></li>
</ul>
</section>
<section id="bilbiographie">
<h2>Bilbiographie<a class="headerlink" href="#bilbiographie" title="Lien vers cette rubrique">¶</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="acharya2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Acharya2016</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/1607.03990">Fast Algorithms for Segmented Regression</a>,
Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt, <a class="reference external" href="https://icml.cc/2016/index.html">ICML 2016</a></p>
</div>
<div class="citation" id="cai2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Cai2020</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://www.jmlr.org/papers/volume21/18-567/18-567.pdf">Online Sufficient Dimension Reduction Through Sliced Inverse Regression</a>,
Zhanrui Cai, Runze Li, Liping Zhu</p>
</div>
<div class="citation" id="nie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Nie2016</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://jmlr.org/papers/volume17/15-320/15-320.pdf">Online PCA with Optimal Regret</a>,
Jiazhong Nie, Wojciech Kotlowski, Manfred K. Warmuth</p>
</div>
<div class="citation" id="preda2010" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Preda2010</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://hal.science/hal-01125940">The NIPALS Algorithm for Missing Functional Data</a>,
Cristian Preda, Gilbert Saporta, Mohamed Hadj Mbarek,
Revue roumaine de mathématiques pures et appliquées 2010, 55 (4), pp.315-326.</p>
</div>
</div>
<p>Voir aussi <a class="reference external" href="https://cran.r-project.org/web/packages/nipals/vignettes/nipals_algorithm.html">The NIPALS algorithm</a>.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../notebooks/ml/piecewise_linear_regression.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Régression linéaire par morceaux</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../notebooks/dsgarden/quantile_regression_example.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Régression quantile illustrée</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2016-2025, Xavier Dupré
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Régression linéaire par morceaux</a><ul>
<li><a class="reference internal" href="#exploration">Exploration</a><ul>
<li><a class="reference internal" href="#probleme-et-regression-lineaire-dans-un-espace-a-une-dimension">Problème et regréssion linéaire dans un espace à une dimension</a></li>
<li><a class="reference internal" href="#implementation-naive-d-une-regression-lineaire-par-morceaux">Implémentation naïve d’une régression linéaire par morceaux</a></li>
<li><a class="reference internal" href="#aparte-sur-la-continuite-de-la-regression-lineaire-par-morceaux">Aparté sur la continuité de la régression linéaire par morceaux</a></li>
<li><a class="reference internal" href="#regression-lineaire-et-correlation">Régression linéaire et corrélation</a></li>
<li><a class="reference internal" href="#idee-de-l-algorithme">Idée de l’algorithme</a></li>
<li><a class="reference internal" href="#un-peu-plus-en-detail-dans-l-algorithme">Un peu plus en détail dans l’algorithme</a></li>
</ul>
</li>
<li><a class="reference internal" href="#synthese-mathematique">Synthèse mathématique</a></li>
<li><a class="reference internal" href="#streaming">Streaming</a><ul>
<li><a class="reference internal" href="#streaming-gram-schmidt">Streaming Gram-Schmidt</a></li>
<li><a class="reference internal" href="#streaming-linear-regression">Streaming Linear Regression</a></li>
<li><a class="reference internal" href="#streaming-linear-regression-version-gram-schmidt">Streaming Linear Regression version Gram-Schmidt</a></li>
</ul>
</li>
<li><a class="reference internal" href="#digressions">Digressions</a></li>
<li><a class="reference internal" href="#notebooks">Notebooks</a></li>
<li><a class="reference internal" href="#implementations">Implémentations</a></li>
<li><a class="reference internal" href="#bilbiographie">Bilbiographie</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=0886690b"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/translations.js?v=e6b791cb"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    </body>
</html>