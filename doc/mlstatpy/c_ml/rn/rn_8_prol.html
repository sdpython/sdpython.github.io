
<!DOCTYPE html>


<html lang="fr" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Prolongements &#8212; Documentation mlstatpy 0.4.0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/translations.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'c_ml/rn/rn_8_prol';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Recherche" href="../../search.html" />
    <link rel="next" title="Analyse en composantes principales (ACP) et Auto Encoders" href="rn_9_auto.html" />
    <link rel="prev" title="Classification" href="rn_7_clas2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/project_ico.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/project_ico.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_clus/index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../license.html">
                        License
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Navigation du site">
    Navigation du site
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_clus/index.html">
                        Clustering
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../index.html">
                        Non linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_lin.html">
                        Régression linéaire
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../index_reg_log.html">
                        Régression logistique
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_nlp/index.html">
                        NLP
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_metric/index.html">
                        Métriques
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_algo/index.html">
                        Algorithmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../c_garden/index.html">
                        Pérégrinations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/index.html">
                        API
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../i_ex.html">
                        Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../defthe_index.html">
                        Listes des définitions et théorèmes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../auto_examples/index.html">
                        Gallery of examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../notebooks/index.html">
                        Galleries de notebooks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../glossary.html">
                        Glossary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../CHANGELOGS.html">
                        Change Logs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../license.html">
                        License
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Navigation de la section">
  <p class="bd-links__title" role="heading" aria-level="1">Navigation de la section</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="rn.html">Réseaux de neurones</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="rn_1_def.html">Définition des réseaux de neurones multi-couches</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_2_reg.html">La régression</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_3_clas.html">La classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_4_densite.html">Démonstration du théorème de la densité des réseaux de neurones</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_5_newton.html">Descente de gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_6_apprentissage.html">Apprentissage d’un réseau de neurones</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_7_clas2.html">Classification</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Prolongements</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_9_auto.html">Analyse en composantes principales (ACP) et Auto Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="rn_biblio.html">Bibliographie</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../kppv.html">Classification à l’aide des plus proches voisins</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../missing_values_mf.html">Liens entre factorisation de matrices, ACP, k-means</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/ml/mf_acp.html">Factorisation et matrice et ACP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/ml/valeurs_manquantes_mf.html">Valeurs manquantes et factorisation de matrices</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/ml/neural_tree.html">Un arbre de décision en réseaux de neurones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/ml/neural_tree_onnx.html">NeuralTreeNet et ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/ml/neural_tree_cost.html">NeuralTreeNet et coût</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Fils d'Ariane">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Fil d'Ariane">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Acceuil">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Non linéaire</a></li>
    
    
    <li class="breadcrumb-item"><a href="rn.html" class="nav-link">Réseaux de neurones</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Prolongements</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="prolongements">
<h1>Prolongements<a class="headerlink" href="#prolongements" title="Lien permanent vers cette rubrique">#</a></h1>
<nav class="contents local" id="sommaire">
<ul class="simple">
<li><p><a class="reference internal" href="#base-d-apprentissage-et-base-de-test" id="id6">Base d’apprentissage et base de test</a></p></li>
<li><p><a class="reference internal" href="#fonction-de-transfert-a-base-radiale" id="id7">Fonction de transfert à base radiale</a></p></li>
<li><p><a class="reference internal" href="#poids-partages" id="id8">Poids partagés</a></p></li>
<li><p><a class="reference internal" href="#derivee-par-rapport-aux-entrees" id="id9">Dérivée par rapport aux entrées</a></p></li>
<li><p><a class="reference internal" href="#regularisation-ou-decay" id="id10">Régularisation ou Decay</a></p></li>
<li><p><a class="reference internal" href="#problemes-de-gradients" id="id11">Problèmes de gradients</a></p></li>
<li><p><a class="reference internal" href="#selection-de-connexions" id="id12">Sélection de connexions</a></p></li>
</ul>
</nav>
<section id="base-d-apprentissage-et-base-de-test">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Base d’apprentissage et base de test</a><a class="headerlink" href="#base-d-apprentissage-et-base-de-test" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Les deux exemples de régression et de classification
<a class="reference internal" href="rn_2_reg.html#rn-section-regression"><span class="std std-ref">La régression</span></a> et <a class="reference internal" href="rn_7_clas2.html#subsection-classifieur"><span class="std std-ref">Problème de classification pour les réseaux de neurones</span></a> ont montré
que la structure du réseau de neurones la mieux adaptée a
une grande importance. Dans ces deux cas, une rapide vérification visuelle
permet de juger de la qualité du modèle obtenu après apprentissage,
mais bien souvent, cette « vision » est inaccessible pour
des dimensions supérieures à deux. Le meilleur moyen de jauger
le modèle appris est de vérifier si l’erreur obtenue sur une base
ayant servi à l’apprentissage (ou <em>base d’apprentissage</em>) est conservée
sur une autre base (ou <em>base de test</em>) que le modèle découvre pour la première fois.</p>
<p>Soit <span class="math notranslate nohighlight">\(B=\acc{\pa{X_i,Y_i} | 1 \infegal i \infegal N}\)</span>
l’ensemble des observations disponibles. Cet ensemble est
aléatoirement scindé en deux sous-ensembles <span class="math notranslate nohighlight">\(B_a\)</span> et <span class="math notranslate nohighlight">\(B_t\)</span>
de telle sorte que :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{l}
B_a \neq \emptyset \text{ et } B_t \neq \emptyset \\
B_a \cup B_t = B \text{ et } B_a \cap B_t = \emptyset \\
\frac{\#{B_a}}{\#{B_a \cup B_t}} = p \in ]0,1[
            \text{, en règle générale, } p \in \cro{\frac{1}{2},\frac{3}{4}}
\end{array}\end{split}\]</div>
<p>Ce découpage est valide si tous les exemples de la base <span class="math notranslate nohighlight">\(B\)</span>
obéissent à la même loi, les deux bases <span class="math notranslate nohighlight">\(B_a\)</span> et <span class="math notranslate nohighlight">\(B_t\)</span>
sont dites <em>homogènes</em>. Le réseau de neurones sera donc appris sur la
base d’apprentissage <span class="math notranslate nohighlight">\(B_a\)</span> et « testé » sur la base de test
<span class="math notranslate nohighlight">\(B_t\)</span>. Le test consiste à vérifier que l’erreur sur <span class="math notranslate nohighlight">\(B_t\)</span>
est sensiblement égale à celle sur <span class="math notranslate nohighlight">\(B_a\)</span>, auquel cas on dit que le
modèle (ou réseau de neurones) généralise bien. Le modèle trouvé
n’est pas pour autant le bon modèle mais il est robuste.
La courbe figure suivante illustre une définition du modèle optimal
comme étant celui qui minimise l’erreur sur la base de test.
Lorsque le modèle choisi n’est pas celui-là, deux cas sont possibles :</p>
<ul class="simple">
<li><p>Le nombre de coefficients est trop petit :
le modèle généralise bien mais il existe d’autres modèles
meilleurs pour lesquels l’erreur d’apprentissage et de test est moindre.</p></li>
<li><p>Le nombre de coefficients est trop grand : le modèle généralise mal,
l’erreur d’apprentissage est faible et l’erreur de test élevée,
le réseau a appris la base d’apprentissage par coeur.</p></li>
</ul>
<div class="admonition-mathdef admonition" id="indexmathe-Figure0">
<p class="admonition-title">Figure F1 : Modèle optimal pour la base de test</p>
<img alt="../../_images/errapptest.png" src="../../_images/errapptest.png" />
</div>
<p>Ce découpage des données en deux bases d’apprentissage et de
test est fréquemment utilisé pour toute estimation de modèles
résultant d’une optimisation réalisée au moyen d’un algorithme itératif.
C’est le cas par exemple des modèles de Markov cachés.
Elle permet de s’assurer qu’un modèle s’adapte bien à de nouvelles données.</p>
</section>
<section id="fonction-de-transfert-a-base-radiale">
<span id="rnn-fonction-base-radiale-rbf"></span><h2><a class="toc-backref" href="#id7" role="doc-backlink">Fonction de transfert à base radiale</a><a class="headerlink" href="#fonction-de-transfert-a-base-radiale" title="Lien permanent vers cette rubrique">#</a></h2>
<p>La fonction de transfert est dans ce cas à base radiale
(souvent abrégée par RBF pour <a class="reference external" href="https://en.wikipedia.org/wiki/Radial_basis_function">radial basis function</a>.
Elle ne s’applique pas au produit scalaire entre le
vecteur des poids et celui des entrées mais
à la distance euclidienne entre ces vecteurs.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Définition0">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-definition-neurone-dist">Définition D1 : neurone distance</p>
<p>Un neurone distance à <span class="math notranslate nohighlight">\(p\)</span> entrées est une fonction
<span class="math notranslate nohighlight">\(f : \R^{p+1} \times \R^p \longrightarrow \R\)</span> définie par :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(g : \R \dans \R\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W \in \R^{p+1}\)</span>, <span class="math notranslate nohighlight">\(W=\pa{w_1,\dots,w_{p+1}} = \pa{W',w_{p+1}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\forall x \in \R^p, \; f\pa{W,x} = e^{-\norm{W'-x}^2 + w_{p+1}}\)</span>
avec <span class="math notranslate nohighlight">\(x = \pa{x_1,\dots,x_p}\)</span></p></li>
</ul>
</div>
<p>Ce neurone est un cas particulier du suivant qui pondère chaque
dimension par un coefficient. Toutefois, ce neurone possède <span class="math notranslate nohighlight">\(2p+1\)</span>
coefficients où <span class="math notranslate nohighlight">\(p\)</span> est le nombre d’entrée.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Définition1">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-definition-neurone-dist-pond">Définition D2 : neurone distance pondérée</p>
<p>Pour un vecteur donné <span class="math notranslate nohighlight">\(W \in \R^p = \pa{w_1,\dots,w_p}\)</span>,
on note <span class="math notranslate nohighlight">\(W_i^j = \pa{w_i,\dots,w_j}\)</span>.
Un neurone distance pondérée à <span class="math notranslate nohighlight">\(p\)</span> entrées est une fonction
<span class="math notranslate nohighlight">\(f : \R^{2p+1} \times \R^p \longrightarrow \R\)</span> définie par :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(g : \R \dans \R\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W \in \R^{2p+1}\)</span>, <span class="math notranslate nohighlight">\(W=\pa{w_1,\dots,w_{2p+1}} = \pa{w_1,w_{2p+1}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\forall x \in \R^p, \; f\pa{W,x} =
\exp \cro {-\cro{\sum_{i=1}^{p} w_{p+i}\pa{w_i - x_i}^2 } + w_{p+1}}\)</span>
avec <span class="math notranslate nohighlight">\(x = \pa{x_1,\dots,x_p}\)</span></p></li>
</ul>
</div>
<p>La fonction de transfert est <span class="math notranslate nohighlight">\(x \longrightarrow e^x\)</span>
est le potentiel de ce neurone donc :
<span class="math notranslate nohighlight">\(y = -\cro{\sum_{i=1}^{p} w_{p+i}\pa{w_i - x_i}^2 } + w_{p+1}\)</span>.</p>
<p>L’algorithme de <a class="reference internal" href="rn_5_newton.html#algo-retropropagation"><span class="std std-ref">rétropropagation</span></a>
est modifié par l’insertion d’un tel neurone dans un réseau ainsi que la rétropropagation.
Le plus simple tout d’abord :</p>
<div class="math notranslate nohighlight" id="equation-eq-no-distance-nn">
\begin{eqnarray*}
1 \infegal i \infegal p, &amp; \dfrac{\partial y}{\partial w_{i}} = &amp; - 2 w_{p+i}\pa{w_i - x_i} \\
p+1 \infegal i \infegal 2p, &amp; \dfrac{\partial y}{\partial w_{i}} = &amp; - \pa{w_i - x_i}^2 \\
i = 2p+1, &amp; \dfrac{\partial y}{\partial w_{i}} = &amp; -1
\end{eqnarray*}</div><p>Pour le neurone distance simple, la ligne <a class="reference internal" href="#equation-eq-no-distance-nn">(1)</a>
est superflue, tous les coefficients <span class="math notranslate nohighlight">\((w_i)_{p+1 \infegal i \infegal 2p}\)</span>
sont égaux à 1. La relation <a class="reference internal" href="rn_5_newton.html#equation-retro-eq-nn-3">(6)</a> reste vraie mais n’aboutit plus à:eq:<cite>algo_retro_5</cite>,
celle-ci devient en supposant que la couche d’indice <span class="math notranslate nohighlight">\(c+1\)</span>
ne contient que des neurones définie par la définition précédente.</p>
<div class="math notranslate nohighlight">
\begin{eqnarray*}
\partialfrac{e}{y_{c,i}}
                            &amp;=&amp; \sum_{l=1}^{C_{c+1}}              \partialfrac{e}{y_{c+1,l}}
                                                                \partialfrac{y_{c+1,l}}{z_{c,i}}
                                                                \partialfrac{z_{c,i}}{y_{c,i}}  \\
     &amp;=&amp; \cro{ \sum_{l=1}^{C_{c+1}}
                             \partialfrac{e}{y_{c+1,l}}
                \pa{ 2 w_{c+1,l,p+i} \pa{ w_{c+1,l,i} - z_{c,i} } } }
                \partialfrac{z_{c,i}}{y_{c,i}}
\end{eqnarray*}</div></section>
<section id="poids-partages">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Poids partagés</a><a class="headerlink" href="#poids-partages" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Les poids partagés sont simplement un ensemble de poids qui sont
contraints à conserver la même valeur. Soit <span class="math notranslate nohighlight">\(G\)</span> un groupe de poids
partagés dont la valeur est <span class="math notranslate nohighlight">\(w_{G}\)</span>. Soit <span class="math notranslate nohighlight">\(X_k\)</span> et <span class="math notranslate nohighlight">\(Y_k\)</span>
un exemple de la base d’apprentissage (entrées et sorties désirées),
l’erreur commise par le réseau de neurones est <span class="math notranslate nohighlight">\(e\left(  W,X_k,Y_k\right)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }
{\partial w_{G}}=\sum_{w\in G}\dfrac{\partial e\left(  W,X_{k},Y_{k}\right) }{\partial
w_G}\dfrac{\partial w_{G}}{\partial w}=\sum_{w\in G}
{\sum} \dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial w_G}\]</div>
<p>Par conséquent, si un poids <span class="math notranslate nohighlight">\(w\)</span> appartient à un groupe <span class="math notranslate nohighlight">\(G\)</span> de poids partagés,
sa valeur à l’itération suivante sera :</p>
<div class="math notranslate nohighlight">
\[w_{t+1}=w_{t}-\varepsilon_{t}\left(  \underset{w\in G}
{\sum}\dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial w}\right)\]</div>
<p>Cette idée est utilisée dans les
<a class="reference external" href="https://fr.wikipedia.org/wiki/R%C3%A9seau_neuronal_convolutif">réseaux neuronaux convolutifs</a>
(<a class="reference external" href="https://fr.wikipedia.org/wiki/Apprentissage_profond">deep learning</a>,
<a class="reference external" href="http://cs231n.github.io/neural-networks-1/#layers">CS231n Convolutional Neural Networks for Visual Recognition</a>).</p>
</section>
<section id="derivee-par-rapport-aux-entrees">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Dérivée par rapport aux entrées</a><a class="headerlink" href="#derivee-par-rapport-aux-entrees" title="Lien permanent vers cette rubrique">#</a></h2>
<p>On note <span class="math notranslate nohighlight">\(\left(  X_k,Y_k\right)\)</span> un exemple de la base d’apprentissage.
Le réseau de neurones est composé de <span class="math notranslate nohighlight">\(C\)</span> couches, <span class="math notranslate nohighlight">\(C_i\)</span> est le
nombre de neurones sur la ième couche, <span class="math notranslate nohighlight">\(C_0\)</span> est le nombre d’entrées.
Les entrées sont appelées <span class="math notranslate nohighlight">\(\left( z_{0,i}\right) _{1\leqslant i\leqslant C_{0}}\)</span>,
<span class="math notranslate nohighlight">\(\left(  y_{1,i}\right)  _{1\leqslant i\leqslant C_{1}}\)</span>
sont les potentiels des neurones de la première couche, on en déduit que, dans le cas d’un neurone classique (non distance) :</p>
<div class="math notranslate nohighlight">
\[\dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial z_{0,i}} =
    \underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left(  W,X_{k}
,Y_{k}\right)  }{\partial y_{1,j}}\dfrac{\partial y_{1,j}}{\partial z_{0,i}
 }=\underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left( W,X_{k}
,Y_{k}\right)  }{\partial y_{1,j}}w_{1,j,i}\]</div>
<p>Comme le potentiel d’un neurone distance n’est pas linéaire par
rapport aux entrées <span class="math notranslate nohighlight">\(\left( y=\overset{N} {\underset{i=1}{\sum}}\left( w_{i}-z_{0,i}\right)  ^{2}+b\right)\)</span>,
la formule devient dans ce cas :</p>
<div class="math notranslate nohighlight">
\[\dfrac{\partial e\left(  W,X_{k},Y_{k}\right)  }{\partial z_{0,i}} =
        \underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left(  W,X_{k}
,Y_{k}\right)  }{\partial y_{1,j}}\dfrac{\partial y_{1,j}}{\partial z_{0,i}
     }=-2\underset{j=1}{\overset{C_{1}}{\sum}}\dfrac{\partial e\left(
W,X_{k},Y_{k}\right)  }{\partial y_{1,j}}\left(  w_{1,j,i}-z_{0,i}\right)\]</div>
</section>
<section id="regularisation-ou-decay">
<span id="rn-decay"></span><h2><a class="toc-backref" href="#id10" role="doc-backlink">Régularisation ou Decay</a><a class="headerlink" href="#regularisation-ou-decay" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Lors de l’apprentissage, comme les fonctions de seuil du réseau de
neurones sont bornées, pour une grande variation des coefficients,
la sortie varie peu. De plus, pour ces grandes valeurs, la dérivée
est quasi nulle et l’apprentissage s’en trouve ralenti. Par conséquent,
il est préférable d’éviter ce cas et c’est pourquoi un terme de
régularisation est ajouté lors de la mise à jour des
coefficients (voir <a class="reference internal" href="rn_biblio.html#bishop1995" id="id1"><span>[Bishop1995]</span></a>). L’idée consiste à ajouter
à l’erreur une pénalité fonction des coefficients du réseau de neurones :
<span class="math notranslate nohighlight">\(E_{reg} = E + \lambda \; \sum_{i} \; w_i^2\)</span>.</p>
<p>Et lors de la mise à jour du poids <span class="math notranslate nohighlight">\(w_i^t\)</span> à l’itération <span class="math notranslate nohighlight">\(t+1\)</span> :
<span class="math notranslate nohighlight">\(w_i^{t+1} = w_i^t - \epsilon_t \cro{ \partialfrac{E}{w_i} - 2\lambda w_i^t }\)</span>.</p>
<p>Le coefficient <span class="math notranslate nohighlight">\(\lambda\)</span> peut décroître avec le nombre
d’itérations et est en général de l’ordre de <span class="math notranslate nohighlight">\(0,01\)</span> pour un
apprentissage avec gradient global, plus faible pour un
apprentissage avec gradient stochastique.</p>
</section>
<section id="problemes-de-gradients">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Problèmes de gradients</a><a class="headerlink" href="#problemes-de-gradients" title="Lien permanent vers cette rubrique">#</a></h2>
<p>La descente du gradient repose sur l’algorithme de <a class="reference internal" href="rn_5_newton.html#algo-retropropagation"><span class="std std-ref">rétropropagation</span></a>
qui propoge l’erreur depuis la dernière couche jusqu’à la première.
Pour peu qu’une fonction de seuil soit saturée. Hors la zone rouge,
le gradient est très atténué.</p>
<p>(<a class="reference download internal" download="" href="../../_downloads/1873cb36a7e959870caf4430c6509499/rn_8_prol-1.py"><code class="xref download docutils literal notranslate"><span class="pre">Source</span> <span class="pre">code</span></code></a>, <a class="reference download internal" download="" href="../../_downloads/60d8ca63d272598ae22ebc089308998e/rn_8_prol-1.png"><code class="xref download docutils literal notranslate"><span class="pre">png</span></code></a>, <a class="reference download internal" download="" href="../../_downloads/3a96f40fa319711a242ceb1f49d50555/rn_8_prol-1.hires.png"><code class="xref download docutils literal notranslate"><span class="pre">hires.png</span></code></a>, <a class="reference download internal" download="" href="../../_downloads/e8362d6036c915138e3f81a41a679cdc/rn_8_prol-1.pdf"><code class="xref download docutils literal notranslate"><span class="pre">pdf</span></code></a>)</p>
<figure class="align-default">
<img alt="../../_images/rn_8_prol-1.png" class="plot-directive" src="../../_images/rn_8_prol-1.png" />
</figure>
<p id="index-0">Après deux couches de fonctions de transferts, le
gradient est souvent diminué. On appelle ce phénomène
le <a class="reference external" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Vanishing gradient problem</a>.
C’est d’autant plus probable que le réseau est gros. Quelques pistes pour y remédier :
<a class="reference external" href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients</a>,
<a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap5.html">Why are deep neural networks hard to train?</a>.
L’article <a class="reference external" href="http://arxiv.org/pdf/1512.03385v1.pdf">Deep Residual Learning for Image Recognition</a>
présente une structure de réseau qui va dnas le même sens.
De la même manière, la norme du gradient peut exploser plus particulièrement dans le cas des
<a class="reference external" href="https://en.wikipedia.org/wiki/Recurrent_neural_network">réseaux de neurones récurrents</a> :
<a class="reference external" href="http://arxiv.org/pdf/1211.5063v1.pdf">Understanding the exploding gradient problem</a>.</p>
</section>
<section id="selection-de-connexions">
<span id="selection-connexion"></span><h2><a class="toc-backref" href="#id12" role="doc-backlink">Sélection de connexions</a><a class="headerlink" href="#selection-de-connexions" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Ce paragraphe présente un algorithme de sélection de l’architecture
d’un réseau de neurones proposé par Cottrel et Al. dans <a class="reference internal" href="rn_biblio.html#cottrel1995" id="id2"><span>[Cottrel1995]</span></a>.
La méthode est applicable à tout réseau de neurones mais n’a été démontrée
que pour la classe de réseau de neurones utilisée pour la
<a class="reference internal" href="rn_2_reg.html#rn-section-regression"><span class="std std-ref">régression</span></a>. Les propriétés qui suivent ne sont
vraies que des réseaux à une couche cachée et dont les sorties
sont linéaires. Soit <span class="math notranslate nohighlight">\(\pa{X_k,Y_k}\)</span> un exemple de la base
d’apprentissage, les résidus de la régression sont supposés normaux
et i.i.d. L’erreur est donc (voir <a class="reference internal" href="rn_4_densite.html#rn-enonce-probleme-regression"><span class="std std-ref">Formulation du problème de la régression</span></a>) :
<span class="math notranslate nohighlight">\(e\left( W,X_k,Y_k\right) =\left(f\left( W,X_k\right)  -Y_k\right)^2\)</span>.</p>
<p>On peut estimer la loi asymptotique des coefficients du réseau de neurones.
Des connexions ayant un rôle peu important peuvent alors être supprimées
sans nuire à l’apprentissage en testant la nullité du coefficient associé.
On note <span class="math notranslate nohighlight">\(\widehat{W}\)</span> les poids trouvés par apprentissage et
<span class="math notranslate nohighlight">\(\overset{\ast}{W}\)</span> les poids optimaux. On définit :</p>
<div class="math notranslate nohighlight" id="equation-rn-selection-suite">
\begin{eqnarray*}
\text{la suite } \widehat{\varepsilon_{k}} &amp;=&amp;   f\left(  \widehat{W} ,X_{k}\right)  -Y_{k}, \;
                             \widehat{\sigma}_{N}^{2}=\dfrac{1}{N}\underset
                                {k=1}{\overset{N}{\sum}}\widehat{\varepsilon_{k}}^{2} \\
\text{la matrice }
\widehat{\Sigma_{N}}      &amp;=&amp;   \dfrac{1}{N}\left[  \nabla_{\widehat{W}%
                                }e\left(  W,X_{k},Y_{k}\right)  \right]
                                \left[  \nabla_{\widehat{W}}
                                e\left(  W,X_{k},Y_{k}\right)  \right]  ^{\prime}
\end{eqnarray*}</div><div class="admonition-mathdef admonition" id="indexmathe-Théorème0">
<div class="docutils container">
</div>
<p class="admonition-title" id="theoreme-loi-asym">Théorème T1 : loi asymptotique des coefficients</p>
<p>Soit <span class="math notranslate nohighlight">\(f\)</span> un réseau de neurone défini par <a class="reference internal" href="rn_1_def.html#rn-definition-perpception-1"><span class="std std-ref">perceptron</span></a>
composé de :</p>
<ul class="simple">
<li><p>une couche d’entrées</p></li>
<li><p>une couche cachée dont les fonctions de transfert sont sigmoïdes</p></li>
<li><p>une couche de sortie dont les fonctions de transfert sont linéaires</p></li>
</ul>
<p>Ce réseau sert de modèle pour la fonction <span class="math notranslate nohighlight">\(f\)</span>
dans le problème de <a class="reference internal" href="rn_2_reg.html#problem-regression"><span class="std std-ref">régression</span></a>
avec un échantillon <span class="math notranslate nohighlight">\(\vecteur{\pa{X_1,Y_1}}{\pa{X_N,Y_N}}\)</span>,
les résidus sont supposés normaux.
La suite <span class="math notranslate nohighlight">\(\pa{\widehat{\epsilon_k}}\)</span> définie par <a class="reference internal" href="#equation-rn-selection-suite">(2)</a> vérifie :</p>
<div class="math notranslate nohighlight">
\[\dfrac{1}{N} \sum_{i=1}^{N} \widehat{\epsilon_k} = 0 = \esp\cro{f\pa{\widehat{W},X} - Y}\]</div>
<p>Et le vecteur aléatoire <span class="math notranslate nohighlight">\(\widehat{W} - W^*\)</span> vérifie :</p>
<div class="math notranslate nohighlight">
\[\sqrt{N} \cro { \widehat{W} - W^* } \; \overset{T \rightarrow + \infty}{\longrightarrow} \;
        \loinormale{0}{\widehat{\sigma_N}^2  \widehat{\Sigma_N}}\]</div>
<p>Où la matrice <span class="math notranslate nohighlight">\(\widehat{\Sigma_N}\)</span> est définie par <a class="reference internal" href="#equation-rn-selection-suite">(2)</a>.</p>
<p>end{xtheorem}</p>
</div>
<div class="admonition-mathdef admonition" id="indexmathe-Figure1">
<div class="docutils container">
</div>
<p class="admonition-title" id="figure-selection-connexion-reseau-fig">Figure F2 : Réseau de neurones pour lequel la sélection de connexions s’applique</p>
<img alt="../../_images/selection_connexion.png" src="../../_images/selection_connexion.png" />
</div>
<p>La démonstration de ce théorème est donnée par l’article <a class="reference internal" href="rn_biblio.html#cottrel1995" id="id3"><span>[Cottrel1995]</span></a>.
Ce théorème mène au corollaire suivant :</p>
<div class="admonition-mathdef admonition" id="indexmathe-Corollaire0">
<p class="admonition-title">Corollaire C1 : nullité d’un coefficient</p>
<p>Les notations utilisées sont celles du théorème sur <a class="reference internal" href="#theoreme-loi-asym"><span class="std std-ref">loi asymptotique des coefficients</span></a>.
Soit <span class="math notranslate nohighlight">\(w_k\)</span> un poids du réseau de neurones
d’indice quelconque <span class="math notranslate nohighlight">\(k\)</span>. Sa valeur estimée est <span class="math notranslate nohighlight">\(\widehat{w_k}\)</span>,
sa valeur optimale <span class="math notranslate nohighlight">\(w^*_k\)</span>. D’après le théorème :</p>
<div class="math notranslate nohighlight">
\[N \dfrac{ \pa{\widehat{w_k} - w^*_k}^2  } { \widehat{\sigma_N}^2 \pa{\widehat{\Sigma_N}^{-1}}_{kk} }
\; \overset{T \rightarrow + \infty}{\longrightarrow} \; \chi^2_1\]</div>
</div>
<p>Ce résultat permet, à partir d’un réseau de neurones, de supprimer les
connexions pour lesquelles l’hypothèse de nullité n’est pas réfutée.
Afin d’aboutir à l’architecture minimale adaptée au problème,
Cottrel et Al. proposent dans <a class="reference internal" href="rn_biblio.html#cottrel1995" id="id4"><span>[Cottrel1995]</span></a> l’algorithme suivant :</p>
<div class="admonition-mathdef admonition" id="indexmathe-Théorème1">
<div class="docutils container">
</div>
<p class="admonition-title" id="rn-algorithme-selection-connexion-1">Théorème T2 : sélection d’architecture</p>
<p>Les notations utilisées sont celles du théorème
<a class="reference internal" href="#theoreme-loi-asym"><span class="std std-ref">loi asymptotique des coefficients</span></a>.
<span class="math notranslate nohighlight">\(f\)</span> est un réseau de neurones
de paramètres <span class="math notranslate nohighlight">\(W\)</span>. On définit la constante <span class="math notranslate nohighlight">\(\tau\)</span>,
en général <span class="math notranslate nohighlight">\(\tau = 3,84\)</span> puisque
<span class="math notranslate nohighlight">\(\pr {X &lt; \tau} = 0,95\)</span> si <span class="math notranslate nohighlight">\(X \sim \chi_1^2\)</span>.</p>
<p><em>Initialisation</em></p>
<p>Une architecture est choisie pour le réseau de neurones <span class="math notranslate nohighlight">\(f\)</span> incluant un nombre <cite>M</cite> de paramètres.</p>
<p><em>Apprentissage</em></p>
<p>Le réseau de neurones <span class="math notranslate nohighlight">\(f\)</span> est appris. On calcule les nombre et matrice
<span class="math notranslate nohighlight">\(\widehat{\sigma_N}^2\)</span> et <span class="math notranslate nohighlight">\(\widehat{\Sigma_N}\)</span>.
La base d’apprentissage contient <span class="math notranslate nohighlight">\(N\)</span> exemples.</p>
<p><em>Test</em></p>
<div class="line-block">
<div class="line">for <span class="math notranslate nohighlight">\(k\)</span> in <span class="math notranslate nohighlight">\(1..M\)</span></div>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(t_k \longleftarrow N \dfrac{ \widehat{w_k} ^2  } { \widehat{\sigma_N}^2 \pa{\widehat{\Sigma_N}^{-1}}_{kk} }\)</span></div>
</div>
</div>
<p><em>Sélection</em></p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(k' \longleftarrow \underset{k}{\arg \min} \; t_k\)</span></div>
<div class="line">si <span class="math notranslate nohighlight">\(t_{k'} &lt; \tau\)</span></div>
<div class="line-block">
<div class="line">Le modèle obtenu est supposé être le modèle optimal. L’algorithme s’arrête.</div>
</div>
<div class="line">sinon</div>
<div class="line-block">
<div class="line">La connexion <span class="math notranslate nohighlight">\(k'\)</span> est supprimée ou le poids <span class="math notranslate nohighlight">\(w_{k'}\)</span> est maintenue à zéro.</div>
<div class="line"><span class="math notranslate nohighlight">\(M \longleftarrow M-1\)</span></div>
<div class="line">Retour à l’apprentissage.</div>
</div>
</div>
</div>
<p>Cet algorithme est sensible au minimum local trouvé lors de l’apprentissage, il est préférable d’utiliser des méthodes
du second ordre afin d’assurer une meilleure convergence du réseau de neurones.</p>
<p>L’étape de sélection ne supprime qu’une seule connexion. Comme l’apprentissage
est coûteux en calcul, il peut être intéressant de supprimer toutes les connexions
<span class="math notranslate nohighlight">\(k\)</span> qui vérifient <span class="math notranslate nohighlight">\(t_k &lt; \tau\)</span>. Il est toutefois conseillé de ne
pas enlever trop de connexions simultanément puisque la suppression d’une connexion nulle peut
réhausser le test d’une autre connexion, nulle à cette même itération, mais non nulle à l’itération suivante.
Dans l’article <a class="reference internal" href="rn_biblio.html#cottrel1995" id="id5"><span>[Cottrel1995]</span></a>, les auteurs valident leur algorithme dans le cas d’une
régression grâce à l’algorithme suivant.</p>
<div class="admonition-mathdef admonition" id="indexmathe-Algorithme0">
<div class="docutils container">
</div>
<p class="admonition-title" id="nn-algorithme-valid-selection">Algorithme A1 : validation de l’algorithme de sélection des coefficients</p>
<p><em>Choix aléatoire d’un modèle</em></p>
<p>Un réseau de neurones est choisi aléatoirement,
soit <span class="math notranslate nohighlight">\(f : \R^p \dans \R\)</span> la fonction qu’il représente.
Une base d’apprentissage <span class="math notranslate nohighlight">\(A\)</span> (ou échantillon)
de <span class="math notranslate nohighlight">\(N\)</span> observations est générée aléatoirement à partir de ce modèle :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{l}
\text{soit } \pa{\epsilon_i}_{1 \infegal i \infegal N} \text{ un bruit blanc} \\
A = \acc{ \left. \pa{X_i,Y_i}_{1 \infegal i \infegal N} \right|
            \forall i \in \intervalle{1}{N}, \; Y_i = f\pa{X_i} + \epsilon_i }
\end{array}\end{split}\]</div>
<p><em>Choix aléatoire d’un modèle</em></p>
<p>L’algorithme de <a class="reference internal" href="#rn-algorithme-selection-connexion-1"><span class="std std-ref">sélection</span></a>
à un réseau de neurones plus riche que le modèle choisi
dans l’étape d’initilisation. Le modèle sélectionné est noté <span class="math notranslate nohighlight">\(g\)</span>.</p>
<p><em>Validation</em></p>
<p>Si <span class="math notranslate nohighlight">\(\norm{f-g} \approx 0\)</span>,
l’algorithme de
<a class="reference internal" href="#rn-algorithme-selection-connexion-1"><span class="std std-ref">sélection</span></a>
est validé.</p>
</div>
<p>La réduction des réseaux de neurones ne se posent plus en ce sens.
Les réseaux de neurones sont aujourd’hui des réseaux de neurones
de neurones profonds qui ne suivent plus cette architecture à une
couche.</p>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="rn_7_clas2.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Classification</p>
      </div>
    </a>
    <a class="right-next"
       href="rn_9_auto.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Analyse en composantes principales (ACP) et Auto Encoders</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Sur cette page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#base-d-apprentissage-et-base-de-test">Base d’apprentissage et base de test</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fonction-de-transfert-a-base-radiale">Fonction de transfert à base radiale</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#poids-partages">Poids partagés</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivee-par-rapport-aux-entrees">Dérivée par rapport aux entrées</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularisation-ou-decay">Régularisation ou Decay</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problemes-de-gradients">Problèmes de gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-de-connexions">Sélection de connexions</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/c_ml/rn/rn_8_prol.rst">
      <i class="fa-solid fa-file-lines"></i> Montrer le code source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2016-2023, Xavier Dupré.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Créé en utilisant <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.0.1.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Construit avec le <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">Thème PyData Sphinx</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>