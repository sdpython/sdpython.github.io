{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Arbre d'ind\u00e9cision\n\nLa construction d'un arbre de d\u00e9cision appliqu\u00e9 \u00e0 une\nclassification binaire suppose qu'on puisse\nd\u00e9terminer un seuil qui s\u00e9pare les deux classes ou tout\ndu moins qui aboutisse \u00e0 deux sous-ensemble dans lesquels\nune classe est majoritaire. Mais certains cas, c'est une\nchose compliqu\u00e9e.\n\n## Un cas simple et un cas compliqu\u00e9\n\nIl faut choisir un seuil sur l'axe des abscisses qui\npermette de classer le jeu de donn\u00e9es.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\n\n\ndef random_set_1d(n, kind):\n    x = numpy.random.rand(n) * 3 - 1\n    if kind:\n        y = numpy.empty(x.shape, dtype=numpy.int32)\n        y[x < 0] = 0\n        y[(x >= 0) & (x <= 1)] = 1\n        y[x > 1] = 0\n    else:\n        y = numpy.empty(x.shape, dtype=numpy.int32)\n        y[x < 0] = 0\n        y[x >= 0] = 1\n    x2 = numpy.random.rand(n)\n    return numpy.vstack([x, x2]).T, y\n\n\ndef plot_ds(X, y, ax=None, title=None):\n    if ax is None:\n        ax = plt.gca()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\", lw=0.5)\n    if title is not None:\n        ax.set_title(title)\n    return ax\n\n\nX1, y1 = random_set_1d(1000, False)\nX2, y2 = random_set_1d(1000, True)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\nplot_ds(X1, y1, ax=ax[0], title=\"easy\")\nplot_ds(X2, y2, ax=ax[1], title=\"difficult\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seuil de d\u00e9cision\n\nLes arbres de d\u00e9cision utilisent comme crit\u00e8re\nle crit\u00e8re de [Gini](https://fr.wikipedia.org/wiki/\nArbre_de_d%C3%A9cision_(apprentissage)#Cas_des_arbres_de_classification)\nou l'[entropie](https://fr.wikipedia.org/wiki/Entropie_de_Shannon).\nL'apprentissage d'une r\u00e9gression logistique\ns'appuie sur la `log-vraisemblance <l-lr-log-likelihood>`\ndu jeu de donn\u00e9es. On regarde l'\u00e9volution de ces crit\u00e8res\nen fonction des diff\u00e9rents seuils possibles.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plog2(p):\n    if p == 0:\n        return 0\n    return p * numpy.log(p) / numpy.log(2)\n\n\ndef logistic(x):\n    return 1.0 / (1.0 + numpy.exp(-x))\n\n\ndef likelihood(x, y, theta=1.0, th=0.0):\n    lr = logistic((x - th) * theta)\n    return y * lr + (1.0 - y) * (1 - lr)\n\n\ndef criteria(X, y):\n    res = numpy.empty((X.shape[0], 8))\n    res[:, 0] = X[:, 0]\n    res[:, 1] = y\n    order = numpy.argsort(res[:, 0])\n    res = res[order, :].copy()\n    x = res[:, 0].copy()\n    y = res[:, 1].copy()\n\n    for i in range(1, res.shape[0] - 1):\n        # gini\n        p1 = numpy.sum(y[:i]) / i\n        p2 = numpy.sum(y[i:]) / (y.shape[0] - i)\n        res[i, 2] = p1\n        res[i, 3] = p2\n        res[i, 4] = 1 - p1**2 - (1 - p1) ** 2 + 1 - p2**2 - (1 - p2) ** 2\n        res[i, 5] = -plog2(p1) - plog2(1 - p1) - plog2(p2) - plog2(1 - p2)\n        th = x[i]\n        res[i, 6] = logistic(th * 10.0)\n        res[i, 7] = numpy.sum(likelihood(x, y, 10.0, th)) / res.shape[0]\n    return DataFrame(\n        res[1:-1], columns=[\"X\", \"y\", \"p1\", \"p2\", \"Gini\", \"Gain\", \"lr\", \"LL-10\"]\n    )\n\n\nX1, y1 = random_set_1d(1000, False)\nX2, y2 = random_set_1d(1000, True)\n\ndf = criteria(X1, y1)\nprint(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et visuellement...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_ds(X, y, ax=None, title=None):\n    if ax is None:\n        ax = plt.gca()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\", lw=0.5)\n    if title is not None:\n        ax.set_title(title)\n    return ax\n\n\ndf1 = criteria(X1, y1)\ndf2 = criteria(X2, y2)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\nplot_ds(X1, y1, ax=ax[0], title=\"easy\")\nplot_ds(X2, y2, ax=ax[1], title=\"difficult\")\ndf1.plot(x=\"X\", y=[\"Gini\", \"Gain\", \"LL-10\", \"p1\", \"p2\"], ax=ax[0], lw=5.0)\ndf2.plot(x=\"X\", y=[\"Gini\", \"Gain\", \"LL-10\", \"p1\", \"p2\"], ax=ax[1], lw=5.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le premier exemple est le cas simple et tous les\nindicateurs trouvent bien la fonti\u00e8re entre les deux classes\ncomme un extremum sur l'intervalle consid\u00e9r\u00e9.\nLe second cas est lin\u00e9airement non s\u00e9parable.\nAucun des indicateurs ne semble trouver une des\ndeux fronti\u00e8res. La log-vraisemblance montre deux\nmaxima. L'un est bien situ\u00e9 sur une fronti\u00e8re, le second\nest situ\u00e9 \u00e0 une extr\u00e9mit\u00e9 de l'intervalle, ce qui revient\n\u00e0 construire un classifier qui retourn\u00e9 une r\u00e9ponse\nconstante. C'est donc inutile.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}